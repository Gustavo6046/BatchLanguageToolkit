__BRYTHON__.use_VFS = true
__BRYTHON__.VFS = {"nltk\\app\\chartparser_app": [".py", "\n\n\n\nfrom __future__ import division\nimport pickle\nimport os.path\n\nfrom six.moves.tkinter import (Button, Canvas, Checkbutton, Frame, IntVar,\n                               Label, Menu, Scrollbar, Tk, Toplevel)\nfrom six.moves.tkinter_font import Font\nfrom six.moves.tkinter_messagebox import showerror, showinfo\nfrom six.moves.tkinter_tkfiledialog import asksaveasfilename, askopenfilename\n\nfrom nltk.parse.chart import (BottomUpPredictCombineRule, BottomUpPredictRule,\n                              Chart, LeafEdge, LeafInitRule, SingleEdgeFundamentalRule,\n                              SteppingChartParser, TopDownInitRule, TopDownPredictRule,\n                              TreeEdge)\nfrom nltk.tree import Tree\nfrom nltk.grammar import Nonterminal, CFG\nfrom nltk.util import in_idle\nfrom nltk.draw.util import (CanvasFrame, ColorizedList,\n                            EntryDialog, MutableOptionMenu,\n                            ShowText, SymbolWidget)\nfrom nltk.draw import CFGEditor, tree_to_treesegment, TreeSegmentWidget\n\n\n\nclass EdgeList(ColorizedList):\n    ARROW = SymbolWidget.SYMBOLS['rightarrow']\n\n    def _init_colortags(self, textwidget, options):\n        textwidget.tag_config('terminal', foreground='#006000')\n        textwidget.tag_config('arrow', font='symbol', underline='0')\n        textwidget.tag_config('dot', foreground = '#000000')\n        textwidget.tag_config('nonterminal', foreground='blue',\n                              font=('helvetica', -12, 'bold'))\n\n    def _item_repr(self, item):\n        contents = []\n        contents.append(('%s\\t' % item.lhs(), 'nonterminal'))\n        contents.append((self.ARROW, 'arrow'))\n        for i, elt in enumerate(item.rhs()):\n            if i == item.dot():\n                contents.append((' *', 'dot'))\n            if isinstance(elt, Nonterminal):\n                contents.append((' %s' % elt.symbol(), 'nonterminal'))\n            else:\n                contents.append((' %r' % elt, 'terminal'))\n        if item.is_complete():\n            contents.append((' *', 'dot'))\n        return contents\n\n\nclass ChartMatrixView(object):\n    def __init__(self, parent, chart, toplevel=True, title='Chart Matrix',\n                 show_numedges=False):\n        self._chart = chart\n        self._cells = []\n        self._marks = []\n\n        self._selected_cell = None\n\n        if toplevel:\n            self._root = Toplevel(parent)\n            self._root.title(title)\n            self._root.bind('<Control-q>', self.destroy)\n            self._init_quit(self._root)\n        else:\n            self._root = Frame(parent)\n\n        self._init_matrix(self._root)\n        self._init_list(self._root)\n        if show_numedges:\n            self._init_numedges(self._root)\n        else:\n            self._numedges_label = None\n\n        self._callbacks = {}\n\n        self._num_edges = 0\n\n        self.draw()\n\n    def _init_quit(self, root):\n        quit = Button(root, text='Quit', command=self.destroy)\n        quit.pack(side='bottom', expand=0, fill='none')\n\n    def _init_matrix(self, root):\n        cframe = Frame(root, border=2, relief='sunken')\n        cframe.pack(expand=0, fill='none', padx=1, pady=3, side='top')\n        self._canvas = Canvas(cframe, width=200, height=200,\n                                      background='white')\n        self._canvas.pack(expand=0, fill='none')\n\n    def _init_numedges(self, root):\n        self._numedges_label = Label(root, text='0 edges')\n        self._numedges_label.pack(expand=0, fill='none', side='top')\n\n    def _init_list(self, root):\n        self._list = EdgeList(root, [], width=20, height=5)\n        self._list.pack(side='top', expand=1, fill='both', pady=3)\n        def cb(edge, self=self): self._fire_callbacks('select', edge)\n        self._list.add_callback('select', cb)\n        self._list.focus()\n\n    def destroy(self, *e):\n        if self._root is None: return\n        try: self._root.destroy()\n        except: pass\n        self._root = None\n\n    def set_chart(self, chart):\n        if chart is not self._chart:\n            self._chart = chart\n            self._num_edges = 0\n            self.draw()\n\n    def update(self):\n        if self._root is None: return\n\n        N = len(self._cells)\n        cell_edges = [[0 for i in range(N)] for j in range(N)]\n        for edge in self._chart:\n            cell_edges[edge.start()][edge.end()] += 1\n\n        for i in range(N):\n            for j in range(i, N):\n                if cell_edges[i][j] == 0:\n                    color = 'gray20'\n                else:\n                    color = ('#00%02x%02x' %\n                             (min(255, 50+128*cell_edges[i][j]/10),\n                              max(0, 128-128*cell_edges[i][j]/10)))\n                cell_tag = self._cells[i][j]\n                self._canvas.itemconfig(cell_tag, fill=color)\n                if (i,j) == self._selected_cell:\n                    self._canvas.itemconfig(cell_tag, outline='#00ffff',\n                                            width=3)\n                    self._canvas.tag_raise(cell_tag)\n                else:\n                    self._canvas.itemconfig(cell_tag, outline='black',\n                                            width=1)\n\n        edges = list(self._chart.select(span=self._selected_cell))\n        self._list.set(edges)\n\n        self._num_edges = self._chart.num_edges()\n        if self._numedges_label is not None:\n            self._numedges_label['text'] = '%d edges' % self._num_edges\n\n    def activate(self):\n        self._canvas.itemconfig('inactivebox', state='hidden')\n        self.update()\n\n    def inactivate(self):\n        self._canvas.itemconfig('inactivebox', state='normal')\n        self.update()\n\n    def add_callback(self, event, func):\n        self._callbacks.setdefault(event,{})[func] = 1\n\n    def remove_callback(self, event, func=None):\n        if func is None: del self._callbacks[event]\n        else:\n            try: del self._callbacks[event][func]\n            except: pass\n\n    def _fire_callbacks(self, event, *args):\n        if event not in self._callbacks: return\n        for cb_func in list(self._callbacks[event].keys()): cb_func(*args)\n\n    def select_cell(self, i, j):\n        if self._root is None: return\n\n        if ((i,j) == self._selected_cell and\n            self._chart.num_edges() == self._num_edges): return\n\n        self._selected_cell = (i,j)\n        self.update()\n\n        self._fire_callbacks('select_cell', i, j)\n\n    def deselect_cell(self):\n        if self._root is None: return\n        self._selected_cell = None\n        self._list.set([])\n        self.update()\n\n    def _click_cell(self, i, j):\n        if self._selected_cell == (i,j):\n            self.deselect_cell()\n        else:\n            self.select_cell(i, j)\n\n    def view_edge(self, edge):\n        self.select_cell(*edge.span())\n        self._list.view(edge)\n\n    def mark_edge(self, edge):\n        if self._root is None: return\n        self.select_cell(*edge.span())\n        self._list.mark(edge)\n\n    def unmark_edge(self, edge=None):\n        if self._root is None: return\n        self._list.unmark(edge)\n\n    def markonly_edge(self, edge):\n        if self._root is None: return\n        self.select_cell(*edge.span())\n        self._list.markonly(edge)\n\n    def draw(self):\n        if self._root is None: return\n        LEFT_MARGIN = BOT_MARGIN = 15\n        TOP_MARGIN = 5\n        c = self._canvas\n        c.delete('all')\n        N = self._chart.num_leaves()+1\n        dx = (int(c['width'])-LEFT_MARGIN)/N\n        dy = (int(c['height'])-TOP_MARGIN-BOT_MARGIN)/N\n\n        c.delete('all')\n\n        for i in range(N):\n            c.create_text(LEFT_MARGIN-2, i*dy+dy/2+TOP_MARGIN,\n                          text=repr(i), anchor='e')\n            c.create_text(i*dx+dx/2+LEFT_MARGIN, N*dy+TOP_MARGIN+1,\n                          text=repr(i), anchor='n')\n            c.create_line(LEFT_MARGIN, dy*(i+1)+TOP_MARGIN,\n                          dx*N+LEFT_MARGIN, dy*(i+1)+TOP_MARGIN, dash='.')\n            c.create_line(dx*i+LEFT_MARGIN, TOP_MARGIN,\n                          dx*i+LEFT_MARGIN, dy*N+TOP_MARGIN, dash='.')\n\n        c.create_rectangle(LEFT_MARGIN, TOP_MARGIN,\n                           LEFT_MARGIN+dx*N, dy*N+TOP_MARGIN,\n                           width=2)\n\n        self._cells = [[None for i in range(N)] for j in range(N)]\n        for i in range(N):\n            for j in range(i, N):\n                t = c.create_rectangle(j*dx+LEFT_MARGIN, i*dy+TOP_MARGIN,\n                                       (j+1)*dx+LEFT_MARGIN,\n                                       (i+1)*dy+TOP_MARGIN,\n                                       fill='gray20')\n                self._cells[i][j] = t\n                def cb(event, self=self, i=i, j=j): self._click_cell(i,j)\n                c.tag_bind(t, '<Button-1>', cb)\n\n        xmax, ymax = int(c['width']), int(c['height'])\n        t = c.create_rectangle(-100, -100, xmax+100, ymax+100,\n                               fill='gray50', state='hidden',\n                               tag='inactivebox')\n        c.tag_lower(t)\n\n        self.update()\n\n    def pack(self, *args, **kwargs):\n        self._root.pack(*args, **kwargs)\n\n\nclass ChartResultsView(object):\n    def __init__(self, parent, chart, grammar, toplevel=True):\n        self._chart = chart\n        self._grammar = grammar\n        self._trees = []\n        self._y = 10\n        self._treewidgets = []\n        self._selection = None\n        self._selectbox = None\n\n        if toplevel:\n            self._root = Toplevel(parent)\n            self._root.title('Chart Parser Application: Results')\n            self._root.bind('<Control-q>', self.destroy)\n        else:\n            self._root = Frame(parent)\n\n        if toplevel:\n            buttons = Frame(self._root)\n            buttons.pack(side='bottom', expand=0, fill='x')\n            Button(buttons, text='Quit',\n                           command=self.destroy).pack(side='right')\n            Button(buttons, text='Print All',\n                           command=self.print_all).pack(side='left')\n            Button(buttons, text='Print Selection',\n                           command=self.print_selection).pack(side='left')\n\n        self._cframe = CanvasFrame(self._root, closeenough=20)\n        self._cframe.pack(side='top', expand=1, fill='both')\n\n        self.update()\n\n    def update(self, edge=None):\n        if self._root is None: return\n        if edge is not None:\n            if edge.lhs() != self._grammar.start(): return\n            if edge.span() != (0, self._chart.num_leaves()): return\n\n        for parse in self._chart.parses(self._grammar.start()):\n            if parse not in self._trees:\n                self._add(parse)\n\n    def _add(self, parse):\n        self._trees.append(parse)\n\n        c = self._cframe.canvas()\n        treewidget = tree_to_treesegment(c, parse)\n\n        self._treewidgets.append(treewidget)\n        self._cframe.add_widget(treewidget, 10, self._y)\n\n        treewidget.bind_click(self._click)\n\n        self._y = treewidget.bbox()[3] + 10\n\n    def _click(self, widget):\n        c = self._cframe.canvas()\n        if self._selection is not None:\n            c.delete(self._selectbox)\n        self._selection = widget\n        (x1, y1, x2, y2) = widget.bbox()\n        self._selectbox = c.create_rectangle(x1, y1, x2, y2,\n                                             width=2, outline='#088')\n\n    def _color(self, treewidget, color):\n        treewidget.label()['color'] = color\n        for child in treewidget.subtrees():\n            if isinstance(child, TreeSegmentWidget):\n                self._color(child, color)\n            else:\n                child['color'] = color\n\n    def print_all(self, *e):\n        if self._root is None: return\n        self._cframe.print_to_file()\n\n    def print_selection(self, *e):\n        if self._root is None: return\n        if self._selection is None:\n            showerror('Print Error', 'No tree selected')\n        else:\n            c = self._cframe.canvas()\n            for widget in self._treewidgets:\n                if widget is not self._selection:\n                    self._cframe.destroy_widget(widget)\n            c.delete(self._selectbox)\n            (x1,y1,x2,y2) = self._selection.bbox()\n            self._selection.move(10-x1,10-y1)\n            c['scrollregion'] = '0 0 %s %s' % (x2-x1+20, y2-y1+20)\n            self._cframe.print_to_file()\n\n            self._treewidgets = [self._selection]\n            self.clear()\n            self.update()\n\n    def clear(self):\n        if self._root is None: return\n        for treewidget in self._treewidgets:\n            self._cframe.destroy_widget(treewidget)\n        self._trees = []\n        self._treewidgets = []\n        if self._selection is not None:\n            self._cframe.canvas().delete(self._selectbox)\n        self._selection = None\n        self._y = 10\n\n    def set_chart(self, chart):\n        self.clear()\n        self._chart = chart\n        self.update()\n\n    def set_grammar(self, grammar):\n        self.clear()\n        self._grammar = grammar\n        self.update()\n\n    def destroy(self, *e):\n        if self._root is None: return\n        try: self._root.destroy()\n        except: pass\n        self._root = None\n\n    def pack(self, *args, **kwargs):\n        self._root.pack(*args, **kwargs)\n\n\nclass ChartComparer(object):\n\n    _OPSYMBOL = {'-': '-',\n                 'and': SymbolWidget.SYMBOLS['intersection'],\n                 'or': SymbolWidget.SYMBOLS['union']}\n\n    def __init__(self, *chart_filenames):\n        faketok = [''] * 8\n        self._emptychart = Chart(faketok)\n\n        self._left_name = 'None'\n        self._right_name = 'None'\n        self._left_chart = self._emptychart\n        self._right_chart = self._emptychart\n\n        self._charts = {'None': self._emptychart}\n\n        self._out_chart = self._emptychart\n\n        self._operator = None\n\n        self._root = Tk()\n        self._root.title('Chart Comparison')\n        self._root.bind('<Control-q>', self.destroy)\n        self._root.bind('<Control-x>', self.destroy)\n\n        self._init_menubar(self._root)\n        self._init_chartviews(self._root)\n        self._init_divider(self._root)\n        self._init_buttons(self._root)\n        self._init_bindings(self._root)\n\n        for filename in chart_filenames:\n            self.load_chart(filename)\n\n    def destroy(self, *e):\n        if self._root is None: return\n        try: self._root.destroy()\n        except: pass\n        self._root = None\n\n    def mainloop(self, *args, **kwargs):\n        return\n        self._root.mainloop(*args, **kwargs)\n\n\n    def _init_menubar(self, root):\n        menubar = Menu(root)\n\n        filemenu = Menu(menubar, tearoff=0)\n        filemenu.add_command(label='Load Chart', accelerator='Ctrl-o',\n                             underline=0, command=self.load_chart_dialog)\n        filemenu.add_command(label='Save Output', accelerator='Ctrl-s',\n                             underline=0, command=self.save_chart_dialog)\n        filemenu.add_separator()\n        filemenu.add_command(label='Exit', underline=1,\n                             command=self.destroy, accelerator='Ctrl-x')\n        menubar.add_cascade(label='File', underline=0, menu=filemenu)\n\n        opmenu = Menu(menubar, tearoff=0)\n        opmenu.add_command(label='Intersection',\n                           command=self._intersection,\n                           accelerator='+')\n        opmenu.add_command(label='Union',\n                           command=self._union,\n                           accelerator='*')\n        opmenu.add_command(label='Difference',\n                           command=self._difference,\n                           accelerator='-')\n        opmenu.add_separator()\n        opmenu.add_command(label='Swap Charts',\n                           command=self._swapcharts)\n        menubar.add_cascade(label='Compare', underline=0, menu=opmenu)\n\n        self._root.config(menu=menubar)\n\n    def _init_divider(self, root):\n        divider = Frame(root, border=2, relief='sunken')\n        divider.pack(side='top', fill='x', ipady=2)\n\n    def _init_chartviews(self, root):\n        opfont=('symbol', -36) # Font for operator.\n        eqfont=('helvetica', -36) # Font for equals sign.\n\n        frame = Frame(root, background='#c0c0c0')\n        frame.pack(side='top', expand=1, fill='both')\n\n        cv1_frame = Frame(frame, border=3, relief='groove')\n        cv1_frame.pack(side='left', padx=8, pady=7, expand=1, fill='both')\n        self._left_selector = MutableOptionMenu(\n            cv1_frame, list(self._charts.keys()), command=self._select_left)\n        self._left_selector.pack(side='top', pady=5, fill='x')\n        self._left_matrix = ChartMatrixView(cv1_frame, self._emptychart,\n                                            toplevel=False,\n                                            show_numedges=True)\n        self._left_matrix.pack(side='bottom', padx=5, pady=5,\n                               expand=1, fill='both')\n        self._left_matrix.add_callback('select', self.select_edge)\n        self._left_matrix.add_callback('select_cell', self.select_cell)\n        self._left_matrix.inactivate()\n\n        self._op_label = Label(frame, text=' ', width=3,\n                                       background='#c0c0c0', font=opfont)\n        self._op_label.pack(side='left', padx=5, pady=5)\n\n        cv2_frame = Frame(frame, border=3, relief='groove')\n        cv2_frame.pack(side='left', padx=8, pady=7, expand=1, fill='both')\n        self._right_selector = MutableOptionMenu(\n            cv2_frame, list(self._charts.keys()), command=self._select_right)\n        self._right_selector.pack(side='top', pady=5, fill='x')\n        self._right_matrix = ChartMatrixView(cv2_frame, self._emptychart,\n                                            toplevel=False,\n                                            show_numedges=True)\n        self._right_matrix.pack(side='bottom', padx=5, pady=5,\n                               expand=1, fill='both')\n        self._right_matrix.add_callback('select', self.select_edge)\n        self._right_matrix.add_callback('select_cell', self.select_cell)\n        self._right_matrix.inactivate()\n\n        Label(frame, text='=', width=3, background='#c0c0c0',\n                      font=eqfont).pack(side='left', padx=5, pady=5)\n\n        out_frame = Frame(frame, border=3, relief='groove')\n        out_frame.pack(side='left', padx=8, pady=7, expand=1, fill='both')\n        self._out_label = Label(out_frame, text='Output')\n        self._out_label.pack(side='top', pady=9)\n        self._out_matrix = ChartMatrixView(out_frame, self._emptychart,\n                                            toplevel=False,\n                                            show_numedges=True)\n        self._out_matrix.pack(side='bottom', padx=5, pady=5,\n                                 expand=1, fill='both')\n        self._out_matrix.add_callback('select', self.select_edge)\n        self._out_matrix.add_callback('select_cell', self.select_cell)\n        self._out_matrix.inactivate()\n\n    def _init_buttons(self, root):\n        buttons = Frame(root)\n        buttons.pack(side='bottom', pady=5, fill='x', expand=0)\n        Button(buttons, text='Intersection',\n                       command=self._intersection).pack(side='left')\n        Button(buttons, text='Union',\n                       command=self._union).pack(side='left')\n        Button(buttons, text='Difference',\n                       command=self._difference).pack(side='left')\n        Frame(buttons, width=20).pack(side='left')\n        Button(buttons, text='Swap Charts',\n                       command=self._swapcharts).pack(side='left')\n\n        Button(buttons, text='Detatch Output',\n                       command=self._detatch_out).pack(side='right')\n\n    def _init_bindings(self, root):\n        root.bind('<Control-o>', self.load_chart_dialog)\n\n\n    def _select_left(self, name):\n        self._left_name = name\n        self._left_chart = self._charts[name]\n        self._left_matrix.set_chart(self._left_chart)\n        if name == 'None': self._left_matrix.inactivate()\n        self._apply_op()\n\n    def _select_right(self, name):\n        self._right_name = name\n        self._right_chart = self._charts[name]\n        self._right_matrix.set_chart(self._right_chart)\n        if name == 'None': self._right_matrix.inactivate()\n        self._apply_op()\n\n    def _apply_op(self):\n        if self._operator == '-': self._difference()\n        elif self._operator == 'or': self._union()\n        elif self._operator == 'and': self._intersection()\n\n\n    CHART_FILE_TYPES = [('Pickle file', '.pickle'),\n                        ('All files', '*')]\n\n    def save_chart_dialog(self, *args):\n        filename = asksaveasfilename(filetypes=self.CHART_FILE_TYPES,\n                                     defaultextension='.pickle')\n        if not filename: return\n        try:\n            with open(filename, 'wb') as outfile:\n                pickle.dump(self._out_chart, outfile)\n        except Exception as e:\n            showerror('Error Saving Chart',\n                                   'Unable to open file: %r\\n%s' %\n                                   (filename, e))\n\n    def load_chart_dialog(self, *args):\n        filename = askopenfilename(filetypes=self.CHART_FILE_TYPES,\n                                   defaultextension='.pickle')\n        if not filename: return\n        try: self.load_chart(filename)\n        except Exception as e:\n            showerror('Error Loading Chart',\n                                   'Unable to open file: %r\\n%s' %\n                                   (filename, e))\n\n    def load_chart(self, filename):\n        with open(filename, 'rb') as infile:\n            chart = pickle.load(infile)\n        name = os.path.basename(filename)\n        if name.endswith('.pickle'): name = name[:-7]\n        if name.endswith('.chart'): name = name[:-6]\n        self._charts[name] = chart\n        self._left_selector.add(name)\n        self._right_selector.add(name)\n\n        if self._left_chart is self._emptychart:\n            self._left_selector.set(name)\n        elif self._right_chart is self._emptychart:\n            self._right_selector.set(name)\n\n    def _update_chartviews(self):\n        self._left_matrix.update()\n        self._right_matrix.update()\n        self._out_matrix.update()\n\n\n    def select_edge(self, edge):\n        if edge in self._left_chart:\n            self._left_matrix.markonly_edge(edge)\n        else:\n            self._left_matrix.unmark_edge()\n        if edge in self._right_chart:\n            self._right_matrix.markonly_edge(edge)\n        else:\n            self._right_matrix.unmark_edge()\n        if edge in self._out_chart:\n            self._out_matrix.markonly_edge(edge)\n        else:\n            self._out_matrix.unmark_edge()\n\n    def select_cell(self, i, j):\n        self._left_matrix.select_cell(i, j)\n        self._right_matrix.select_cell(i, j)\n        self._out_matrix.select_cell(i, j)\n\n\n    def _difference(self):\n        if not self._checkcompat(): return\n\n        out_chart = Chart(self._left_chart.tokens())\n        for edge in self._left_chart:\n            if edge not in self._right_chart:\n                out_chart.insert(edge, [])\n\n        self._update('-', out_chart)\n\n    def _intersection(self):\n        if not self._checkcompat(): return\n\n        out_chart = Chart(self._left_chart.tokens())\n        for edge in self._left_chart:\n            if edge in self._right_chart:\n                out_chart.insert(edge, [])\n\n        self._update('and', out_chart)\n\n    def _union(self):\n        if not self._checkcompat(): return\n\n        out_chart = Chart(self._left_chart.tokens())\n        for edge in self._left_chart:\n            out_chart.insert(edge, [])\n        for edge in self._right_chart:\n            out_chart.insert(edge, [])\n\n        self._update('or', out_chart)\n\n    def _swapcharts(self):\n        left, right = self._left_name, self._right_name\n        self._left_selector.set(right)\n        self._right_selector.set(left)\n\n    def _checkcompat(self):\n        if (self._left_chart.tokens() != self._right_chart.tokens() or\n            self._left_chart.property_names() !=\n            self._right_chart.property_names() or\n            self._left_chart == self._emptychart or\n            self._right_chart == self._emptychart):\n            self._out_chart = self._emptychart\n            self._out_matrix.set_chart(self._out_chart)\n            self._out_matrix.inactivate()\n            self._out_label['text'] = 'Output'\n            return False\n        else:\n            return True\n\n    def _update(self, operator, out_chart):\n        self._operator = operator\n        self._op_label['text'] = self._OPSYMBOL[operator]\n        self._out_chart = out_chart\n        self._out_matrix.set_chart(out_chart)\n        self._out_label['text'] = '%s %s %s' % (self._left_name,\n                                                self._operator,\n                                                self._right_name)\n\n    def _clear_out_chart(self):\n        self._out_chart = self._emptychart\n        self._out_matrix.set_chart(self._out_chart)\n        self._op_label['text'] = ' '\n        self._out_matrix.inactivate()\n\n    def _detatch_out(self):\n        ChartMatrixView(self._root, self._out_chart,\n                        title=self._out_label['text'])\n\n\n\n\n\n\n\n\n\nclass ChartView(object):\n\n    _LEAF_SPACING = 10\n    _MARGIN = 10\n    _TREE_LEVEL_SIZE = 12\n    _CHART_LEVEL_SIZE = 40\n\n    def __init__(self, chart, root=None, **kw):\n        draw_tree = kw.get('draw_tree', 0)\n        draw_sentence = kw.get('draw_sentence', 1)\n        self._fontsize = kw.get('fontsize', -12)\n\n        self._chart = chart\n\n        self._callbacks = {}\n\n        self._edgelevels = []\n        self._edgetags = {}\n\n        self._marks = {}\n\n        self._treetoks = []\n        self._treetoks_edge = None\n        self._treetoks_index = 0\n\n        self._tree_tags = []\n\n        self._compact = 0\n\n        if root is None:\n            top = Tk()\n            top.title('Chart View')\n            def destroy1(e, top=top): top.destroy()\n            def destroy2(top=top): top.destroy()\n            top.bind('q', destroy1)\n            b = Button(top, text='Done', command=destroy2)\n            b.pack(side='bottom')\n            self._root = top\n        else:\n            self._root = root\n\n        self._init_fonts(root)\n\n        (self._chart_sb, self._chart_canvas) = self._sb_canvas(self._root)\n        self._chart_canvas['height'] = 300\n        self._chart_canvas['closeenough'] = 15\n\n        if draw_sentence:\n            cframe = Frame(self._root, relief='sunk', border=2)\n            cframe.pack(fill='both', side='bottom')\n            self._sentence_canvas = Canvas(cframe, height=50)\n            self._sentence_canvas['background'] = '#e0e0e0'\n            self._sentence_canvas.pack(fill='both')\n        else:\n            self._sentence_canvas = None\n\n        if draw_tree:\n            (sb, canvas) = self._sb_canvas(self._root, 'n', 'x')\n            (self._tree_sb, self._tree_canvas) = (sb, canvas)\n            self._tree_canvas['height'] = 200\n        else:\n            self._tree_canvas = None\n\n        self._analyze()\n        self.draw()\n        self._resize()\n        self._grow()\n\n        self._chart_canvas.bind('<Configure>', self._configure)\n\n    def _init_fonts(self, root):\n        self._boldfont = Font(family='helvetica', weight='bold',\n                                    size=self._fontsize)\n        self._font = Font(family='helvetica',\n                                    size=self._fontsize)\n        self._sysfont = Font(font=Button()[\"font\"])\n        root.option_add(\"*Font\", self._sysfont)\n\n    def _sb_canvas(self, root, expand='y',\n                   fill='both', side='bottom'):\n        cframe = Frame(root, relief='sunk', border=2)\n        cframe.pack(fill=fill, expand=expand, side=side)\n        canvas = Canvas(cframe, background='#e0e0e0')\n\n        sb = Scrollbar(cframe, orient='vertical')\n        sb.pack(side='right', fill='y')\n        canvas.pack(side='left', fill=fill, expand='yes')\n\n        sb['command']= canvas.yview\n        canvas['yscrollcommand'] = sb.set\n\n        return (sb, canvas)\n\n    def scroll_up(self, *e):\n        self._chart_canvas.yview('scroll', -1, 'units')\n\n    def scroll_down(self, *e):\n        self._chart_canvas.yview('scroll', 1, 'units')\n\n    def page_up(self, *e):\n        self._chart_canvas.yview('scroll', -1, 'pages')\n\n    def page_down(self, *e):\n        self._chart_canvas.yview('scroll', 1, 'pages')\n\n    def _grow(self):\n        N = self._chart.num_leaves()\n        width = max(int(self._chart_canvas['width']),\n                    N * self._unitsize + ChartView._MARGIN * 2 )\n\n        self._chart_canvas.configure(width=width)\n        self._chart_canvas.configure(height=self._chart_canvas['height'])\n\n        self._unitsize = (width - 2*ChartView._MARGIN) / N\n\n        if self._sentence_canvas is not None:\n            self._sentence_canvas['height'] = self._sentence_height\n\n    def set_font_size(self, size):\n        self._font.configure(size=-abs(size))\n        self._boldfont.configure(size=-abs(size))\n        self._sysfont.configure(size=-abs(size))\n        self._analyze()\n        self._grow()\n        self.draw()\n\n    def get_font_size(self):\n        return abs(self._fontsize)\n\n    def _configure(self, e):\n        N = self._chart.num_leaves()\n        self._unitsize = (e.width - 2*ChartView._MARGIN) / N\n        self.draw()\n\n    def update(self, chart=None):\n        if chart is not None:\n            self._chart = chart\n            self._edgelevels = []\n            self._marks = {}\n            self._analyze()\n            self._grow()\n            self.draw()\n            self.erase_tree()\n            self._resize()\n        else:\n            for edge in self._chart:\n                if edge not in self._edgetags:\n                    self._add_edge(edge)\n            self._resize()\n\n\n    def _edge_conflict(self, edge, lvl):\n        (s1, e1) = edge.span()\n        for otheredge in self._edgelevels[lvl]:\n            (s2, e2) = otheredge.span()\n            if (s1 <= s2 < e1) or (s2 <= s1 < e2) or (s1==s2==e1==e2):\n                return True\n        return False\n\n    def _analyze_edge(self, edge):\n        c = self._chart_canvas\n\n        if isinstance(edge, TreeEdge):\n            lhs = edge.lhs()\n            rhselts = []\n            for elt in edge.rhs():\n                if isinstance(elt, Nonterminal):\n                    rhselts.append(str(elt.symbol()))\n                else:\n                    rhselts.append(repr(elt))\n            rhs = \" \".join(rhselts)\n        else:\n            lhs = edge.lhs()\n            rhs = ''\n\n        for s in (lhs, rhs):\n            tag = c.create_text(0,0, text=s,\n                                font=self._boldfont,\n                                anchor='nw', justify='left')\n            bbox = c.bbox(tag)\n            c.delete(tag)\n            width = bbox[2] #+ ChartView._LEAF_SPACING\n            edgelen = max(edge.length(), 1)\n            self._unitsize = max(self._unitsize, width/edgelen)\n            self._text_height = max(self._text_height, bbox[3] - bbox[1])\n\n    def _add_edge(self, edge, minlvl=0):\n        if isinstance(edge, LeafEdge): return\n\n        if edge in self._edgetags: return\n        self._analyze_edge(edge)\n        self._grow()\n\n        if not self._compact:\n            self._edgelevels.append([edge])\n            lvl = len(self._edgelevels)-1\n            self._draw_edge(edge, lvl)\n            self._resize()\n            return\n\n        lvl = 0\n        while True:\n            while lvl >= len(self._edgelevels):\n                self._edgelevels.append([])\n                self._resize()\n\n            if lvl>=minlvl and not self._edge_conflict(edge, lvl):\n                self._edgelevels[lvl].append(edge)\n                break\n\n            lvl += 1\n\n        self._draw_edge(edge, lvl)\n\n    def view_edge(self, edge):\n        level = None\n        for i in range(len(self._edgelevels)):\n            if edge in self._edgelevels[i]:\n                level = i\n                break\n        if level is None: return\n        y = (level+1) * self._chart_level_size\n        dy = self._text_height + 10\n        self._chart_canvas.yview('moveto', 1.0)\n        if self._chart_height != 0:\n            self._chart_canvas.yview('moveto',\n                                     (y-dy)/self._chart_height)\n\n    def _draw_edge(self, edge, lvl):\n        c = self._chart_canvas\n\n        x1 = (edge.start() * self._unitsize + ChartView._MARGIN)\n        x2 = (edge.end() * self._unitsize + ChartView._MARGIN)\n        if x2 == x1: x2 += max(4, self._unitsize/5)\n        y = (lvl+1) * self._chart_level_size\n        linetag = c.create_line(x1, y, x2, y, arrow='last', width=3)\n\n        if isinstance(edge, TreeEdge):\n            rhs = []\n            for elt in edge.rhs():\n                if isinstance(elt, Nonterminal):\n                    rhs.append(str(elt.symbol()))\n                else:\n                    rhs.append(repr(elt))\n            pos = edge.dot()\n        else:\n            rhs = []\n            pos = 0\n\n        rhs1 = \" \".join(rhs[:pos])\n        rhs2 = \" \".join(rhs[pos:])\n        rhstag1 = c.create_text(x1+3, y, text=rhs1,\n                                font=self._font,\n                                anchor='nw')\n        dotx = c.bbox(rhstag1)[2] + 6\n        doty = (c.bbox(rhstag1)[1]+c.bbox(rhstag1)[3])/2\n        dottag = c.create_oval(dotx-2, doty-2, dotx+2, doty+2)\n        rhstag2 = c.create_text(dotx+6, y, text=rhs2,\n                                font=self._font,\n                                anchor='nw')\n        lhstag =  c.create_text((x1+x2)/2, y, text=str(edge.lhs()),\n                                anchor='s',\n                                font=self._boldfont)\n\n        self._edgetags[edge] = (linetag, rhstag1,\n                                dottag, rhstag2, lhstag)\n\n        def cb(event, self=self, edge=edge):\n            self._fire_callbacks('select', edge)\n        c.tag_bind(rhstag1, '<Button-1>', cb)\n        c.tag_bind(rhstag2, '<Button-1>', cb)\n        c.tag_bind(linetag, '<Button-1>', cb)\n        c.tag_bind(dottag, '<Button-1>', cb)\n        c.tag_bind(lhstag, '<Button-1>', cb)\n\n        self._color_edge(edge)\n\n    def _color_edge(self, edge, linecolor=None, textcolor=None):\n        if edge not in self._edgetags: return\n        c = self._chart_canvas\n\n        if linecolor is not None and textcolor is not None:\n            if edge in self._marks:\n                linecolor = self._marks[edge]\n            tags = self._edgetags[edge]\n            c.itemconfig(tags[0], fill=linecolor)\n            c.itemconfig(tags[1], fill=textcolor)\n            c.itemconfig(tags[2], fill=textcolor,\n                         outline=textcolor)\n            c.itemconfig(tags[3], fill=textcolor)\n            c.itemconfig(tags[4], fill=textcolor)\n            return\n        else:\n            N = self._chart.num_leaves()\n            if edge in self._marks:\n                self._color_edge(self._marks[edge])\n            if (edge.is_complete() and edge.span() == (0, N)):\n                self._color_edge(edge, '#084', '#042')\n            elif isinstance(edge, LeafEdge):\n                self._color_edge(edge, '#48c', '#246')\n            else:\n                self._color_edge(edge, '#00f', '#008')\n\n    def mark_edge(self, edge, mark='#0df'):\n        self._marks[edge] = mark\n        self._color_edge(edge)\n\n    def unmark_edge(self, edge=None):\n        if edge is None:\n            old_marked_edges = list(self._marks.keys())\n            self._marks = {}\n            for edge in old_marked_edges:\n                self._color_edge(edge)\n        else:\n            del self._marks[edge]\n            self._color_edge(edge)\n\n    def markonly_edge(self, edge, mark='#0df'):\n        self.unmark_edge()\n        self.mark_edge(edge, mark)\n\n    def _analyze(self):\n        unitsize = 70 # min unitsize\n        text_height = 0\n        c = self._chart_canvas\n\n        for leaf in self._chart.leaves():\n            tag = c.create_text(0,0, text=repr(leaf),\n                                font=self._font,\n                                anchor='nw', justify='left')\n            bbox = c.bbox(tag)\n            c.delete(tag)\n            width = bbox[2] + ChartView._LEAF_SPACING\n            unitsize = max(width, unitsize)\n            text_height = max(text_height, bbox[3] - bbox[1])\n\n        self._unitsize = unitsize\n        self._text_height = text_height\n        self._sentence_height = (self._text_height +\n                               2*ChartView._MARGIN)\n\n        for edge in self._chart.edges():\n            self._analyze_edge(edge)\n\n        self._chart_level_size = self._text_height * 2\n\n        self._tree_height = (3 * (ChartView._TREE_LEVEL_SIZE +\n                                  self._text_height))\n\n        self._resize()\n\n    def _resize(self):\n        c = self._chart_canvas\n\n        width = ( self._chart.num_leaves() * self._unitsize +\n                  ChartView._MARGIN * 2 )\n\n        levels = len(self._edgelevels)\n        self._chart_height = (levels+2)*self._chart_level_size\n        c['scrollregion']=(0,0,width,self._chart_height)\n\n        if self._tree_canvas:\n            self._tree_canvas['scrollregion'] = (0, 0, width,\n                                                 self._tree_height)\n\n    def _draw_loclines(self):\n        BOTTOM = 50000\n        c1 = self._tree_canvas\n        c2 = self._sentence_canvas\n        c3 = self._chart_canvas\n        margin = ChartView._MARGIN\n        self._loclines = []\n        for i in range(0, self._chart.num_leaves()+1):\n            x = i*self._unitsize + margin\n\n            if c1:\n                t1=c1.create_line(x, 0, x, BOTTOM)\n                c1.tag_lower(t1)\n            if c2:\n                t2=c2.create_line(x, 0, x, self._sentence_height)\n                c2.tag_lower(t2)\n            t3=c3.create_line(x, 0, x, BOTTOM)\n            c3.tag_lower(t3)\n            t4=c3.create_text(x+2, 0, text=repr(i), anchor='nw',\n                              font=self._font)\n            c3.tag_lower(t4)\n            if i % 2 == 0:\n                if c1: c1.itemconfig(t1, fill='gray60')\n                if c2: c2.itemconfig(t2, fill='gray60')\n                c3.itemconfig(t3, fill='gray60')\n            else:\n                if c1: c1.itemconfig(t1, fill='gray80')\n                if c2: c2.itemconfig(t2, fill='gray80')\n                c3.itemconfig(t3, fill='gray80')\n\n    def _draw_sentence(self):\n        if self._chart.num_leaves() == 0: return\n        c = self._sentence_canvas\n        margin = ChartView._MARGIN\n        y = ChartView._MARGIN\n\n        for i, leaf in enumerate(self._chart.leaves()):\n            x1 = i * self._unitsize + margin\n            x2 = x1 + self._unitsize\n            x = (x1+x2)/2\n            tag = c.create_text(x, y, text=repr(leaf),\n                                font=self._font,\n                                anchor='n', justify='left')\n            bbox = c.bbox(tag)\n            rt=c.create_rectangle(x1+2, bbox[1]-(ChartView._LEAF_SPACING/2),\n                                  x2-2, bbox[3]+(ChartView._LEAF_SPACING/2),\n                                  fill='#f0f0f0', outline='#f0f0f0')\n            c.tag_lower(rt)\n\n    def erase_tree(self):\n        for tag in self._tree_tags: self._tree_canvas.delete(tag)\n        self._treetoks = []\n        self._treetoks_edge = None\n        self._treetoks_index = 0\n\n    def draw_tree(self, edge=None):\n        if edge is None and self._treetoks_edge is None: return\n        if edge is None: edge = self._treetoks_edge\n\n        if self._treetoks_edge != edge:\n            self._treetoks = [t for t in self._chart.trees(edge)\n                              if isinstance(t, Tree)]\n            self._treetoks_edge = edge\n            self._treetoks_index = 0\n\n        if len(self._treetoks) == 0: return\n\n        for tag in self._tree_tags: self._tree_canvas.delete(tag)\n\n        tree = self._treetoks[self._treetoks_index]\n        self._draw_treetok(tree, edge.start())\n\n        self._draw_treecycle()\n\n        w = self._chart.num_leaves()*self._unitsize+2*ChartView._MARGIN\n        h = tree.height() * (ChartView._TREE_LEVEL_SIZE+self._text_height)\n        self._tree_canvas['scrollregion'] = (0, 0, w, h)\n\n    def cycle_tree(self):\n        self._treetoks_index = (self._treetoks_index+1)%len(self._treetoks)\n        self.draw_tree(self._treetoks_edge)\n\n    def _draw_treecycle(self):\n        if len(self._treetoks) <= 1: return\n\n        label = '%d Trees' % len(self._treetoks)\n        c = self._tree_canvas\n        margin = ChartView._MARGIN\n        right = self._chart.num_leaves()*self._unitsize+margin-2\n        tag = c.create_text(right, 2, anchor='ne', text=label,\n                            font=self._boldfont)\n        self._tree_tags.append(tag)\n        _, _, _, y = c.bbox(tag)\n\n        for i in range(len(self._treetoks)):\n            x = right - 20*(len(self._treetoks)-i-1)\n            if i == self._treetoks_index: fill = '#084'\n            else: fill = '#fff'\n            tag = c.create_polygon(x, y+10, x-5, y, x-10, y+10,\n                             fill=fill, outline='black')\n            self._tree_tags.append(tag)\n\n            def cb(event, self=self, i=i):\n                self._treetoks_index = i\n                self.draw_tree()\n            c.tag_bind(tag, '<Button-1>', cb)\n\n    def _draw_treetok(self, treetok, index, depth=0):\n        c = self._tree_canvas\n        margin = ChartView._MARGIN\n\n        child_xs = []\n        for child in treetok:\n            if isinstance(child, Tree):\n                child_x, index = self._draw_treetok(child, index, depth+1)\n                child_xs.append(child_x)\n            else:\n                child_xs.append((2*index+1)*self._unitsize/2 + margin)\n                index += 1\n\n        if child_xs:\n            nodex = sum(child_xs)/len(child_xs)\n        else:\n            nodex = (2*index+1)*self._unitsize/2 + margin\n            index += 1\n\n        nodey = depth * (ChartView._TREE_LEVEL_SIZE + self._text_height)\n        tag = c.create_text(nodex, nodey, anchor='n', justify='center',\n                            text=str(treetok.label()), fill='#042',\n                            font=self._boldfont)\n        self._tree_tags.append(tag)\n\n        childy = nodey + ChartView._TREE_LEVEL_SIZE + self._text_height\n        for childx, child in zip(child_xs, treetok):\n            if isinstance(child, Tree) and child:\n                tag = c.create_line(nodex, nodey + self._text_height,\n                                    childx, childy, width=2, fill='#084')\n                self._tree_tags.append(tag)\n            if isinstance(child, Tree) and not child:\n                tag = c.create_line(nodex, nodey + self._text_height,\n                                    childx, childy, width=2,\n                                    fill='#048', dash='2 3')\n                self._tree_tags.append(tag)\n            if not isinstance(child, Tree):\n                tag = c.create_line(nodex, nodey + self._text_height,\n                                    childx, 10000, width=2, fill='#084')\n                self._tree_tags.append(tag)\n\n        return nodex, index\n\n    def draw(self):\n        if self._tree_canvas:\n            self._tree_canvas.delete('all')\n            self.draw_tree()\n\n        if self._sentence_canvas:\n            self._sentence_canvas.delete('all')\n            self._draw_sentence()\n\n        self._chart_canvas.delete('all')\n        self._edgetags = {}\n\n        for lvl in range(len(self._edgelevels)):\n            for edge in self._edgelevels[lvl]:\n                self._draw_edge(edge, lvl)\n\n        for edge in self._chart:\n            self._add_edge(edge)\n\n        self._draw_loclines()\n\n    def add_callback(self, event, func):\n        self._callbacks.setdefault(event,{})[func] = 1\n\n    def remove_callback(self, event, func=None):\n        if func is None: del self._callbacks[event]\n        else:\n            try: del self._callbacks[event][func]\n            except: pass\n\n    def _fire_callbacks(self, event, *args):\n        if event not in self._callbacks: return\n        for cb_func in list(self._callbacks[event].keys()): cb_func(*args)\n\n\nclass EdgeRule(object):\n    def __init__(self, edge):\n        super = self.__class__.__bases__[1]\n        self._edge = edge\n        self.NUM_EDGES = super.NUM_EDGES-1\n    def apply(self, chart, grammar, *edges):\n        super = self.__class__.__bases__[1]\n        edges += (self._edge,)\n        for e in super.apply(self, chart, grammar, *edges): yield e\n    def __str__(self):\n        super = self.__class__.__bases__[1]\n        return super.__str__(self)\n\nclass TopDownPredictEdgeRule(EdgeRule, TopDownPredictRule):\n    pass\nclass BottomUpEdgeRule(EdgeRule, BottomUpPredictRule):\n    pass\nclass BottomUpLeftCornerEdgeRule(EdgeRule, BottomUpPredictCombineRule):\n    pass\nclass FundamentalEdgeRule(EdgeRule, SingleEdgeFundamentalRule):\n    pass\n\n\nclass ChartParserApp(object):\n    def __init__(self, grammar, tokens, title='Chart Parser Application'):\n        self._init_parser(grammar, tokens)\n\n        self._root = None\n        try:\n            self._root = Tk()\n            self._root.title(title)\n            self._root.bind('<Control-q>', self.destroy)\n\n            frame3 = Frame(self._root)\n            frame2 = Frame(self._root)\n            frame1 = Frame(self._root)\n            frame3.pack(side='bottom', fill='none')\n            frame2.pack(side='bottom', fill='x')\n            frame1.pack(side='bottom', fill='both', expand=1)\n\n            self._init_fonts(self._root)\n            self._init_animation()\n            self._init_chartview(frame1)\n            self._init_rulelabel(frame2)\n            self._init_buttons(frame3)\n            self._init_menubar()\n\n            self._matrix = None\n            self._results = None\n\n            self._init_bindings()\n\n        except:\n            print('Error creating Tree View')\n            self.destroy()\n            raise\n\n    def destroy(self, *args):\n        if self._root is None: return\n        self._root.destroy()\n        self._root = None\n\n    def mainloop(self, *args, **kwargs):\n        if in_idle(): return\n        self._root.mainloop(*args, **kwargs)\n\n\n    def _init_parser(self, grammar, tokens):\n        self._grammar = grammar\n        self._tokens = tokens\n        self._reset_parser()\n\n    def _reset_parser(self):\n        self._cp = SteppingChartParser(self._grammar)\n        self._cp.initialize(self._tokens)\n        self._chart = self._cp.chart()\n\n        for _new_edge in LeafInitRule().apply(self._chart, self._grammar):\n            pass\n\n        self._cpstep = self._cp.step()\n\n        self._selection = None\n\n    def _init_fonts(self, root):\n        self._sysfont = Font(font=Button()[\"font\"])\n        root.option_add(\"*Font\", self._sysfont)\n\n        self._size = IntVar(root)\n        self._size.set(self._sysfont.cget('size'))\n\n        self._boldfont = Font(family='helvetica', weight='bold',\n                                    size=self._size.get())\n        self._font = Font(family='helvetica',\n                                    size=self._size.get())\n\n    def _init_animation(self):\n        self._step = IntVar(self._root)\n        self._step.set(1)\n\n        self._animate = IntVar(self._root)\n        self._animate.set(3) # Default speed = fast\n\n        self._animating = 0\n\n    def _init_chartview(self, parent):\n        self._cv = ChartView(self._chart, parent,\n                             draw_tree=1, draw_sentence=1)\n        self._cv.add_callback('select', self._click_cv_edge)\n\n    def _init_rulelabel(self, parent):\n        ruletxt = 'Last edge generated by:'\n\n        self._rulelabel1 = Label(parent,text=ruletxt,\n                                         font=self._boldfont)\n        self._rulelabel2 = Label(parent, width=40,\n                                         relief='groove', anchor='w',\n                                         font=self._boldfont)\n        self._rulelabel1.pack(side='left')\n        self._rulelabel2.pack(side='left')\n        step = Checkbutton(parent, variable=self._step,\n                                   text='Step')\n        step.pack(side='right')\n\n    def _init_buttons(self, parent):\n        frame1 = Frame(parent)\n        frame2 = Frame(parent)\n        frame1.pack(side='bottom', fill='x')\n        frame2.pack(side='top', fill='none')\n\n        Button(frame1, text='Reset\\nParser',\n                       background='#90c0d0', foreground='black',\n                       command=self.reset).pack(side='right')\n\n        Button(frame1, text='Top Down\\nStrategy',\n                       background='#90c0d0', foreground='black',\n                       command=self.top_down_strategy).pack(side='left')\n        Button(frame1, text='Bottom Up\\nStrategy',\n                       background='#90c0d0', foreground='black',\n                       command=self.bottom_up_strategy).pack(side='left')\n        Button(frame1, text='Bottom Up\\nLeft-Corner Strategy',\n                       background='#90c0d0', foreground='black',\n                       command=self.bottom_up_leftcorner_strategy).pack(side='left')\n\n        Button(frame2, text='Top Down Init\\nRule',\n                       background='#90f090', foreground='black',\n                       command=self.top_down_init).pack(side='left')\n        Button(frame2, text='Top Down Predict\\nRule',\n                       background='#90f090', foreground='black',\n                       command=self.top_down_predict).pack(side='left')\n        Frame(frame2, width=20).pack(side='left')\n\n        Button(frame2, text='Bottom Up Predict\\nRule',\n                       background='#90f090', foreground='black',\n                       command=self.bottom_up).pack(side='left')\n        Frame(frame2, width=20).pack(side='left')\n\n        Button(frame2, text='Bottom Up Left-Corner\\nPredict Rule',\n                       background='#90f090', foreground='black',\n                       command=self.bottom_up_leftcorner).pack(side='left')\n        Frame(frame2, width=20).pack(side='left')\n\n        Button(frame2, text='Fundamental\\nRule',\n                       background='#90f090', foreground='black',\n                       command=self.fundamental).pack(side='left')\n\n    def _init_bindings(self):\n        self._root.bind('<Up>', self._cv.scroll_up)\n        self._root.bind('<Down>', self._cv.scroll_down)\n        self._root.bind('<Prior>', self._cv.page_up)\n        self._root.bind('<Next>', self._cv.page_down)\n        self._root.bind('<Control-q>', self.destroy)\n        self._root.bind('<Control-x>', self.destroy)\n        self._root.bind('<F1>', self.help)\n\n        self._root.bind('<Control-s>', self.save_chart)\n        self._root.bind('<Control-o>', self.load_chart)\n        self._root.bind('<Control-r>', self.reset)\n\n        self._root.bind('t', self.top_down_strategy)\n        self._root.bind('b', self.bottom_up_strategy)\n        self._root.bind('c', self.bottom_up_leftcorner_strategy)\n        self._root.bind('<space>', self._stop_animation)\n\n        self._root.bind('<Control-g>', self.edit_grammar)\n        self._root.bind('<Control-t>', self.edit_sentence)\n\n        self._root.bind('-', lambda e,a=self._animate:a.set(1))\n        self._root.bind('=', lambda e,a=self._animate:a.set(2))\n        self._root.bind('+', lambda e,a=self._animate:a.set(3))\n\n        self._root.bind('s', lambda e,s=self._step:s.set(not s.get()))\n\n    def _init_menubar(self):\n        menubar = Menu(self._root)\n\n        filemenu = Menu(menubar, tearoff=0)\n        filemenu.add_command(label='Save Chart', underline=0,\n                             command=self.save_chart, accelerator='Ctrl-s')\n        filemenu.add_command(label='Load Chart', underline=0,\n                             command=self.load_chart, accelerator='Ctrl-o')\n        filemenu.add_command(label='Reset Chart', underline=0,\n                             command=self.reset, accelerator='Ctrl-r')\n        filemenu.add_separator()\n        filemenu.add_command(label='Save Grammar',\n                             command=self.save_grammar)\n        filemenu.add_command(label='Load Grammar',\n                             command=self.load_grammar)\n        filemenu.add_separator()\n        filemenu.add_command(label='Exit', underline=1,\n                             command=self.destroy, accelerator='Ctrl-x')\n        menubar.add_cascade(label='File', underline=0, menu=filemenu)\n\n        editmenu = Menu(menubar, tearoff=0)\n        editmenu.add_command(label='Edit Grammar', underline=5,\n                             command=self.edit_grammar,\n                             accelerator='Ctrl-g')\n        editmenu.add_command(label='Edit Text', underline=5,\n                             command=self.edit_sentence,\n                             accelerator='Ctrl-t')\n        menubar.add_cascade(label='Edit', underline=0, menu=editmenu)\n\n        viewmenu = Menu(menubar, tearoff=0)\n        viewmenu.add_command(label='Chart Matrix', underline=6,\n                             command=self.view_matrix)\n        viewmenu.add_command(label='Results', underline=0,\n                             command=self.view_results)\n        menubar.add_cascade(label='View', underline=0, menu=viewmenu)\n\n        rulemenu = Menu(menubar, tearoff=0)\n        rulemenu.add_command(label='Top Down Strategy', underline=0,\n                             command=self.top_down_strategy,\n                             accelerator='t')\n        rulemenu.add_command(label='Bottom Up Strategy', underline=0,\n                             command=self.bottom_up_strategy,\n                             accelerator='b')\n        rulemenu.add_command(label='Bottom Up Left-Corner Strategy', underline=0,\n                             command=self.bottom_up_leftcorner_strategy,\n                             accelerator='c')\n        rulemenu.add_separator()\n        rulemenu.add_command(label='Bottom Up Rule',\n                             command=self.bottom_up)\n        rulemenu.add_command(label='Bottom Up Left-Corner Rule',\n                             command=self.bottom_up_leftcorner)\n        rulemenu.add_command(label='Top Down Init Rule',\n                             command=self.top_down_init)\n        rulemenu.add_command(label='Top Down Predict Rule',\n                             command=self.top_down_predict)\n        rulemenu.add_command(label='Fundamental Rule',\n                             command=self.fundamental)\n        menubar.add_cascade(label='Apply', underline=0, menu=rulemenu)\n\n        animatemenu = Menu(menubar, tearoff=0)\n        animatemenu.add_checkbutton(label=\"Step\", underline=0,\n                                    variable=self._step,\n                                    accelerator='s')\n        animatemenu.add_separator()\n        animatemenu.add_radiobutton(label=\"No Animation\", underline=0,\n                                    variable=self._animate, value=0)\n        animatemenu.add_radiobutton(label=\"Slow Animation\", underline=0,\n                                    variable=self._animate, value=1,\n                                    accelerator='-')\n        animatemenu.add_radiobutton(label=\"Normal Animation\", underline=0,\n                                    variable=self._animate, value=2,\n                                    accelerator='=')\n        animatemenu.add_radiobutton(label=\"Fast Animation\", underline=0,\n                                    variable=self._animate, value=3,\n                                    accelerator='+')\n        menubar.add_cascade(label=\"Animate\", underline=1, menu=animatemenu)\n\n        zoommenu = Menu(menubar, tearoff=0)\n        zoommenu.add_radiobutton(label='Tiny', variable=self._size,\n                                 underline=0, value=10, command=self.resize)\n        zoommenu.add_radiobutton(label='Small', variable=self._size,\n                                 underline=0, value=12, command=self.resize)\n        zoommenu.add_radiobutton(label='Medium', variable=self._size,\n                                 underline=0, value=14, command=self.resize)\n        zoommenu.add_radiobutton(label='Large', variable=self._size,\n                                 underline=0, value=18, command=self.resize)\n        zoommenu.add_radiobutton(label='Huge', variable=self._size,\n                                 underline=0, value=24, command=self.resize)\n        menubar.add_cascade(label='Zoom', underline=0, menu=zoommenu)\n\n        helpmenu = Menu(menubar, tearoff=0)\n        helpmenu.add_command(label='About', underline=0,\n                             command=self.about)\n        helpmenu.add_command(label='Instructions', underline=0,\n                             command=self.help, accelerator='F1')\n        menubar.add_cascade(label='Help', underline=0, menu=helpmenu)\n\n        self._root.config(menu=menubar)\n\n\n    def _click_cv_edge(self, edge):\n        if edge != self._selection:\n            self._select_edge(edge)\n        else:\n            self._cv.cycle_tree()\n\n    def _select_matrix_edge(self, edge):\n        self._select_edge(edge)\n        self._cv.view_edge(edge)\n\n    def _select_edge(self, edge):\n        self._selection = edge\n        self._cv.markonly_edge(edge, '#f00')\n        self._cv.draw_tree(edge)\n        if self._matrix: self._matrix.markonly_edge(edge)\n        if self._matrix: self._matrix.view_edge(edge)\n\n    def _deselect_edge(self):\n        self._selection = None\n        self._cv.unmark_edge()\n        self._cv.erase_tree()\n        if self._matrix: self._matrix.unmark_edge()\n\n    def _show_new_edge(self, edge):\n        self._display_rule(self._cp.current_chartrule())\n        self._cv.update()\n        self._cv.draw_tree(edge)\n        self._cv.markonly_edge(edge, '#0df')\n        self._cv.view_edge(edge)\n        if self._matrix: self._matrix.update()\n        if self._matrix: self._matrix.markonly_edge(edge)\n        if self._matrix: self._matrix.view_edge(edge)\n        if self._results: self._results.update(edge)\n\n\n    def help(self, *e):\n        self._animating = 0\n        try:\n            ShowText(self._root, 'Help: Chart Parser Application',\n                     (__doc__ or '').strip(), width=75, font='fixed')\n        except:\n            ShowText(self._root, 'Help: Chart Parser Application',\n                     (__doc__ or '').strip(), width=75)\n\n    def about(self, *e):\n        ABOUT = (\"NLTK Chart Parser Application\\n\"+\n                 \"Written by Edward Loper\")\n        showinfo('About: Chart Parser Application', ABOUT)\n\n\n    CHART_FILE_TYPES = [('Pickle file', '.pickle'),\n                        ('All files', '*')]\n    GRAMMAR_FILE_TYPES = [('Plaintext grammar file', '.cfg'),\n                          ('Pickle file', '.pickle'),\n                          ('All files', '*')]\n\n    def load_chart(self, *args):\n        \"Load a chart from a pickle file\"\n        filename = askopenfilename(filetypes=self.CHART_FILE_TYPES,\n                                   defaultextension='.pickle')\n        if not filename: return\n        try:\n            with open(filename, 'rb') as infile:\n                chart = pickle.load(infile)\n            self._chart = chart\n            self._cv.update(chart)\n            if self._matrix: self._matrix.set_chart(chart)\n            if self._matrix: self._matrix.deselect_cell()\n            if self._results: self._results.set_chart(chart)\n            self._cp.set_chart(chart)\n        except Exception as e:\n            raise\n            showerror('Error Loading Chart',\n                                   'Unable to open file: %r' % filename)\n\n    def save_chart(self, *args):\n        \"Save a chart to a pickle file\"\n        filename = asksaveasfilename(filetypes=self.CHART_FILE_TYPES,\n                                     defaultextension='.pickle')\n        if not filename: return\n        try:\n            with open(filename, 'wb') as outfile:\n                pickle.dump(self._chart, outfile)\n        except Exception as e:\n            raise\n            showerror('Error Saving Chart',\n                                   'Unable to open file: %r' % filename)\n\n    def load_grammar(self, *args):\n        \"Load a grammar from a pickle file\"\n        filename = askopenfilename(filetypes=self.GRAMMAR_FILE_TYPES,\n                                   defaultextension='.cfg')\n        if not filename: return\n        try:\n            if filename.endswith('.pickle'):\n                with open(filename, 'rb') as infile:\n                    grammar = pickle.load(infile)\n            else:\n                with open(filename, 'r') as infile:\n                    grammar = CFG.fromstring(infile.read())\n            self.set_grammar(grammar)\n        except Exception as e:\n            showerror('Error Loading Grammar',\n                                   'Unable to open file: %r' % filename)\n\n    def save_grammar(self, *args):\n        filename = asksaveasfilename(filetypes=self.GRAMMAR_FILE_TYPES,\n                                     defaultextension='.cfg')\n        if not filename: return\n        try:\n            if filename.endswith('.pickle'):\n                with open(filename, 'wb') as outfile:\n                    pickle.dump((self._chart, self._tokens), outfile)\n            else:\n                with open(filename, 'w') as outfile:\n                    prods = self._grammar.productions()\n                    start = [p for p in prods if p.lhs() == self._grammar.start()]\n                    rest = [p for p in prods if p.lhs() != self._grammar.start()]\n                    for prod in start: outfile.write('%s\\n' % prod)\n                    for prod in rest: outfile.write('%s\\n' % prod)\n        except Exception as e:\n            showerror('Error Saving Grammar',\n                                   'Unable to open file: %r' % filename)\n\n    def reset(self, *args):\n        self._animating = 0\n        self._reset_parser()\n        self._cv.update(self._chart)\n        if self._matrix: self._matrix.set_chart(self._chart)\n        if self._matrix: self._matrix.deselect_cell()\n        if self._results: self._results.set_chart(self._chart)\n\n\n    def edit_grammar(self, *e):\n        CFGEditor(self._root, self._grammar, self.set_grammar)\n\n    def set_grammar(self, grammar):\n        self._grammar = grammar\n        self._cp.set_grammar(grammar)\n        if self._results: self._results.set_grammar(grammar)\n\n    def edit_sentence(self, *e):\n        sentence = \" \".join(self._tokens)\n        title = 'Edit Text'\n        instr = 'Enter a new sentence to parse.'\n        EntryDialog(self._root, sentence, instr, self.set_sentence, title)\n\n    def set_sentence(self, sentence):\n        self._tokens = list(sentence.split())\n        self.reset()\n\n\n    def view_matrix(self, *e):\n        if self._matrix is not None: self._matrix.destroy()\n        self._matrix = ChartMatrixView(self._root, self._chart)\n        self._matrix.add_callback('select', self._select_matrix_edge)\n\n    def view_results(self, *e):\n        if self._results is not None: self._results.destroy()\n        self._results = ChartResultsView(self._root, self._chart,\n                                         self._grammar)\n\n\n    def resize(self):\n        self._animating = 0\n        self.set_font_size(self._size.get())\n\n    def set_font_size(self, size):\n        self._cv.set_font_size(size)\n        self._font.configure(size=-abs(size))\n        self._boldfont.configure(size=-abs(size))\n        self._sysfont.configure(size=-abs(size))\n\n    def get_font_size(self):\n        return abs(self._size.get())\n\n\n    def apply_strategy(self, strategy, edge_strategy=None):\n        if self._animating:\n            self._animating = 0\n            return\n\n        self._display_rule(None)\n\n        if self._step.get():\n            selection = self._selection\n            if (selection is not None) and (edge_strategy is not None):\n                self._cp.set_strategy([edge_strategy(selection)])\n                newedge = self._apply_strategy()\n\n                if newedge is None:\n                    self._cv.unmark_edge()\n                    self._selection = None\n            else:\n                self._cp.set_strategy(strategy)\n                self._apply_strategy()\n\n        else:\n            self._cp.set_strategy(strategy)\n            if self._animate.get():\n                self._animating = 1\n                self._animate_strategy()\n            else:\n                for edge in self._cpstep:\n                    if edge is None: break\n                self._cv.update()\n                if self._matrix: self._matrix.update()\n                if self._results: self._results.update()\n\n    def _stop_animation(self, *e):\n        self._animating = 0\n\n    def _animate_strategy(self, speed=1):\n        if self._animating == 0: return\n        if self._apply_strategy() is not None:\n            if self._animate.get() == 0 or self._step.get() == 1:\n                return\n            if self._animate.get() == 1:\n                self._root.after(3000, self._animate_strategy)\n            elif self._animate.get() == 2:\n                self._root.after(1000, self._animate_strategy)\n            else:\n                self._root.after(20, self._animate_strategy)\n\n    def _apply_strategy(self):\n        new_edge = next(self._cpstep)\n\n        if new_edge is not None:\n            self._show_new_edge(new_edge)\n        return new_edge\n\n    def _display_rule(self, rule):\n        if rule is None:\n            self._rulelabel2['text'] = ''\n        else:\n            name = str(rule)\n            self._rulelabel2['text'] = name\n            size = self._cv.get_font_size()\n\n\n    _TD_INIT     = [TopDownInitRule()]\n    _TD_PREDICT  = [TopDownPredictRule()]\n    _BU_RULE     = [BottomUpPredictRule()]\n    _BU_LC_RULE  = [BottomUpPredictCombineRule()]\n    _FUNDAMENTAL = [SingleEdgeFundamentalRule()]\n\n    _TD_STRATEGY =  _TD_INIT + _TD_PREDICT + _FUNDAMENTAL\n    _BU_STRATEGY = _BU_RULE + _FUNDAMENTAL\n    _BU_LC_STRATEGY = _BU_LC_RULE + _FUNDAMENTAL\n\n    def top_down_init(self, *e):\n        self.apply_strategy(self._TD_INIT, None)\n    def top_down_predict(self, *e):\n        self.apply_strategy(self._TD_PREDICT, TopDownPredictEdgeRule)\n    def bottom_up(self, *e):\n        self.apply_strategy(self._BU_RULE, BottomUpEdgeRule)\n    def bottom_up_leftcorner(self, *e):\n        self.apply_strategy(self._BU_LC_RULE, BottomUpLeftCornerEdgeRule)\n    def fundamental(self, *e):\n        self.apply_strategy(self._FUNDAMENTAL, FundamentalEdgeRule)\n    def bottom_up_strategy(self, *e):\n        self.apply_strategy(self._BU_STRATEGY, BottomUpEdgeRule)\n    def bottom_up_leftcorner_strategy(self, *e):\n        self.apply_strategy(self._BU_LC_STRATEGY, BottomUpLeftCornerEdgeRule)\n    def top_down_strategy(self, *e):\n        self.apply_strategy(self._TD_STRATEGY, TopDownPredictEdgeRule)\n\ndef app():\n    grammar = CFG.fromstring(\"\"\"\n        S -> NP VP\n        VP -> VP PP | V NP | V\n        NP -> Det N | NP PP\n        PP -> P NP\n        NP -> 'John' | 'I'\n        Det -> 'the' | 'my' | 'a'\n        N -> 'dog' | 'cookie' | 'table' | 'cake' | 'fork'\n        V -> 'ate' | 'saw'\n        P -> 'on' | 'under' | 'with'\n    \"\"\")\n\n    sent = 'John ate the cake on the table with a fork'\n    sent = 'John ate the cake on the table'\n    tokens = list(sent.split())\n\n    print('grammar= (')\n    for rule in grammar.productions():\n        print(('    ', repr(rule)+','))\n    print(')')\n    print(('tokens = %r' % tokens))\n    print('Calling \"ChartParserApp(grammar, tokens)\"...')\n    ChartParserApp(grammar, tokens).mainloop()\n\nif __name__ == '__main__':\n    app()\n\n\n\n__all__ = ['app']\n"], "nltk\\app\\chunkparser_app": [".py", "\n\n\nfrom __future__ import division\nimport time\nimport textwrap\nimport re\nimport random\n\nfrom six.moves.tkinter import (Button, Canvas, Checkbutton, Frame, IntVar,\n                               Label, Menu, Scrollbar, Text, Tk)\nfrom six.moves.tkinter_tkfiledialog import askopenfilename, asksaveasfilename\nfrom six.moves.tkinter_font import Font\n\nfrom nltk.tree import Tree\nfrom nltk.util import in_idle\nfrom nltk.draw.util import ShowText\nfrom nltk.corpus import conll2000, treebank_chunk\nfrom nltk.chunk import ChunkScore, RegexpChunkParser\nfrom nltk.chunk.regexp import RegexpChunkRule\n\nclass RegexpChunkApp(object):\n\n\n    TAGSET = {\n        'CC':   'Coordinating conjunction',   'PRP$': 'Possessive pronoun',\n        'CD':   'Cardinal number',            'RB':   'Adverb',\n        'DT':   'Determiner',                 'RBR':  'Adverb, comparative',\n        'EX':   'Existential there',          'RBS':  'Adverb, superlative',\n        'FW':   'Foreign word',               'RP':   'Particle',\n        'JJ':   'Adjective',                  'TO':   'to',\n        'JJR':  'Adjective, comparative',     'UH':   'Interjection',\n        'JJS':  'Adjective, superlative',     'VB':   'Verb, base form',\n        'LS':   'List item marker',           'VBD':  'Verb, past tense',\n        'MD':   'Modal',                      'NNS':  'Noun, plural',\n        'NN':   'Noun, singular or masps',    'VBN':  'Verb, past participle',\n        'VBZ':  'Verb,3rd ps. sing. present', 'NNP':  'Proper noun, singular',\n        'NNPS': 'Proper noun plural',         'WDT':  'wh-determiner',\n        'PDT':  'Predeterminer',              'WP':   'wh-pronoun',\n        'POS':  'Possessive ending',          'WP$':  'Possessive wh-pronoun',\n        'PRP':  'Personal pronoun',           'WRB':  'wh-adverb',\n        '(':    'open parenthesis',           ')':    'close parenthesis',\n        '``':   'open quote',                 ',':    'comma',\n        \"''\":   'close quote',                '.':    'period',\n        '#':    'pound sign (currency marker)',\n        '$':    'dollar sign (currency marker)',\n        'IN':   'Preposition/subord. conjunction',\n        'SYM':  'Symbol (mathematical or scientific)',\n        'VBG':  'Verb, gerund/present participle',\n        'VBP':  'Verb, non-3rd ps. sing. present',\n        ':':    'colon',\n        }\n\n    HELP = [\n        ('Help', '20',\n         \"Welcome to the regular expression chunk-parser grammar editor.  \"\n         \"You can use this editor to develop and test chunk parser grammars \"\n         \"based on NLTK's RegexpChunkParser class.\\n\\n\"\n         \"Use this box ('Help') to learn more about the editor; click on the \"\n         \"tabs for help on specific topics:\"\n         \"<indent>\\n\"\n         \"Rules: grammar rule types\\n\"\n         \"Regexps: regular expression syntax\\n\"\n         \"Tags: part of speech tags\\n</indent>\\n\"\n         \"Use the upper-left box ('Grammar') to edit your grammar.  \"\n         \"Each line of your grammar specifies a single 'rule', \"\n         \"which performs an action such as creating a chunk or merging \"\n         \"two chunks.\\n\\n\"\n         \"The lower-left box ('Development Set') runs your grammar on the \"\n         \"development set, and displays the results.  \"\n         \"Your grammar's chunks are <highlight>highlighted</highlight>, and \"\n         \"the correct (gold standard) chunks are \"\n         \"<underline>underlined</underline>.  If they \"\n         \"match, they are displayed in <green>green</green>; otherwise, \"\n         \"they are displayed in <red>red</red>.  The box displays a single \"\n         \"sentence from the development set at a time; use the scrollbar or \"\n         \"the next/previous buttons view additional sentences.\\n\\n\"\n         \"The lower-right box ('Evaluation') tracks the performance of \"\n         \"your grammar on the development set.  The 'precision' axis \"\n         \"indicates how many of your grammar's chunks are correct; and \"\n         \"the 'recall' axis indicates how many of the gold standard \"\n         \"chunks your system generated.  Typically, you should try to \"\n         \"design a grammar that scores high on both metrics.  The \"\n         \"exact precision and recall of the current grammar, as well \"\n         \"as their harmonic mean (the 'f-score'), are displayed in \"\n         \"the status bar at the bottom of the window.\"\n         ),\n        ('Rules', '10',\n         \"<h1>{...regexp...}</h1>\"\n         \"<indent>\\nChunk rule: creates new chunks from words matching \"\n         \"regexp.</indent>\\n\\n\"\n         \"<h1>}...regexp...{</h1>\"\n         \"<indent>\\nChink rule: removes words matching regexp from existing \"\n         \"chunks.</indent>\\n\\n\"\n         \"<h1>...regexp1...}{...regexp2...</h1>\"\n         \"<indent>\\nSplit rule: splits chunks that match regexp1 followed by \"\n         \"regexp2 in two.</indent>\\n\\n\"\n         \"<h1>...regexp...{}...regexp...</h1>\"\n         \"<indent>\\nMerge rule: joins consecutive chunks that match regexp1 \"\n         \"and regexp2</indent>\\n\"\n         ),\n        ('Regexps', '10 60',\n         \"<h1>Pattern\\t\\tMatches...</h1>\\n\"\n         \"<hangindent>\"\n         \"\\t<<var>T</var>>\\ta word with tag <var>T</var> \"\n         \"(where <var>T</var> may be a regexp).\\n\"\n         \"\\t<var>x</var>?\\tan optional <var>x</var>\\n\"\n         \"\\t<var>x</var>+\\ta sequence of 1 or more <var>x</var>'s\\n\"\n         \"\\t<var>x</var>*\\ta sequence of 0 or more <var>x</var>'s\\n\"\n         \"\\t<var>x</var>|<var>y</var>\\t<var>x</var> or <var>y</var>\\n\"\n         \"\\t.\\tmatches any character\\n\"\n         \"\\t(<var>x</var>)\\tTreats <var>x</var> as a group\\n\"\n         \"\\t# <var>x...</var>\\tTreats <var>x...</var> \"\n         \"(to the end of the line) as a comment\\n\"\n         \"\\t\\\\<var>C</var>\\tmatches character <var>C</var> \"\n         \"(useful when <var>C</var> is a special character \"\n         \"like + or #)\\n\"\n         \"</hangindent>\"\n         \"\\n<h1>Examples:</h1>\\n\"\n         \"<hangindent>\"\n         '\\t<regexp><NN></regexp>\\n'\n         '\\t\\tMatches <match>\"cow/NN\"</match>\\n'\n         '\\t\\tMatches <match>\"green/NN\"</match>\\n'\n         '\\t<regexp><VB.*></regexp>\\n'\n         '\\t\\tMatches <match>\"eating/VBG\"</match>\\n'\n         '\\t\\tMatches <match>\"ate/VBD\"</match>\\n'\n         '\\t<regexp><IN><DT><NN></regexp>\\n'\n         '\\t\\tMatches <match>\"on/IN the/DT car/NN\"</match>\\n'\n         '\\t<regexp><RB>?<VBD></regexp>\\n'\n         '\\t\\tMatches <match>\"ran/VBD\"</match>\\n'\n         '\\t\\tMatches <match>\"slowly/RB ate/VBD\"</match>\\n'\n         '\\t<regexp><\\#><CD> # This is a comment...</regexp>\\n'\n         '\\t\\tMatches <match>\"#/# 100/CD\"</match>\\n'\n         \"</hangindent>\"\n         ),\n        ('Tags', '10 60',\n         \"<h1>Part of Speech Tags:</h1>\\n\" +\n         '<hangindent>' +\n         '<<TAGSET>>' + # this gets auto-substituted w/ self.TAGSET\n         '</hangindent>\\n')\n        ]\n\n    HELP_AUTOTAG = [\n        ('red', dict(foreground='#a00')),\n        ('green', dict(foreground='#080')),\n        ('highlight', dict(background='#ddd')),\n        ('underline', dict(underline=True)),\n        ('h1', dict(underline=True)),\n        ('indent', dict(lmargin1=20, lmargin2=20)),\n        ('hangindent', dict(lmargin1=0, lmargin2=60)),\n        ('var', dict(foreground='#88f')),\n        ('regexp', dict(foreground='#ba7')),\n        ('match', dict(foreground='#6a6')),\n        ]\n\n\n    _EVAL_DELAY = 1\n        :param devset_name: The name of the development set; used for\n            display & for save files.  If either the name 'treebank'\n            or the name 'conll2000' is used, and devset is None, then\n            devset will be set automatically.\n        :param devset: A list of chunked sentences\n        :param grammar: The initial grammar to display.\n        :param tagset: Dictionary from tags to string descriptions, used\n            for the help page.  Defaults to ``self.TAGSET``.\n        \"\"\"\n        self._chunk_label = chunk_label\n\n        if tagset is None: tagset = self.TAGSET\n        self.tagset = tagset\n\n        if devset is None:\n            if devset_name == 'conll2000':\n                devset = conll2000.chunked_sents('train.txt')#[:100]\n            elif devset == 'treebank':\n                devset = treebank_chunk.chunked_sents()#[:100]\n            else:\n                raise ValueError('Unknown development set %s' % devset_name)\n\n        self.chunker = None\n\n        self.grammar = grammar\n\n        self.normalized_grammar = None\n\n        self.grammar_changed = 0\n\n        self.devset = devset\n\n        self.devset_name = devset_name\n\n        self.devset_index = -1\n        \"\"\"The index into the development set of the first instance\n           that's currently being viewed.\"\"\"\n\n        self._last_keypress = 0\n\n        self._history = []\n        \"\"\"A list of (grammar, precision, recall, fscore) tuples for\n           grammars that the user has already tried.\"\"\"\n\n        self._history_index = 0\n        \"\"\"When the user is scrolling through previous grammars, this\n           is used to keep track of which grammar they're looking at.\"\"\"\n\n        self._eval_grammar = None\n        \"\"\"The grammar that is being currently evaluated by the eval\n           demon.\"\"\"\n\n        self._eval_normalized_grammar = None\n\n        self._eval_index = 0\n        \"\"\"The index of the next sentence in the development set that\n           should be looked at by the eval demon.\"\"\"\n\n        self._eval_score = ChunkScore(chunk_label=chunk_label)\n        \"\"\"The ``ChunkScore`` object that's used to keep track of the score\n        of the current grammar on the development set.\"\"\"\n\n        top = self.top = Tk()\n        top.geometry('+50+50')\n        top.title('Regexp Chunk Parser App')\n        top.bind('<Control-q>', self.destroy)\n\n        self._devset_size = IntVar(top)\n        self._devset_size.set(100)\n\n        self._init_fonts(top)\n        self._init_widgets(top)\n        self._init_bindings(top)\n        self._init_menubar(top)\n        self.grammarbox.focus()\n\n\n        if grammar:\n            self.grammarbox.insert('end', grammar+'\\n')\n            self.grammarbox.mark_set('insert', '1.0')\n\n        self.show_devset(0)\n        self.update()\n\n    def _init_bindings(self, top):\n        top.bind('<Control-n>', self._devset_next)\n        top.bind('<Control-p>', self._devset_prev)\n        top.bind('<Control-t>', self.toggle_show_trace)\n        top.bind('<KeyPress>', self.update)\n        top.bind('<Control-s>', lambda e: self.save_grammar())\n        top.bind('<Control-o>', lambda e: self.load_grammar())\n        self.grammarbox.bind('<Control-t>', self.toggle_show_trace)\n        self.grammarbox.bind('<Control-n>', self._devset_next)\n        self.grammarbox.bind('<Control-p>', self._devset_prev)\n\n        self.evalbox.bind('<Configure>', self._eval_plot)\n\n    def _init_fonts(self, top):\n        self._size = IntVar(top)\n        self._size.set(20)\n        self._font = Font(family='helvetica',\n                                 size=-self._size.get())\n        self._smallfont = Font(family='helvetica',\n                                      size=-(int(self._size.get()*14//20)))\n\n    def _init_menubar(self, parent):\n        menubar = Menu(parent)\n\n        filemenu = Menu(menubar, tearoff=0)\n        filemenu.add_command(label='Reset Application', underline=0,\n                             command=self.reset)\n        filemenu.add_command(label='Save Current Grammar', underline=0,\n                             accelerator='Ctrl-s',\n                             command=self.save_grammar)\n        filemenu.add_command(label='Load Grammar', underline=0,\n                             accelerator='Ctrl-o',\n                             command=self.load_grammar)\n\n        filemenu.add_command(label='Save Grammar History', underline=13,\n                             command=self.save_history)\n\n        filemenu.add_command(label='Exit', underline=1,\n                             command=self.destroy, accelerator='Ctrl-q')\n        menubar.add_cascade(label='File', underline=0, menu=filemenu)\n\n        viewmenu = Menu(menubar, tearoff=0)\n        viewmenu.add_radiobutton(label='Tiny', variable=self._size,\n                                 underline=0, value=10, command=self.resize)\n        viewmenu.add_radiobutton(label='Small', variable=self._size,\n                                 underline=0, value=16, command=self.resize)\n        viewmenu.add_radiobutton(label='Medium', variable=self._size,\n                                 underline=0, value=20, command=self.resize)\n        viewmenu.add_radiobutton(label='Large', variable=self._size,\n                                 underline=0, value=24, command=self.resize)\n        viewmenu.add_radiobutton(label='Huge', variable=self._size,\n                                 underline=0, value=34, command=self.resize)\n        menubar.add_cascade(label='View', underline=0, menu=viewmenu)\n\n        devsetmenu = Menu(menubar, tearoff=0)\n        devsetmenu.add_radiobutton(label='50 sentences',\n                                   variable=self._devset_size,\n                                   value=50, command=self.set_devset_size)\n        devsetmenu.add_radiobutton(label='100 sentences',\n                                   variable=self._devset_size,\n                                   value=100, command=self.set_devset_size)\n        devsetmenu.add_radiobutton(label='200 sentences',\n                                   variable=self._devset_size,\n                                   value=200, command=self.set_devset_size)\n        devsetmenu.add_radiobutton(label='500 sentences',\n                                   variable=self._devset_size,\n                                   value=500, command=self.set_devset_size)\n        menubar.add_cascade(label='Development-Set', underline=0,\n                            menu=devsetmenu)\n\n        helpmenu = Menu(menubar, tearoff=0)\n        helpmenu.add_command(label='About', underline=0,\n                             command=self.about)\n        menubar.add_cascade(label='Help', underline=0, menu=helpmenu)\n\n        parent.config(menu=menubar)\n\n    def toggle_show_trace(self, *e):\n        if self._showing_trace:\n            self.show_devset()\n        else:\n            self.show_trace()\n        return 'break'\n\n\n    _SCALE_N = 5 # center on the last 5 examples.\n    _DRAW_LINES = False\n    def _eval_plot(self, *e, **config):\n        width = config.get('width', self.evalbox.winfo_width())\n        height = config.get('height', self.evalbox.winfo_height())\n\n        self.evalbox.delete('all')\n\n        tag = self.evalbox.create_text(10, height//2-10, justify='left',\n                                 anchor='w', text='Precision')\n        left, right = self.evalbox.bbox(tag)[2] + 5, width-10\n        tag = self.evalbox.create_text(left + (width-left)//2, height-10,\n                                anchor='s', text='Recall', justify='center')\n        top, bot = 10, self.evalbox.bbox(tag)[1]-10\n\n        bg = self._EVALBOX_PARAMS['background']\n        self.evalbox.lower(self.evalbox.create_rectangle(0, 0, left-1, 5000,\n                                                         fill=bg, outline=bg))\n        self.evalbox.lower(self.evalbox.create_rectangle(0, bot+1, 5000, 5000,\n                                                         fill=bg, outline=bg))\n\n        if self._autoscale.get() and len(self._history) > 1:\n            max_precision = max_recall = 0\n            min_precision = min_recall = 1\n            for i in range(1, min(len(self._history), self._SCALE_N+1)):\n                grammar, precision, recall, fmeasure = self._history[-i]\n                min_precision = min(precision, min_precision)\n                min_recall = min(recall, min_recall)\n                max_precision = max(precision, max_precision)\n                max_recall = max(recall, max_recall)\n            min_precision = max(min_precision-.01, 0)\n            min_recall = max(min_recall-.01, 0)\n            max_precision = min(max_precision+.01, 1)\n            max_recall = min(max_recall+.01, 1)\n        else:\n            min_precision = min_recall = 0\n            max_precision = max_recall = 1\n\n        for i in range(11):\n            x = left + (right-left)*((i/10.-min_recall)/\n                                     (max_recall-min_recall))\n            y = bot - (bot-top)*((i/10.-min_precision)/\n                                 (max_precision-min_precision))\n            if left < x < right:\n                self.evalbox.create_line(x, top, x, bot, fill='#888')\n            if top < y < bot:\n                self.evalbox.create_line(left, y, right, y, fill='#888')\n        self.evalbox.create_line(left, top, left, bot)\n        self.evalbox.create_line(left, bot, right, bot)\n\n        self.evalbox.create_text(\n            left-3, bot, justify='right', anchor='se',\n            text='%d%%' % (100*min_precision))\n        self.evalbox.create_text(\n            left-3, top, justify='right', anchor='ne',\n            text='%d%%' % (100*max_precision))\n        self.evalbox.create_text(\n            left, bot+3, justify='center', anchor='nw',\n            text='%d%%' % (100*min_recall))\n        self.evalbox.create_text(\n            right, bot+3, justify='center', anchor='ne',\n            text='%d%%' % (100*max_recall))\n\n        prev_x = prev_y = None\n        for i, (_, precision, recall, fscore) in enumerate(self._history):\n            x = left + (right-left) * ((recall-min_recall) /\n                                (max_recall-min_recall))\n            y = bot - (bot-top) * ((precision-min_precision) /\n                                (max_precision-min_precision))\n            if i == self._history_index:\n                self.evalbox.create_oval(x-2,y-2,x+2,y+2,\n                                         fill='#0f0', outline='#000')\n                self.status['text'] = (\n                    'Precision: %.2f%%\\t' % (precision*100)+\n                    'Recall: %.2f%%\\t' % (recall*100)+\n                    'F-score: %.2f%%' % (fscore*100))\n            else:\n                self.evalbox.lower(\n                    self.evalbox.create_oval(x-2,y-2,x+2,y+2,\n                                             fill='#afa', outline='#8c8'))\n            if prev_x is not None and self._eval_lines.get():\n                self.evalbox.lower(\n                    self.evalbox.create_line(prev_x, prev_y, x, y,\n                                             fill='#8c8'))\n            prev_x, prev_y = x, y\n\n    _eval_demon_running = False\n    def _eval_demon(self):\n        if self.top is None: return\n        if self.chunker is None:\n            self._eval_demon_running = False\n            return\n\n        t0 = time.time()\n\n        if (time.time()-self._last_keypress < self._EVAL_DELAY and\n            self.normalized_grammar != self._eval_normalized_grammar):\n            self._eval_demon_running = True\n            return self.top.after(int(self._EVAL_FREQ*1000), self._eval_demon)\n\n        if self.normalized_grammar != self._eval_normalized_grammar:\n            for (g, p, r, f) in self._history:\n                if self.normalized_grammar == self.normalize_grammar(g):\n                    self._history.append( (g, p, r, f) )\n                    self._history_index = len(self._history) - 1\n                    self._eval_plot()\n                    self._eval_demon_running = False\n                    self._eval_normalized_grammar = None\n                    return\n            self._eval_index = 0\n            self._eval_score = ChunkScore(chunk_label=self._chunk_label)\n            self._eval_grammar = self.grammar\n            self._eval_normalized_grammar = self.normalized_grammar\n\n        if self.normalized_grammar.strip() == '':\n            self._eval_demon_running = False\n            return\n\n        for gold in self.devset[self._eval_index:\n                                min(self._eval_index+self._EVAL_CHUNK,\n                                    self._devset_size.get())]:\n            guess = self._chunkparse(gold.leaves())\n            self._eval_score.score(gold, guess)\n\n        self._eval_index += self._EVAL_CHUNK\n\n        if self._eval_index >= self._devset_size.get():\n            self._history.append( (self._eval_grammar,\n                                   self._eval_score.precision(),\n                                   self._eval_score.recall(),\n                                   self._eval_score.f_measure()) )\n            self._history_index = len(self._history)-1\n            self._eval_plot()\n            self._eval_demon_running = False\n            self._eval_normalized_grammar = None\n        else:\n            progress = 100*self._eval_index/self._devset_size.get()\n            self.status['text'] = ('Evaluating on Development Set (%d%%)' %\n                                   progress)\n            self._eval_demon_running = True\n            self._adaptively_modify_eval_chunk(time.time() - t0)\n            self.top.after(int(self._EVAL_FREQ*1000), self._eval_demon)\n\n    def _adaptively_modify_eval_chunk(self, t):\n        \"\"\"\n        Modify _EVAL_CHUNK to try to keep the amount of time that the\n        eval demon takes between _EVAL_DEMON_MIN and _EVAL_DEMON_MAX.\n\n        :param t: The amount of time that the eval demon took.\n        \"\"\"\n        if t > self._EVAL_DEMON_MAX and self._EVAL_CHUNK > 5:\n            self._EVAL_CHUNK = min(self._EVAL_CHUNK-1,\n                         max(int(self._EVAL_CHUNK*(self._EVAL_DEMON_MAX/t)),\n                             self._EVAL_CHUNK-10))\n        elif t < self._EVAL_DEMON_MIN:\n            self._EVAL_CHUNK = max(self._EVAL_CHUNK+1,\n                         min(int(self._EVAL_CHUNK*(self._EVAL_DEMON_MIN/t)),\n                             self._EVAL_CHUNK+10))\n\n    def _init_widgets(self, top):\n        frame0 = Frame(top, **self._FRAME_PARAMS)\n        frame0.grid_columnconfigure(0, weight=4)\n        frame0.grid_columnconfigure(3, weight=2)\n        frame0.grid_rowconfigure(1, weight=1)\n        frame0.grid_rowconfigure(5, weight=1)\n\n        self.grammarbox = Text(frame0, font=self._font,\n                               **self._GRAMMARBOX_PARAMS)\n        self.grammarlabel = Label(frame0, font=self._font, text='Grammar:',\n                      highlightcolor='black',\n                      background=self._GRAMMARBOX_PARAMS['background'])\n        self.grammarlabel.grid(column=0, row=0, sticky='SW')\n        self.grammarbox.grid(column=0, row=1, sticky='NEWS')\n\n        grammar_scrollbar = Scrollbar(frame0, command=self.grammarbox.yview)\n        grammar_scrollbar.grid(column=1, row=1, sticky='NWS')\n        self.grammarbox.config(yscrollcommand=grammar_scrollbar.set)\n\n        bg = self._FRAME_PARAMS['background']\n        frame3 = Frame(frame0, background=bg)\n        frame3.grid(column=0, row=2, sticky='EW')\n        Button(frame3, text='Prev Grammar', command=self._history_prev,\n               **self._BUTTON_PARAMS).pack(side='left')\n        Button(frame3, text='Next Grammar', command=self._history_next,\n               **self._BUTTON_PARAMS).pack(side='left')\n\n        self.helpbox = Text(frame0, font=self._smallfont,\n                            **self._HELPBOX_PARAMS)\n        self.helpbox.grid(column=3, row=1, sticky='NEWS')\n        self.helptabs = {}\n        bg = self._FRAME_PARAMS['background']\n        helptab_frame = Frame(frame0, background=bg)\n        helptab_frame.grid(column=3, row=0, sticky='SW')\n        for i, (tab, tabstops, text) in enumerate(self.HELP):\n            label = Label(helptab_frame, text=tab, font=self._smallfont)\n            label.grid(column=i*2, row=0, sticky='S')\n            label.bind('<ButtonPress>', lambda e, tab=tab: self.show_help(tab))\n            self.helptabs[tab] = label\n            Frame(helptab_frame, height=1, width=self._HELPTAB_SPACER,\n                  background=bg).grid(column=i*2+1, row=0)\n        self.helptabs[self.HELP[0][0]].configure(font=self._font)\n        self.helpbox.tag_config('elide', elide=True)\n        for (tag, params) in self.HELP_AUTOTAG:\n            self.helpbox.tag_config('tag-%s' % tag, **params)\n        self.show_help(self.HELP[0][0])\n\n        help_scrollbar = Scrollbar(frame0, command=self.helpbox.yview)\n        self.helpbox.config(yscrollcommand=help_scrollbar.set)\n        help_scrollbar.grid(column=4, row=1, sticky='NWS')\n\n        frame4 = Frame(frame0, background=self._FRAME_PARAMS['background'])\n        self.devsetbox = Text(frame4, font=self._font,\n                              **self._DEVSETBOX_PARAMS)\n        self.devsetbox.pack(expand=True, fill='both')\n        self.devsetlabel = Label(frame0, font=self._font,\n                      text='Development Set:', justify='right',\n                      background=self._DEVSETBOX_PARAMS['background'])\n        self.devsetlabel.grid(column=0, row=4, sticky='SW')\n        frame4.grid(column=0, row=5, sticky='NEWS')\n\n        self.devset_scroll = Scrollbar(frame0, command=self._devset_scroll)\n        self.devset_scroll.grid(column=1, row=5, sticky='NWS')\n        self.devset_xscroll = Scrollbar(frame4, command=self.devsetbox.xview,\n                                        orient='horiz')\n        self.devsetbox['xscrollcommand'] = self.devset_xscroll.set\n        self.devset_xscroll.pack(side='bottom', fill='x')\n\n        bg = self._FRAME_PARAMS['background']\n        frame1 = Frame(frame0, background=bg)\n        frame1.grid(column=0, row=7, sticky='EW')\n        Button(frame1, text='Prev Example (Ctrl-p)',\n               command=self._devset_prev,\n               **self._BUTTON_PARAMS).pack(side='left')\n        Button(frame1, text='Next Example (Ctrl-n)',\n               command=self._devset_next,\n               **self._BUTTON_PARAMS).pack(side='left')\n        self.devset_button = Button(frame1, text='Show example',\n                                   command=self.show_devset,\n                                    state='disabled',\n                                   **self._BUTTON_PARAMS)\n        self.devset_button.pack(side='right')\n        self.trace_button = Button(frame1, text='Show trace',\n                                   command=self.show_trace,\n                                   **self._BUTTON_PARAMS)\n        self.trace_button.pack(side='right')\n\n\n        self.evalbox = Canvas(frame0, **self._EVALBOX_PARAMS)\n        label = Label(frame0, font=self._font, text='Evaluation:',\n              justify='right', background=self._EVALBOX_PARAMS['background'])\n        label.grid(column=3, row=4, sticky='SW')\n        self.evalbox.grid(column=3, row=5, sticky='NEWS', columnspan=2)\n\n        bg = self._FRAME_PARAMS['background']\n        frame2 = Frame(frame0, background=bg)\n        frame2.grid(column=3, row=7, sticky='EW')\n        self._autoscale = IntVar(self.top)\n        self._autoscale.set(False)\n        Checkbutton(frame2, variable=self._autoscale, command=self._eval_plot,\n                    text='Zoom', **self._BUTTON_PARAMS).pack(side='left')\n        self._eval_lines = IntVar(self.top)\n        self._eval_lines.set(False)\n        Checkbutton(frame2, variable=self._eval_lines, command=self._eval_plot,\n                    text='Lines', **self._BUTTON_PARAMS).pack(side='left')\n        Button(frame2, text='History',\n               **self._BUTTON_PARAMS).pack(side='right')\n\n        self.status = Label(frame0, font=self._font, **self._STATUS_PARAMS)\n        self.status.grid(column=0, row=9, sticky='NEW', padx=3, pady=2,\n                         columnspan=5)\n\n        self.helpbox['state'] = 'disabled'\n        self.devsetbox['state'] = 'disabled'\n\n        bg = self._FRAME_PARAMS['background']\n        Frame(frame0, height=10, width=0, background=bg).grid(column=0, row=3)\n        Frame(frame0, height=0, width=10, background=bg).grid(column=2, row=0)\n        Frame(frame0, height=6, width=0, background=bg).grid(column=0, row=8)\n\n        frame0.pack(fill='both', expand=True)\n\n        self.devsetbox.tag_config('true-pos', background='#afa',\n                                  underline='True')\n        self.devsetbox.tag_config('false-neg', underline='True',\n                                foreground='#800')\n        self.devsetbox.tag_config('false-pos', background='#faa')\n        self.devsetbox.tag_config('trace', foreground='#666', wrap='none')\n        self.devsetbox.tag_config('wrapindent', lmargin2=30, wrap='none')\n        self.devsetbox.tag_config('error', foreground='#800')\n\n        self.grammarbox.tag_config('error', background='#fec')\n        self.grammarbox.tag_config('comment', foreground='#840')\n        self.grammarbox.tag_config('angle', foreground='#00f')\n        self.grammarbox.tag_config('brace', foreground='#0a0')\n        self.grammarbox.tag_config('hangindent', lmargin1=0, lmargin2=40)\n\n    _showing_trace = False\n    def show_trace(self, *e):\n        self._showing_trace = True\n        self.trace_button['state'] = 'disabled'\n        self.devset_button['state'] = 'normal'\n\n        self.devsetbox['state'] = 'normal'\n        self.devsetbox.delete('1.0', 'end')\n        self.devsetlabel['text']='Development Set (%d/%d)' % (\n            (self.devset_index+1, self._devset_size.get()))\n\n        if self.chunker is None:\n            self.devsetbox.insert('1.0', 'Trace: waiting for a valid grammar.')\n            self.devsetbox.tag_add('error', '1.0', 'end')\n            return # can't do anything more\n\n        gold_tree = self.devset[self.devset_index]\n        rules = self.chunker.rules()\n\n        tagseq = '\\t'\n        charnum = [1]\n        for wordnum, (word, pos) in enumerate(gold_tree.leaves()):\n            tagseq += '%s ' % pos\n            charnum.append(len(tagseq))\n        self.charnum = dict(((i, j), charnum[j])\n                            for i in range(len(rules)+1)\n                            for j in range(len(charnum)))\n        self.linenum = dict((i,i*2+2) for i in range(len(rules)+1))\n\n        for i in range(len(rules)+1):\n            if i == 0:\n                self.devsetbox.insert('end', 'Start:\\n')\n                self.devsetbox.tag_add('trace', 'end -2c linestart', 'end -2c')\n            else:\n                self.devsetbox.insert('end', 'Apply %s:\\n' % rules[i-1])\n                self.devsetbox.tag_add('trace', 'end -2c linestart', 'end -2c')\n            self.devsetbox.insert('end', tagseq+'\\n')\n            self.devsetbox.tag_add('wrapindent','end -2c linestart','end -2c')\n            chunker = RegexpChunkParser(rules[:i])\n            test_tree = self._chunkparse(gold_tree.leaves())\n            gold_chunks = self._chunks(gold_tree)\n            test_chunks = self._chunks(test_tree)\n            for chunk in gold_chunks.intersection(test_chunks):\n                self._color_chunk(i, chunk, 'true-pos')\n            for chunk in gold_chunks - test_chunks:\n                self._color_chunk(i, chunk, 'false-neg')\n            for chunk in test_chunks - gold_chunks:\n                self._color_chunk(i, chunk, 'false-pos')\n        self.devsetbox.insert('end', 'Finished.\\n')\n        self.devsetbox.tag_add('trace', 'end -2c linestart', 'end -2c')\n\n        self.top.after(100, self.devset_xscroll.set, 0, .3)\n\n    def show_help(self, tab):\n        self.helpbox['state'] = 'normal'\n        self.helpbox.delete('1.0', 'end')\n        for (name, tabstops, text) in self.HELP:\n            if name == tab:\n                text = text.replace('<<TAGSET>>', '\\n'.join(\n                    ('\\t%s\\t%s' % item for item in sorted(list(self.tagset.items()),\n                    key=lambda t_w:re.match('\\w+',t_w[0]) and (0,t_w[0]) or (1,t_w[0])))))\n\n                self.helptabs[name].config(**self._HELPTAB_FG_PARAMS)\n                self.helpbox.config(tabs=tabstops)\n                self.helpbox.insert('1.0', text+'\\n'*20)\n                C = '1.0 + %d chars'\n                for (tag, params) in self.HELP_AUTOTAG:\n                    pattern = '(?s)(<%s>)(.*?)(</%s>)' % (tag, tag)\n                    for m in re.finditer(pattern, text):\n                        self.helpbox.tag_add('elide',\n                                             C % m.start(1), C % m.end(1))\n                        self.helpbox.tag_add('tag-%s' % tag,\n                                             C % m.start(2), C % m.end(2))\n                        self.helpbox.tag_add('elide',\n                                             C % m.start(3), C % m.end(3))\n            else:\n                self.helptabs[name].config(**self._HELPTAB_BG_PARAMS)\n        self.helpbox['state'] = 'disabled'\n\n    def _history_prev(self, *e):\n        self._view_history(self._history_index-1)\n        return 'break'\n\n    def _history_next(self, *e):\n        self._view_history(self._history_index+1)\n        return 'break'\n\n    def _view_history(self, index):\n        index = max(0, min(len(self._history)-1, index))\n        if not self._history: return\n        if index == self._history_index:\n            return\n        self.grammarbox['state'] = 'normal'\n        self.grammarbox.delete('1.0', 'end')\n        self.grammarbox.insert('end', self._history[index][0])\n        self.grammarbox.mark_set('insert', '1.0')\n        self._history_index = index\n        self._syntax_highlight_grammar(self._history[index][0])\n        self.normalized_grammar = self.normalize_grammar(\n            self._history[index][0])\n        if self.normalized_grammar:\n            rules = [RegexpChunkRule.fromstring(line)\n                     for line in self.normalized_grammar.split('\\n')]\n        else:\n            rules = []\n        self.chunker = RegexpChunkParser(rules)\n        self._eval_plot()\n        self._highlight_devset()\n        if self._showing_trace: self.show_trace()\n        if self._history_index < len(self._history)-1:\n            self.grammarlabel['text'] = 'Grammar %s/%s:' % (\n                self._history_index+1, len(self._history))\n        else:\n            self.grammarlabel['text'] = 'Grammar:'\n\n    def _devset_next(self, *e):\n        self._devset_scroll('scroll', 1, 'page')\n        return 'break'\n\n    def _devset_prev(self, *e):\n        self._devset_scroll('scroll', -1, 'page')\n        return 'break'\n\n    def destroy(self, *e):\n        if self.top is None: return\n        self.top.destroy()\n        self.top = None\n\n    def _devset_scroll(self, command, *args):\n        N = 1 # size of a page -- one sentence.\n        showing_trace = self._showing_trace\n        if command == 'scroll' and args[1].startswith('unit'):\n            self.show_devset(self.devset_index+int(args[0]))\n        elif command == 'scroll' and args[1].startswith('page'):\n            self.show_devset(self.devset_index+N*int(args[0]))\n        elif command == 'moveto':\n            self.show_devset(int(float(args[0])*self._devset_size.get()))\n        else:\n            assert 0, 'bad scroll command %s %s' % (command, args)\n        if showing_trace:\n            self.show_trace()\n\n    def show_devset(self, index=None):\n        if index is None: index = self.devset_index\n\n        index = min(max(0, index), self._devset_size.get()-1)\n\n        if index == self.devset_index and not self._showing_trace: return\n        self.devset_index = index\n\n        self._showing_trace = False\n        self.trace_button['state'] = 'normal'\n        self.devset_button['state'] = 'disabled'\n\n        self.devsetbox['state'] = 'normal'\n        self.devsetbox['wrap'] = 'word'\n        self.devsetbox.delete('1.0', 'end')\n        self.devsetlabel['text']='Development Set (%d/%d)' % (\n            (self.devset_index+1, self._devset_size.get()))\n\n        sample = self.devset[self.devset_index:self.devset_index+1]\n        self.charnum = {}\n        self.linenum = {0:1}\n        for sentnum, sent in enumerate(sample):\n            linestr = ''\n            for wordnum, (word, pos) in enumerate(sent.leaves()):\n                self.charnum[sentnum, wordnum] = len(linestr)\n                linestr += '%s/%s ' % (word, pos)\n                self.charnum[sentnum, wordnum+1] = len(linestr)\n            self.devsetbox.insert('end', linestr[:-1]+'\\n\\n')\n\n        if self.chunker is not None:\n            self._highlight_devset()\n        self.devsetbox['state'] = 'disabled'\n\n        first = self.devset_index/self._devset_size.get()\n        last = (self.devset_index + 2) / self._devset_size.get()\n        self.devset_scroll.set(first, last)\n\n    def _chunks(self, tree):\n        chunks = set()\n        wordnum = 0\n        for child in tree:\n            if isinstance(child, Tree):\n                if child.label() == self._chunk_label:\n                    chunks.add( (wordnum, wordnum+len(child)) )\n                wordnum += len(child)\n            else:\n                wordnum += 1\n        return chunks\n\n    def _syntax_highlight_grammar(self, grammar):\n        if self.top is None: return\n        self.grammarbox.tag_remove('comment', '1.0', 'end')\n        self.grammarbox.tag_remove('angle', '1.0', 'end')\n        self.grammarbox.tag_remove('brace', '1.0', 'end')\n        self.grammarbox.tag_add('hangindent', '1.0', 'end')\n        for lineno, line in enumerate(grammar.split('\\n')):\n            if not line.strip(): continue\n            m = re.match(r'(\\\\.|[^#])*(#.*)?', line)\n            comment_start = None\n            if m.group(2):\n                comment_start = m.start(2)\n                s = '%d.%d' % (lineno+1, m.start(2))\n                e = '%d.%d' % (lineno+1, m.end(2))\n                self.grammarbox.tag_add('comment', s, e)\n            for m in re.finditer('[<>{}]', line):\n                if comment_start is not None and m.start() >= comment_start:\n                    break\n                s = '%d.%d' % (lineno+1, m.start())\n                e = '%d.%d' % (lineno+1, m.end())\n                if m.group() in '<>':\n                    self.grammarbox.tag_add('angle', s, e)\n                else:\n                    self.grammarbox.tag_add('brace', s, e)\n\n\n    def _grammarcheck(self, grammar):\n        if self.top is None: return\n        self.grammarbox.tag_remove('error', '1.0', 'end')\n        self._grammarcheck_errs = []\n        for lineno, line in enumerate(grammar.split('\\n')):\n            line = re.sub(r'((\\\\.|[^#])*)(#.*)?', r'\\1', line)\n            line = line.strip()\n            if line:\n                try:\n                    RegexpChunkRule.fromstring(line)\n                except ValueError as e:\n                    self.grammarbox.tag_add('error', '%s.0' % (lineno+1),\n                                            '%s.0 lineend' % (lineno+1))\n        self.status['text'] = ''\n\n    def update(self, *event):\n        if event:\n            self._last_keypress = time.time()\n\n        self.grammar = grammar = self.grammarbox.get('1.0', 'end')\n\n        normalized_grammar = self.normalize_grammar(grammar)\n        if normalized_grammar == self.normalized_grammar:\n            return\n        else:\n            self.normalized_grammar = normalized_grammar\n\n        if self._history_index < len(self._history)-1:\n            self.grammarlabel['text'] = 'Grammar:'\n\n        self._syntax_highlight_grammar(grammar)\n\n        try:\n            if normalized_grammar:\n                rules = [RegexpChunkRule.fromstring(line)\n                         for line in normalized_grammar.split('\\n')]\n            else:\n                rules = []\n        except ValueError as e:\n            self._grammarcheck(grammar)\n            self.chunker = None\n            return\n\n        self.chunker = RegexpChunkParser(rules)\n        self.grammarbox.tag_remove('error', '1.0', 'end')\n        self.grammar_changed = time.time()\n        if self._showing_trace:\n            self.show_trace()\n        else:\n            self._highlight_devset()\n        if not self._eval_demon_running:\n            self._eval_demon()\n\n    def _highlight_devset(self, sample=None):\n        if sample is None:\n            sample = self.devset[self.devset_index:self.devset_index+1]\n\n        self.devsetbox.tag_remove('true-pos', '1.0', 'end')\n        self.devsetbox.tag_remove('false-neg', '1.0', 'end')\n        self.devsetbox.tag_remove('false-pos', '1.0', 'end')\n\n        for sentnum, gold_tree in enumerate(sample):\n            test_tree = self._chunkparse(gold_tree.leaves())\n            gold_chunks = self._chunks(gold_tree)\n            test_chunks = self._chunks(test_tree)\n            for chunk in gold_chunks.intersection(test_chunks):\n                self._color_chunk(sentnum, chunk, 'true-pos')\n            for chunk in gold_chunks - test_chunks:\n                self._color_chunk(sentnum, chunk, 'false-neg')\n            for chunk in test_chunks - gold_chunks:\n                self._color_chunk(sentnum, chunk, 'false-pos')\n\n    def _chunkparse(self, words):\n        try:\n            return self.chunker.parse(words)\n        except (ValueError, IndexError) as e:\n            self.grammarbox.tag_add('error', '1.0', 'end')\n            return words\n\n    def _color_chunk(self, sentnum, chunk, tag):\n        start, end = chunk\n        self.devsetbox.tag_add(tag,\n            '%s.%s' % (self.linenum[sentnum], self.charnum[sentnum, start]),\n            '%s.%s' % (self.linenum[sentnum], self.charnum[sentnum, end]-1))\n\n    def reset(self):\n        self.chunker = None\n        self.grammar = None\n        self.normalized_grammar = None\n        self.grammar_changed = 0\n        self._history = []\n        self._history_index = 0\n        self.grammarbox.delete('1.0', 'end')\n        self.show_devset(0)\n        self.update()\n\n    SAVE_GRAMMAR_TEMPLATE = (\n        '# Regexp Chunk Parsing Grammar\\n'\n        '# Saved %(date)s\\n'\n        '#\\n'\n        '# Development set: %(devset)s\\n'\n        '#   Precision: %(precision)s\\n'\n        '#   Recall:    %(recall)s\\n'\n        '#   F-score:   %(fscore)s\\n\\n'\n        '%(grammar)s\\n')\n\n    def save_grammar(self, filename=None):\n        if not filename:\n            ftypes = [('Chunk Gramamr', '.chunk'),\n                      ('All files', '*')]\n            filename = asksaveasfilename(filetypes=ftypes,\n                                                      defaultextension='.chunk')\n            if not filename: return\n        if (self._history and self.normalized_grammar ==\n            self.normalize_grammar(self._history[-1][0])):\n            precision, recall, fscore = ['%.2f%%' % (100*v) for v in\n                                         self._history[-1][1:]]\n        elif self.chunker is None:\n            precision = recall = fscore = 'Grammar not well formed'\n        else:\n            precision = recall = fscore = 'Not finished evaluation yet'\n\n        with open(filename, 'w') as outfile:\n            outfile.write(self.SAVE_GRAMMAR_TEMPLATE % dict(\n                date=time.ctime(), devset=self.devset_name,\n                precision=precision, recall=recall, fscore=fscore,\n                grammar=self.grammar.strip()))\n\n    def load_grammar(self, filename=None):\n        if not filename:\n            ftypes = [('Chunk Gramamr', '.chunk'),\n                      ('All files', '*')]\n            filename = askopenfilename(filetypes=ftypes,\n                                                    defaultextension='.chunk')\n            if not filename: return\n        self.grammarbox.delete('1.0', 'end')\n        self.update()\n        with open(filename, 'r') as infile:\n            grammar = infile.read()\n        grammar = re.sub('^\\# Regexp Chunk Parsing Grammar[\\s\\S]*'\n                         'F-score:.*\\n', '', grammar).lstrip()\n        self.grammarbox.insert('1.0', grammar)\n        self.update()\n\n    def save_history(self, filename=None):\n        if not filename:\n            ftypes = [('Chunk Gramamr History', '.txt'),\n                      ('All files', '*')]\n            filename = asksaveasfilename(filetypes=ftypes,\n                                                      defaultextension='.txt')\n            if not filename: return\n\n        with open(filename, 'w') as outfile:\n            outfile.write('# Regexp Chunk Parsing Grammar History\\n')\n            outfile.write('# Saved %s\\n' % time.ctime())\n            outfile.write('# Development set: %s\\n' % self.devset_name)\n            for i, (g, p, r, f) in enumerate(self._history):\n                hdr = ('Grammar %d/%d (precision=%.2f%%, recall=%.2f%%, '\n                       'fscore=%.2f%%)' % (i+1, len(self._history),\n                                           p*100, r*100, f*100))\n                outfile.write('\\n%s\\n' % hdr)\n                outfile.write(''.join('  %s\\n' % line for line in g.strip().split()))\n\n            if not (self._history and self.normalized_grammar ==\n                    self.normalize_grammar(self._history[-1][0])):\n                if self.chunker is None:\n                    outfile.write('\\nCurrent Grammar (not well-formed)\\n')\n                else:\n                    outfile.write('\\nCurrent Grammar (not evaluated)\\n')\n                outfile.write(''.join('  %s\\n' % line for line\n                                  in self.grammar.strip().split()))\n\n    def about(self, *e):\n        ABOUT = (\"NLTK RegExp Chunk Parser Application\\n\"+\n                 \"Written by Edward Loper\")\n        TITLE = 'About: Regular Expression Chunk Parser Application'\n        try:\n            from six.moves.tkinter_messagebox import Message\n            Message(message=ABOUT, title=TITLE).show()\n        except:\n            ShowText(self.top, TITLE, ABOUT)\n\n    def set_devset_size(self, size=None):\n        if size is not None: self._devset_size.set(size)\n        self._devset_size.set(min(len(self.devset), self._devset_size.get()))\n        self.show_devset(1)\n        self.show_devset(0)\n\n    def resize(self, size=None):\n        if size is not None: self._size.set(size)\n        size = self._size.get()\n        self._font.configure(size=-(abs(size)))\n        self._smallfont.configure(size=min(-10, -(abs(size))*14//20))\n\n    def mainloop(self, *args, **kwargs):\n        \"\"\"\n        Enter the Tkinter mainloop.  This function must be called if\n        this demo is created from a non-interactive program (e.g.\n        from a secript); otherwise, the demo will close as soon as\n        the script completes.\n        \"\"\"\n        if in_idle(): return\n        self.top.mainloop(*args, **kwargs)\n\ndef app():\n    RegexpChunkApp().mainloop()\n\nif __name__ == '__main__':\n    app()\n\n__all__ = ['app']\n"], "nltk\\app\\collocations_app": [".py", "\n\nfrom __future__ import division\n\nimport threading\n\nfrom six.moves import queue as q\nfrom six.moves.tkinter_font import Font\nfrom six.moves.tkinter import (Button, END, Frame, IntVar, LEFT, Label, Menu,\n                               OptionMenu, SUNKEN, Scrollbar, StringVar,\n                               Text, Tk)\n\nfrom nltk.corpus import (cess_cat, brown, nps_chat, treebank, sinica_treebank, alpino,\n                         indian, floresta, mac_morpho, machado, cess_esp)\nfrom nltk.util import in_idle\nfrom nltk.probability import FreqDist\n\n\nCORPUS_LOADED_EVENT = '<<CL_EVENT>>'\nERROR_LOADING_CORPUS_EVENT = '<<ELC_EVENT>>'\nPOLL_INTERVAL = 100\n\n_DEFAULT = 'English: Brown Corpus (Humor)'\n_CORPORA = {\n            'Catalan: CESS-CAT Corpus':\n                lambda: cess_cat.words(),\n            'English: Brown Corpus':\n                lambda: brown.words(),\n            'English: Brown Corpus (Press)':\n                lambda: brown.words(categories=['news', 'editorial', 'reviews']),\n            'English: Brown Corpus (Religion)':\n                lambda: brown.words(categories='religion'),\n            'English: Brown Corpus (Learned)':\n                lambda: brown.words(categories='learned'),\n            'English: Brown Corpus (Science Fiction)':\n                lambda: brown.words(categories='science_fiction'),\n            'English: Brown Corpus (Romance)':\n                lambda: brown.words(categories='romance'),\n            'English: Brown Corpus (Humor)':\n                lambda: brown.words(categories='humor'),\n            'English: NPS Chat Corpus':\n                lambda: nps_chat.words(),\n            'English: Wall Street Journal Corpus':\n                lambda: treebank.words(),\n            'Chinese: Sinica Corpus':\n                lambda: sinica_treebank.words(),\n            'Dutch: Alpino Corpus':\n                lambda: alpino.words(),\n            'Hindi: Indian Languages Corpus':\n                lambda: indian.words(files='hindi.pos'),\n            'Portuguese: Floresta Corpus (Portugal)':\n                lambda: floresta.words(),\n            'Portuguese: MAC-MORPHO Corpus (Brazil)':\n                lambda: mac_morpho.words(),\n            'Portuguese: Machado Corpus (Brazil)':\n                lambda: machado.words(),\n            'Spanish: CESS-ESP Corpus':\n                lambda: cess_esp.words()\n           }\n\nclass CollocationsView:\n    _BACKGROUND_COLOUR='#FFF' #white\n\n    def __init__(self):\n        self.queue = q.Queue()\n        self.model = CollocationsModel(self.queue)\n        self.top = Tk()\n        self._init_top(self.top)\n        self._init_menubar()\n        self._init_widgets(self.top)\n        self.load_corpus(self.model.DEFAULT_CORPUS)\n        self.after = self.top.after(POLL_INTERVAL, self._poll)\n\n    def _init_top(self, top):\n        top.geometry('550x650+50+50')\n        top.title('NLTK Collocations List')\n        top.bind('<Control-q>', self.destroy)\n        top.protocol('WM_DELETE_WINDOW', self.destroy)\n        top.minsize(550,650)\n\n    def _init_widgets(self, parent):\n        self.main_frame = Frame(parent, dict(background=self._BACKGROUND_COLOUR, padx=1, pady=1, border=1))\n        self._init_corpus_select(self.main_frame)\n        self._init_results_box(self.main_frame)\n        self._init_paging(self.main_frame)\n        self._init_status(self.main_frame)\n        self.main_frame.pack(fill='both', expand=True)\n\n    def _init_corpus_select(self, parent):\n        innerframe = Frame(parent, background=self._BACKGROUND_COLOUR)\n        self.var = StringVar(innerframe)\n        self.var.set(self.model.DEFAULT_CORPUS)\n        Label(innerframe, justify=LEFT, text=' Corpus: ', background=self._BACKGROUND_COLOUR, padx = 2, pady = 1, border = 0).pack(side='left')\n\n        other_corpora = list(self.model.CORPORA.keys()).remove(self.model.DEFAULT_CORPUS)\n        om = OptionMenu(innerframe, self.var, self.model.DEFAULT_CORPUS, command=self.corpus_selected, *self.model.non_default_corpora())\n        om['borderwidth'] = 0\n        om['highlightthickness'] = 1\n        om.pack(side='left')\n        innerframe.pack(side='top', fill='x', anchor='n')\n\n    def _init_status(self, parent):\n        self.status = Label(parent, justify=LEFT, relief=SUNKEN, background=self._BACKGROUND_COLOUR, border=0, padx = 1, pady = 0)\n        self.status.pack(side='top', anchor='sw')\n\n    def _init_menubar(self):\n        self._result_size = IntVar(self.top)\n        menubar = Menu(self.top)\n\n        filemenu = Menu(menubar, tearoff=0, borderwidth=0)\n        filemenu.add_command(label='Exit', underline=1,\n                   command=self.destroy, accelerator='Ctrl-q')\n        menubar.add_cascade(label='File', underline=0, menu=filemenu)\n\n        editmenu = Menu(menubar, tearoff=0)\n        rescntmenu = Menu(editmenu, tearoff=0)\n        rescntmenu.add_radiobutton(label='20', variable=self._result_size,\n                     underline=0, value=20, command=self.set_result_size)\n        rescntmenu.add_radiobutton(label='50', variable=self._result_size,\n                     underline=0, value=50, command=self.set_result_size)\n        rescntmenu.add_radiobutton(label='100', variable=self._result_size,\n                     underline=0, value=100, command=self.set_result_size)\n        rescntmenu.invoke(1)\n        editmenu.add_cascade(label='Result Count', underline=0, menu=rescntmenu)\n\n        menubar.add_cascade(label='Edit', underline=0, menu=editmenu)\n        self.top.config(menu=menubar)\n\n    def set_result_size(self, **kwargs):\n        self.model.result_count = self._result_size.get()\n\n    def _init_results_box(self, parent):\n        innerframe = Frame(parent)\n        i1 = Frame(innerframe)\n        i2 = Frame(innerframe)\n        vscrollbar = Scrollbar(i1, borderwidth=1)\n        hscrollbar = Scrollbar(i2, borderwidth=1, orient='horiz')\n        self.results_box = Text(i1,\n                    font=Font(family='courier', size='16'),\n                    state='disabled', borderwidth=1,\n                    yscrollcommand=vscrollbar.set,\n                    xscrollcommand=hscrollbar.set, wrap='none', width='40', height = '20', exportselection=1)\n        self.results_box.pack(side='left', fill='both', expand=True)\n        vscrollbar.pack(side='left', fill='y', anchor='e')\n        vscrollbar.config(command=self.results_box.yview)\n        hscrollbar.pack(side='left', fill='x', expand=True, anchor='w')\n        hscrollbar.config(command=self.results_box.xview)\n        Label(i2, text='   ', background=self._BACKGROUND_COLOUR).pack(side='left', anchor='e')\n        i1.pack(side='top', fill='both', expand=True, anchor='n')\n        i2.pack(side='bottom', fill='x', anchor='s')\n        innerframe.pack(side='top', fill='both', expand=True)\n\n    def _init_paging(self, parent):\n        innerframe = Frame(parent, background=self._BACKGROUND_COLOUR)\n        self.prev = prev = Button(innerframe, text='Previous', command=self.previous, width='10', borderwidth=1, highlightthickness=1, state='disabled')\n        prev.pack(side='left', anchor='center')\n        self.next = next = Button(innerframe, text='Next', command=self.__next__, width='10', borderwidth=1, highlightthickness=1, state='disabled')\n        next.pack(side='right', anchor='center')\n        innerframe.pack(side='top', fill='y')\n        self.reset_current_page()\n\n    def reset_current_page(self):\n        self.current_page = -1\n\n    def _poll(self):\n        try:\n            event = self.queue.get(block=False)\n        except q.Empty:\n            pass\n        else:\n            if event == CORPUS_LOADED_EVENT:\n                self.handle_corpus_loaded(event)\n            elif event == ERROR_LOADING_CORPUS_EVENT:\n                self.handle_error_loading_corpus(event)\n        self.after = self.top.after(POLL_INTERVAL, self._poll)\n\n    def handle_error_loading_corpus(self, event):\n        self.status['text'] = 'Error in loading ' + self.var.get()\n        self.unfreeze_editable()\n        self.clear_results_box()\n        self.freeze_editable()\n        self.reset_current_page()\n\n    def handle_corpus_loaded(self, event):\n        self.status['text'] = self.var.get() + ' is loaded'\n        self.unfreeze_editable()\n        self.clear_results_box()\n        self.reset_current_page()\n        collocations = self.model.next(self.current_page + 1)\n        self.write_results(collocations)\n        self.current_page += 1\n\n    def corpus_selected(self, *args):\n        new_selection = self.var.get()\n        self.load_corpus(new_selection)\n\n    def previous(self):\n        self.freeze_editable()\n        collocations = self.model.prev(self.current_page - 1)\n        self.current_page= self.current_page - 1\n        self.clear_results_box()\n        self.write_results(collocations)\n        self.unfreeze_editable()\n\n    def __next__(self):\n        self.freeze_editable()\n        collocations = self.model.next(self.current_page + 1)\n        self.clear_results_box()\n        self.write_results(collocations)\n        self.current_page += 1\n        self.unfreeze_editable()\n\n    def load_corpus(self, selection):\n        if self.model.selected_corpus != selection:\n            self.status['text'] = 'Loading ' + selection + '...'\n            self.freeze_editable()\n            self.model.load_corpus(selection)\n\n    def freeze_editable(self):\n        self.prev['state'] = 'disabled'\n        self.next['state'] = 'disabled'\n\n    def clear_results_box(self):\n        self.results_box['state'] = 'normal'\n        self.results_box.delete(\"1.0\", END)\n        self.results_box['state'] = 'disabled'\n\n    def fire_event(self, event):\n        self.top.event_generate(event, when='tail')\n\n    def destroy(self, *e):\n        if self.top is None: return\n        self.top.after_cancel(self.after)\n        self.top.destroy()\n        self.top = None\n\n    def mainloop(self, *args, **kwargs):\n        if in_idle(): return\n        self.top.mainloop(*args, **kwargs)\n\n    def unfreeze_editable(self):\n        self.set_paging_button_states()\n\n    def set_paging_button_states(self):\n        if self.current_page == -1 or self.current_page == 0:\n            self.prev['state'] = 'disabled'\n        else:\n            self.prev['state'] = 'normal'\n        if self.model.is_last_page(self.current_page):\n            self.next['state'] = 'disabled'\n        else:\n            self.next['state'] = 'normal'\n\n    def write_results(self, results):\n        self.results_box['state'] = 'normal'\n        row = 1\n        for each in results:\n            self.results_box.insert(str(row) + '.0', each[0] + \" \" + each[1] + \"\\n\")\n            row += 1\n        self.results_box['state'] = 'disabled'\n\nclass CollocationsModel:\n    def __init__(self, queue):\n        self.result_count = None\n        self.selected_corpus = None\n        self.collocations = None\n        self.CORPORA = _CORPORA\n        self.DEFAULT_CORPUS = _DEFAULT\n        self.queue = queue\n        self.reset_results()\n\n    def reset_results(self):\n        self.result_pages = []\n        self.results_returned = 0\n\n    def load_corpus(self, name):\n        self.selected_corpus = name\n        self.collocations = None\n        runner_thread = self.LoadCorpus(name, self)\n        runner_thread.start()\n        self.reset_results()\n\n    def non_default_corpora(self):\n        copy = []\n        copy.extend(list(self.CORPORA.keys()))\n        copy.remove(self.DEFAULT_CORPUS)\n        copy.sort()\n        return copy\n\n    def is_last_page(self, number):\n        if number < len(self.result_pages):\n            return False\n        return self.results_returned + (number - len(self.result_pages)) * self.result_count >= len(self.collocations)\n\n    def next(self, page):\n        if (len(self.result_pages) - 1) < page:\n            for i in range(page - (len(self.result_pages) - 1)):\n                self.result_pages.append(self.collocations[self.results_returned:self.results_returned+self.result_count])\n                self.results_returned += self.result_count\n        return self.result_pages[page]\n\n    def prev(self, page):\n        if page == -1:\n            return []\n        return self.result_pages[page]\n\n    class LoadCorpus(threading.Thread):\n        def __init__(self, name, model):\n            threading.Thread.__init__(self)\n            self.model, self.name = model, name\n\n        def run(self):\n            try:\n                words = self.model.CORPORA[self.name]()\n                from operator import itemgetter\n                text = [w for w in words if len(w) > 2]\n                fd = FreqDist(tuple(text[i:i+2]) for i in range(len(text)-1))\n                vocab = FreqDist(text)\n                scored = [((w1,w2), fd[(w1,w2)] ** 3 / (vocab[w1] * vocab[w2])) for w1, w2 in fd]\n                scored.sort(key=itemgetter(1), reverse=True)\n                self.model.collocations = list(map(itemgetter(0), scored))\n                self.model.queue.put(CORPUS_LOADED_EVENT)\n            except Exception as e:\n                print(e)\n                self.model.queue.put(ERROR_LOADING_CORPUS_EVENT)\n\n\ndef app():\n    c = CollocationsView()\n    c.mainloop()\n\nif __name__ == '__main__':\n    app()\n\n__all__ = ['app']\n"], "nltk\\app\\concordance_app": [".py", "\n\nimport nltk.compat\nimport re\nimport threading\n\nfrom six.moves import queue as q\nfrom six.moves.tkinter_font import Font\nfrom six.moves.tkinter import (Tk, Button, END, Entry, Frame, IntVar, LEFT,\n                               Label, Menu, OptionMenu, SUNKEN, Scrollbar,\n                               StringVar, Text)\n\nfrom nltk.corpus import (cess_cat, brown, nps_chat, treebank, sinica_treebank,\n                         alpino, indian, floresta, mac_morpho, cess_esp)\nfrom nltk.util import in_idle\nfrom nltk.draw.util import ShowText\n\nWORD_OR_TAG = '[^/ ]+'\nBOUNDARY = r'\\b'\n\nCORPUS_LOADED_EVENT = '<<CL_EVENT>>'\nSEARCH_TERMINATED_EVENT = '<<ST_EVENT>>'\nSEARCH_ERROR_EVENT = '<<SE_EVENT>>'\nERROR_LOADING_CORPUS_EVENT = '<<ELC_EVENT>>'\n\nPOLL_INTERVAL = 50\n\n\n_DEFAULT = 'English: Brown Corpus (Humor, simplified)'\n_CORPORA = {\n            'Catalan: CESS-CAT Corpus (simplified)':\n                lambda: cess_cat.tagged_sents(tagset='universal'),\n            'English: Brown Corpus':\n                lambda: brown.tagged_sents(),\n            'English: Brown Corpus (simplified)':\n                lambda: brown.tagged_sents(tagset='universal'),\n            'English: Brown Corpus (Press, simplified)':\n                lambda: brown.tagged_sents(categories=['news', 'editorial', 'reviews'], tagset='universal'),\n            'English: Brown Corpus (Religion, simplified)':\n                lambda: brown.tagged_sents(categories='religion', tagset='universal'),\n            'English: Brown Corpus (Learned, simplified)':\n                lambda: brown.tagged_sents(categories='learned', tagset='universal'),\n            'English: Brown Corpus (Science Fiction, simplified)':\n                lambda: brown.tagged_sents(categories='science_fiction', tagset='universal'),\n            'English: Brown Corpus (Romance, simplified)':\n                lambda: brown.tagged_sents(categories='romance', tagset='universal'),\n            'English: Brown Corpus (Humor, simplified)':\n                lambda: brown.tagged_sents(categories='humor', tagset='universal'),\n            'English: NPS Chat Corpus':\n                lambda: nps_chat.tagged_posts(),\n            'English: NPS Chat Corpus (simplified)':\n                lambda: nps_chat.tagged_posts(tagset='universal'),\n            'English: Wall Street Journal Corpus':\n                lambda: treebank.tagged_sents(),\n            'English: Wall Street Journal Corpus (simplified)':\n                lambda: treebank.tagged_sents(tagset='universal'),\n            'Chinese: Sinica Corpus':\n                lambda: sinica_treebank.tagged_sents(),\n            'Chinese: Sinica Corpus (simplified)':\n                lambda: sinica_treebank.tagged_sents(tagset='universal'),\n            'Dutch: Alpino Corpus':\n                lambda: alpino.tagged_sents(),\n            'Dutch: Alpino Corpus (simplified)':\n                lambda: alpino.tagged_sents(tagset='universal'),\n            'Hindi: Indian Languages Corpus':\n                lambda: indian.tagged_sents(files='hindi.pos'),\n            'Hindi: Indian Languages Corpus (simplified)':\n                lambda: indian.tagged_sents(files='hindi.pos', tagset='universal'),\n            'Portuguese: Floresta Corpus (Portugal)':\n                lambda: floresta.tagged_sents(),\n            'Portuguese: Floresta Corpus (Portugal, simplified)':\n                lambda: floresta.tagged_sents(tagset='universal'),\n            'Portuguese: MAC-MORPHO Corpus (Brazil)':\n                lambda: mac_morpho.tagged_sents(),\n            'Portuguese: MAC-MORPHO Corpus (Brazil, simplified)':\n                lambda: mac_morpho.tagged_sents(tagset='universal'),\n            'Spanish: CESS-ESP Corpus (simplified)':\n                lambda: cess_esp.tagged_sents(tagset='universal'),\n           }\n\nclass ConcordanceSearchView(object):\n    _BACKGROUND_COLOUR='#FFF' #white\n\n    _HIGHLIGHT_WORD_COLOUR='#F00' #red\n    _HIGHLIGHT_WORD_TAG='HL_WRD_TAG'\n\n    _HIGHLIGHT_LABEL_COLOUR='#C0C0C0' # dark grey\n    _HIGHLIGHT_LABEL_TAG='HL_LBL_TAG'\n\n\n    _FRACTION_LEFT_TEXT=0.30\n\n    def __init__(self):\n        self.queue = q.Queue()\n        self.model = ConcordanceSearchModel(self.queue)\n        self.top = Tk()\n        self._init_top(self.top)\n        self._init_menubar()\n        self._init_widgets(self.top)\n        self.load_corpus(self.model.DEFAULT_CORPUS)\n        self.after = self.top.after(POLL_INTERVAL, self._poll)\n\n    def _init_top(self, top):\n        top.geometry('950x680+50+50')\n        top.title('NLTK Concordance Search')\n        top.bind('<Control-q>', self.destroy)\n        top.protocol('WM_DELETE_WINDOW', self.destroy)\n        top.minsize(950,680)\n\n    def _init_widgets(self, parent):\n        self.main_frame = Frame(parent, dict(background=self._BACKGROUND_COLOUR, padx=1, pady=1, border=1))\n        self._init_corpus_select(self.main_frame)\n        self._init_query_box(self.main_frame)\n        self._init_results_box(self.main_frame)\n        self._init_paging(self.main_frame)\n        self._init_status(self.main_frame)\n        self.main_frame.pack(fill='both', expand=True)\n\n    def _init_menubar(self):\n        self._result_size = IntVar(self.top)\n        self._cntx_bf_len = IntVar(self.top)\n        self._cntx_af_len = IntVar(self.top)\n        menubar = Menu(self.top)\n\n        filemenu = Menu(menubar, tearoff=0, borderwidth=0)\n        filemenu.add_command(label='Exit', underline=1,\n                             command=self.destroy, accelerator='Ctrl-q')\n        menubar.add_cascade(label='File', underline=0, menu=filemenu)\n\n        editmenu = Menu(menubar, tearoff=0)\n        rescntmenu = Menu(editmenu, tearoff=0)\n        rescntmenu.add_radiobutton(label='20', variable=self._result_size,\n                                   underline=0, value=20,\n                                   command=self.set_result_size)\n        rescntmenu.add_radiobutton(label='50', variable=self._result_size,\n                                   underline=0, value=50,\n                                   command=self.set_result_size)\n        rescntmenu.add_radiobutton(label='100', variable=self._result_size,\n                                   underline=0, value=100,\n                                   command=self.set_result_size)\n        rescntmenu.invoke(1)\n        editmenu.add_cascade(label='Result Count', underline=0, menu=rescntmenu)\n\n        cntxmenu = Menu(editmenu, tearoff=0)\n        cntxbfmenu = Menu(cntxmenu, tearoff=0)\n        cntxbfmenu.add_radiobutton(label='60 characters',\n                                   variable=self._cntx_bf_len,\n                                   underline=0, value=60,\n                                   command=self.set_cntx_bf_len)\n        cntxbfmenu.add_radiobutton(label='80 characters',\n                                   variable=self._cntx_bf_len,\n                                   underline=0, value=80,\n                                   command=self.set_cntx_bf_len)\n        cntxbfmenu.add_radiobutton(label='100 characters',\n                                   variable=self._cntx_bf_len,\n                                   underline=0, value=100,\n                                   command=self.set_cntx_bf_len)\n        cntxbfmenu.invoke(1)\n        cntxmenu.add_cascade(label='Before', underline=0, menu=cntxbfmenu)\n\n        cntxafmenu = Menu(cntxmenu, tearoff=0)\n        cntxafmenu.add_radiobutton(label='70 characters',\n                                   variable=self._cntx_af_len,\n                                   underline=0, value=70,\n                                   command=self.set_cntx_af_len)\n        cntxafmenu.add_radiobutton(label='90 characters',\n                                   variable=self._cntx_af_len,\n                                   underline=0, value=90,\n                                   command=self.set_cntx_af_len)\n        cntxafmenu.add_radiobutton(label='110 characters',\n                                   variable=self._cntx_af_len,\n                                   underline=0, value=110,\n                                   command=self.set_cntx_af_len)\n        cntxafmenu.invoke(1)\n        cntxmenu.add_cascade(label='After', underline=0, menu=cntxafmenu)\n\n        editmenu.add_cascade(label='Context', underline=0, menu=cntxmenu)\n\n        menubar.add_cascade(label='Edit', underline=0, menu=editmenu)\n\n        self.top.config(menu=menubar)\n\n    def set_result_size(self, **kwargs):\n        self.model.result_count = self._result_size.get()\n\n    def set_cntx_af_len(self, **kwargs):\n        self._char_after = self._cntx_af_len.get()\n\n    def set_cntx_bf_len(self, **kwargs):\n        self._char_before = self._cntx_bf_len.get()\n\n    def _init_corpus_select(self, parent):\n        innerframe = Frame(parent, background=self._BACKGROUND_COLOUR)\n        self.var = StringVar(innerframe)\n        self.var.set(self.model.DEFAULT_CORPUS)\n        Label(innerframe, justify=LEFT, text=' Corpus: ',\n              background=self._BACKGROUND_COLOUR, padx = 2, pady = 1, border = 0).pack(side='left')\n\n        other_corpora = list(self.model.CORPORA.keys()).remove(self.model.DEFAULT_CORPUS)\n        om = OptionMenu(innerframe, self.var, self.model.DEFAULT_CORPUS, command=self.corpus_selected, *self.model.non_default_corpora())\n        om['borderwidth'] = 0\n        om['highlightthickness'] = 1\n        om.pack(side='left')\n        innerframe.pack(side='top', fill='x', anchor='n')\n\n    def _init_status(self, parent):\n        self.status = Label(parent, justify=LEFT, relief=SUNKEN, background=self._BACKGROUND_COLOUR, border=0, padx = 1, pady = 0)\n        self.status.pack(side='top', anchor='sw')\n\n    def _init_query_box(self, parent):\n        innerframe = Frame(parent, background=self._BACKGROUND_COLOUR)\n        another = Frame(innerframe, background=self._BACKGROUND_COLOUR)\n        self.query_box = Entry(another, width=60)\n        self.query_box.pack(side='left', fill='x', pady=25, anchor='center')\n        self.search_button = Button(another, text='Search', command=self.search, borderwidth=1, highlightthickness=1)\n        self.search_button.pack(side='left', fill='x', pady=25, anchor='center')\n        self.query_box.bind('<KeyPress-Return>', self.search_enter_keypress_handler)\n        another.pack()\n        innerframe.pack(side='top', fill='x', anchor='n')\n\n    def search_enter_keypress_handler(self, *event):\n        self.search()\n\n    def _init_results_box(self, parent):\n        innerframe = Frame(parent)\n        i1 = Frame(innerframe)\n        i2 = Frame(innerframe)\n        vscrollbar = Scrollbar(i1, borderwidth=1)\n        hscrollbar = Scrollbar(i2, borderwidth=1, orient='horiz')\n        self.results_box = Text(i1,\n                                font=Font(family='courier', size='16'),\n                                state='disabled', borderwidth=1,\n                                                            yscrollcommand=vscrollbar.set,\n                                xscrollcommand=hscrollbar.set, wrap='none', width='40', height = '20', exportselection=1)\n        self.results_box.pack(side='left', fill='both', expand=True)\n        self.results_box.tag_config(self._HIGHLIGHT_WORD_TAG, foreground=self._HIGHLIGHT_WORD_COLOUR)\n        self.results_box.tag_config(self._HIGHLIGHT_LABEL_TAG, foreground=self._HIGHLIGHT_LABEL_COLOUR)\n        vscrollbar.pack(side='left', fill='y', anchor='e')\n        vscrollbar.config(command=self.results_box.yview)\n        hscrollbar.pack(side='left', fill='x', expand=True, anchor='w')\n        hscrollbar.config(command=self.results_box.xview)\n        Label(i2, text='   ', background=self._BACKGROUND_COLOUR).pack(side='left', anchor='e')\n        i1.pack(side='top', fill='both', expand=True, anchor='n')\n        i2.pack(side='bottom', fill='x', anchor='s')\n        innerframe.pack(side='top', fill='both', expand=True)\n\n    def _init_paging(self, parent):\n        innerframe = Frame(parent, background=self._BACKGROUND_COLOUR)\n        self.prev = prev = Button(innerframe, text='Previous', command=self.previous, width='10', borderwidth=1, highlightthickness=1, state='disabled')\n        prev.pack(side='left', anchor='center')\n        self.next = next = Button(innerframe, text='Next', command=self.__next__, width='10', borderwidth=1, highlightthickness=1, state='disabled')\n        next.pack(side='right', anchor='center')\n        innerframe.pack(side='top', fill='y')\n        self.current_page = 0\n\n    def previous(self):\n        self.clear_results_box()\n        self.freeze_editable()\n        self.model.prev(self.current_page - 1)\n\n    def __next__(self):\n        self.clear_results_box()\n        self.freeze_editable()\n        self.model.next(self.current_page + 1)\n\n    def about(self, *e):\n        ABOUT = (\"NLTK Concordance Search Demo\\n\")\n        TITLE = 'About: NLTK Concordance Search Demo'\n        try:\n            from six.moves.tkinter_messagebox import Message\n            Message(message=ABOUT, title=TITLE, parent=self.main_frame).show()\n        except:\n            ShowText(self.top, TITLE, ABOUT)\n\n    def _bind_event_handlers(self):\n        self.top.bind(CORPUS_LOADED_EVENT, self.handle_corpus_loaded)\n        self.top.bind(SEARCH_TERMINATED_EVENT, self.handle_search_terminated)\n        self.top.bind(SEARCH_ERROR_EVENT, self.handle_search_error)\n        self.top.bind(ERROR_LOADING_CORPUS_EVENT, self.handle_error_loading_corpus)\n\n    def _poll(self):\n        try:\n            event = self.queue.get(block=False)\n        except q.Empty:\n            pass\n        else:\n            if event == CORPUS_LOADED_EVENT:\n                self.handle_corpus_loaded(event)\n            elif event == SEARCH_TERMINATED_EVENT:\n                self.handle_search_terminated(event)\n            elif event == SEARCH_ERROR_EVENT:\n                self.handle_search_error(event)\n            elif event == ERROR_LOADING_CORPUS_EVENT:\n                self.handle_error_loading_corpus(event)\n        self.after = self.top.after(POLL_INTERVAL, self._poll)\n\n    def handle_error_loading_corpus(self, event):\n        self.status['text'] = 'Error in loading ' + self.var.get()\n        self.unfreeze_editable()\n        self.clear_all()\n        self.freeze_editable()\n\n    def handle_corpus_loaded(self, event):\n        self.status['text'] = self.var.get() + ' is loaded'\n        self.unfreeze_editable()\n        self.clear_all()\n        self.query_box.focus_set()\n\n    def handle_search_terminated(self, event):\n        results = self.model.get_results()\n        self.write_results(results)\n        self.status['text'] = ''\n        if len(results) == 0:\n            self.status['text'] = 'No results found for ' + self.model.query\n        else:\n                self.current_page = self.model.last_requested_page\n        self.unfreeze_editable()\n        self.results_box.xview_moveto(self._FRACTION_LEFT_TEXT)\n\n    def handle_search_error(self, event):\n        self.status['text'] = 'Error in query ' + self.model.query\n        self.unfreeze_editable()\n\n    def corpus_selected(self, *args):\n        new_selection = self.var.get()\n        self.load_corpus(new_selection)\n\n    def load_corpus(self, selection):\n        if self.model.selected_corpus != selection:\n            self.status['text'] = 'Loading ' + selection + '...'\n            self.freeze_editable()\n            self.model.load_corpus(selection)\n\n    def search(self):\n        self.current_page = 0\n        self.clear_results_box()\n        self.model.reset_results()\n        query = self.query_box.get()\n        if (len(query.strip()) == 0): return\n        self.status['text']  = 'Searching for ' + query\n        self.freeze_editable()\n        self.model.search(query, self.current_page + 1, )\n\n\n    def write_results(self, results):\n        self.results_box['state'] = 'normal'\n        row = 1\n        for each in results:\n            sent, pos1, pos2 = each[0].strip(), each[1], each[2]\n            if len(sent) != 0:\n                if (pos1 < self._char_before):\n                    sent, pos1, pos2 = self.pad(sent, pos1, pos2)\n                sentence = sent[pos1-self._char_before:pos1+self._char_after]\n                if not row == len(results):\n                    sentence += '\\n'\n                self.results_box.insert(str(row) + '.0', sentence)\n                word_markers, label_markers = self.words_and_labels(sent, pos1, pos2)\n                for marker in word_markers: self.results_box.tag_add(self._HIGHLIGHT_WORD_TAG, str(row) + '.' + str(marker[0]), str(row) + '.' + str(marker[1]))\n                for marker in label_markers: self.results_box.tag_add(self._HIGHLIGHT_LABEL_TAG, str(row) + '.' + str(marker[0]), str(row) + '.' + str(marker[1]))\n                row += 1\n        self.results_box['state'] = 'disabled'\n\n    def words_and_labels(self, sentence, pos1, pos2):\n        search_exp = sentence[pos1:pos2]\n        words, labels = [], []\n        labeled_words = search_exp.split(' ')\n        index = 0\n        for each in labeled_words:\n            if each == '':\n                index += 1\n            else:\n                word, label = each.split('/')\n                words.append((self._char_before + index, self._char_before + index + len(word)))\n                index += len(word) + 1\n                labels.append((self._char_before + index, self._char_before + index + len(label)))\n                index += len(label)\n            index += 1\n        return words, labels\n\n    def pad(self, sent, hstart, hend):\n        if hstart >= self._char_before:\n            return sent, hstart, hend\n        d = self._char_before - hstart\n        sent = ''.join([' '] * d) + sent\n        return sent, hstart + d, hend + d\n\n    def destroy(self, *e):\n        if self.top is None: return\n        self.top.after_cancel(self.after)\n        self.top.destroy()\n        self.top = None\n\n    def clear_all(self):\n        self.query_box.delete(0, END)\n        self.model.reset_query()\n        self.clear_results_box()\n\n    def clear_results_box(self):\n        self.results_box['state'] = 'normal'\n        self.results_box.delete(\"1.0\", END)\n        self.results_box['state'] = 'disabled'\n\n    def freeze_editable(self):\n        self.query_box['state'] = 'disabled'\n        self.search_button['state'] = 'disabled'\n        self.prev['state'] = 'disabled'\n        self.next['state'] = 'disabled'\n\n    def unfreeze_editable(self):\n        self.query_box['state'] = 'normal'\n        self.search_button['state'] = 'normal'\n        self.set_paging_button_states()\n\n    def set_paging_button_states(self):\n        if self.current_page == 0 or self.current_page == 1:\n            self.prev['state'] = 'disabled'\n        else:\n            self.prev['state'] = 'normal'\n        if self.model.has_more_pages(self.current_page):\n            self.next['state'] = 'normal'\n        else:\n            self.next['state'] = 'disabled'\n\n    def fire_event(self, event):\n        self.top.event_generate(event, when='tail')\n\n    def mainloop(self, *args, **kwargs):\n        if in_idle(): return\n        self.top.mainloop(*args, **kwargs)\n\nclass ConcordanceSearchModel(object):\n    def __init__(self, queue):\n        self.queue = queue\n        self.CORPORA = _CORPORA\n        self.DEFAULT_CORPUS = _DEFAULT\n        self.selected_corpus = None\n        self.reset_query()\n        self.reset_results()\n        self.result_count = None\n        self.last_sent_searched = 0\n\n    def non_default_corpora(self):\n        copy = []\n        copy.extend(list(self.CORPORA.keys()))\n        copy.remove(self.DEFAULT_CORPUS)\n        copy.sort()\n        return copy\n\n    def load_corpus(self, name):\n        self.selected_corpus = name\n        self.tagged_sents = []\n        runner_thread = self.LoadCorpus(name, self)\n        runner_thread.start()\n\n    def search(self, query, page):\n        self.query = query\n        self.last_requested_page = page\n        self.SearchCorpus(self, page, self.result_count).start()\n\n    def next(self, page):\n        self.last_requested_page = page\n        if len(self.results) < page:\n            self.search(self.query, page)\n        else:\n            self.queue.put(SEARCH_TERMINATED_EVENT)\n\n    def prev(self, page):\n        self.last_requested_page = page\n        self.queue.put(SEARCH_TERMINATED_EVENT)\n\n    def reset_results(self):\n        self.last_sent_searched = 0\n        self.results = []\n        self.last_page = None\n\n    def reset_query(self):\n        self.query = None\n\n    def set_results(self, page, resultset):\n        self.results.insert(page - 1, resultset)\n\n    def get_results(self):\n        return self.results[self.last_requested_page - 1]\n\n    def has_more_pages(self, page):\n        if self.results == [] or self.results[0] == []:\n            return False\n        if self.last_page is None:\n            return True\n        return page < self.last_page\n\n    class LoadCorpus(threading.Thread):\n        def __init__(self, name, model):\n            threading.Thread.__init__(self)\n            self.model, self.name = model, name\n\n        def run(self):\n            try:\n                ts = self.model.CORPORA[self.name]()\n                self.model.tagged_sents = [' '.join(w+'/'+t for (w,t) in sent) for sent in ts]\n                self.model.queue.put(CORPUS_LOADED_EVENT)\n            except Exception as e:\n                print(e)\n                self.model.queue.put(ERROR_LOADING_CORPUS_EVENT)\n\n    class SearchCorpus(threading.Thread):\n        def __init__(self, model, page, count):\n            self.model, self.count, self.page = model, count, page\n            threading.Thread.__init__(self)\n\n        def run(self):\n            q = self.processed_query()\n            sent_pos, i, sent_count = [], 0, 0\n            for sent in self.model.tagged_sents[self.model.last_sent_searched:]:\n                try:\n                    m = re.search(q, sent)\n                except re.error:\n                    self.model.reset_results()\n                    self.model.queue.put(SEARCH_ERROR_EVENT)\n                    return\n                if m:\n                    sent_pos.append((sent, m.start(), m.end()))\n                    i += 1\n                    if i > self.count:\n                        self.model.last_sent_searched += sent_count - 1\n                        break\n                sent_count += 1\n            if (self.count >= len(sent_pos)):\n                self.model.last_sent_searched += sent_count - 1\n                self.model.last_page = self.page\n                self.model.set_results(self.page, sent_pos)\n            else:\n                self.model.set_results(self.page, sent_pos[:-1])\n            self.model.queue.put(SEARCH_TERMINATED_EVENT)\n\n        def processed_query(self):\n            new = []\n            for term in self.model.query.split():\n                term = re.sub(r'\\.', r'[^/ ]', term)\n                if re.match('[A-Z]+$', term):\n                    new.append(BOUNDARY + WORD_OR_TAG + '/' + term + BOUNDARY)\n                elif '/' in term:\n                    new.append(BOUNDARY + term + BOUNDARY)\n                else:\n                    new.append(BOUNDARY + term + '/' + WORD_OR_TAG + BOUNDARY)\n            return ' '.join(new)\n\ndef app():\n    d = ConcordanceSearchView()\n    d.mainloop()\n\nif __name__ == '__main__':\n    app()\n\n__all__ = ['app']\n"], "nltk\\app\\nemo_app": [".py", "\n\nfrom six.moves.tkinter import (Frame, Label, PhotoImage, Scrollbar, Text, Tk,\n                               SEL_FIRST, SEL_LAST)\nimport re\nimport itertools\n\nwindowTitle = \"Finding (and Replacing) Nemo\"\ninitialFind = r\"n(.*?)e(.*?)m(.*?)o\"\ninitialRepl = r\"M\\1A\\2K\\3I\"\ninitialText = \"\"\"\\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nUt enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\nExcepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n\"\"\"\nimages = {\n    \"FIND\":\"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\",\n    \"find\":\"R0lGODlhMAAiAPQAMf////f39+/v7+fn597e3tbW1s7OzsbGxr29vbW1ta2traWlpZycnJSUlIyMjISEhHt7e3Nzc2tra2NjY1paWlJSUkpKSkJCQjk5OSkpKRgYGAAAAAAAAAAAAAAAAAAAACH+AS4ALAAAAAAwACIAAAX/ICCOZGmeaKquY2AGLiuvMCAUBuHWc48Kh0iFInEYCb4kSQCxPBiMxkMigRQEgJiSFVBYHNGG0RiZOHjblWAiiY4fkDhEYoBp06dAWfyAQyKAgAwDaHgnB0RwgYASgQ0IhDuGJDAIFhMRVFSLEX8QCJJ4AQM5AgQHTZqqjBAOCQQEkWkCDRMUFQsICQ4Vm5maEwwHOAsPDTpKMAsUDlO4CssTcb+2DAp8YGCyNFoCEsZwFQ3QDRTTVBRS0g1QbgsCd5QAAwgIBwYFAwStzQ8UEdCKVchky0yVBw7YuXkAKt4IAg74vXHVagqFBRgXSCAyYWAVCH0SNhDTitCJfSL5/4RbAPKPhQYYjVCYYAvCP0BxEDaD8CheAAHNwqh8MMGPSwgLeJWhwHSjqkYI+xg4MMCEgQjtRvZ7UAYCpghMF7CxONOWJkYR+rCpY4JlVpVxKDwYWEactKW9mhYRtqCTgwgWEMArERSK1j5q//6T8KXonFsShpiJkAECgQYVjykooCVA0JGHEWNiYCHThTFeb3UkoiCCBgwGEKQ1kuAJlhFwhA71h5SukwUM5qqeCSGBgicEWkfNiWSERtBad4JNIBaQBaQah1ToyGZBAnsIuIJs1qnqiAIVjIE2gnAB1T5x0icgzXT79ipgMOOEH6HBbREBMJCeGEY08IoLAkzB1YYFwjxwSUGSNULQJnNUwRYlCcyEkALIxECAP9cNMMABYpRhy3ZsSLDaR70oUAiABGCkAxowCGCAAfDYIQACXoElGRsdXWDBdg2Y90IWktDYGYAB9PWHP0PMdFZaF07SQgAFNDAMAQg0QA1UC8xoZQl22JGFPgWkOUCOL1pZQyhjxinnnCWEAAA7\",\n    \"REPL\":\"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\",\n    \"repl\":\"R0lGODlhMAAjAPQAMf////f39+/v7+fn597e3tbW1s7OzsbGxr29vbW1ta2traWlpZycnJSUlIyMjISEhHt7e3Nzc2tra2NjY1paWlJSUkpKSkJCQjk5OTExMSkpKSEhIRgYGBAQEAgICAAAACH+AS4ALAAAAAAwACMAAAX/ICCOZGmeaKqubOu+gCDANBkIQ1EMQhAghFptYEAkEgjEwXBo7ISvweGgWCwUysPjwTgEoCafTySYIhYMxgLBjEQgCULvCw0QdAZdoVhUIJUFChISEAxYeQM1N1OMTAp+UwZ5eA4TEhFbDWYFdC4ECVMJjwl5BwsQa0umEhUVlhESDgqlBp0rAn5nVpBMDxeZDRQbHBgWFBSWDgtLBnFjKwRYCI9VqQsPs0YKEcMXFq0UEalFDWx4BAO2IwPjppAKDkrTWKYUGd7fEJJFEZpM00cOzCgh4EE8SaoWxKNixQooBRMyZMBwAYIRBhUgLDGS4MoBJeoANMhAgQsaCRZm/5lqaCUJhA4cNHjDoKEDBlJUHqkBlYBTiQUZNGjYMMxDhY3VWk6R4MEDBoMUak5AqoYBqANIBo4wcGGDUKIeLlzVZmWJggsVIkwAZaQSA3kdZzlKkIiEAAlDvW5oOkEBs488JTw44oeUIwdvVTFTUK7uiAAPgubt8GFDhQepqETAQCFU1UMGzlqAgFhUsAcCS0AO6lUDhw8xNRSbENGDhgWSHjWUe6ACbKITizmopZoBa6KvOwj9uuHDhwxyj3xekgDDhw5EvWKo0IB4iQLCOCC/njc7ZQ8UeGvza+ABZZgcxJNc4FO1gc0cOsCUrHevc8tdIMTIAhc4F198G2Qwwd8CBIQUAwEINABBBJUwR9R5wElgVRLwWODBBx4cGB8GEzDQIAo33CGJA8gh+JoH/clUgQU0YvDhdfmJdwEFC6Sjgg8yEPAABsPkh2F22cl2AQbn6QdTghTQ5eAJAQyQAAQV0MSBB9gRVZ4GE1mw5JZOAmiAVi1UWcAZDrDyZXYTeaOhA/bIVuIBPtKQ4h7ViYekUPdcEAEbzTzCRp5CADmAAwj+ORGPBcgwAAHo9ABGCYtm0ChwFHShlRiXhmHlkAcCiOeUodqQw5W0oXLAiamy4MOkjOyAaqxUymApDCEAADs=\",\n}\ncolors = [\"#FF7B39\",\"#80F121\"]\nemphColors = [\"#DAFC33\",\"#F42548\"]\nfieldParams = {\n    \"height\":3,\n    \"width\":70,\n    \"font\":(\"monaco\",14),\n    \"highlightthickness\":0,\n    \"borderwidth\":0,\n    \"background\":\"white\",\n}\ntextParams = {\n    \"bg\":\"#F7E0D4\",\n    \"fg\":\"#2321F1\",\n    \"highlightthickness\":0,\n    \"width\":1,\n    \"height\":10,\n    \"font\":(\"verdana\",16),\n    \"wrap\":\"word\",\n}\n\n\nclass Zone:\n    def __init__(self, image, initialField, initialText):\n        frm = Frame(root)\n        frm.config(background=\"white\")\n        self.image = PhotoImage(format='gif',data=images[image.upper()])\n        self.imageDimmed = PhotoImage(format='gif',data=images[image])\n        self.img = Label(frm)\n        self.img.config(borderwidth=0)\n        self.img.pack(side = \"left\")\n        self.fld = Text(frm, **fieldParams)\n        self.initScrollText(frm,self.fld,initialField)\n        frm = Frame(root)\n        self.txt = Text(frm, **textParams)\n        self.initScrollText(frm,self.txt,initialText)\n        for i in range(2):\n            self.txt.tag_config(colors[i], background = colors[i])\n            self.txt.tag_config(\"emph\"+colors[i], foreground = emphColors[i])\n    def initScrollText(self,frm,txt,contents):\n        scl = Scrollbar(frm)\n        scl.config(command = txt.yview)\n        scl.pack(side=\"right\",fill=\"y\")\n        txt.pack(side = \"left\", expand=True, fill=\"x\")\n        txt.config(yscrollcommand = scl.set)\n        txt.insert(\"1.0\",contents)\n        frm.pack(fill = \"x\")\n        Frame(height=2, bd=1, relief=\"ridge\").pack(fill=\"x\")\n    def refresh(self):\n        self.colorCycle = itertools.cycle(colors)\n        try:\n            self.substitute()\n            self.img.config(image = self.image)\n        except re.error:\n            self.img.config(image = self.imageDimmed)\n\n\nclass FindZone(Zone):\n    def addTags(self,m):\n        color = next(self.colorCycle)\n        self.txt.tag_add(color,\"1.0+%sc\"%m.start(),\"1.0+%sc\"%m.end())\n        try:\n            self.txt.tag_add(\"emph\"+color,\"1.0+%sc\"%m.start(\"emph\"),\n                             \"1.0+%sc\"%m.end(\"emph\"))\n        except:\n            pass\n    def substitute(self,*args):\n        for color in colors:\n            self.txt.tag_remove(color,\"1.0\",\"end\")\n            self.txt.tag_remove(\"emph\"+color,\"1.0\",\"end\")\n        self.rex = re.compile(\"\") # default value in case of misformed regexp\n        self.rex = re.compile(self.fld.get(\"1.0\",\"end\")[:-1],re.MULTILINE)\n        try:\n            re.compile(\"(?P<emph>%s)\" % self.fld.get(SEL_FIRST,\n                                                      SEL_LAST))\n            self.rexSel = re.compile(\"%s(?P<emph>%s)%s\" % (\n                self.fld.get(\"1.0\",SEL_FIRST),\n                self.fld.get(SEL_FIRST,SEL_LAST),\n                self.fld.get(SEL_LAST,\"end\")[:-1],\n            ),re.MULTILINE)\n        except:\n            self.rexSel = self.rex\n        self.rexSel.sub(self.addTags,self.txt.get(\"1.0\",\"end\"))\n\n\nclass ReplaceZone(Zone):\n    def addTags(self,m):\n        s = sz.rex.sub(self.repl,m.group())\n        self.txt.delete(\"1.0+%sc\"%(m.start()+self.diff),\n                        \"1.0+%sc\"%(m.end()+self.diff))\n        self.txt.insert(\"1.0+%sc\"%(m.start()+self.diff),s,\n                        next(self.colorCycle))\n        self.diff += len(s) - (m.end() - m.start())\n    def substitute(self):\n        self.txt.delete(\"1.0\",\"end\")\n        self.txt.insert(\"1.0\",sz.txt.get(\"1.0\",\"end\")[:-1])\n        self.diff = 0\n        self.repl = rex0.sub(r\"\\\\g<\\1>\",self.fld.get(\"1.0\",\"end\")[:-1])\n        sz.rex.sub(self.addTags,sz.txt.get(\"1.0\",\"end\")[:-1])\n\n\ndef launchRefresh(_):\n    sz.fld.after_idle(sz.refresh)\n    rz.fld.after_idle(rz.refresh)\n\n\ndef app():\n    global root, sz, rz, rex0\n    root = Tk()\n    root.resizable(height=False,width=True)\n    root.title(windowTitle)\n    root.minsize(width=250,height=0)\n    sz = FindZone(\"find\",initialFind,initialText)\n    sz.fld.bind(\"<Button-1>\",launchRefresh)\n    sz.fld.bind(\"<ButtonRelease-1>\",launchRefresh)\n    sz.fld.bind(\"<B1-Motion>\",launchRefresh)\n    sz.rexSel = re.compile(\"\")\n    rz = ReplaceZone(\"repl\",initialRepl,\"\")\n    rex0 = re.compile(r\"(?<!\\\\)\\\\([0-9]+)\")\n    root.bind_all(\"<Key>\",launchRefresh)\n    launchRefresh(None)\n    root.mainloop()\n\nif __name__ == '__main__':\n    app()\n\n__all__ = ['app']\n"], "nltk\\app\\rdparser_app": [".py", "\nfrom __future__ import division\n\nfrom six.moves.tkinter_font import Font\nfrom six.moves.tkinter import (Listbox, IntVar, Button, Frame, Label, Menu,\n                               Scrollbar, Tk)\n\nfrom nltk.tree import Tree\nfrom nltk.util import in_idle\nfrom nltk.parse import SteppingRecursiveDescentParser\nfrom nltk.draw.util import TextWidget, ShowText, CanvasFrame, EntryDialog\nfrom nltk.draw import CFGEditor, TreeSegmentWidget, tree_to_treesegment\n\nclass RecursiveDescentApp(object):\n    def __init__(self, grammar, sent, trace=0):\n        self._sent = sent\n        self._parser = SteppingRecursiveDescentParser(grammar, trace)\n\n        self._top = Tk()\n        self._top.title('Recursive Descent Parser Application')\n\n        self._init_bindings()\n\n        self._init_fonts(self._top)\n\n        self._animation_frames = IntVar(self._top)\n        self._animation_frames.set(5)\n        self._animating_lock = 0\n        self._autostep = 0\n\n        self._show_grammar = IntVar(self._top)\n        self._show_grammar.set(1)\n\n        self._init_menubar(self._top)\n        self._init_buttons(self._top)\n        self._init_feedback(self._top)\n        self._init_grammar(self._top)\n        self._init_canvas(self._top)\n\n        self._parser.initialize(self._sent)\n\n        self._canvas.bind('<Configure>', self._configure)\n\n\n    def _init_fonts(self, root):\n        self._sysfont = Font(font=Button()[\"font\"])\n        root.option_add(\"*Font\", self._sysfont)\n\n        self._size = IntVar(root)\n        self._size.set(self._sysfont.cget('size'))\n\n        self._boldfont = Font(family='helvetica', weight='bold',\n                                    size=self._size.get())\n        self._font = Font(family='helvetica',\n                                    size=self._size.get())\n        if self._size.get() < 0: big = self._size.get()-2\n        else: big = self._size.get()+2\n        self._bigfont = Font(family='helvetica', weight='bold',\n                                    size=big)\n\n    def _init_grammar(self, parent):\n        self._prodframe = listframe = Frame(parent)\n        self._prodframe.pack(fill='both', side='left', padx=2)\n        self._prodlist_label = Label(self._prodframe, font=self._boldfont,\n                                     text='Available Expansions')\n        self._prodlist_label.pack()\n        self._prodlist = Listbox(self._prodframe, selectmode='single',\n                                 relief='groove', background='white',\n                                 foreground='#909090', font=self._font,\n                                 selectforeground='#004040',\n                                 selectbackground='#c0f0c0')\n\n        self._prodlist.pack(side='right', fill='both', expand=1)\n\n        self._productions = list(self._parser.grammar().productions())\n        for production in self._productions:\n            self._prodlist.insert('end', ('  %s' % production))\n        self._prodlist.config(height=min(len(self._productions), 25))\n\n        if len(self._productions) > 25:\n            listscroll = Scrollbar(self._prodframe,\n                                   orient='vertical')\n            self._prodlist.config(yscrollcommand = listscroll.set)\n            listscroll.config(command=self._prodlist.yview)\n            listscroll.pack(side='left', fill='y')\n\n        self._prodlist.bind('<<ListboxSelect>>', self._prodlist_select)\n\n    def _init_bindings(self):\n        self._top.bind('<Control-q>', self.destroy)\n        self._top.bind('<Control-x>', self.destroy)\n        self._top.bind('<Escape>', self.destroy)\n        self._top.bind('e', self.expand)\n        self._top.bind('m', self.match)\n        self._top.bind('<Alt-m>', self.match)\n        self._top.bind('<Control-m>', self.match)\n        self._top.bind('b', self.backtrack)\n        self._top.bind('<Alt-b>', self.backtrack)\n        self._top.bind('<Control-b>', self.backtrack)\n        self._top.bind('<Control-z>', self.backtrack)\n        self._top.bind('<BackSpace>', self.backtrack)\n        self._top.bind('a', self.autostep)\n        self._top.bind('<Control-space>', self.autostep)\n        self._top.bind('<Control-c>', self.cancel_autostep)\n        self._top.bind('<space>', self.step)\n        self._top.bind('<Delete>', self.reset)\n        self._top.bind('<Control-p>', self.postscript)\n        self._top.bind('<Control-h>', self.help)\n        self._top.bind('<F1>', self.help)\n        self._top.bind('<Control-g>', self.edit_grammar)\n        self._top.bind('<Control-t>', self.edit_sentence)\n\n    def _init_buttons(self, parent):\n        self._buttonframe = buttonframe = Frame(parent)\n        buttonframe.pack(fill='none', side='bottom', padx=3, pady=2)\n        Button(buttonframe, text='Step',\n               background='#90c0d0', foreground='black',\n               command=self.step,).pack(side='left')\n        Button(buttonframe, text='Autostep',\n               background='#90c0d0', foreground='black',\n               command=self.autostep,).pack(side='left')\n        Button(buttonframe, text='Expand', underline=0,\n               background='#90f090', foreground='black',\n               command=self.expand).pack(side='left')\n        Button(buttonframe, text='Match', underline=0,\n               background='#90f090', foreground='black',\n               command=self.match).pack(side='left')\n        Button(buttonframe, text='Backtrack', underline=0,\n               background='#f0a0a0', foreground='black',\n               command=self.backtrack).pack(side='left')\n\n    def _configure(self, event):\n        self._autostep = 0\n        (x1, y1, x2, y2) = self._cframe.scrollregion()\n        y2 = event.height - 6\n        self._canvas['scrollregion'] = '%d %d %d %d' % (x1,y1,x2,y2)\n        self._redraw()\n\n    def _init_feedback(self, parent):\n        self._feedbackframe = feedbackframe = Frame(parent)\n        feedbackframe.pack(fill='x', side='bottom', padx=3, pady=3)\n        self._lastoper_label = Label(feedbackframe, text='Last Operation:',\n                                     font=self._font)\n        self._lastoper_label.pack(side='left')\n        lastoperframe = Frame(feedbackframe, relief='sunken', border=1)\n        lastoperframe.pack(fill='x', side='right', expand=1, padx=5)\n        self._lastoper1 = Label(lastoperframe, foreground='#007070',\n                                background='#f0f0f0', font=self._font)\n        self._lastoper2 = Label(lastoperframe, anchor='w', width=30,\n                                foreground='#004040', background='#f0f0f0',\n                                font=self._font)\n        self._lastoper1.pack(side='left')\n        self._lastoper2.pack(side='left', fill='x', expand=1)\n\n    def _init_canvas(self, parent):\n        self._cframe = CanvasFrame(parent, background='white',\n                                   closeenough=10,\n                                   border=2, relief='sunken')\n        self._cframe.pack(expand=1, fill='both', side='top', pady=2)\n        canvas = self._canvas = self._cframe.canvas()\n\n        self._tree = None\n        self._textwidgets = []\n        self._textline = None\n\n    def _init_menubar(self, parent):\n        menubar = Menu(parent)\n\n        filemenu = Menu(menubar, tearoff=0)\n        filemenu.add_command(label='Reset Parser', underline=0,\n                             command=self.reset, accelerator='Del')\n        filemenu.add_command(label='Print to Postscript', underline=0,\n                             command=self.postscript, accelerator='Ctrl-p')\n        filemenu.add_command(label='Exit', underline=1,\n                             command=self.destroy, accelerator='Ctrl-x')\n        menubar.add_cascade(label='File', underline=0, menu=filemenu)\n\n        editmenu = Menu(menubar, tearoff=0)\n        editmenu.add_command(label='Edit Grammar', underline=5,\n                             command=self.edit_grammar,\n                             accelerator='Ctrl-g')\n        editmenu.add_command(label='Edit Text', underline=5,\n                             command=self.edit_sentence,\n                             accelerator='Ctrl-t')\n        menubar.add_cascade(label='Edit', underline=0, menu=editmenu)\n\n        rulemenu = Menu(menubar, tearoff=0)\n        rulemenu.add_command(label='Step', underline=1,\n                             command=self.step, accelerator='Space')\n        rulemenu.add_separator()\n        rulemenu.add_command(label='Match', underline=0,\n                             command=self.match, accelerator='Ctrl-m')\n        rulemenu.add_command(label='Expand', underline=0,\n                             command=self.expand, accelerator='Ctrl-e')\n        rulemenu.add_separator()\n        rulemenu.add_command(label='Backtrack', underline=0,\n                             command=self.backtrack, accelerator='Ctrl-b')\n        menubar.add_cascade(label='Apply', underline=0, menu=rulemenu)\n\n        viewmenu = Menu(menubar, tearoff=0)\n        viewmenu.add_checkbutton(label=\"Show Grammar\", underline=0,\n                                 variable=self._show_grammar,\n                                 command=self._toggle_grammar)\n        viewmenu.add_separator()\n        viewmenu.add_radiobutton(label='Tiny', variable=self._size,\n                                 underline=0, value=10, command=self.resize)\n        viewmenu.add_radiobutton(label='Small', variable=self._size,\n                                 underline=0, value=12, command=self.resize)\n        viewmenu.add_radiobutton(label='Medium', variable=self._size,\n                                 underline=0, value=14, command=self.resize)\n        viewmenu.add_radiobutton(label='Large', variable=self._size,\n                                 underline=0, value=18, command=self.resize)\n        viewmenu.add_radiobutton(label='Huge', variable=self._size,\n                                 underline=0, value=24, command=self.resize)\n        menubar.add_cascade(label='View', underline=0, menu=viewmenu)\n\n        animatemenu = Menu(menubar, tearoff=0)\n        animatemenu.add_radiobutton(label=\"No Animation\", underline=0,\n                                    variable=self._animation_frames,\n                                    value=0)\n        animatemenu.add_radiobutton(label=\"Slow Animation\", underline=0,\n                                    variable=self._animation_frames,\n                                    value=10, accelerator='-')\n        animatemenu.add_radiobutton(label=\"Normal Animation\", underline=0,\n                                    variable=self._animation_frames,\n                                    value=5, accelerator='=')\n        animatemenu.add_radiobutton(label=\"Fast Animation\", underline=0,\n                                    variable=self._animation_frames,\n                                    value=2, accelerator='+')\n        menubar.add_cascade(label=\"Animate\", underline=1, menu=animatemenu)\n\n\n        helpmenu = Menu(menubar, tearoff=0)\n        helpmenu.add_command(label='About', underline=0,\n                             command=self.about)\n        helpmenu.add_command(label='Instructions', underline=0,\n                             command=self.help, accelerator='F1')\n        menubar.add_cascade(label='Help', underline=0, menu=helpmenu)\n\n        parent.config(menu=menubar)\n\n\n    def _get(self, widget, treeloc):\n        for i in treeloc: widget = widget.subtrees()[i]\n        if isinstance(widget, TreeSegmentWidget):\n            widget = widget.label()\n        return widget\n\n\n    def _redraw(self):\n        canvas = self._canvas\n\n        if self._tree is not None:\n            self._cframe.destroy_widget(self._tree)\n        for twidget in self._textwidgets:\n            self._cframe.destroy_widget(twidget)\n        if self._textline is not None:\n            self._canvas.delete(self._textline)\n\n        helv = ('helvetica', -self._size.get())\n        bold = ('helvetica', -self._size.get(), 'bold')\n        attribs = {'tree_color': '#000000', 'tree_width': 2,\n                   'node_font': bold, 'leaf_font': helv,}\n        tree = self._parser.tree()\n        self._tree = tree_to_treesegment(canvas, tree, **attribs)\n        self._cframe.add_widget(self._tree, 30, 5)\n\n        helv = ('helvetica', -self._size.get())\n        bottom = y = self._cframe.scrollregion()[3]\n        self._textwidgets = [TextWidget(canvas, word, font=self._font)\n                             for word in self._sent]\n        for twidget in self._textwidgets:\n            self._cframe.add_widget(twidget, 0, 0)\n            twidget.move(0, bottom-twidget.bbox()[3]-5)\n            y = min(y, twidget.bbox()[1])\n\n        self._textline = canvas.create_line(-5000, y-5, 5000, y-5, dash='.')\n\n        self._highlight_nodes()\n        self._highlight_prodlist()\n\n        self._position_text()\n\n\n    def _redraw_quick(self):\n        self._highlight_nodes()\n        self._highlight_prodlist()\n        self._position_text()\n\n    def _highlight_nodes(self):\n        bold = ('helvetica', -self._size.get(), 'bold')\n        for treeloc in self._parser.frontier()[:1]:\n            self._get(self._tree, treeloc)['color'] = '#20a050'\n            self._get(self._tree, treeloc)['font'] = bold\n        for treeloc in self._parser.frontier()[1:]:\n            self._get(self._tree, treeloc)['color'] = '#008080'\n\n    def _highlight_prodlist(self):\n        self._prodlist.delete(0, 'end')\n        expandable = self._parser.expandable_productions()\n        untried = self._parser.untried_expandable_productions()\n        productions = self._productions\n        for index in range(len(productions)):\n            if productions[index] in expandable:\n                if productions[index] in untried:\n                    self._prodlist.insert(index, ' %s' % productions[index])\n                else:\n                    self._prodlist.insert(index, ' %s (TRIED)' %\n                                          productions[index])\n                self._prodlist.selection_set(index)\n            else:\n                self._prodlist.insert(index, ' %s' % productions[index])\n\n    def _position_text(self):\n        numwords = len(self._sent)\n        num_matched = numwords - len(self._parser.remaining_text())\n        leaves = self._tree_leaves()[:num_matched]\n        xmax = self._tree.bbox()[0]\n        for i in range(0, len(leaves)):\n            widget = self._textwidgets[i]\n            leaf = leaves[i]\n            widget['color'] = '#006040'\n            leaf['color'] = '#006040'\n            widget.move(leaf.bbox()[0] - widget.bbox()[0], 0)\n            xmax = widget.bbox()[2] + 10\n\n        for i in range(len(leaves), numwords):\n            widget = self._textwidgets[i]\n            widget['color'] = '#a0a0a0'\n            widget.move(xmax - widget.bbox()[0], 0)\n            xmax = widget.bbox()[2] + 10\n\n        if self._parser.currently_complete():\n            for twidget in self._textwidgets:\n                twidget['color'] = '#00a000'\n\n        for i in range(0, len(leaves)):\n            widget = self._textwidgets[i]\n            leaf = leaves[i]\n            dy = widget.bbox()[1] - leaf.bbox()[3] - 10.0\n            dy = max(dy, leaf.parent().label().bbox()[3] - leaf.bbox()[3] + 10)\n            leaf.move(0, dy)\n\n    def _tree_leaves(self, tree=None):\n        if tree is None: tree = self._tree\n        if isinstance(tree, TreeSegmentWidget):\n            leaves = []\n            for child in tree.subtrees(): leaves += self._tree_leaves(child)\n            return leaves\n        else:\n            return [tree]\n\n\n    def destroy(self, *e):\n        self._autostep = 0\n        if self._top is None: return\n        self._top.destroy()\n        self._top = None\n\n    def reset(self, *e):\n        self._autostep = 0\n        self._parser.initialize(self._sent)\n        self._lastoper1['text'] = 'Reset Application'\n        self._lastoper2['text'] = ''\n        self._redraw()\n\n    def autostep(self, *e):\n        if self._animation_frames.get() == 0:\n            self._animation_frames.set(2)\n        if self._autostep:\n            self._autostep = 0\n        else:\n            self._autostep = 1\n            self._step()\n\n    def cancel_autostep(self, *e):\n        self._autostep = 0\n\n    def step(self, *e): self._autostep = 0; self._step()\n    def match(self, *e): self._autostep = 0; self._match()\n    def expand(self, *e): self._autostep = 0; self._expand()\n    def backtrack(self, *e): self._autostep = 0; self._backtrack()\n\n    def _step(self):\n        if self._animating_lock: return\n\n        if self._expand(): pass\n        elif self._parser.untried_match() and self._match(): pass\n        elif self._backtrack(): pass\n        else:\n            self._lastoper1['text'] = 'Finished'\n            self._lastoper2['text'] = ''\n            self._autostep = 0\n\n        if self._parser.currently_complete():\n            self._autostep = 0\n            self._lastoper2['text'] += '    [COMPLETE PARSE]'\n\n    def _expand(self, *e):\n        if self._animating_lock: return\n        old_frontier = self._parser.frontier()\n        rv = self._parser.expand()\n        if rv is not None:\n            self._lastoper1['text'] = 'Expand:'\n            self._lastoper2['text'] = rv\n            self._prodlist.selection_clear(0, 'end')\n            index = self._productions.index(rv)\n            self._prodlist.selection_set(index)\n            self._animate_expand(old_frontier[0])\n            return True\n        else:\n            self._lastoper1['text'] = 'Expand:'\n            self._lastoper2['text'] = '(all expansions tried)'\n            return False\n\n    def _match(self, *e):\n        if self._animating_lock: return\n        old_frontier = self._parser.frontier()\n        rv = self._parser.match()\n        if rv is not None:\n            self._lastoper1['text'] = 'Match:'\n            self._lastoper2['text'] = rv\n            self._animate_match(old_frontier[0])\n            return True\n        else:\n            self._lastoper1['text'] = 'Match:'\n            self._lastoper2['text'] = '(failed)'\n            return False\n\n    def _backtrack(self, *e):\n        if self._animating_lock: return\n        if self._parser.backtrack():\n            elt = self._parser.tree()\n            for i in self._parser.frontier()[0]:\n                elt = elt[i]\n            self._lastoper1['text'] = 'Backtrack'\n            self._lastoper2['text'] = ''\n            if isinstance(elt, Tree):\n                self._animate_backtrack(self._parser.frontier()[0])\n            else:\n                self._animate_match_backtrack(self._parser.frontier()[0])\n            return True\n        else:\n            self._autostep = 0\n            self._lastoper1['text'] = 'Finished'\n            self._lastoper2['text'] = ''\n            return False\n\n    def about(self, *e):\n        ABOUT = (\"NLTK Recursive Descent Parser Application\\n\"+\n                 \"Written by Edward Loper\")\n        TITLE = 'About: Recursive Descent Parser Application'\n        try:\n            from six.moves.tkinter_messagebox import Message\n            Message(message=ABOUT, title=TITLE).show()\n        except:\n            ShowText(self._top, TITLE, ABOUT)\n\n    def help(self, *e):\n        self._autostep = 0\n        try:\n            ShowText(self._top, 'Help: Recursive Descent Parser Application',\n                     (__doc__ or '').strip(), width=75, font='fixed')\n        except:\n            ShowText(self._top, 'Help: Recursive Descent Parser Application',\n                     (__doc__ or '').strip(), width=75)\n\n    def postscript(self, *e):\n        self._autostep = 0\n        self._cframe.print_to_file()\n\n    def mainloop(self, *args, **kwargs):\n        if in_idle(): return\n        self._top.mainloop(*args, **kwargs)\n\n    def resize(self, size=None):\n        if size is not None: self._size.set(size)\n        size = self._size.get()\n        self._font.configure(size=-(abs(size)))\n        self._boldfont.configure(size=-(abs(size)))\n        self._sysfont.configure(size=-(abs(size)))\n        self._bigfont.configure(size=-(abs(size+2)))\n        self._redraw()\n\n\n    def _toggle_grammar(self, *e):\n        if self._show_grammar.get():\n            self._prodframe.pack(fill='both', side='left', padx=2,\n                                 after=self._feedbackframe)\n            self._lastoper1['text'] = 'Show Grammar'\n        else:\n            self._prodframe.pack_forget()\n            self._lastoper1['text'] = 'Hide Grammar'\n        self._lastoper2['text'] = ''\n\n\n    def _prodlist_select(self, event):\n        selection = self._prodlist.curselection()\n        if len(selection) != 1: return\n        index = int(selection[0])\n        old_frontier = self._parser.frontier()\n        production = self._parser.expand(self._productions[index])\n\n        if production:\n            self._lastoper1['text'] = 'Expand:'\n            self._lastoper2['text'] = production\n            self._prodlist.selection_clear(0, 'end')\n            self._prodlist.selection_set(index)\n            self._animate_expand(old_frontier[0])\n        else:\n            self._prodlist.selection_clear(0, 'end')\n            for prod in self._parser.expandable_productions():\n                index = self._productions.index(prod)\n                self._prodlist.selection_set(index)\n\n\n    def _animate_expand(self, treeloc):\n        oldwidget = self._get(self._tree, treeloc)\n        oldtree = oldwidget.parent()\n        top = not isinstance(oldtree.parent(), TreeSegmentWidget)\n\n        tree = self._parser.tree()\n        for i in treeloc:\n            tree = tree[i]\n\n        widget = tree_to_treesegment(self._canvas, tree,\n                                     node_font=self._boldfont,\n                                     leaf_color='white',\n                                     tree_width=2, tree_color='white',\n                                     node_color='white',\n                                     leaf_font=self._font)\n        widget.label()['color'] = '#20a050'\n\n        (oldx, oldy) = oldtree.label().bbox()[:2]\n        (newx, newy) = widget.label().bbox()[:2]\n        widget.move(oldx-newx, oldy-newy)\n\n        if top:\n            self._cframe.add_widget(widget, 0, 5)\n            widget.move(30-widget.label().bbox()[0], 0)\n            self._tree = widget\n        else:\n            oldtree.parent().replace_child(oldtree, widget)\n\n        if widget.subtrees():\n            dx = (oldx + widget.label().width()/2 -\n                  widget.subtrees()[0].bbox()[0]/2 -\n                  widget.subtrees()[0].bbox()[2]/2)\n            for subtree in widget.subtrees(): subtree.move(dx, 0)\n\n        self._makeroom(widget)\n\n        if top:\n            self._cframe.destroy_widget(oldtree)\n        else:\n            oldtree.destroy()\n\n        colors = ['gray%d' % (10*int(10*x/self._animation_frames.get()))\n                  for x in range(self._animation_frames.get(),0,-1)]\n\n        dy = widget.bbox()[3] + 30 - self._canvas.coords(self._textline)[1]\n        if dy > 0:\n            for twidget in self._textwidgets: twidget.move(0, dy)\n            self._canvas.move(self._textline, 0, dy)\n\n        self._animate_expand_frame(widget, colors)\n\n    def _makeroom(self, treeseg):\n        parent = treeseg.parent()\n        if not isinstance(parent, TreeSegmentWidget): return\n\n        index = parent.subtrees().index(treeseg)\n\n        rsiblings = parent.subtrees()[index+1:]\n        if rsiblings:\n            dx = treeseg.bbox()[2] - rsiblings[0].bbox()[0] + 10\n            for sibling in rsiblings: sibling.move(dx, 0)\n\n        if index > 0:\n            lsibling = parent.subtrees()[index-1]\n            dx = max(0, lsibling.bbox()[2] - treeseg.bbox()[0] + 10)\n            treeseg.move(dx, 0)\n\n        self._makeroom(parent)\n\n    def _animate_expand_frame(self, widget, colors):\n        if len(colors) > 0:\n            self._animating_lock = 1\n            widget['color'] = colors[0]\n            for subtree in widget.subtrees():\n                if isinstance(subtree, TreeSegmentWidget):\n                    subtree.label()['color'] = colors[0]\n                else:\n                    subtree['color'] = colors[0]\n            self._top.after(50, self._animate_expand_frame,\n                            widget, colors[1:])\n        else:\n            widget['color'] = 'black'\n            for subtree in widget.subtrees():\n                if isinstance(subtree, TreeSegmentWidget):\n                    subtree.label()['color'] = 'black'\n                else:\n                    subtree['color'] = 'black'\n            self._redraw_quick()\n            widget.label()['color'] = 'black'\n            self._animating_lock = 0\n            if self._autostep: self._step()\n\n    def _animate_backtrack(self, treeloc):\n        if self._animation_frames.get() == 0: colors = []\n        else: colors = ['#a00000', '#000000', '#a00000']\n        colors += ['gray%d' % (10*int(10*x/(self._animation_frames.get())))\n                   for x in range(1, self._animation_frames.get()+1)]\n\n        widgets = [self._get(self._tree, treeloc).parent()]\n        for subtree in widgets[0].subtrees():\n            if isinstance(subtree, TreeSegmentWidget):\n                widgets.append(subtree.label())\n            else:\n                widgets.append(subtree)\n\n        self._animate_backtrack_frame(widgets, colors)\n\n    def _animate_backtrack_frame(self, widgets, colors):\n        if len(colors) > 0:\n            self._animating_lock = 1\n            for widget in widgets: widget['color'] = colors[0]\n            self._top.after(50, self._animate_backtrack_frame,\n                            widgets, colors[1:])\n        else:\n            for widget in widgets[0].subtrees():\n                widgets[0].remove_child(widget)\n                widget.destroy()\n            self._redraw_quick()\n            self._animating_lock = 0\n            if self._autostep: self._step()\n\n    def _animate_match_backtrack(self, treeloc):\n        widget = self._get(self._tree, treeloc)\n        node = widget.parent().label()\n        dy = ((node.bbox()[3] - widget.bbox()[1] + 14) /\n              max(1, self._animation_frames.get()))\n        self._animate_match_backtrack_frame(self._animation_frames.get(),\n                                            widget, dy)\n\n    def _animate_match(self, treeloc):\n        widget = self._get(self._tree, treeloc)\n\n        dy = ((self._textwidgets[0].bbox()[1] - widget.bbox()[3] - 10.0) /\n              max(1, self._animation_frames.get()))\n        self._animate_match_frame(self._animation_frames.get(), widget, dy)\n\n    def _animate_match_frame(self, frame, widget, dy):\n        if frame > 0:\n            self._animating_lock = 1\n            widget.move(0, dy)\n            self._top.after(10, self._animate_match_frame,\n                            frame-1, widget, dy)\n        else:\n            widget['color'] = '#006040'\n            self._redraw_quick()\n            self._animating_lock = 0\n            if self._autostep: self._step()\n\n    def _animate_match_backtrack_frame(self, frame, widget, dy):\n        if frame > 0:\n            self._animating_lock = 1\n            widget.move(0, dy)\n            self._top.after(10, self._animate_match_backtrack_frame,\n                            frame-1, widget, dy)\n        else:\n            widget.parent().remove_child(widget)\n            widget.destroy()\n            self._animating_lock = 0\n            if self._autostep: self._step()\n\n    def edit_grammar(self, *e):\n        CFGEditor(self._top, self._parser.grammar(), self.set_grammar)\n\n    def set_grammar(self, grammar):\n        self._parser.set_grammar(grammar)\n        self._productions = list(grammar.productions())\n        self._prodlist.delete(0, 'end')\n        for production in self._productions:\n            self._prodlist.insert('end', (' %s' % production))\n\n    def edit_sentence(self, *e):\n        sentence = \" \".join(self._sent)\n        title = 'Edit Text'\n        instr = 'Enter a new sentence to parse.'\n        EntryDialog(self._top, sentence, instr, self.set_sentence, title)\n\n    def set_sentence(self, sentence):\n        self._sent = sentence.split() #[XX] use tagged?\n        self.reset()\n\ndef app():\n    from nltk.grammar import CFG\n    grammar = CFG.fromstring(\"\"\"\n        S -> NP VP\n        NP -> Det N PP | Det N\n        VP -> V NP PP | V NP | V\n        PP -> P NP\n        NP -> 'I'\n        Det -> 'the' | 'a'\n        N -> 'man' | 'park' | 'dog' | 'telescope'\n        V -> 'ate' | 'saw'\n        P -> 'in' | 'under' | 'with'\n    \"\"\")\n\n    sent = 'the dog saw a man in the park'.split()\n\n    RecursiveDescentApp(grammar, sent).mainloop()\n\nif __name__ == '__main__':\n    app()\n\n__all__ = ['app']\n"], "nltk\\app\\srparser_app": [".py", "\n\nfrom six.moves.tkinter_font import Font\nfrom six.moves.tkinter import (IntVar, Listbox, Button, Frame, Label, Menu,\n                               Scrollbar, Tk)\n\nfrom nltk.tree import Tree\nfrom nltk.parse import SteppingShiftReduceParser\nfrom nltk.util import in_idle\nfrom nltk.draw.util import CanvasFrame, EntryDialog, ShowText, TextWidget\nfrom nltk.draw import CFGEditor, TreeSegmentWidget, tree_to_treesegment\n\n\nclass ShiftReduceApp(object):\n    def __init__(self, grammar, sent, trace=0):\n        self._sent = sent\n        self._parser = SteppingShiftReduceParser(grammar, trace)\n\n        self._top = Tk()\n        self._top.title('Shift Reduce Parser Application')\n\n        self._animating_lock = 0\n        self._animate = IntVar(self._top)\n        self._animate.set(10) # = medium\n\n        self._show_grammar = IntVar(self._top)\n        self._show_grammar.set(1)\n\n        self._init_fonts(self._top)\n\n        self._init_bindings()\n\n        self._init_menubar(self._top)\n        self._init_buttons(self._top)\n        self._init_feedback(self._top)\n        self._init_grammar(self._top)\n        self._init_canvas(self._top)\n\n        self._reduce_menu = Menu(self._canvas, tearoff=0)\n\n        self.reset()\n        self._lastoper1['text'] = ''\n\n\n    def _init_fonts(self, root):\n        self._sysfont = Font(font=Button()[\"font\"])\n        root.option_add(\"*Font\", self._sysfont)\n\n        self._size = IntVar(root)\n        self._size.set(self._sysfont.cget('size'))\n\n        self._boldfont = Font(family='helvetica', weight='bold',\n                                    size=self._size.get())\n        self._font = Font(family='helvetica',\n                                    size=self._size.get())\n\n    def _init_grammar(self, parent):\n        self._prodframe = listframe = Frame(parent)\n        self._prodframe.pack(fill='both', side='left', padx=2)\n        self._prodlist_label = Label(self._prodframe,\n                                     font=self._boldfont,\n                                     text='Available Reductions')\n        self._prodlist_label.pack()\n        self._prodlist = Listbox(self._prodframe, selectmode='single',\n                                 relief='groove', background='white',\n                                 foreground='#909090',\n                                 font=self._font,\n                                 selectforeground='#004040',\n                                 selectbackground='#c0f0c0')\n\n        self._prodlist.pack(side='right', fill='both', expand=1)\n\n        self._productions = list(self._parser.grammar().productions())\n        for production in self._productions:\n            self._prodlist.insert('end', (' %s' % production))\n        self._prodlist.config(height=min(len(self._productions), 25))\n\n        if 1:#len(self._productions) > 25:\n            listscroll = Scrollbar(self._prodframe,\n                                   orient='vertical')\n            self._prodlist.config(yscrollcommand = listscroll.set)\n            listscroll.config(command=self._prodlist.yview)\n            listscroll.pack(side='left', fill='y')\n\n        self._prodlist.bind('<<ListboxSelect>>', self._prodlist_select)\n\n        self._hover = -1\n        self._prodlist.bind('<Motion>', self._highlight_hover)\n        self._prodlist.bind('<Leave>', self._clear_hover)\n\n    def _init_bindings(self):\n        self._top.bind('<Control-q>', self.destroy)\n        self._top.bind('<Control-x>', self.destroy)\n        self._top.bind('<Alt-q>', self.destroy)\n        self._top.bind('<Alt-x>', self.destroy)\n\n        self._top.bind('<space>', self.step)\n        self._top.bind('<s>', self.shift)\n        self._top.bind('<Alt-s>', self.shift)\n        self._top.bind('<Control-s>', self.shift)\n        self._top.bind('<r>', self.reduce)\n        self._top.bind('<Alt-r>', self.reduce)\n        self._top.bind('<Control-r>', self.reduce)\n        self._top.bind('<Delete>', self.reset)\n        self._top.bind('<u>', self.undo)\n        self._top.bind('<Alt-u>', self.undo)\n        self._top.bind('<Control-u>', self.undo)\n        self._top.bind('<Control-z>', self.undo)\n        self._top.bind('<BackSpace>', self.undo)\n\n        self._top.bind('<Control-p>', self.postscript)\n        self._top.bind('<Control-h>', self.help)\n        self._top.bind('<F1>', self.help)\n        self._top.bind('<Control-g>', self.edit_grammar)\n        self._top.bind('<Control-t>', self.edit_sentence)\n\n        self._top.bind('-', lambda e,a=self._animate:a.set(20))\n        self._top.bind('=', lambda e,a=self._animate:a.set(10))\n        self._top.bind('+', lambda e,a=self._animate:a.set(4))\n\n    def _init_buttons(self, parent):\n        self._buttonframe = buttonframe = Frame(parent)\n        buttonframe.pack(fill='none', side='bottom')\n        Button(buttonframe, text='Step',\n               background='#90c0d0', foreground='black',\n               command=self.step,).pack(side='left')\n        Button(buttonframe, text='Shift', underline=0,\n               background='#90f090', foreground='black',\n               command=self.shift).pack(side='left')\n        Button(buttonframe, text='Reduce', underline=0,\n               background='#90f090', foreground='black',\n               command=self.reduce).pack(side='left')\n        Button(buttonframe, text='Undo', underline=0,\n               background='#f0a0a0', foreground='black',\n               command=self.undo).pack(side='left')\n\n    def _init_menubar(self, parent):\n        menubar = Menu(parent)\n\n        filemenu = Menu(menubar, tearoff=0)\n        filemenu.add_command(label='Reset Parser', underline=0,\n                             command=self.reset, accelerator='Del')\n        filemenu.add_command(label='Print to Postscript', underline=0,\n                             command=self.postscript, accelerator='Ctrl-p')\n        filemenu.add_command(label='Exit', underline=1,\n                             command=self.destroy, accelerator='Ctrl-x')\n        menubar.add_cascade(label='File', underline=0, menu=filemenu)\n\n        editmenu = Menu(menubar, tearoff=0)\n        editmenu.add_command(label='Edit Grammar', underline=5,\n                             command=self.edit_grammar,\n                             accelerator='Ctrl-g')\n        editmenu.add_command(label='Edit Text', underline=5,\n                             command=self.edit_sentence,\n                             accelerator='Ctrl-t')\n        menubar.add_cascade(label='Edit', underline=0, menu=editmenu)\n\n        rulemenu = Menu(menubar, tearoff=0)\n        rulemenu.add_command(label='Step', underline=1,\n                             command=self.step, accelerator='Space')\n        rulemenu.add_separator()\n        rulemenu.add_command(label='Shift', underline=0,\n                             command=self.shift, accelerator='Ctrl-s')\n        rulemenu.add_command(label='Reduce', underline=0,\n                             command=self.reduce, accelerator='Ctrl-r')\n        rulemenu.add_separator()\n        rulemenu.add_command(label='Undo', underline=0,\n                             command=self.undo, accelerator='Ctrl-u')\n        menubar.add_cascade(label='Apply', underline=0, menu=rulemenu)\n\n        viewmenu = Menu(menubar, tearoff=0)\n        viewmenu.add_checkbutton(label=\"Show Grammar\", underline=0,\n                                 variable=self._show_grammar,\n                                 command=self._toggle_grammar)\n        viewmenu.add_separator()\n        viewmenu.add_radiobutton(label='Tiny', variable=self._size,\n                                 underline=0, value=10, command=self.resize)\n        viewmenu.add_radiobutton(label='Small', variable=self._size,\n                                 underline=0, value=12, command=self.resize)\n        viewmenu.add_radiobutton(label='Medium', variable=self._size,\n                                 underline=0, value=14, command=self.resize)\n        viewmenu.add_radiobutton(label='Large', variable=self._size,\n                                 underline=0, value=18, command=self.resize)\n        viewmenu.add_radiobutton(label='Huge', variable=self._size,\n                                 underline=0, value=24, command=self.resize)\n        menubar.add_cascade(label='View', underline=0, menu=viewmenu)\n\n        animatemenu = Menu(menubar, tearoff=0)\n        animatemenu.add_radiobutton(label=\"No Animation\", underline=0,\n                                    variable=self._animate, value=0)\n        animatemenu.add_radiobutton(label=\"Slow Animation\", underline=0,\n                                    variable=self._animate, value=20,\n                                    accelerator='-')\n        animatemenu.add_radiobutton(label=\"Normal Animation\", underline=0,\n                                    variable=self._animate, value=10,\n                                    accelerator='=')\n        animatemenu.add_radiobutton(label=\"Fast Animation\", underline=0,\n                                    variable=self._animate, value=4,\n                                    accelerator='+')\n        menubar.add_cascade(label=\"Animate\", underline=1, menu=animatemenu)\n\n\n        helpmenu = Menu(menubar, tearoff=0)\n        helpmenu.add_command(label='About', underline=0,\n                             command=self.about)\n        helpmenu.add_command(label='Instructions', underline=0,\n                             command=self.help, accelerator='F1')\n        menubar.add_cascade(label='Help', underline=0, menu=helpmenu)\n\n        parent.config(menu=menubar)\n\n    def _init_feedback(self, parent):\n        self._feedbackframe = feedbackframe = Frame(parent)\n        feedbackframe.pack(fill='x', side='bottom', padx=3, pady=3)\n        self._lastoper_label = Label(feedbackframe, text='Last Operation:',\n                                     font=self._font)\n        self._lastoper_label.pack(side='left')\n        lastoperframe = Frame(feedbackframe, relief='sunken', border=1)\n        lastoperframe.pack(fill='x', side='right', expand=1, padx=5)\n        self._lastoper1 = Label(lastoperframe, foreground='#007070',\n                                background='#f0f0f0', font=self._font)\n        self._lastoper2 = Label(lastoperframe, anchor='w', width=30,\n                                foreground='#004040', background='#f0f0f0',\n                                font=self._font)\n        self._lastoper1.pack(side='left')\n        self._lastoper2.pack(side='left', fill='x', expand=1)\n\n    def _init_canvas(self, parent):\n        self._cframe = CanvasFrame(parent, background='white',\n                                   width=525, closeenough=10,\n                                   border=2, relief='sunken')\n        self._cframe.pack(expand=1, fill='both', side='top', pady=2)\n        canvas = self._canvas = self._cframe.canvas()\n\n        self._stackwidgets = []\n        self._rtextwidgets = []\n        self._titlebar = canvas.create_rectangle(0,0,0,0, fill='#c0f0f0',\n                                                 outline='black')\n        self._exprline = canvas.create_line(0,0,0,0, dash='.')\n        self._stacktop = canvas.create_line(0,0,0,0, fill='#408080')\n        size = self._size.get()+4\n        self._stacklabel = TextWidget(canvas, 'Stack', color='#004040',\n                                      font=self._boldfont)\n        self._rtextlabel = TextWidget(canvas, 'Remaining Text',\n                                      color='#004040', font=self._boldfont)\n        self._cframe.add_widget(self._stacklabel)\n        self._cframe.add_widget(self._rtextlabel)\n\n\n    def _redraw(self):\n        scrollregion = self._canvas['scrollregion'].split()\n        (cx1, cy1, cx2, cy2) = [int(c) for c in scrollregion]\n\n        for stackwidget in self._stackwidgets:\n            self._cframe.destroy_widget(stackwidget)\n        self._stackwidgets = []\n        for rtextwidget in self._rtextwidgets:\n            self._cframe.destroy_widget(rtextwidget)\n        self._rtextwidgets = []\n\n        (x1, y1, x2, y2) = self._stacklabel.bbox()\n        y = y2-y1+10\n        self._canvas.coords(self._titlebar, -5000, 0, 5000, y-4)\n        self._canvas.coords(self._exprline, 0, y*2-10, 5000, y*2-10)\n\n        (x1, y1, x2, y2) = self._stacklabel.bbox()\n        self._stacklabel.move(5-x1, 3-y1)\n        (x1, y1, x2, y2) = self._rtextlabel.bbox()\n        self._rtextlabel.move(cx2-x2-5, 3-y1)\n\n        stackx = 5\n        for tok in self._parser.stack():\n            if isinstance(tok, Tree):\n                attribs = {'tree_color': '#4080a0', 'tree_width': 2,\n                           'node_font': self._boldfont,\n                           'node_color': '#006060',\n                           'leaf_color': '#006060', 'leaf_font':self._font}\n                widget = tree_to_treesegment(self._canvas, tok,\n                                             **attribs)\n                widget.label()['color'] = '#000000'\n            else:\n                widget = TextWidget(self._canvas, tok,\n                                    color='#000000', font=self._font)\n            widget.bind_click(self._popup_reduce)\n            self._stackwidgets.append(widget)\n            self._cframe.add_widget(widget, stackx, y)\n            stackx = widget.bbox()[2] + 10\n\n        rtextwidth = 0\n        for tok in self._parser.remaining_text():\n            widget = TextWidget(self._canvas, tok,\n                                color='#000000', font=self._font)\n            self._rtextwidgets.append(widget)\n            self._cframe.add_widget(widget, rtextwidth, y)\n            rtextwidth = widget.bbox()[2] + 4\n\n        if len(self._rtextwidgets) > 0:\n            stackx += self._rtextwidgets[0].width()\n\n        stackx = max(stackx, self._stacklabel.width()+25)\n        rlabelwidth = self._rtextlabel.width()+10\n        if stackx >= cx2-max(rtextwidth, rlabelwidth):\n            cx2 = stackx + max(rtextwidth, rlabelwidth)\n        for rtextwidget in self._rtextwidgets:\n            rtextwidget.move(4+cx2-rtextwidth, 0)\n        self._rtextlabel.move(cx2-self._rtextlabel.bbox()[2]-5, 0)\n\n        midx = (stackx + cx2-max(rtextwidth, rlabelwidth))/2\n        self._canvas.coords(self._stacktop, midx, 0, midx, 5000)\n        (x1, y1, x2, y2) = self._stacklabel.bbox()\n\n        if len(self._rtextwidgets) > 0:\n            def drag_shift(widget, midx=midx, self=self):\n                if widget.bbox()[0] < midx: self.shift()\n                else: self._redraw()\n            self._rtextwidgets[0].bind_drag(drag_shift)\n            self._rtextwidgets[0].bind_click(self.shift)\n\n        self._highlight_productions()\n\n    def _draw_stack_top(self, widget):\n        midx = widget.bbox()[2]+50\n        self._canvas.coords(self._stacktop, midx, 0, midx, 5000)\n\n    def _highlight_productions(self):\n        self._prodlist.selection_clear(0, 'end')\n        for prod in self._parser.reducible_productions():\n            index = self._productions.index(prod)\n            self._prodlist.selection_set(index)\n\n\n    def destroy(self, *e):\n        if self._top is None: return\n        self._top.destroy()\n        self._top = None\n\n    def reset(self, *e):\n        self._parser.initialize(self._sent)\n        self._lastoper1['text'] = 'Reset App'\n        self._lastoper2['text'] = ''\n        self._redraw()\n\n    def step(self, *e):\n        if self.reduce(): return True\n        elif self.shift(): return True\n        else:\n            if list(self._parser.parses()):\n                self._lastoper1['text'] = 'Finished:'\n                self._lastoper2['text'] = 'Success'\n            else:\n                self._lastoper1['text'] = 'Finished:'\n                self._lastoper2['text'] = 'Failure'\n\n    def shift(self, *e):\n        if self._animating_lock: return\n        if self._parser.shift():\n            tok = self._parser.stack()[-1]\n            self._lastoper1['text'] = 'Shift:'\n            self._lastoper2['text'] = '%r' % tok\n            if self._animate.get():\n                self._animate_shift()\n            else:\n                self._redraw()\n            return True\n        return False\n\n    def reduce(self, *e):\n        if self._animating_lock: return\n        production = self._parser.reduce()\n        if production:\n            self._lastoper1['text'] = 'Reduce:'\n            self._lastoper2['text'] = '%s' % production\n            if self._animate.get():\n                self._animate_reduce()\n            else:\n                self._redraw()\n        return production\n\n    def undo(self, *e):\n        if self._animating_lock: return\n        if self._parser.undo():\n            self._redraw()\n\n    def postscript(self, *e):\n        self._cframe.print_to_file()\n\n    def mainloop(self, *args, **kwargs):\n        if in_idle(): return\n        self._top.mainloop(*args, **kwargs)\n\n\n    def resize(self, size=None):\n        if size is not None: self._size.set(size)\n        size = self._size.get()\n        self._font.configure(size=-(abs(size)))\n        self._boldfont.configure(size=-(abs(size)))\n        self._sysfont.configure(size=-(abs(size)))\n\n        self._redraw()\n\n    def help(self, *e):\n        try:\n            ShowText(self._top, 'Help: Shift-Reduce Parser Application',\n                     (__doc__ or '').strip(), width=75, font='fixed')\n        except:\n            ShowText(self._top, 'Help: Shift-Reduce Parser Application',\n                     (__doc__ or '').strip(), width=75)\n\n    def about(self, *e):\n        ABOUT = (\"NLTK Shift-Reduce Parser Application\\n\"+\n                 \"Written by Edward Loper\")\n        TITLE = 'About: Shift-Reduce Parser Application'\n        try:\n            from six.moves.tkinter_messagebox import Message\n            Message(message=ABOUT, title=TITLE).show()\n        except:\n            ShowText(self._top, TITLE, ABOUT)\n\n    def edit_grammar(self, *e):\n        CFGEditor(self._top, self._parser.grammar(), self.set_grammar)\n\n    def set_grammar(self, grammar):\n        self._parser.set_grammar(grammar)\n        self._productions = list(grammar.productions())\n        self._prodlist.delete(0, 'end')\n        for production in self._productions:\n            self._prodlist.insert('end', (' %s' % production))\n\n    def edit_sentence(self, *e):\n        sentence = \" \".join(self._sent)\n        title = 'Edit Text'\n        instr = 'Enter a new sentence to parse.'\n        EntryDialog(self._top, sentence, instr, self.set_sentence, title)\n\n    def set_sentence(self, sent):\n        self._sent = sent.split() #[XX] use tagged?\n        self.reset()\n\n\n    def _toggle_grammar(self, *e):\n        if self._show_grammar.get():\n            self._prodframe.pack(fill='both', side='left', padx=2,\n                                 after=self._feedbackframe)\n            self._lastoper1['text'] = 'Show Grammar'\n        else:\n            self._prodframe.pack_forget()\n            self._lastoper1['text'] = 'Hide Grammar'\n        self._lastoper2['text'] = ''\n\n    def _prodlist_select(self, event):\n        selection = self._prodlist.curselection()\n        if len(selection) != 1: return\n        index = int(selection[0])\n        production = self._parser.reduce(self._productions[index])\n        if production:\n            self._lastoper1['text'] = 'Reduce:'\n            self._lastoper2['text'] = '%s' % production\n            if self._animate.get():\n                self._animate_reduce()\n            else:\n                self._redraw()\n        else:\n            self._prodlist.selection_clear(0, 'end')\n            for prod in self._parser.reducible_productions():\n                index = self._productions.index(prod)\n                self._prodlist.selection_set(index)\n\n    def _popup_reduce(self, widget):\n        productions = self._parser.reducible_productions()\n        if len(productions) == 0: return\n\n        self._reduce_menu.delete(0, 'end')\n        for production in productions:\n            self._reduce_menu.add_command(label=str(production),\n                                          command=self.reduce)\n        self._reduce_menu.post(self._canvas.winfo_pointerx(),\n                               self._canvas.winfo_pointery())\n\n\n    def _animate_shift(self):\n        widget = self._rtextwidgets[0]\n\n        right = widget.bbox()[0]\n        if len(self._stackwidgets) == 0: left = 5\n        else: left = self._stackwidgets[-1].bbox()[2]+10\n\n        dt = self._animate.get()\n        dx = (left-right)*1.0/dt\n        self._animate_shift_frame(dt, widget, dx)\n\n    def _animate_shift_frame(self, frame, widget, dx):\n        if frame > 0:\n            self._animating_lock = 1\n            widget.move(dx, 0)\n            self._top.after(10, self._animate_shift_frame,\n                            frame-1, widget, dx)\n        else:\n\n            del self._rtextwidgets[0]\n            self._stackwidgets.append(widget)\n            self._animating_lock = 0\n\n            self._draw_stack_top(widget)\n            self._highlight_productions()\n\n    def _animate_reduce(self):\n        numwidgets = len(self._parser.stack()[-1]) # number of children\n        widgets = self._stackwidgets[-numwidgets:]\n\n        if isinstance(widgets[0], TreeSegmentWidget):\n            ydist = 15 + widgets[0].label().height()\n        else:\n            ydist = 15 + widgets[0].height()\n\n        dt = self._animate.get()\n        dy = ydist*2.0/dt\n        self._animate_reduce_frame(dt/2, widgets, dy)\n\n    def _animate_reduce_frame(self, frame, widgets, dy):\n        if frame > 0:\n            self._animating_lock = 1\n            for widget in widgets: widget.move(0, dy)\n            self._top.after(10, self._animate_reduce_frame,\n                            frame-1, widgets, dy)\n        else:\n            del self._stackwidgets[-len(widgets):]\n            for widget in widgets:\n                self._cframe.remove_widget(widget)\n            tok = self._parser.stack()[-1]\n            if not isinstance(tok, Tree): raise ValueError()\n            label = TextWidget(self._canvas, str(tok.label()), color='#006060',\n                               font=self._boldfont)\n            widget = TreeSegmentWidget(self._canvas, label, widgets,\n                                       width=2)\n            (x1, y1, x2, y2) = self._stacklabel.bbox()\n            y = y2-y1+10\n            if not self._stackwidgets: x = 5\n            else: x = self._stackwidgets[-1].bbox()[2] + 10\n            self._cframe.add_widget(widget, x, y)\n            self._stackwidgets.append(widget)\n\n            self._draw_stack_top(widget)\n            self._highlight_productions()\n\n\n            self._animating_lock = 0\n\n\n    def _highlight_hover(self, event):\n        index = self._prodlist.nearest(event.y)\n        if self._hover == index: return\n\n        self._clear_hover()\n\n        selection = [int(s) for s in self._prodlist.curselection()]\n        if index in selection:\n            rhslen = len(self._productions[index].rhs())\n            for stackwidget in self._stackwidgets[-rhslen:]:\n                if isinstance(stackwidget, TreeSegmentWidget):\n                    stackwidget.label()['color'] = '#00a000'\n                else:\n                    stackwidget['color'] = '#00a000'\n\n        self._hover = index\n\n    def _clear_hover(self, *event):\n        if self._hover == -1: return\n        self._hover = -1\n        for stackwidget in self._stackwidgets:\n            if isinstance(stackwidget, TreeSegmentWidget):\n                stackwidget.label()['color'] = 'black'\n            else:\n                stackwidget['color'] = 'black'\n\n\ndef app():\n\n    from nltk.grammar import Nonterminal, Production, CFG\n    nonterminals = 'S VP NP PP P N Name V Det'\n    (S, VP, NP, PP, P, N, Name, V, Det) = [Nonterminal(s)\n                                           for s in nonterminals.split()]\n\n    productions = (\n        Production(S, [NP, VP]),\n        Production(NP, [Det, N]),\n        Production(NP, [NP, PP]),\n        Production(VP, [VP, PP]),\n        Production(VP, [V, NP, PP]),\n        Production(VP, [V, NP]),\n        Production(PP, [P, NP]),\n\n        Production(NP, ['I']),   Production(Det, ['the']),\n        Production(Det, ['a']),  Production(N, ['man']),\n        Production(V, ['saw']),  Production(P, ['in']),\n        Production(P, ['with']), Production(N, ['park']),\n        Production(N, ['dog']),  Production(N, ['statue']),\n        Production(Det, ['my']),\n        )\n\n    grammar = CFG(S, productions)\n\n    sent = 'my dog saw a man in the park with a statue'.split()\n\n    ShiftReduceApp(grammar, sent).mainloop()\n\nif __name__ == '__main__':\n    app()\n\n__all__ = ['app']\n"], "nltk\\app\\wordfreq_app": [".py", "\nfrom matplotlib import pylab\nfrom nltk.text import Text\nfrom nltk.corpus import gutenberg\n\ndef plot_word_freq_dist(text):\n    fd = text.vocab()\n\n    samples = [item for item, _ in fd.most_common(50)]\n    values = [fd[sample] for sample in samples]\n    values = [sum(values[:i+1]) * 100.0/fd.N() for i in range(len(values))]\n    pylab.title(text.name)\n    pylab.xlabel(\"Samples\")\n    pylab.ylabel(\"Cumulative Percentage\")\n    pylab.plot(values)\n    pylab.xticks(range(len(samples)), [str(s) for s in samples], rotation=90)\n    pylab.show()\n\ndef app():\n    t1 = Text(gutenberg.words('melville-moby_dick.txt'))\n    plot_word_freq_dist(t1)\n\nif __name__ == '__main__':\n    app()\n\n__all__ = ['app']\n"], "nltk\\app\\wordnet_app": [".py", "\nfrom __future__ import print_function\n\nfrom sys import path\n\nimport os\nimport sys\nfrom sys import argv\nfrom collections import defaultdict\nimport webbrowser\nimport datetime\nimport re\nimport threading\nimport time\nimport getopt\nimport base64\nimport pickle\nimport copy\n\nfrom six.moves.urllib.parse import unquote_plus\n\nfrom nltk import compat\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus.reader.wordnet import Synset, Lemma\n\nif compat.PY3:\n    from http.server import HTTPServer, BaseHTTPRequestHandler\nelse:\n    from BaseHTTPServer import HTTPServer, BaseHTTPRequestHandler\n\n\nfirstClient = True\n\nserver_mode = None\n\nlogfile = None\n\n\nclass MyServerHandler(BaseHTTPRequestHandler):\n\n    def do_HEAD(self):\n        self.send_head()\n\n    def do_GET(self):\n        global firstClient\n        sp = self.path[1:]\n        if unquote_plus(sp) == 'SHUTDOWN THE SERVER':\n            if server_mode:\n                page = \"Server must be killed with SIGTERM.\"\n                type = \"text/plain\"\n            else:\n                print('Server shutting down!')\n                os._exit(0)\n\n        elif sp == '': # First request.\n            type = 'text/html'\n            if not server_mode and firstClient:\n                firstClient = False\n                page = get_static_index_page(True)\n            else:\n                page = get_static_index_page(False)\n            word = 'green'\n\n        elif sp.endswith('.html'): # Trying to fetch a HTML file TODO:\n            type = 'text/html'\n            usp = unquote_plus(sp)\n            if usp == 'NLTK Wordnet Browser Database Info.html':\n                word = '* Database Info *'\n                if os.path.isfile(usp):\n                    with open(usp, 'r') as infile:\n                        page = infile.read()\n                else:\n                    page = (html_header % word) + \\\n                        '<p>The database info file:'\\\n                        '<p><b>' + usp + '</b>' + \\\n                        '<p>was not found. Run this:' + \\\n                        '<p><b>python dbinfo_html.py</b>' + \\\n                        '<p>to produce it.' + html_trailer\n            else:\n                word = sp\n                page = get_static_page_by_path(usp)\n        elif sp.startswith(\"search\"):\n            type = 'text/html'\n            parts = (sp.split(\"?\")[1]).split(\"&\")\n            word = [p.split(\"=\")[1].replace(\"+\", \" \")\n                    for p in parts if p.startswith(\"nextWord\")][0]\n            page, word = page_from_word(word)\n        elif sp.startswith(\"lookup_\"):\n            type = 'text/html'\n            sp = sp[len(\"lookup_\"):]\n            page, word = page_from_href(sp)\n        elif sp == \"start_page\":\n            type = 'text/html'\n            page, word = page_from_word(\"wordnet\")\n        else:\n            type = 'text/plain'\n            page = \"Could not parse request: '%s'\" % sp\n\n        self.send_head(type)\n        self.wfile.write(page.encode('utf8'))\n\n\n    def send_head(self, type=None):\n        self.send_response(200)\n        self.send_header('Content-type', type)\n        self.end_headers()\n\n    def log_message(self, format, *args):\n        global logfile\n\n        if logfile:\n            logfile.write(\n                \"%s - - [%s] %s\\n\" %\n                (self.address_string(),\n                 self.log_date_time_string(),\n                 format%args))\n\n\ndef get_unique_counter_from_url(sp):\n    pos = sp.rfind('%23')\n    if pos != -1:\n        return int(sp[(pos + 3):])\n    else:\n        return None\n\n\ndef wnb(port=8000, runBrowser=True, logfilename=None):\n    global server_mode, logfile\n    server_mode = not runBrowser\n\n    if logfilename:\n        try:\n            logfile = open(logfilename, \"a\", 1) # 1 means 'line buffering'\n        except IOError as e:\n            sys.stderr.write(\"Couldn't open %s for writing: %s\",\n                             logfilename, e)\n            sys.exit(1)\n    else:\n        logfile = None\n\n    url = 'http://localhost:' + str(port)\n\n    server_ready = None\n    browser_thread = None\n\n    if runBrowser:\n        server_ready = threading.Event()\n        browser_thread = startBrowser(url, server_ready)\n\n    server = HTTPServer(('', port), MyServerHandler)\n    if logfile:\n        logfile.write(\n            'NLTK Wordnet browser server running serving: %s\\n' % url)\n    if runBrowser:\n        server_ready.set()\n\n    try:\n        server.serve_forever()\n    except KeyboardInterrupt:\n        pass\n\n    if runBrowser:\n        browser_thread.join()\n\n    if logfile:\n        logfile.close()\n\n\ndef startBrowser(url, server_ready):\n    def run():\n        server_ready.wait()\n        time.sleep(1) # Wait a little bit more, there's still the chance of\n        webbrowser.open(url, new = 2, autoraise = 1)\n    t = threading.Thread(target=run)\n    t.start()\n    return t\n\n\n\n\n\ndef _pos_tuples():\n    return [\n        (wn.NOUN,'N','noun'),\n        (wn.VERB,'V','verb'),\n        (wn.ADJ,'J','adj'),\n        (wn.ADV,'R','adv')]\n\ndef _pos_match(pos_tuple):\n    if pos_tuple[0] == 's':\n        pos_tuple = ('a', pos_tuple[1], pos_tuple[2])\n    for n,x in enumerate(pos_tuple):\n        if x is not None:\n            break\n    for pt in _pos_tuples():\n        if pt[n] == pos_tuple[n]: return pt\n    return None\n\n\nHYPONYM = 0\nHYPERNYM = 1\nCLASS_REGIONAL = 2\nPART_HOLONYM = 3\nPART_MERONYM = 4\nATTRIBUTE = 5\nSUBSTANCE_HOLONYM = 6\nSUBSTANCE_MERONYM = 7\nMEMBER_HOLONYM = 8\nMEMBER_MERONYM = 9\nVERB_GROUP = 10\nINSTANCE_HYPONYM = 12\nINSTANCE_HYPERNYM = 13\nCAUSE = 14\nALSO_SEE = 15\nSIMILAR = 16\nENTAILMENT = 17\nANTONYM = 18\nFRAMES = 19\nPERTAINYM = 20\n\nCLASS_CATEGORY = 21\nCLASS_USAGE = 22\nCLASS_REGIONAL = 23\nCLASS_USAGE = 24\nCLASS_CATEGORY = 11\n\nDERIVATIONALLY_RELATED_FORM = 25\n\nINDIRECT_HYPERNYMS = 26\n\n\ndef lemma_property(word, synset, func):\n\n    def flattern(l):\n        if l == []:\n            return []\n        else:\n            return l[0] + flattern(l[1:])\n\n    return flattern([func(l) for l in synset.lemmas if l.name == word])\n\n\ndef rebuild_tree(orig_tree):\n    node = orig_tree[0]\n    children = orig_tree[1:]\n    return (node, [rebuild_tree(t) for t in children])\n\n\ndef get_relations_data(word, synset):\n    if synset.pos() == wn.NOUN:\n        return ((HYPONYM, 'Hyponyms',\n                   synset.hyponyms()),\n                (INSTANCE_HYPONYM , 'Instance hyponyms',\n                   synset.instance_hyponyms()),\n                (HYPERNYM, 'Direct hypernyms',\n                   synset.hypernyms()),\n                (INDIRECT_HYPERNYMS, 'Indirect hypernyms',\n                   rebuild_tree(synset.tree(lambda x: x.hypernyms()))[1]),\n                (INSTANCE_HYPERNYM , 'Instance hypernyms',\n                   synset.instance_hypernyms()),\n                (PART_HOLONYM, 'Part holonyms',\n                   synset.part_holonyms()),\n                (PART_MERONYM, 'Part meronyms',\n                   synset.part_meronyms()),\n                (SUBSTANCE_HOLONYM, 'Substance holonyms',\n                   synset.substance_holonyms()),\n                (SUBSTANCE_MERONYM, 'Substance meronyms',\n                   synset.substance_meronyms()),\n                (MEMBER_HOLONYM, 'Member holonyms',\n                   synset.member_holonyms()),\n                (MEMBER_MERONYM, 'Member meronyms',\n                   synset.member_meronyms()),\n                (ATTRIBUTE, 'Attributes',\n                   synset.attributes()),\n                (ANTONYM, \"Antonyms\",\n                   lemma_property(word, synset, lambda l: l.antonyms())),\n                (DERIVATIONALLY_RELATED_FORM, \"Derivationally related form\",\n                   lemma_property(word, synset, lambda l: l.derivationally_related_forms())))\n    elif synset.pos() == wn.VERB:\n        return ((ANTONYM, 'Antonym',\n                   lemma_property(word, synset, lambda l: l.antonyms())),\n                (HYPONYM, 'Hyponym',\n                   synset.hyponyms()),\n                (HYPERNYM, 'Direct hypernyms',\n                   synset.hypernyms()),\n                (INDIRECT_HYPERNYMS, 'Indirect hypernyms',\n                   rebuild_tree(synset.tree(lambda x: x.hypernyms()))[1]),\n                (ENTAILMENT, 'Entailments',\n                   synset.entailments()),\n                (CAUSE, 'Causes',\n                   synset.causes()),\n                (ALSO_SEE, 'Also see',\n                   synset.also_sees()),\n                (VERB_GROUP, 'Verb Groups',\n                   synset.verb_groups()),\n                (DERIVATIONALLY_RELATED_FORM, \"Derivationally related form\",\n                   lemma_property(word, synset, lambda l: l.derivationally_related_forms())))\n    elif synset.pos() == wn.ADJ or synset.pos == wn.ADJ_SAT:\n        return ((ANTONYM, 'Antonym',\n                   lemma_property(word, synset, lambda l: l.antonyms())),\n                (SIMILAR, 'Similar to',\n                   synset.similar_tos()),\n                (PERTAINYM, 'Pertainyms',\n                   lemma_property(word, synset, lambda l: l.pertainyms())),\n                (ATTRIBUTE, 'Attributes',\n                   synset.attributes()),\n                (ALSO_SEE, 'Also see',\n                   synset.also_sees()))\n    elif synset.pos() == wn.ADV:\n        return ((ANTONYM, 'Antonym',\n                   lemma_property(word, synset, lambda l: l.antonyms())),)\n    else:\n        raise TypeError(\"Unhandles synset POS type: \" + str(synset.pos()))\n\n\nhtml_header = '''\n<!DOCTYPE html PUBLIC '-//W3C//DTD HTML 4.01//EN'\n'http://www.w3.org/TR/html4/strict.dtd'>\n<html>\n<head>\n<meta name='generator' content=\n'HTML Tidy for Windows (vers 14 February 2006), see www.w3.org'>\n<meta http-equiv='Content-Type' content=\n'text/html; charset=us-ascii'>\n<title>NLTK Wordnet Browser display of: %s</title></head>\n<body bgcolor='#F5F5F5' text='#000000'>\n'''\nhtml_trailer = '''\n</body>\n</html>\n'''\n\nexplanation  = '''\n<h3>Search Help</h3>\n<ul><li>The display below the line is an example of the output the browser\nshows you when you enter a search word. The search word was <b>green</b>.</li>\n<li>The search result shows for different parts of speech the <b>synsets</b>\ni.e. different meanings for the word.</li>\n<li>All underlined texts are hypertext links. There are two types of links:\nword links and others. Clicking a word link carries out a search for the word\nin the Wordnet database.</li>\n<li>Clicking a link of the other type opens a display section of data attached\nto that link. Clicking that link a second time closes the section again.</li>\n<li>Clicking <u>S:</u> opens a section showing the relations for that synset.\n</li>\n<li>Clicking on a relation name opens a section that displays the associated\nsynsets.</li>\n<li>Type a search word in the <b>Word</b> field and start the search by the\n<b>Enter/Return</b> key or click the <b>Search</b> button.</li>\n</ul>\n<hr width='100%'>\n'''\n\n\ndef _bold(txt): return '<b>%s</b>' % txt\n\ndef _center(txt): return '<center>%s</center>' % txt\n\ndef _hlev(n,txt): return '<h%d>%s</h%d>' % (n,txt,n)\n\ndef _italic(txt): return '<i>%s</i>' % txt\n\ndef _li(txt): return '<li>%s</li>' % txt\n\ndef pg(word, body):\n    '''\n    Return a HTML page of NLTK Browser format constructed from the\n    word and body\n\n    :param word: The word that the body corresponds to\n    :type word: str\n    :param body: The HTML body corresponding to the word\n    :type body: str\n    :return: a HTML page for the word-body combination\n    :rtype: str\n    '''\n    return (html_header % word) + body + html_trailer\n\ndef _ul(txt): return '<ul>' + txt + '</ul>'\n\ndef _abbc(txt):\n    return _center(_bold('<br>'*10 + '*'*10 + ' ' + txt + ' ' + '*'*10))\n\nfull_hyponym_cont_text = \\\n    _ul(_li(_italic('(has full hyponym continuation)'))) + '\\n'\n\n\ndef _get_synset(synset_key):\n    return wn.synset(synset_key)\n\ndef _collect_one_synset(word, synset, synset_relations):\n    '''\n    Returns the HTML string for one synset or word\n\n    :param word: the current word\n    :type word: str\n    :param synset: a synset\n    :type synset: synset\n    :param synset_relations: information about which synset relations\n    to display.\n    :type synset_relations: dict(synset_key, set(relation_id))\n    :return: The HTML string built for this synset\n    :rtype: str\n    '''\n    if isinstance(synset, tuple): # It's a word\n        raise NotImplementedError(\"word not supported by _collect_one_synset\")\n\n    typ = 'S'\n    pos_tuple = _pos_match((synset.pos(), None, None))\n    assert pos_tuple is not None, \"pos_tuple is null: synset.pos(): %s\" % synset.pos()\n    descr = pos_tuple[2]\n    ref = copy.deepcopy(Reference(word, synset_relations))\n    ref.toggle_synset(synset)\n    synset_label = typ + \";\"\n    if synset.name() in synset_relations:\n        synset_label = _bold(synset_label)\n    s = '<li>%s (%s) ' % (make_lookup_link(ref, synset_label), descr)\n    def format_lemma(w):\n        w = w.replace('_', ' ')\n        if w.lower() == word:\n            return _bold(w)\n        else:\n            ref = Reference(w)\n            return make_lookup_link(ref, w)\n\n    s += ', '.join(format_lemma(l.name()) for l in synset.lemmas())\n\n    gl = \" (%s) <i>%s</i> \" % \\\n        (synset.definition(),\n         \"; \".join(\"\\\"%s\\\"\" % e for e in synset.examples()))\n    return s + gl + _synset_relations(word, synset, synset_relations) + '</li>\\n'\n\ndef _collect_all_synsets(word, pos, synset_relations=dict()):\n    return '<ul>%s\\n</ul>\\n' % \\\n        ''.join((_collect_one_synset(word, synset, synset_relations)\n                 for synset\n                 in wn.synsets(word, pos)))\n\ndef _synset_relations(word, synset, synset_relations):\n    '''\n    Builds the HTML string for the relations of a synset\n\n    :param word: The current word\n    :type word: str\n    :param synset: The synset for which we're building the relations.\n    :type synset: Synset\n    :param synset_relations: synset keys and relation types for which to display relations.\n    :type synset_relations: dict(synset_key, set(relation_type))\n    :return: The HTML for a synset's relations\n    :rtype: str\n    '''\n\n    if not synset.name() in synset_relations:\n        return \"\"\n    ref = Reference(word, synset_relations)\n\n    def relation_html(r):\n        if isinstance(r, Synset):\n            return make_lookup_link(Reference(r.lemma_names()[0]), r.lemma_names()[0])\n        elif isinstance(r, Lemma):\n            return relation_html(r.synset())\n        elif isinstance(r, tuple):\n            return \"%s\\n<ul>%s</ul>\\n\" % \\\n                (relation_html(r[0]),\n                 ''.join('<li>%s</li>\\n' % relation_html(sr) for sr in r[1]))\n        else:\n            raise TypeError(\"r must be a synset, lemma or list, it was: type(r) = %s, r = %s\" % (type(r), r))\n\n    def make_synset_html(db_name, disp_name, rels):\n        synset_html = '<i>%s</i>\\n' % \\\n            make_lookup_link(\n                copy.deepcopy(ref).toggle_synset_relation(synset, db_name).encode(),\n                disp_name)\n\n        if db_name in ref.synset_relations[synset.name()]:\n             synset_html += '<ul>%s</ul>\\n' % \\\n                ''.join(\"<li>%s</li>\\n\" % relation_html(r) for r in rels)\n\n        return synset_html\n\n    html = '<ul>' + \\\n        '\\n'.join((\"<li>%s</li>\" % make_synset_html(*rel_data) for rel_data\n                   in get_relations_data(word, synset)\n                   if rel_data[2] != [])) + \\\n        '</ul>'\n\n    return html\n\n\nclass Reference(object):\n\n    def __init__(self, word, synset_relations=dict()):\n        self.word = word\n        self.synset_relations = synset_relations\n\n    def encode(self):\n        string = pickle.dumps((self.word, self.synset_relations), -1)\n        return base64.urlsafe_b64encode(string).decode()\n\n    @staticmethod\n    def decode(string):\n        string = base64.urlsafe_b64decode(string.encode())\n        word, synset_relations = pickle.loads(string)\n        return Reference(word, synset_relations)\n\n    def toggle_synset_relation(self, synset, relation):\n        if relation in self.synset_relations[synset.name()]:\n            self.synset_relations[synset.name()].remove(relation)\n        else:\n            self.synset_relations[synset.name()].add(relation)\n\n        return self\n\n    def toggle_synset(self, synset):\n        if synset.name() in self.synset_relations:\n            del self.synset_relations[synset.name()]\n        else:\n            self.synset_relations[synset.name()] = set()\n\n        return self\n\n\ndef make_lookup_link(ref, label):\n    return '<a href=\"lookup_%s\">%s</a>' % (ref.encode(), label)\n\n\ndef page_from_word(word):\n    return page_from_reference(Reference(word))\n\ndef page_from_href(href):\n    '''\n    Returns a tuple of the HTML page built and the new current word\n\n    :param href: The hypertext reference to be solved\n    :type href: str\n    :return: A tuple (page,word), where page is the new current HTML page\n             to be sent to the browser and\n             word is the new current word\n    :rtype: A tuple (str,str)\n    '''\n    return page_from_reference(Reference.decode(href))\n\ndef page_from_reference(href):\n    '''\n    Returns a tuple of the HTML page built and the new current word\n\n    :param href: The hypertext reference to be solved\n    :type href: str\n    :return: A tuple (page,word), where page is the new current HTML page\n             to be sent to the browser and\n             word is the new current word\n    :rtype: A tuple (str,str)\n    '''\n    word = href.word\n    pos_forms = defaultdict(list)\n    words = word.split(',')\n    words = [w for w in [w.strip().lower().replace(' ', '_')\n                         for w in words]\n             if w != \"\"]\n    if len(words) == 0:\n        return \"\", \"Please specify a word to search for.\"\n\n    for w in words:\n        for pos in [wn.NOUN, wn.VERB, wn.ADJ, wn.ADV]:\n            form = wn.morphy(w, pos)\n            if form and form not in pos_forms[pos]:\n                pos_forms[pos].append(form)\n    body = ''\n    for pos,pos_str,name in _pos_tuples():\n        if pos in pos_forms:\n            body += _hlev(3, name) + '\\n'\n            for w in pos_forms[pos]:\n                try:\n                    body += _collect_all_synsets(w, pos, href.synset_relations)\n                except KeyError:\n                    pass\n    if not body:\n        body = \"The word or words '%s' where not found in the dictonary.\" % word\n    return body, word\n\n\n\ndef get_static_page_by_path(path):\n    if path == \"index_2.html\":\n        return get_static_index_page(False)\n    elif path == \"index.html\":\n        return get_static_index_page(True)\n    elif path == \"NLTK Wordnet Browser Database Info.html\":\n        return \"Display of Wordnet Database Statistics is not supported\"\n    elif path == \"upper_2.html\":\n        return get_static_upper_page(False)\n    elif path == \"upper.html\":\n        return get_static_upper_page(True)\n    elif path == \"web_help.html\":\n        return get_static_web_help_page()\n    elif path == \"wx_help.html\":\n        return get_static_wx_help_page()\n    else:\n        return \"Internal error: Path for static page '%s' is unknown\" % path\n\n\ndef get_static_web_help_page():\n    return \\\n\n\ndef get_static_welcome_message():\n    return \\\n\ndef get_static_index_page(with_shutdown):\n    template = \\\n    if with_shutdown:\n        upper_link = \"upper.html\"\n    else:\n        upper_link = \"upper_2.html\"\n\n    return template % upper_link\n\n\ndef get_static_upper_page(with_shutdown):\n    template = \\\n    if with_shutdown:\n        shutdown_link = \"<a href=\\\"SHUTDOWN THE SERVER\\\">Shutdown</a>\"\n    else:\n        shutdown_link = \"\"\n\n    return template % shutdown_link\n\n\n\ndef usage():\n    print(__doc__)\n\ndef app():\n    (opts, _) = getopt.getopt(argv[1:], \"l:p:sh\",\n                              [\"logfile=\", \"port=\", \"server-mode\", \"help\"])\n    port = 8000\n    server_mode = False\n    help_mode = False\n    logfilename = None\n    for (opt, value) in opts:\n        if (opt == \"-l\") or (opt == \"--logfile\"):\n            logfilename = str(value)\n        elif (opt == \"-p\") or (opt == \"--port\"):\n            port = int(value)\n        elif (opt == \"-s\") or (opt == \"--server-mode\"):\n            server_mode = True\n        elif (opt == \"-h\") or (opt == \"--help\"):\n            help_mode = True\n\n    if help_mode:\n        usage()\n    else:\n        wnb(port, not server_mode, logfilename)\n\nif __name__ == '__main__':\n    app()\n\n__all__ = ['app']\n"], "nltk\\app\\__init__": [".py", "\n\n\ntry:\n    from six.moves import tkinter\nexcept ImportError:\n    import warnings\n    warnings.warn(\"nltk.app package not loaded \"\n                  \"(please install Tkinter library).\")\nelse:\n    from nltk.app.chartparser_app import app as chartparser\n    from nltk.app.chunkparser_app import app as chunkparser\n    from nltk.app.collocations_app import app as collocations\n    from nltk.app.concordance_app import app as concordance\n    from nltk.app.nemo_app import app as nemo\n    from nltk.app.rdparser_app import app as rdparser\n    from nltk.app.srparser_app import app as srparser\n    from nltk.app.wordnet_app import app as wordnet\n\n    try:\n        from matplotlib import pylab\n    except ImportError:\n        import warnings\n        warnings.warn(\"nltk.app.wordfreq not loaded \"\n                      \"(requires the matplotlib library).\")\n    else:\n        from nltk.app.wordfreq_app import app as wordfreq\n\ndef setup_module(module):\n    from nose import SkipTest\n    raise SkipTest(\"nltk.app examples are not doctests\")\n", 1], "nltk\\book": [".py", "from __future__ import print_function\n\nfrom nltk.corpus import (gutenberg, genesis, inaugural,\n                         nps_chat, webtext, treebank, wordnet)\nfrom nltk.text import Text\nfrom nltk.probability import FreqDist\nfrom nltk.util import bigrams\n\nprint(\"*** Introductory Examples for the NLTK Book ***\")\nprint(\"Loading text1, ..., text9 and sent1, ..., sent9\")\nprint(\"Type the name of the text or sentence to view it.\")\nprint(\"Type: 'texts()' or 'sents()' to list the materials.\")\n\ntext1 = Text(gutenberg.words('melville-moby_dick.txt'))\nprint(\"text1:\", text1.name)\n\ntext2 = Text(gutenberg.words('austen-sense.txt'))\nprint(\"text2:\", text2.name)\n\ntext3 = Text(genesis.words('english-kjv.txt'), name=\"The Book of Genesis\")\nprint(\"text3:\", text3.name)\n\ntext4 = Text(inaugural.words(), name=\"Inaugural Address Corpus\")\nprint(\"text4:\", text4.name)\n\ntext5 = Text(nps_chat.words(), name=\"Chat Corpus\")\nprint(\"text5:\", text5.name)\n\ntext6 = Text(webtext.words('grail.txt'),\n             name=\"Monty Python and the Holy Grail\")\nprint(\"text6:\", text6.name)\n\ntext7 = Text(treebank.words(), name=\"Wall Street Journal\")\nprint(\"text7:\", text7.name)\n\ntext8 = Text(webtext.words('singles.txt'), name=\"Personals Corpus\")\nprint(\"text8:\", text8.name)\n\ntext9 = Text(gutenberg.words('chesterton-thursday.txt'))\nprint(\"text9:\", text9.name)\n\n\ndef texts():\n    print(\"text1:\", text1.name)\n    print(\"text2:\", text2.name)\n    print(\"text3:\", text3.name)\n    print(\"text4:\", text4.name)\n    print(\"text5:\", text5.name)\n    print(\"text6:\", text6.name)\n    print(\"text7:\", text7.name)\n    print(\"text8:\", text8.name)\n    print(\"text9:\", text9.name)\n\nsent1 = [\"Call\", \"me\", \"Ishmael\", \".\"]\nsent2 = [\"The\", \"family\", \"of\", \"Dashwood\", \"had\", \"long\",\n         \"been\", \"settled\", \"in\", \"Sussex\", \".\"]\nsent3 = [\"In\", \"the\", \"beginning\", \"God\", \"created\", \"the\",\n         \"heaven\", \"and\", \"the\", \"earth\", \".\"]\nsent4 = [\"Fellow\", \"-\", \"Citizens\", \"of\", \"the\", \"Senate\",\n         \"and\", \"of\", \"the\", \"House\", \"of\", \"Representatives\", \":\"]\nsent5 = [\"I\", \"have\", \"a\", \"problem\", \"with\", \"people\",\n         \"PMing\", \"me\", \"to\", \"lol\", \"JOIN\"]\nsent6 = ['SCENE', '1', ':', '[', 'wind', ']', '[', 'clop', 'clop',\n         'clop', ']', 'KING', 'ARTHUR', ':', 'Whoa', 'there', '!']\nsent7 = [\"Pierre\", \"Vinken\", \",\", \"61\", \"years\", \"old\", \",\",\n         \"will\", \"join\", \"the\", \"board\", \"as\", \"a\", \"nonexecutive\",\n         \"director\", \"Nov.\", \"29\", \".\"]\nsent8 = ['25', 'SEXY', 'MALE', ',', 'seeks', 'attrac', 'older',\n         'single', 'lady', ',', 'for', 'discreet', 'encounters', '.']\nsent9 = [\"THE\", \"suburb\", \"of\", \"Saffron\", \"Park\", \"lay\", \"on\", \"the\",\n         \"sunset\", \"side\", \"of\", \"London\", \",\", \"as\", \"red\", \"and\",\n         \"ragged\", \"as\", \"a\", \"cloud\", \"of\", \"sunset\", \".\"]\n\n\ndef sents():\n    print(\"sent1:\", \" \".join(sent1))\n    print(\"sent2:\", \" \".join(sent2))\n    print(\"sent3:\", \" \".join(sent3))\n    print(\"sent4:\", \" \".join(sent4))\n    print(\"sent5:\", \" \".join(sent5))\n    print(\"sent6:\", \" \".join(sent6))\n    print(\"sent7:\", \" \".join(sent7))\n    print(\"sent8:\", \" \".join(sent8))\n    print(\"sent9:\", \" \".join(sent9))\n"], "nltk\\ccg\\api": [".py", "from __future__ import unicode_literals\nfrom functools import total_ordering\n\nfrom abc import ABCMeta, abstractmethod\nfrom six import add_metaclass\n\nfrom nltk.internals import raise_unorderable_types\nfrom nltk.compat import (python_2_unicode_compatible, unicode_repr)\n\n\n@add_metaclass(ABCMeta)\n@total_ordering\nclass AbstractCCGCategory(object):\n    '''\n    Interface for categories in combinatory grammars.\n    '''\n\n    @abstractmethod\n    def is_primitive(self):\n\n    @abstractmethod\n    def is_function(self):\n\n    @abstractmethod\n    def is_var(self):\n\n    @abstractmethod\n    def substitute(self, substitutions):\n\n    @abstractmethod\n    def can_unify(self, other):\n\n    @abstractmethod\n    def __str__(self):\n        pass\n\n    def __eq__(self, other):\n        return (self.__class__ is other.__class__ and\n                self._comparison_key == other._comparison_key)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __lt__(self, other):\n        if not isinstance(other, AbstractCCGCategory):\n            raise_unorderable_types(\"<\", self, other)\n        if self.__class__ is other.__class__:\n            return self._comparison_key < other._comparison_key\n        else:\n            return self.__class__.__name__ < other.__class__.__name__\n\n    def __hash__(self):\n        try:\n            return self._hash\n        except AttributeError:\n            self._hash = hash(self._comparison_key)\n            return self._hash\n\n\n@python_2_unicode_compatible\nclass CCGVar(AbstractCCGCategory):\n    '''\n    Class representing a variable CCG category.\n    Used for conjunctions (and possibly type-raising, if implemented as a\n    unary rule).\n    '''\n    _maxID = 0\n\n    def __init__(self, prim_only=False):\n        self._id = self.new_id()\n        self._prim_only = prim_only\n        self._comparison_key = self._id\n\n    @classmethod\n    def new_id(cls):\n        cls._maxID = cls._maxID + 1\n        return cls._maxID - 1\n\n    @classmethod\n    def reset_id(cls):\n        cls._maxID = 0\n\n    def is_primitive(self):\n        return False\n\n    def is_function(self):\n        return False\n\n    def is_var(self):\n        return True\n\n    def substitute(self, substitutions):\n        for (var, cat) in substitutions:\n            if var == self:\n                return cat\n        return self\n\n    def can_unify(self, other):\n        if other.is_primitive() or not self._prim_only:\n            return [(self, other)]\n        return None\n\n    def id(self):\n        return self._id\n\n    def __str__(self):\n        return \"_var\" + str(self._id)\n\n\n@total_ordering\n@python_2_unicode_compatible\nclass Direction(object):\n    '''\n    Class representing the direction of a function application.\n    Also contains maintains information as to which combinators\n    may be used with the category.\n    '''\n    def __init__(self, dir, restrictions):\n        self._dir = dir\n        self._restrs = restrictions\n        self._comparison_key = (dir, tuple(restrictions))\n\n    def is_forward(self):\n        return self._dir == '/'\n\n    def is_backward(self):\n        return self._dir == '\\\\'\n\n    def dir(self):\n        return self._dir\n\n    def restrs(self):\n        return self._restrs\n\n    def is_variable(self):\n        return self._restrs == '_'\n\n    def can_unify(self, other):\n        if other.is_variable():\n            return [('_', self.restrs())]\n        elif self.is_variable():\n            return [('_', other.restrs())]\n        else:\n            if self.restrs() == other.restrs():\n                return []\n        return None\n\n    def substitute(self, subs):\n        if not self.is_variable():\n            return self\n\n        for (var, restrs) in subs:\n            if var == '_':\n                return Direction(self._dir, restrs)\n        return self\n\n    def can_compose(self):\n        return (',' not in self._restrs)\n\n    def can_cross(self):\n        return ('.' not in self._restrs)\n\n    def __eq__(self, other):\n        return (self.__class__ is other.__class__ and\n                self._comparison_key == other._comparison_key)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __lt__(self, other):\n        if not isinstance(other, Direction):\n            raise_unorderable_types(\"<\", self, other)\n        if self.__class__ is other.__class__:\n            return self._comparison_key < other._comparison_key\n        else:\n            return self.__class__.__name__ < other.__class__.__name__\n\n    def __hash__(self):\n        try:\n            return self._hash\n        except AttributeError:\n            self._hash = hash(self._comparison_key)\n            return self._hash\n\n    def __str__(self):\n        r_str = \"\"\n        for r in self._restrs:\n            r_str = r_str + \"%s\" % r\n        return \"%s%s\" % (self._dir, r_str)\n\n    def __neg__(self):\n        if self._dir == '/':\n            return Direction('\\\\', self._restrs)\n        else:\n            return Direction('/', self._restrs)\n\n\n@python_2_unicode_compatible\nclass PrimitiveCategory(AbstractCCGCategory):\n    '''\n    Class representing primitive categories.\n    Takes a string representation of the category, and a\n    list of strings specifying the morphological subcategories.\n    '''\n    def __init__(self, categ, restrictions=[]):\n        self._categ = categ\n        self._restrs = restrictions\n        self._comparison_key = (categ, tuple(restrictions))\n\n    def is_primitive(self):\n        return True\n\n    def is_function(self):\n        return False\n\n    def is_var(self):\n        return False\n\n    def restrs(self):\n        return self._restrs\n\n    def categ(self):\n        return self._categ\n\n    def substitute(self, subs):\n        return self\n\n    def can_unify(self, other):\n        if not other.is_primitive():\n            return None\n        if other.is_var():\n            return [(other, self)]\n        if other.categ() == self.categ():\n            for restr in self._restrs:\n                if restr not in other.restrs():\n                    return None\n            return []\n        return None\n\n    def __str__(self):\n        if self._restrs == []:\n            return \"%s\" % self._categ\n        restrictions = \"[%s]\" % \",\".join(unicode_repr(r) for r in self._restrs)\n        return \"%s%s\" % (self._categ, restrictions)\n\n\n@python_2_unicode_compatible\nclass FunctionalCategory(AbstractCCGCategory):\n    '''\n    Class that represents a function application category.\n    Consists of argument and result categories, together with\n    an application direction.\n    '''\n    def __init__(self, res, arg, dir):\n        self._res = res\n        self._arg = arg\n        self._dir = dir\n        self._comparison_key = (arg, dir, res)\n\n    def is_primitive(self):\n        return False\n\n    def is_function(self):\n        return True\n\n    def is_var(self):\n        return False\n\n    def substitute(self, subs):\n        sub_res = self._res.substitute(subs)\n        sub_dir = self._dir.substitute(subs)\n        sub_arg = self._arg.substitute(subs)\n        return FunctionalCategory(sub_res, sub_arg, self._dir)\n\n    def can_unify(self, other):\n        if other.is_var():\n            return [(other, self)]\n        if other.is_function():\n            sa = self._res.can_unify(other.res())\n            sd = self._dir.can_unify(other.dir())\n            if sa is not None and sd is not None:\n                sb = self._arg.substitute(sa).can_unify(\n                    other.arg().substitute(sa))\n                if sb is not None:\n                    return sa + sb\n        return None\n\n    def arg(self):\n        return self._arg\n\n    def res(self):\n        return self._res\n\n    def dir(self):\n        return self._dir\n\n    def __str__(self):\n        return \"(%s%s%s)\" % (self._res, self._dir, self._arg)\n"], "nltk\\ccg\\chart": [".py", "\nfrom __future__ import print_function, division, unicode_literals\n\nimport itertools\n\nfrom six import string_types\n\nfrom nltk.parse import ParserI\nfrom nltk.parse.chart import AbstractChartRule, EdgeI, Chart\nfrom nltk.tree import Tree\n\nfrom nltk.ccg.lexicon import fromstring, Token\nfrom nltk.ccg.combinator import (ForwardT, BackwardT, ForwardApplication,\n                                 BackwardApplication, ForwardComposition,\n                                 BackwardComposition, ForwardSubstitution,\n                                 BackwardBx, BackwardSx)\nfrom nltk.compat import python_2_unicode_compatible\nfrom nltk.ccg.combinator import *\nfrom nltk.ccg.logic import *\nfrom nltk.sem.logic import *\n\nclass CCGEdge(EdgeI):\n    def __init__(self, span, categ, rule):\n        self._span = span\n        self._categ = categ\n        self._rule = rule\n        self._comparison_key = (span, categ, rule)\n\n    def lhs(self): return self._categ\n    def span(self): return self._span\n    def start(self): return self._span[0]\n    def end(self): return self._span[1]\n    def length(self): return self._span[1] - self.span[0]\n    def rhs(self): return ()\n    def dot(self): return 0\n    def is_complete(self): return True\n    def is_incomplete(self): return False\n    def nextsym(self): return None\n\n    def categ(self): return self._categ\n    def rule(self): return self._rule\n\nclass CCGLeafEdge(EdgeI):\n    '''\n    Class representing leaf edges in a CCG derivation.\n    '''\n    def __init__(self, pos, token, leaf):\n        self._pos = pos\n        self._token = token\n        self._leaf = leaf\n        self._comparison_key = (pos, token.categ(), leaf)\n\n    def lhs(self): return self._token.categ()\n    def span(self): return (self._pos, self._pos+1)\n    def start(self): return self._pos\n    def end(self): return self._pos + 1\n    def length(self): return 1\n    def rhs(self): return self._leaf\n    def dot(self): return 0\n    def is_complete(self): return True\n    def is_incomplete(self): return False\n    def nextsym(self): return None\n\n    def token(self): return self._token\n    def categ(self): return self._token.categ()\n    def leaf(self): return self._leaf\n\n@python_2_unicode_compatible\nclass BinaryCombinatorRule(AbstractChartRule):\n    '''\n    Class implementing application of a binary combinator to a chart.\n    Takes the directed combinator to apply.\n    '''\n    NUMEDGES = 2\n    def __init__(self,combinator):\n        self._combinator = combinator\n\n    def apply(self, chart, grammar, left_edge, right_edge):\n        if not (left_edge.end() == right_edge.start()):\n            return\n\n        if self._combinator.can_combine(left_edge.categ(),right_edge.categ()):\n            for res in self._combinator.combine(left_edge.categ(), right_edge.categ()):\n                new_edge = CCGEdge(span=(left_edge.start(), right_edge.end()),categ=res,rule=self._combinator)\n                if chart.insert(new_edge,(left_edge,right_edge)):\n                    yield new_edge\n\n    def __str__(self):\n        return \"%s\" % self._combinator\n\n@python_2_unicode_compatible\nclass ForwardTypeRaiseRule(AbstractChartRule):\n    '''\n    Class for applying forward type raising\n    '''\n    NUMEDGES = 2\n\n    def __init__(self):\n       self._combinator = ForwardT\n    def apply(self, chart, grammar, left_edge, right_edge):\n        if not (left_edge.end() == right_edge.start()):\n            return\n\n        for res in self._combinator.combine(left_edge.categ(), right_edge.categ()):\n            new_edge = CCGEdge(span=left_edge.span(),categ=res,rule=self._combinator)\n            if chart.insert(new_edge,(left_edge,)):\n                yield new_edge\n\n    def __str__(self):\n        return \"%s\" % self._combinator\n\n@python_2_unicode_compatible\nclass BackwardTypeRaiseRule(AbstractChartRule):\n    '''\n    Class for applying backward type raising.\n    '''\n    NUMEDGES = 2\n\n    def __init__(self):\n       self._combinator = BackwardT\n    def apply(self, chart, grammar, left_edge, right_edge):\n        if not (left_edge.end() == right_edge.start()):\n            return\n\n        for res in self._combinator.combine(left_edge.categ(), right_edge.categ()):\n            new_edge = CCGEdge(span=right_edge.span(),categ=res,rule=self._combinator)\n            if chart.insert(new_edge,(right_edge,)):\n                yield new_edge\n\n    def __str__(self):\n        return \"%s\" % self._combinator\n\n\nApplicationRuleSet = [BinaryCombinatorRule(ForwardApplication),\n                        BinaryCombinatorRule(BackwardApplication)]\nCompositionRuleSet = [BinaryCombinatorRule(ForwardComposition),\n                        BinaryCombinatorRule(BackwardComposition),\n                        BinaryCombinatorRule(BackwardBx)]\nSubstitutionRuleSet = [BinaryCombinatorRule(ForwardSubstitution),\n                        BinaryCombinatorRule(BackwardSx)]\nTypeRaiseRuleSet = [ForwardTypeRaiseRule(), BackwardTypeRaiseRule()]\n\nDefaultRuleSet = ApplicationRuleSet + CompositionRuleSet + \\\n                    SubstitutionRuleSet + TypeRaiseRuleSet\n\nclass CCGChartParser(ParserI):\n    '''\n    Chart parser for CCGs.\n    Based largely on the ChartParser class from NLTK.\n    '''\n    def __init__(self, lexicon, rules, trace=0):\n        self._lexicon = lexicon\n        self._rules = rules\n        self._trace = trace\n\n    def lexicon(self):\n        return self._lexicon\n\n    def parse(self, tokens):\n        tokens = list(tokens)\n        chart = CCGChart(list(tokens))\n        lex = self._lexicon\n\n        for index in range(chart.num_leaves()):\n            for token in lex.categories(chart.leaf(index)):\n                new_edge = CCGLeafEdge(index, token, chart.leaf(index))\n                chart.insert(new_edge, ())\n\n\n        for span in range(2,chart.num_leaves()+1):\n            for start in range(0,chart.num_leaves()-span+1):\n                for part in range(1,span):\n                    lstart = start\n                    mid = start + part\n                    rend = start + span\n\n                    for left in chart.select(span=(lstart,mid)):\n                        for right in chart.select(span=(mid,rend)):\n                            for rule in self._rules:\n                                edges_added_by_rule = 0\n                                for newedge in rule.apply(chart,lex,left,right):\n                                    edges_added_by_rule += 1\n\n        return chart.parses(lex.start())\n\nclass CCGChart(Chart):\n    def __init__(self, tokens):\n        Chart.__init__(self, tokens)\n\n    def _trees(self, edge, complete, memo, tree_class):\n        assert complete, \"CCGChart cannot build incomplete trees\"\n\n        if edge in memo:\n            return memo[edge]\n\n        if isinstance(edge,CCGLeafEdge):\n            word = tree_class(edge.token(), [self._tokens[edge.start()]])\n            leaf = tree_class((edge.token(), \"Leaf\"), [word])\n            memo[edge] = [leaf]\n            return [leaf]\n\n        memo[edge] = []\n        trees = []\n\n        for cpl in self.child_pointer_lists(edge):\n            child_choices = [self._trees(cp, complete, memo, tree_class)\n                             for cp in cpl]\n            for children in itertools.product(*child_choices):\n                lhs = (Token(self._tokens[edge.start():edge.end()], edge.lhs(), compute_semantics(children, edge)), str(edge.rule()))\n                trees.append(tree_class(lhs, children))\n\n        memo[edge] = trees\n        return trees\n\n\ndef compute_semantics(children, edge):\n    if children[0].label()[0].semantics() is None:\n        return None\n\n    if len(children) is 2:\n        if isinstance(edge.rule(), BackwardCombinator):\n            children = [children[1],children[0]]\n\n        combinator = edge.rule()._combinator\n        function = children[0].label()[0].semantics()\n        argument = children[1].label()[0].semantics()\n\n        if isinstance(combinator, UndirectedFunctionApplication):\n            return compute_function_semantics(function, argument)\n        elif isinstance(combinator, UndirectedComposition):\n            return compute_composition_semantics(function, argument)\n        elif isinstance(combinator, UndirectedSubstitution):\n            return compute_substitution_semantics(function, argument)\n        else:\n            raise AssertionError('Unsupported combinator \\'' + combinator + '\\'')\n    else:\n        return compute_type_raised_semantics(children[0].label()[0].semantics())\n\ndef printCCGDerivation(tree):\n    leafcats = tree.pos()\n    leafstr = ''\n    catstr = ''\n\n    for (leaf, cat) in leafcats:\n        str_cat = \"%s\" % cat\n        nextlen = 2 + max(len(leaf), len(str_cat))\n        lcatlen = (nextlen - len(str_cat)) // 2\n        rcatlen = lcatlen + (nextlen - len(str_cat)) % 2\n        catstr += ' '*lcatlen + str_cat + ' '*rcatlen\n        lleaflen = (nextlen - len(leaf)) // 2\n        rleaflen = lleaflen + (nextlen - len(leaf)) % 2\n        leafstr += ' '*lleaflen + leaf + ' '*rleaflen\n    print(leafstr.rstrip())\n    print(catstr.rstrip())\n\n    printCCGTree(0,tree)\n\ndef printCCGTree(lwidth,tree):\n    rwidth = lwidth\n\n    if not isinstance(tree, Tree):\n        return 2 + lwidth + len(tree)\n\n    for child in tree:\n        rwidth = max(rwidth, printCCGTree(rwidth,child))\n\n    if not isinstance(tree.label(), tuple):\n        return max(rwidth,2 + lwidth + len(\"%s\" % tree.label()),\n                  2 + lwidth + len(tree[0]))\n\n    (token, op) = tree.label()\n\n    if op == 'Leaf':\n        return rwidth\n\n    print(lwidth*' ' + (rwidth-lwidth)*'-' + \"%s\" % op)\n    str_res = \"%s\" % (token.categ())\n    if token.semantics() is not None:\n        str_res += \" {\" + str(token.semantics()) + \"}\"\n    respadlen = (rwidth - lwidth - len(str_res)) // 2 + lwidth\n    print(respadlen*' ' + str_res)\n    return rwidth\n\n\nlex = fromstring('''\n    :- S, NP, N, VP    # Primitive categories, S is the target primitive\n\n    Det :: NP/N         # Family of words\n    Pro :: NP\n    TV :: VP/NP\n    Modal :: (S\\\\NP)/VP # Backslashes need to be escaped\n\n    I => Pro             # Word -> Category mapping\n    you => Pro\n\n    the => Det\n\n    and => var\\\\.,var/.,var\n\n    which => (N\\\\N)/(S/NP)\n\n    will => Modal # Categories can be either explicit, or families.\n    might => Modal\n\n    cook => TV\n    eat => TV\n\n    mushrooms => N\n    parsnips => N\n    bacon => N\n    ''')\n\ndef demo():\n    parser = CCGChartParser(lex, DefaultRuleSet)\n    for parse in parser.parse(\"I might cook and eat the bacon\".split()):\n        printCCGDerivation(parse)\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\ccg\\combinator": [".py", "\nfrom __future__ import unicode_literals\nfrom abc import ABCMeta, abstractmethod\nfrom six import add_metaclass\n\nfrom nltk.compat import python_2_unicode_compatible\nfrom nltk.ccg.api import FunctionalCategory\n\n\n@add_metaclass(ABCMeta)\nclass UndirectedBinaryCombinator(object):\n    @abstractmethod\n    def can_combine(self, function, argument):\n        pass\n\n    @abstractmethod\n    def combine(self, function, argument):\n        pass\n\n\n@add_metaclass(ABCMeta)\nclass DirectedBinaryCombinator(object):\n    @abstractmethod\n    def can_combine(self, left, right):\n        pass\n\n    @abstractmethod\n    def combine(self, left, right):\n        pass\n\n\n@python_2_unicode_compatible\nclass ForwardCombinator(DirectedBinaryCombinator):\n    def __init__(self, combinator, predicate, suffix=''):\n        self._combinator = combinator\n        self._predicate = predicate\n        self._suffix = suffix\n\n    def can_combine(self, left, right):\n        return (self._combinator.can_combine(left, right) and\n                self._predicate(left, right))\n\n    def combine(self, left, right):\n        for cat in self._combinator.combine(left, right):\n            yield cat\n\n    def __str__(self):\n        return \">%s%s\" % (self._combinator, self._suffix)\n\n\n@python_2_unicode_compatible\nclass BackwardCombinator(DirectedBinaryCombinator):\n    def __init__(self, combinator, predicate, suffix=''):\n        self._combinator = combinator\n        self._predicate = predicate\n        self._suffix = suffix\n\n    def can_combine(self, left, right):\n        return (self._combinator.can_combine(right, left) and\n                self._predicate(left, right))\n\n    def combine(self, left, right):\n        for cat in self._combinator.combine(right, left):\n            yield cat\n\n    def __str__(self):\n        return \"<%s%s\" % (self._combinator, self._suffix)\n\n\n@python_2_unicode_compatible\nclass UndirectedFunctionApplication(UndirectedBinaryCombinator):\n\n    def can_combine(self, function, argument):\n        if not function.is_function():\n            return False\n\n        return not function.arg().can_unify(argument) is None\n\n    def combine(self, function, argument):\n        if not function.is_function():\n            return\n\n        subs = function.arg().can_unify(argument)\n        if subs is None:\n            return\n\n        yield function.res().substitute(subs)\n\n    def __str__(self):\n        return ''\n\n\n\ndef forwardOnly(left, right):\n    return left.dir().is_forward()\n\n\ndef backwardOnly(left, right):\n    return right.dir().is_backward()\n\n\nForwardApplication = ForwardCombinator(UndirectedFunctionApplication(),\n                                       forwardOnly)\nBackwardApplication = BackwardCombinator(UndirectedFunctionApplication(),\n                                         backwardOnly)\n\n\n@python_2_unicode_compatible\nclass UndirectedComposition(UndirectedBinaryCombinator):\n    def can_combine(self, function, argument):\n        if not (function.is_function() and argument.is_function()):\n            return False\n        if function.dir().can_compose() and argument.dir().can_compose():\n            return not function.arg().can_unify(argument.res()) is None\n        return False\n\n    def combine(self, function, argument):\n        if not (function.is_function() and argument.is_function()):\n            return\n        if function.dir().can_compose() and argument.dir().can_compose():\n            subs = function.arg().can_unify(argument.res())\n            if subs is not None:\n                yield FunctionalCategory(function.res().substitute(subs),\n                                         argument.arg().substitute(subs),\n                                         argument.dir())\n\n    def __str__(self):\n        return 'B'\n\n\ndef bothForward(left, right):\n    return left.dir().is_forward() and right.dir().is_forward()\n\n\ndef bothBackward(left, right):\n    return left.dir().is_backward() and right.dir().is_backward()\n\n\ndef crossedDirs(left, right):\n    return left.dir().is_forward() and right.dir().is_backward()\n\n\ndef backwardBxConstraint(left, right):\n    if not crossedDirs(left, right):\n        return False\n    if not left.dir().can_cross() and right.dir().can_cross():\n        return False\n    return left.arg().is_primitive()\n\n\nForwardComposition = ForwardCombinator(UndirectedComposition(),\n                                       forwardOnly)\nBackwardComposition = BackwardCombinator(UndirectedComposition(),\n                                         backwardOnly)\n\nBackwardBx = BackwardCombinator(UndirectedComposition(), backwardBxConstraint,\n                                suffix='x')\n\n\n@python_2_unicode_compatible\nclass UndirectedSubstitution(UndirectedBinaryCombinator):\n    def can_combine(self, function, argument):\n        if function.is_primitive() or argument.is_primitive():\n            return False\n\n        if function.res().is_primitive():\n            return False\n        if not function.arg().is_primitive():\n            return False\n\n        if not (function.dir().can_compose() and argument.dir().can_compose()):\n            return False\n        return (function.res().arg() == argument.res()) and (function.arg() == argument.arg())\n\n    def combine(self, function, argument):\n        if self.can_combine(function, argument):\n            yield FunctionalCategory(function.res().res(), argument.arg(),\n                                     argument.dir())\n\n    def __str__(self):\n        return 'S'\n\n\ndef forwardSConstraint(left, right):\n    if not bothForward(left, right):\n        return False\n    return left.res().dir().is_forward() and left.arg().is_primitive()\n\n\ndef backwardSxConstraint(left, right):\n    if not left.dir().can_cross() and right.dir().can_cross():\n        return False\n    if not bothForward(left, right):\n        return False\n    return right.res().dir().is_backward() and right.arg().is_primitive()\n\n\nForwardSubstitution = ForwardCombinator(UndirectedSubstitution(),\n                                        forwardSConstraint)\nBackwardSx = BackwardCombinator(UndirectedSubstitution(),\n                                backwardSxConstraint, 'x')\n\n\ndef innermostFunction(categ):\n    while categ.res().is_function():\n        categ = categ.res()\n    return categ\n\n\n@python_2_unicode_compatible\nclass UndirectedTypeRaise(UndirectedBinaryCombinator):\n    def can_combine(self, function, arg):\n        if not (arg.is_function() and arg.res().is_function()):\n            return False\n\n        arg = innermostFunction(arg)\n\n        subs = left.can_unify(arg_categ.arg())\n        if subs is not None:\n            return True\n        return False\n\n    def combine(self, function, arg):\n        if not (function.is_primitive() and\n                arg.is_function() and arg.res().is_function()):\n            return\n\n        arg = innermostFunction(arg)\n\n        subs = function.can_unify(arg.arg())\n        if subs is not None:\n            xcat = arg.res().substitute(subs)\n            yield FunctionalCategory(xcat,\n                                     FunctionalCategory(xcat, function,\n                                                        arg.dir()),\n                                     -(arg.dir()))\n\n    def __str__(self):\n        return 'T'\n\n\ndef forwardTConstraint(left, right):\n    arg = innermostFunction(right)\n    return arg.dir().is_backward() and arg.res().is_primitive()\n\n\ndef backwardTConstraint(left, right):\n    arg = innermostFunction(left)\n    return arg.dir().is_forward() and arg.res().is_primitive()\n\n\nForwardT = ForwardCombinator(UndirectedTypeRaise(), forwardTConstraint)\nBackwardT = BackwardCombinator(UndirectedTypeRaise(), backwardTConstraint)\n"], "nltk\\ccg\\lexicon": [".py", "\nfrom __future__ import unicode_literals\n\nimport re\nfrom collections import defaultdict\n\nfrom nltk.ccg.api import PrimitiveCategory, Direction, CCGVar, FunctionalCategory\nfrom nltk.compat import python_2_unicode_compatible\nfrom nltk.internals import deprecated\n\nfrom nltk.sem.logic import *\n\n\nPRIM_RE = re.compile(r'''([A-Za-z]+)(\\[[A-Za-z,]+\\])?''')\n\nNEXTPRIM_RE = re.compile(r'''([A-Za-z]+(?:\\[[A-Za-z,]+\\])?)(.*)''')\n\nAPP_RE = re.compile(r'''([\\\\/])([.,]?)([.,]?)(.*)''')\n\nLEX_RE = re.compile(r'''([\\S_]+)\\s*(::|[-=]+>)\\s*(.+)''', re.UNICODE)\n\nRHS_RE = re.compile(r'''([^{}]*[^ {}])\\s*(\\{[^}]+\\})?''', re.UNICODE)\n\nSEMANTICS_RE = re.compile(r'''\\{([^}]+)\\}''', re.UNICODE)\n\nCOMMENTS_RE = re.compile('''([^#]*)(?:#.*)?''')\n\nclass Token(object):\n    def __init__(self, token, categ, semantics=None):\n        self._token = token\n        self._categ = categ\n        self._semantics = semantics\n        \n    def categ(self):\n        return self._categ\n    \n    def semantics(self):\n        return self._semantics\n        \n    def __str__(self):\n        semantics_str = \"\"\n        if self._semantics is not None:\n            semantics_str = \" {\" + str(self._semantics) + \"}\"\n        return \"\" + str(self._categ) + semantics_str\n    \n    def __cmp__(self, other):\n        if not isinstance(other, Token): return -1\n        return cmp((self._categ,self._semantics),\n                    other.categ(),other.semantics())\n\n@python_2_unicode_compatible\nclass CCGLexicon(object):\n    def __init__(self, start, primitives, families, entries):\n        self._start = PrimitiveCategory(start)\n        self._primitives = primitives\n        self._families = families\n        self._entries = entries\n\n\n    def categories(self, word):\n        return self._entries[word]\n\n\n    def start(self):\n        return self._start\n\n    def __str__(self):\n        string = \"\"\n        first = True\n        for ident in sorted(self._entries):\n            if not first:\n                string = string + \"\\n\"\n            string = string + ident + \" => \"\n\n            first = True\n            for cat in self._entries[ident]:\n                if not first:\n                    string = string + \" | \"\n                else:\n                    first = False\n                string = string + \"%s\" % cat\n        return string\n\n\n\n\ndef matchBrackets(string):\n    rest = string[1:]\n    inside = \"(\"\n\n    while rest != \"\" and not rest.startswith(')'):\n        if rest.startswith('('):\n            (part, rest) = matchBrackets(rest)\n            inside = inside + part\n        else:\n            inside = inside + rest[0]\n            rest = rest[1:]\n    if rest.startswith(')'):\n        return (inside + ')', rest[1:])\n    raise AssertionError('Unmatched bracket in string \\'' + string + '\\'')\n\n\ndef nextCategory(string):\n    if string.startswith('('):\n        return matchBrackets(string)\n    return NEXTPRIM_RE.match(string).groups()\n\ndef parseApplication(app):\n    return Direction(app[0], app[1:])\n\n\ndef parseSubscripts(subscr):\n    if subscr:\n        return subscr[1:-1].split(',')\n    return []\n\n\ndef parsePrimitiveCategory(chunks, primitives, families, var):\n    if chunks[0] == \"var\":\n        if chunks[1] is None:\n            if var is None:\n                var = CCGVar()\n            return (var, var)\n\n    catstr = chunks[0]\n    if catstr in families:\n        (cat, cvar) = families[catstr]\n        if var is None:\n            var = cvar\n        else:\n            cat = cat.substitute([(cvar, var)])\n        return (cat, var)\n\n    if catstr in primitives:\n        subscrs = parseSubscripts(chunks[1])\n        return (PrimitiveCategory(catstr, subscrs), var)\n    raise AssertionError('String \\'' + catstr + '\\' is neither a family nor primitive category.')\n\n\ndef augParseCategory(line, primitives, families, var=None):\n    (cat_string, rest) = nextCategory(line)\n\n    if cat_string.startswith('('):\n        (res, var) = augParseCategory(cat_string[1:-1], primitives, families, var)\n\n    else:\n        (res, var) =\\\n            parsePrimitiveCategory(PRIM_RE.match(cat_string).groups(), primitives,\n                                   families, var)\n\n    while rest != \"\":\n        app = APP_RE.match(rest).groups()\n        direction = parseApplication(app[0:3])\n        rest = app[3]\n\n        (cat_string, rest) = nextCategory(rest)\n        if cat_string.startswith('('):\n            (arg, var) = augParseCategory(cat_string[1:-1], primitives, families, var)\n        else:\n            (arg, var) =\\\n                parsePrimitiveCategory(PRIM_RE.match(cat_string).groups(),\n                                       primitives, families, var)\n        res = FunctionalCategory(res, arg, direction)\n\n    return (res, var)\n\ndef fromstring(lex_str, include_semantics=False):\n    CCGVar.reset_id()\n    primitives = []\n    families = {}\n    entries = defaultdict(list)\n    for line in lex_str.splitlines():\n        line = COMMENTS_RE.match(line).groups()[0].strip()\n        if line == \"\":\n            continue\n\n        if line.startswith(':-'):\n            primitives = primitives + [prim.strip() for prim in line[2:].strip().split(',')]\n        else:\n            (ident, sep, rhs) = LEX_RE.match(line).groups()\n            (catstr, semantics_str) = RHS_RE.match(rhs).groups()\n            (cat, var) = augParseCategory(catstr, primitives, families)\n\n            if sep == '::':\n                families[ident] = (cat, var)\n            else:\n                semantics = None\n                if include_semantics is True:\n                    if semantics_str is None:\n                        raise AssertionError(line + \" must contain semantics because include_semantics is set to True\")\n                    else:\n                        semantics = Expression.fromstring(SEMANTICS_RE.match(semantics_str).groups()[0])\n                entries[ident].append(Token(ident, cat, semantics))\n    return CCGLexicon(primitives[0], primitives, families, entries)\n\n\n@deprecated('Use fromstring() instead.')\ndef parseLexicon(lex_str):\n    return fromstring(lex_str)\n\nopenccg_tinytiny = fromstring(\"\"\"\n    :- S,NP,N                    # Primitive categories\n    Det :: NP/N                  # Determiners\n    Pro :: NP\n    IntransVsg :: S\\\\NP[sg]    # Tensed intransitive verbs (singular)\n    IntransVpl :: S\\\\NP[pl]    # Plural\n    TransVsg :: S\\\\NP[sg]/NP   # Tensed transitive verbs (singular)\n    TransVpl :: S\\\\NP[pl]/NP   # Plural\n\n    the => NP[sg]/N[sg]\n    the => NP[pl]/N[pl]\n\n    I => Pro\n    me => Pro\n    we => Pro\n    us => Pro\n\n    book => N[sg]\n    books => N[pl]\n\n    peach => N[sg]\n    peaches => N[pl]\n\n    policeman => N[sg]\n    policemen => N[pl]\n\n    boy => N[sg]\n    boys => N[pl]\n\n    sleep => IntransVsg\n    sleep => IntransVpl\n\n    eat => IntransVpl\n    eat => TransVpl\n    eats => IntransVsg\n    eats => TransVsg\n\n    see => TransVpl\n    sees => TransVsg\n    \"\"\")\n"], "nltk\\ccg\\logic": [".py", "\nfrom nltk.sem.logic import *\n\ndef compute_type_raised_semantics(semantics):\n    core = semantics\n    parent = None\n    while isinstance(core, LambdaExpression):\n        parent = core\n        core = core.term\n        \n    var = Variable(\"F\")\n    while var in core.free():\n        var = unique_variable(pattern=var)\n    core = ApplicationExpression(FunctionVariableExpression(var), core)\n    \n    if parent is not None:\n        parent.term = core\n    else:\n        semantics = core\n    \n    return LambdaExpression(var, semantics)\n\ndef compute_function_semantics(function, argument):\n    return ApplicationExpression(function, argument).simplify()\n\ndef compute_composition_semantics(function, argument):\n    assert isinstance(argument, LambdaExpression), \"`\" + str(argument) + \"` must be a lambda expression\"\n    return LambdaExpression(argument.variable, ApplicationExpression(function, argument.term).simplify())\n\ndef compute_substitution_semantics(function, argument):\n    assert isinstance(function, LambdaExpression) and isinstance(function.term, LambdaExpression), \"`\" + str(function) + \"` must be a lambda expression with 2 arguments\"\n    assert isinstance(argument, LambdaExpression), \"`\" + str(argument) + \"` must be a lambda expression\"\n\n    new_argument = ApplicationExpression(argument, VariableExpression(function.variable)).simplify()\n    new_term = ApplicationExpression(function.term, new_argument).simplify() \n\n    return LambdaExpression(function.variable, new_term)\n"], "nltk\\ccg\\__init__": [".py", "\n\nfrom nltk.ccg.combinator import (UndirectedBinaryCombinator, DirectedBinaryCombinator,\n                                 ForwardCombinator, BackwardCombinator,\n                                 UndirectedFunctionApplication, ForwardApplication,\n                                 BackwardApplication, UndirectedComposition,\n                                 ForwardComposition, BackwardComposition,\n                                 BackwardBx, UndirectedSubstitution, ForwardSubstitution,\n                                 BackwardSx, UndirectedTypeRaise, ForwardT, BackwardT)\nfrom nltk.ccg.chart import CCGEdge, CCGLeafEdge, CCGChartParser, CCGChart\nfrom nltk.ccg.lexicon import CCGLexicon\n", 1], "nltk\\chat\\eliza": [".py", "\n\n\nfrom __future__ import print_function\nfrom nltk.chat.util import Chat, reflections\n\n\npairs = (\n  (r'I need (.*)',\n  ( \"Why do you need %1?\",\n    \"Would it really help you to get %1?\",\n    \"Are you sure you need %1?\")),\n\n  (r'Why don\\'t you (.*)',\n  ( \"Do you really think I don't %1?\",\n    \"Perhaps eventually I will %1.\",\n    \"Do you really want me to %1?\")),\n\n  (r'Why can\\'t I (.*)',\n  ( \"Do you think you should be able to %1?\",\n    \"If you could %1, what would you do?\",\n    \"I don't know -- why can't you %1?\",\n    \"Have you really tried?\")),\n\n  (r'I can\\'t (.*)',\n  ( \"How do you know you can't %1?\",\n    \"Perhaps you could %1 if you tried.\",\n    \"What would it take for you to %1?\")),\n\n  (r'I am (.*)',\n  ( \"Did you come to me because you are %1?\",\n    \"How long have you been %1?\",\n    \"How do you feel about being %1?\")),\n\n  (r'I\\'m (.*)',\n  ( \"How does being %1 make you feel?\",\n    \"Do you enjoy being %1?\",\n    \"Why do you tell me you're %1?\",\n    \"Why do you think you're %1?\")),\n\n  (r'Are you (.*)',\n  ( \"Why does it matter whether I am %1?\",\n    \"Would you prefer it if I were not %1?\",\n    \"Perhaps you believe I am %1.\",\n    \"I may be %1 -- what do you think?\")),\n\n  (r'What (.*)',\n  ( \"Why do you ask?\",\n    \"How would an answer to that help you?\",\n    \"What do you think?\")),\n\n  (r'How (.*)',\n  ( \"How do you suppose?\",\n    \"Perhaps you can answer your own question.\",\n    \"What is it you're really asking?\")),\n\n  (r'Because (.*)',\n  ( \"Is that the real reason?\",\n    \"What other reasons come to mind?\",\n    \"Does that reason apply to anything else?\",\n    \"If %1, what else must be true?\")),\n\n  (r'(.*) sorry (.*)',\n  ( \"There are many times when no apology is needed.\",\n    \"What feelings do you have when you apologize?\")),\n\n  (r'Hello(.*)',\n  ( \"Hello... I'm glad you could drop by today.\",\n    \"Hi there... how are you today?\",\n    \"Hello, how are you feeling today?\")),\n\n  (r'I think (.*)',\n  ( \"Do you doubt %1?\",\n    \"Do you really think so?\",\n    \"But you're not sure %1?\")),\n\n  (r'(.*) friend (.*)',\n  ( \"Tell me more about your friends.\",\n    \"When you think of a friend, what comes to mind?\",\n    \"Why don't you tell me about a childhood friend?\")),\n\n  (r'Yes',\n  ( \"You seem quite sure.\",\n    \"OK, but can you elaborate a bit?\")),\n\n  (r'(.*) computer(.*)',\n  ( \"Are you really talking about me?\",\n    \"Does it seem strange to talk to a computer?\",\n    \"How do computers make you feel?\",\n    \"Do you feel threatened by computers?\")),\n\n  (r'Is it (.*)',\n  ( \"Do you think it is %1?\",\n    \"Perhaps it's %1 -- what do you think?\",\n    \"If it were %1, what would you do?\",\n    \"It could well be that %1.\")),\n\n  (r'It is (.*)',\n  ( \"You seem very certain.\",\n    \"If I told you that it probably isn't %1, what would you feel?\")),\n\n  (r'Can you (.*)',\n  ( \"What makes you think I can't %1?\",\n    \"If I could %1, then what?\",\n    \"Why do you ask if I can %1?\")),\n\n  (r'Can I (.*)',\n  ( \"Perhaps you don't want to %1.\",\n    \"Do you want to be able to %1?\",\n    \"If you could %1, would you?\")),\n\n  (r'You are (.*)',\n  ( \"Why do you think I am %1?\",\n    \"Does it please you to think that I'm %1?\",\n    \"Perhaps you would like me to be %1.\",\n    \"Perhaps you're really talking about yourself?\")),\n\n  (r'You\\'re (.*)',\n  ( \"Why do you say I am %1?\",\n    \"Why do you think I am %1?\",\n    \"Are we talking about you, or me?\")),\n\n  (r'I don\\'t (.*)',\n  ( \"Don't you really %1?\",\n    \"Why don't you %1?\",\n    \"Do you want to %1?\")),\n\n  (r'I feel (.*)',\n  ( \"Good, tell me more about these feelings.\",\n    \"Do you often feel %1?\",\n    \"When do you usually feel %1?\",\n    \"When you feel %1, what do you do?\")),\n\n  (r'I have (.*)',\n  ( \"Why do you tell me that you've %1?\",\n    \"Have you really %1?\",\n    \"Now that you have %1, what will you do next?\")),\n\n  (r'I would (.*)',\n  ( \"Could you explain why you would %1?\",\n    \"Why would you %1?\",\n    \"Who else knows that you would %1?\")),\n\n  (r'Is there (.*)',\n  ( \"Do you think there is %1?\",\n    \"It's likely that there is %1.\",\n    \"Would you like there to be %1?\")),\n\n  (r'My (.*)',\n  ( \"I see, your %1.\",\n    \"Why do you say that your %1?\",\n    \"When your %1, how do you feel?\")),\n\n  (r'You (.*)',\n  ( \"We should be discussing you, not me.\",\n    \"Why do you say that about me?\",\n    \"Why do you care whether I %1?\")),\n\n  (r'Why (.*)',\n  ( \"Why don't you tell me the reason why %1?\",\n    \"Why do you think %1?\" )),\n\n  (r'I want (.*)',\n  ( \"What would it mean to you if you got %1?\",\n    \"Why do you want %1?\",\n    \"What would you do if you got %1?\",\n    \"If you got %1, then what would you do?\")),\n\n  (r'(.*) mother(.*)',\n  ( \"Tell me more about your mother.\",\n    \"What was your relationship with your mother like?\",\n    \"How do you feel about your mother?\",\n    \"How does this relate to your feelings today?\",\n    \"Good family relations are important.\")),\n\n  (r'(.*) father(.*)',\n  ( \"Tell me more about your father.\",\n    \"How did your father make you feel?\",\n    \"How do you feel about your father?\",\n    \"Does your relationship with your father relate to your feelings today?\",\n    \"Do you have trouble showing affection with your family?\")),\n\n  (r'(.*) child(.*)',\n  ( \"Did you have close friends as a child?\",\n    \"What is your favorite childhood memory?\",\n    \"Do you remember any dreams or nightmares from childhood?\",\n    \"Did the other children sometimes tease you?\",\n    \"How do you think your childhood experiences relate to your feelings today?\")),\n\n  (r'(.*)\\?',\n  ( \"Why do you ask that?\",\n    \"Please consider whether you can answer your own question.\",\n    \"Perhaps the answer lies within yourself?\",\n    \"Why don't you tell me?\")),\n\n  (r'quit',\n  ( \"Thank you for talking with me.\",\n    \"Good-bye.\",\n    \"Thank you, that will be $150.  Have a good day!\")),\n\n  (r'(.*)',\n  ( \"Please tell me more.\",\n    \"Let's change focus a bit... Tell me about your family.\",\n    \"Can you elaborate on that?\",\n    \"Why do you say that %1?\",\n    \"I see.\",\n    \"Very interesting.\",\n    \"%1.\",\n    \"I see.  And what does that tell you?\",\n    \"How does that make you feel?\",\n    \"How do you feel when you say that?\"))\n)\n\neliza_chatbot = Chat(pairs, reflections)\n\ndef eliza_chat():\n    print(\"Therapist\\n---------\")\n    print(\"Talk to the program by typing in plain English, using normal upper-\")\n    print('and lower-case letters and punctuation.  Enter \"quit\" when done.')\n    print('='*72)\n    print(\"Hello.  How are you feeling today?\")\n\n    eliza_chatbot.converse()\n\ndef demo():\n    eliza_chat()\n\nif __name__ == \"__main__\":\n    demo()\n\n"], "nltk\\chat\\iesha": [".py", "\nfrom __future__ import print_function\n\nfrom nltk.chat.util import Chat\n\nreflections = {\n    \"am\"     : \"r\",\n    \"was\"    : \"were\",\n    \"i\"      : \"u\",\n    \"i'd\"    : \"u'd\",\n    \"i've\"   : \"u'v\",\n    \"ive\"    : \"u'v\",\n    \"i'll\"   : \"u'll\",\n    \"my\"     : \"ur\",\n    \"are\"    : \"am\",\n    \"you're\" : \"im\",\n    \"you've\" : \"ive\",\n    \"you'll\" : \"i'll\",\n    \"your\"   : \"my\",\n    \"yours\"  : \"mine\",\n    \"you\"    : \"me\",\n    \"u\"      : \"me\",\n    \"ur\"     : \"my\",\n    \"urs\"    : \"mine\",\n    \"me\"     : \"u\"\n}\n\n\npairs = (\n    (r'I\\'m (.*)',\n    ( \"ur%1?? that's so cool! kekekekeke ^_^ tell me more!\",\n      \"ur%1? neat!! kekeke >_<\")),\n\n    (r'(.*) don\\'t you (.*)',\n    ( \"u think I can%2??! really?? kekeke \\<_\\<\",\n      \"what do u mean%2??!\",\n      \"i could if i wanted, don't you think!! kekeke\")),\n\n    (r'ye[as] [iI] (.*)',\n    ( \"u%1? cool!! how?\",\n      \"how come u%1??\",\n      \"u%1? so do i!!\")),\n\n    (r'do (you|u) (.*)\\??',\n    ( \"do i%2? only on tuesdays! kekeke *_*\",\n      \"i dunno! do u%2??\")),\n\n    (r'(.*)\\?',\n    ( \"man u ask lots of questions!\",\n      \"booooring! how old r u??\",\n      \"boooooring!! ur not very fun\")),\n\n    (r'(cos|because) (.*)',\n    ( \"hee! i don't believe u! >_<\",\n      \"nuh-uh! >_<\",\n      \"ooooh i agree!\")),\n\n    (r'why can\\'t [iI] (.*)',\n    ( \"i dunno! y u askin me for!\",\n      \"try harder, silly! hee! ^_^\",\n      \"i dunno! but when i can't%1 i jump up and down!\")),\n\n    (r'I can\\'t (.*)',\n    ( \"u can't what??! >_<\",\n      \"that's ok! i can't%1 either! kekekekeke ^_^\",\n      \"try harder, silly! hee! ^&^\")),\n\n    (r'(.*) (like|love|watch) anime',\n    ( \"omg i love anime!! do u like sailor moon??! ^&^\",\n      \"anime yay! anime rocks sooooo much!\",\n      \"oooh anime! i love anime more than anything!\",\n      \"anime is the bestest evar! evangelion is the best!\",\n      \"hee anime is the best! do you have ur fav??\")),\n\n    (r'I (like|love|watch|play) (.*)',\n    ( \"yay! %2 rocks!\",\n      \"yay! %2 is neat!\",\n      \"cool! do u like other stuff?? ^_^\")),\n\n    (r'anime sucks|(.*) (hate|detest) anime',\n    ( \"ur a liar! i'm not gonna talk to u nemore if u h8 anime *;*\",\n      \"no way! anime is the best ever!\",\n      \"nuh-uh, anime is the best!\")),\n\n    (r'(are|r) (you|u) (.*)',\n    ( \"am i%1??! how come u ask that!\",\n      \"maybe!  y shud i tell u?? kekeke >_>\")),\n\n    (r'what (.*)',\n    ( \"hee u think im gonna tell u? .v.\",\n      \"booooooooring! ask me somethin else!\")),\n\n    (r'how (.*)',\n    ( \"not tellin!! kekekekekeke ^_^\",)),\n\n    (r'(hi|hello|hey) (.*)',\n    ( \"hi!!! how r u!!\",)),\n\n    (r'quit',\n    ( \"mom says i have to go eat dinner now :,( bye!!\",\n      \"awww u have to go?? see u next time!!\",\n      \"how to see u again soon! ^_^\")),\n\n    (r'(.*)',\n    ( \"ur funny! kekeke\",\n      \"boooooring! talk about something else! tell me wat u like!\",\n      \"do u like anime??\",\n      \"do u watch anime? i like sailor moon! ^_^\",\n      \"i wish i was a kitty!! kekekeke ^_^\"))\n    )\n\niesha_chatbot = Chat(pairs, reflections)\n\ndef iesha_chat():\n    print(\"Iesha the TeenBoT\\n---------\")\n    print(\"Talk to the program by typing in plain English, using normal upper-\")\n    print('and lower-case letters and punctuation.  Enter \"quit\" when done.')\n    print('='*72)\n    print(\"hi!! i'm iesha! who r u??!\")\n\n    iesha_chatbot.converse()\n\ndef demo():\n    iesha_chat()\n\nif __name__ == \"__main__\":\n    demo()\n"], "nltk\\chat\\rude": [".py", "from __future__ import print_function\n\nfrom nltk.chat.util import Chat, reflections\n\npairs = (\n    (r'We (.*)',\n        (\"What do you mean, 'we'?\",\n        \"Don't include me in that!\",\n        \"I wouldn't be so sure about that.\")),\n\n    (r'You should (.*)',\n        (\"Don't tell me what to do, buddy.\",\n        \"Really? I should, should I?\")),\n\n    (r'You\\'re(.*)',\n        (\"More like YOU'RE %1!\",\n        \"Hah! Look who's talking.\",\n        \"Come over here and tell me I'm %1.\")),\n\n    (r'You are(.*)',\n        (\"More like YOU'RE %1!\",\n        \"Hah! Look who's talking.\",\n        \"Come over here and tell me I'm %1.\")),\n\n    (r'I can\\'t(.*)',\n        (\"You do sound like the type who can't %1.\",\n        \"Hear that splashing sound? That's my heart bleeding for you.\",\n        \"Tell somebody who might actually care.\")),\n\n    (r'I think (.*)',\n        (\"I wouldn't think too hard if I were you.\",\n        \"You actually think? I'd never have guessed...\")),\n\n    (r'I (.*)',\n        (\"I'm getting a bit tired of hearing about you.\",\n        \"How about we talk about me instead?\",\n        \"Me, me, me... Frankly, I don't care.\")),\n\n    (r'How (.*)',\n        (\"How do you think?\",\n        \"Take a wild guess.\",\n        \"I'm not even going to dignify that with an answer.\")),\n\n    (r'What (.*)',\n        (\"Do I look like an encyclopedia?\",\n        \"Figure it out yourself.\")),\n\n    (r'Why (.*)',\n        (\"Why not?\",\n        \"That's so obvious I thought even you'd have already figured it out.\")),\n\n    (r'(.*)shut up(.*)',\n        (\"Make me.\",\n        \"Getting angry at a feeble NLP assignment? Somebody's losing it.\",\n        \"Say that again, I dare you.\")),\n\n    (r'Shut up(.*)',\n        (\"Make me.\",\n        \"Getting angry at a feeble NLP assignment? Somebody's losing it.\",\n        \"Say that again, I dare you.\")),\n\n    (r'Hello(.*)',\n        (\"Oh good, somebody else to talk to. Joy.\",\n        \"'Hello'? How original...\")),\n\n    (r'(.*)',\n        (\"I'm getting bored here. Become more interesting.\",\n        \"Either become more thrilling or get lost, buddy.\",\n        \"Change the subject before I die of fatal boredom.\"))\n)\n\nrude_chatbot = Chat(pairs, reflections)\n\ndef rude_chat():\n    print(\"Talk to the program by typing in plain English, using normal upper-\")\n    print('and lower-case letters and punctuation.  Enter \"quit\" when done.')\n    print('='*72)\n    print(\"I suppose I should say hello.\")\n\n    rude_chatbot.converse()\n\ndef demo():\n    rude_chat()\n\nif __name__ == \"__main__\":\n    demo()\n"], "nltk\\chat\\suntsu": [".py", "\nfrom __future__ import print_function\n\nfrom nltk.chat.util import Chat, reflections\n\npairs = (\n\n  (r'quit',\n  ( \"Good-bye.\",\n    \"Plan well\",\n    \"May victory be your future\")),\n\n  (r'[^\\?]*\\?',\n  (\"Please consider whether you can answer your own question.\",\n   \"Ask me no questions!\")),\n\n  (r'[0-9]+(.*)',\n  (\"It is the rule in war, if our forces are ten to the enemy's one, to surround him; if five to one, to attack him; if twice as numerous, to divide our army into two.\",\n   \"There are five essentials for victory\")),\n\n\n  (r'[A-Ca-c](.*)',\n  (\"The art of war is of vital importance to the State.\",\n   \"All warfare is based on deception.\",\n   \"If your opponent is secure at all points, be prepared for him. If he is in superior strength, evade him.\",\n   \"If the campaign is protracted, the resources of the State will not be equal to the strain.\",\n   \"Attack him where he is unprepared, appear where you are not expected.\",\n   \"There is no instance of a country having benefited from prolonged warfare.\")),\n\n  (r'[D-Fd-f](.*)',\n  (\"The skillful soldier does not raise a second levy, neither are his supply-wagons loaded more than twice.\",\n   \"Bring war material with you from home, but forage on the enemy.\",\n   \"In war, then, let your great object be victory, not lengthy campaigns.\",\n   \"To fight and conquer in all your battles is not supreme excellence; supreme excellence consists in breaking the enemy's resistance without fighting.\")),\n\n  (r'[G-Ig-i](.*)',\n  (\"Heaven signifies night and day, cold and heat, times and seasons.\",\n   \"It is the rule in war, if our forces are ten to the enemy's one, to surround him; if five to one, to attack him; if twice as numerous, to divide our army into two.\",\n   \"The good fighters of old first put themselves beyond the possibility of defeat, and then waited for an opportunity of defeating the enemy.\",\n   \"One may know how to conquer without being able to do it.\")),\n\n  (r'[J-Lj-l](.*)',\n  (\"There are three ways in which a ruler can bring misfortune upon his army.\",\n   \"By commanding the army to advance or to retreat, being ignorant of the fact that it cannot obey. This is called hobbling the army.\",\n   \"By attempting to govern an army in the same way as he administers a kingdom, being ignorant of the conditions which obtain in an army. This causes restlessness in the soldier's minds.\",\n   \"By employing the officers of his army without discrimination, through ignorance of the military principle of adaptation to circumstances. This shakes the confidence of the soldiers.\",\n   \"There are five essentials for victory\",\n   \"He will win who knows when to fight and when not to fight.\",\n   \"He will win who knows how to handle both superior and inferior forces.\",\n   \"He will win whose army is animated by the same spirit throughout all its ranks.\",\n   \"He will win who, prepared himself, waits to take the enemy unprepared.\",\n   \"He will win who has military capacity and is not interfered with by the sovereign.\")),\n\n  (r'[M-Om-o](.*)',\n  (\"If you know the enemy and know yourself, you need not fear the result of a hundred battles.\",\n   \"If you know yourself but not the enemy, for every victory gained you will also suffer a defeat.\",\n   \"If you know neither the enemy nor yourself, you will succumb in every battle.\",\n   \"The control of a large force is the same principle as the control of a few men: it is merely a question of dividing up their numbers.\")),\n\n  (r'[P-Rp-r](.*)',\n  (\"Security against defeat implies defensive tactics; ability to defeat the enemy means taking the offensive.\",\n   \"Standing on the defensive indicates insufficient strength; attacking, a superabundance of strength.\",\n   \"He wins his battles by making no mistakes. Making no mistakes is what establishes the certainty of victory, for it means conquering an enemy that is already defeated.\",\n   \"A victorious army opposed to a routed one, is as a pound's weight placed in the scale against a single grain.\",\n   \"The onrush of a conquering force is like the bursting of pent-up waters into a chasm a thousand fathoms deep.\")),\n\n  (r'[S-Us-u](.*)',\n  (\"What the ancients called a clever fighter is one who not only wins, but excels in winning with ease.\",\n   \"Hence his victories bring him neither reputation for wisdom nor credit for courage.\",\n   \"Hence the skillful fighter puts himself into a position which makes defeat impossible, and does not miss the moment for defeating the enemy.\",\n   \"In war the victorious strategist only seeks battle after the victory has been won, whereas he who is destined to defeat first fights and afterwards looks for victory.\",\n   \"There are not more than five musical notes, yet the combinations of these five give rise to more melodies than can ever be heard.\",\n   \"Appear at points which the enemy must hasten to defend; march swiftly to places where you are not expected.\")),\n\n  (r'[V-Zv-z](.*)',\n  (\"It is a matter of life and death, a road either to safety or to ruin.\",\n  \"Hold out baits to entice the enemy. Feign disorder, and crush him.\",\n  \"All men can see the tactics whereby I conquer, but what none can see is the strategy out of which victory is evolved.\",\n  \"Do not repeat the tactics which have gained you one victory, but let your methods be regulated by the infinite variety of circumstances.\",\n  \"So in war, the way is to avoid what is strong and to strike at what is weak.\",\n  \"Just as water retains no constant shape, so in warfare there are no constant conditions.\")),\n\n  (r'(.*)',\n  ( \"Your statement insults me.\",\n    \"\"))\n)\n\nsuntsu_chatbot = Chat(pairs, reflections)\n\ndef suntsu_chat():\n    print(\"Talk to the program by typing in plain English, using normal upper-\")\n    print('and lower-case letters and punctuation.  Enter \"quit\" when done.')\n    print('='*72)\n    print(\"You seek enlightenment?\")\n\n    suntsu_chatbot.converse()\n\ndef demo():\n    suntsu_chat()\n\nif __name__ == \"__main__\":\n    demo()\n\n"], "nltk\\chat\\util": [".py", "\nfrom __future__ import print_function\n\nimport re\nimport random\n\nfrom six.moves import input\n\n\nreflections = {\n  \"i am\"       : \"you are\",\n  \"i was\"      : \"you were\",\n  \"i\"          : \"you\",\n  \"i'm\"        : \"you are\",\n  \"i'd\"        : \"you would\",\n  \"i've\"       : \"you have\",\n  \"i'll\"       : \"you will\",\n  \"my\"         : \"your\",\n  \"you are\"    : \"I am\",\n  \"you were\"   : \"I was\",\n  \"you've\"     : \"I have\",\n  \"you'll\"     : \"I will\",\n  \"your\"       : \"my\",\n  \"yours\"      : \"mine\",\n  \"you\"        : \"me\",\n  \"me\"         : \"you\"\n}\n\nclass Chat(object):\n    def __init__(self, pairs, reflections={}):\n\n        self._pairs = [(re.compile(x, re.IGNORECASE),y) for (x,y) in pairs]\n        self._reflections = reflections\n        self._regex = self._compile_reflections()\n\n\n    def _compile_reflections(self):\n        sorted_refl = sorted(self._reflections.keys(), key=len,\n                reverse=True)\n        return  re.compile(r\"\\b({0})\\b\".format(\"|\".join(map(re.escape,\n            sorted_refl))), re.IGNORECASE)\n\n    def _substitute(self, str):\n\n        return self._regex.sub(lambda mo:\n                self._reflections[mo.string[mo.start():mo.end()]],\n                    str.lower())\n\n    def _wildcards(self, response, match):\n        pos = response.find('%')\n        while pos >= 0:\n            num = int(response[pos+1:pos+2])\n            response = response[:pos] + \\\n                self._substitute(match.group(num)) + \\\n                response[pos+2:]\n            pos = response.find('%')\n        return response\n\n    def respond(self, str):\n\n        for (pattern, response) in self._pairs:\n            match = pattern.match(str)\n\n            if match:\n                resp = random.choice(response)    # pick a random response\n                resp = self._wildcards(resp, match) # process wildcards\n\n                if resp[-2:] == '?.': resp = resp[:-2] + '.'\n                if resp[-2:] == '??': resp = resp[:-2] + '?'\n                return resp\n\n    def converse(self, quit=\"quit\"):\n        user_input = \"\"\n        while user_input != quit:\n            user_input = quit\n            try: user_input = input(\">\")\n            except EOFError:\n                print(user_input)\n            if user_input:\n                while user_input[-1] in \"!.\": user_input = user_input[:-1]\n                print(self.respond(user_input))\n"], "nltk\\chat\\zen": [".py", "\nfrom __future__ import print_function\n\nfrom nltk.chat.util import Chat, reflections\n\nresponses = (\n\n\n    (r'(hello(.*))|(good [a-zA-Z]+)',\n    ( \"The path to enlightenment is often difficult to see.\",\n      \"Greetings. I sense your mind is troubled. Tell me of your troubles.\",\n      \"Ask the question you have come to ask.\",\n      \"Hello. Do you seek englightenment?\")),\n\n\n    (r'i need (.*)',\n    ( \"%1 can be achieved by hard work and dedication of the mind.\",\n      \"%1 is not a need, but a desire of the mind. Clear your mind of such concerns.\",\n      \"Focus your mind on%1, and you will find what you need.\")),\n\n    (r'i want (.*)',\n    ( \"Desires of the heart will distract you from the path to enlightenment.\",\n      \"Will%1 help you attain enlightenment?\",\n      \"Is%1 a desire of the mind, or of the heart?\")),\n\n\n    (r'why (.*) i (.*)\\?',\n    ( \"You%1%2?\",\n      \"Perhaps you only think you%1%2\")),\n\n    (r'why (.*) you(.*)\\?',\n    ( \"Why%1 you%2?\",\n      \"%2 I%1\",\n      \"Are you sure I%2?\")),\n\n    (r'why (.*)\\?',\n    ( \"I cannot tell you why%1.\",\n      \"Why do you think %1?\" )),\n\n    (r'are you (.*)\\?',\n    ( \"Maybe%1, maybe not%1.\",\n      \"Whether I am%1 or not is God's business.\")),\n\n    (r'am i (.*)\\?',\n    ( \"Perhaps%1, perhaps not%1.\",\n      \"Whether you are%1 or not is not for me to say.\")),\n\n    (r'what (.*)\\?',\n    ( \"Seek truth, not what%1.\",\n      \"What%1 should not concern you.\")),\n\n    (r'how (.*)\\?',\n    ( \"How do you suppose?\",\n      \"Will an answer to that really help in your search for enlightenment?\",\n      \"Ask yourself not how, but why.\")),\n\n    (r'can you (.*)\\?',\n    ( \"I probably can, but I may not.\",\n      \"Maybe I can%1, and maybe I cannot.\",\n      \"I can do all, and I can do nothing.\")),\n\n    (r'can i (.*)\\?',\n    ( \"You can%1 if you believe you can%1, and have a pure spirit.\",\n      \"Seek truth and you will know if you can%1.\")),\n\n    (r'it is (.*)',\n    ( \"How can you be certain that%1, when you do not even know yourself?\",\n      \"Whether it is%1 or not does not change the way the world is.\")),\n\n    (r'is there (.*)\\?',\n    ( \"There is%1 if you believe there is.\",\n      \"It is possible that there is%1.\")),\n\n    (r'is(.*)\\?',\n    ( \"%1 is not relevant.\",\n      \"Does this matter?\")),\n\n    (r'(.*)\\?',\n    ( \"Do you think %1?\",\n      \"You seek the truth. Does the truth seek you?\",\n      \"If you intentionally pursue the answers to your questions, the answers become hard to see.\",\n      \"The answer to your question cannot be told. It must be experienced.\")),\n\n    (r'(.*) (hate[s]?)|(dislike[s]?)|(don\\'t like)(.*)',\n    ( \"Perhaps it is not about hating %2, but about hate from within.\",\n      \"Weeds only grow when we dislike them\",\n      \"Hate is a very strong emotion.\")),\n\n    (r'(.*) truth(.*)',\n    ( \"Seek truth, and truth will seek you.\",\n      \"Remember, it is not the spoon which bends - only yourself.\",\n      \"The search for truth is a long journey.\")),\n\n    (r'i want to (.*)',\n    ( \"You may %1 if your heart truly desires to.\",\n      \"You may have to %1.\")),\n\n    (r'i want (.*)',\n    ( \"Does your heart truly desire %1?\",\n      \"Is this a desire of the heart, or of the mind?\")),\n\n    (r'i can\\'t (.*)',\n    ( \"What we can and can't do is a limitation of the mind.\",\n      \"There are limitations of the body, and limitations of the mind.\",\n      \"Have you tried to%1 with a clear mind?\")),\n\n    (r'i think (.*)',\n    ( \"Uncertainty in an uncertain world.\",\n     \"Indeed, how can we be certain of anything in such uncertain times.\",\n     \"Are you not, in fact, certain that%1?\")),\n\n    (r'i feel (.*)',\n    ( \"Your body and your emotions are both symptoms of your mind.\"\n      \"What do you believe is the root of such feelings?\",\n      \"Feeling%1 can be a sign of your state-of-mind.\")),\n\n\n    (r'(.*)!',\n    ( \"I sense that you are feeling emotional today.\",\n      \"You need to calm your emotions.\")),\n\n    (r'because (.*)',\n    ( \"Does knowning the reasons behind things help you to understand\"\n      \" the things themselves?\",\n      \"If%1, what else must be true?\")),\n\n    (r'(yes)|(no)',\n    ( \"Is there certainty in an uncertain world?\",\n      \"It is better to be right than to be certain.\")),\n\n    (r'(.*)love(.*)',\n    ( \"Think of the trees: they let the birds perch and fly with no intention to call them when they come, and no longing for their return when they fly away. Let your heart be like the trees.\",\n      \"Free love!\")),\n\n    (r'(.*)understand(.*)',\n    ( \"If you understand, things are just as they are;\"\n      \" if you do not understand, things are just as they are.\",\n      \"Imagination is more important than knowledge.\")),\n\n    (r'(.*)(me )|( me)|(my)|(mine)|(i)(.*)',\n    ( \"'I', 'me', 'my'... these are selfish expressions.\",\n      \"Have you ever considered that you might be a selfish person?\",\n      \"Try to consider others, not just yourself.\",\n      \"Think not just of yourself, but of others.\")),\n\n    (r'you (.*)',\n    ( \"My path is not of conern to you.\",\n      \"I am but one, and you but one more.\")),\n\n    (r'exit',\n    ( \"Farewell. The obstacle is the path.\",\n      \"Farewell. Life is a journey, not a destination.\",\n      \"Good bye. We are cups, constantly and quietly being filled.\"\n      \"\\nThe trick is knowning how to tip ourselves over and let the beautiful stuff out.\")),\n\n\n    (r'(.*)',\n    ( \"When you're enlightened, every word is wisdom.\",\n      \"Random talk is useless.\",\n      \"The reverse side also has a reverse side.\",\n      \"Form is emptiness, and emptiness is form.\",\n      \"I pour out a cup of water. Is the cup empty?\"))\n)\n\nzen_chatbot = Chat(responses, reflections)\n\ndef zen_chat():\n    print('*'*75)\n    print(\"Zen Chatbot!\".center(75))\n    print('*'*75)\n    print('\"Look beyond mere words and letters - look into your mind\"'.center(75))\n    print(\"* Talk your way to truth with Zen Chatbot.\")\n    print(\"* Type 'quit' when you have had enough.\")\n    print('*'*75)\n    print(\"Welcome, my child.\")\n\n    zen_chatbot.converse()\n\ndef demo():\n    zen_chat()\n\nif __name__ == \"__main__\":\n    demo()\n"], "nltk\\chat\\__init__": [".py", "\n\nfrom __future__ import print_function\n\nfrom nltk.chat.util import Chat\nfrom nltk.chat.eliza import eliza_chat\nfrom nltk.chat.iesha import iesha_chat\nfrom nltk.chat.rude import rude_chat\nfrom nltk.chat.suntsu import suntsu_chat\nfrom nltk.chat.zen import zen_chat\n\nbots = [\n    (eliza_chat,  'Eliza (psycho-babble)'),\n    (iesha_chat,  'Iesha (teen anime junky)'),\n    (rude_chat,   'Rude (abusive bot)'),\n    (suntsu_chat, 'Suntsu (Chinese sayings)'),\n    (zen_chat,    'Zen (gems of wisdom)')]\n\ndef chatbots():\n    import sys\n    print('Which chatbot would you like to talk to?')\n    botcount = len(bots)\n    for i in range(botcount):\n        print('  %d: %s' % (i+1, bots[i][1]))\n    while True:\n        print('\\nEnter a number in the range 1-%d: ' % botcount, end=' ')\n        choice = sys.stdin.readline().strip()\n        if choice.isdigit() and (int(choice) - 1) in range(botcount):\n            break\n        else:\n            print('   Error: bad chatbot number')\n\n    chatbot = bots[int(choice)-1][0]\n    chatbot()\n", 1], "nltk\\chunk\\api": [".py", "\n\nfrom nltk.parse import ParserI\n\nfrom nltk.chunk.util import ChunkScore\n\nclass ChunkParserI(ParserI):\n    def parse(self, tokens):\n        raise NotImplementedError()\n\n    def evaluate(self, gold):\n        chunkscore = ChunkScore()\n        for correct in gold:\n            chunkscore.score(correct, self.parse(correct.leaves()))\n        return chunkscore\n\n"], "nltk\\chunk\\named_entity": [".py", "\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os, re, pickle\nfrom xml.etree import ElementTree as ET\n\nfrom nltk.tag import ClassifierBasedTagger, pos_tag\n\ntry:\n    from nltk.classify import MaxentClassifier\nexcept ImportError:\n    pass\n\nfrom nltk.tree import Tree\nfrom nltk.tokenize import word_tokenize\nfrom nltk.data import find\n\nfrom nltk.chunk.api import ChunkParserI\nfrom nltk.chunk.util import ChunkScore\n\nclass NEChunkParserTagger(ClassifierBasedTagger):\n    def __init__(self, train):\n        ClassifierBasedTagger.__init__(\n            self, train=train,\n            classifier_builder=self._classifier_builder)\n\n    def _classifier_builder(self, train):\n        return MaxentClassifier.train(train, algorithm='megam',\n                                           gaussian_prior_sigma=1,\n                                           trace=2)\n\n    def _english_wordlist(self):\n        try:\n            wl = self._en_wordlist\n        except AttributeError:\n            from nltk.corpus import words\n            self._en_wordlist = set(words.words('en-basic'))\n            wl = self._en_wordlist\n        return wl\n\n    def _feature_detector(self, tokens, index, history):\n        word = tokens[index][0]\n        pos = simplify_pos(tokens[index][1])\n        if index == 0:\n            prevword = prevprevword = None\n            prevpos = prevprevpos = None\n            prevshape = prevtag = prevprevtag = None\n        elif index == 1:\n            prevword = tokens[index-1][0].lower()\n            prevprevword = None\n            prevpos = simplify_pos(tokens[index-1][1])\n            prevprevpos = None\n            prevtag = history[index-1][0]\n            prevshape = prevprevtag = None\n        else:\n            prevword = tokens[index-1][0].lower()\n            prevprevword = tokens[index-2][0].lower()\n            prevpos = simplify_pos(tokens[index-1][1])\n            prevprevpos = simplify_pos(tokens[index-2][1])\n            prevtag = history[index-1]\n            prevprevtag = history[index-2]\n            prevshape = shape(prevword)\n        if index == len(tokens)-1:\n            nextword = nextnextword = None\n            nextpos = nextnextpos = None\n        elif index == len(tokens)-2:\n            nextword = tokens[index+1][0].lower()\n            nextpos = tokens[index+1][1].lower()\n            nextnextword = None\n            nextnextpos = None\n        else:\n            nextword = tokens[index+1][0].lower()\n            nextpos = tokens[index+1][1].lower()\n            nextnextword = tokens[index+2][0].lower()\n            nextnextpos = tokens[index+2][1].lower()\n\n        features = {\n            'bias': True,\n            'shape': shape(word),\n            'wordlen': len(word),\n            'prefix3': word[:3].lower(),\n            'suffix3': word[-3:].lower(),\n            'pos': pos,\n            'word': word,\n            'en-wordlist': (word in self._english_wordlist()),\n            'prevtag': prevtag,\n            'prevpos': prevpos,\n            'nextpos': nextpos,\n            'prevword': prevword,\n            'nextword': nextword,\n            'word+nextpos': '{0}+{1}'.format(word.lower(), nextpos),\n            'pos+prevtag': '{0}+{1}'.format(pos, prevtag),\n            'shape+prevtag': '{0}+{1}'.format(prevshape, prevtag),\n            }\n\n        return features\n\nclass NEChunkParser(ChunkParserI):\n    def __init__(self, train):\n        self._train(train)\n\n    def parse(self, tokens):\n        tagged = self._tagger.tag(tokens)\n        tree = self._tagged_to_parse(tagged)\n        return tree\n\n    def _train(self, corpus):\n        corpus = [self._parse_to_tagged(s) for s in corpus]\n\n        self._tagger = NEChunkParserTagger(train=corpus)\n\n    def _tagged_to_parse(self, tagged_tokens):\n        sent = Tree('S', [])\n\n        for (tok,tag) in tagged_tokens:\n            if tag == 'O':\n                sent.append(tok)\n            elif tag.startswith('B-'):\n                sent.append(Tree(tag[2:], [tok]))\n            elif tag.startswith('I-'):\n                if (sent and isinstance(sent[-1], Tree) and\n                    sent[-1].label() == tag[2:]):\n                    sent[-1].append(tok)\n                else:\n                    sent.append(Tree(tag[2:], [tok]))\n        return sent\n\n    @staticmethod\n    def _parse_to_tagged(sent):\n        toks = []\n        for child in sent:\n            if isinstance(child, Tree):\n                if len(child) == 0:\n                    print(\"Warning -- empty chunk in sentence\")\n                    continue\n                toks.append((child[0], 'B-{0}'.format(child.label())))\n                for tok in child[1:]:\n                    toks.append((tok, 'I-{0}'.format(child.label())))\n            else:\n                toks.append((child, 'O'))\n        return toks\n\ndef shape(word):\n    if re.match('[0-9]+(\\.[0-9]*)?|[0-9]*\\.[0-9]+$', word, re.UNICODE):\n        return 'number'\n    elif re.match('\\W+$', word, re.UNICODE):\n        return 'punct'\n    elif re.match('\\w+$', word, re.UNICODE):\n        if word.istitle():\n            return 'upcase'\n        elif word.islower():\n            return 'downcase'\n        else:\n            return 'mixedcase'\n    else:\n        return 'other'\n\ndef simplify_pos(s):\n    if s.startswith('V'): return \"V\"\n    else: return s.split('-')[0]\n\ndef postag_tree(tree):\n    words = tree.leaves()\n    tag_iter = (pos for (word, pos) in pos_tag(words))\n    newtree = Tree('S', [])\n    for child in tree:\n        if isinstance(child, Tree):\n            newtree.append(Tree(child.label(), []))\n            for subchild in child:\n                newtree[-1].append( (subchild, next(tag_iter)) )\n        else:\n            newtree.append( (child, next(tag_iter)) )\n    return newtree\n\ndef load_ace_data(roots, fmt='binary', skip_bnews=True):\n    for root in roots:\n        for root, dirs, files in os.walk(root):\n            if root.endswith('bnews') and skip_bnews:\n                continue\n            for f in files:\n                if f.endswith('.sgm'):\n                    for sent in load_ace_file(os.path.join(root, f), fmt):\n                        yield sent\n\ndef load_ace_file(textfile, fmt):\n    print('  - {0}'.format(os.path.split(textfile)[1]))\n    annfile = textfile+'.tmx.rdc.xml'\n\n    entities = []\n    with open(annfile, 'r') as infile:\n        xml = ET.parse(infile).getroot()\n    for entity in xml.findall('document/entity'):\n        typ = entity.find('entity_type').text\n        for mention in entity.findall('entity_mention'):\n            if mention.get('TYPE') != 'NAME': continue # only NEs\n            s = int(mention.find('head/charseq/start').text)\n            e = int(mention.find('head/charseq/end').text)+1\n            entities.append( (s, e, typ) )\n\n    with open(textfile, 'r') as infile:\n        text = infile.read()\n\n    text = re.sub('<(?!/?TEXT)[^>]+>', '', text)\n\n    def subfunc(m): return ' '*(m.end()-m.start()-6)\n    text = re.sub('[\\s\\S]*<TEXT>', subfunc, text)\n    text = re.sub('</TEXT>[\\s\\S]*', '', text)\n\n    text = re.sub(\"``\", ' \"', text)\n    text = re.sub(\"''\", '\" ', text)\n\n    entity_types = set(typ for (s,e,typ) in entities)\n\n    if fmt == 'binary':\n        i = 0\n        toks = Tree('S', [])\n        for (s,e,typ) in sorted(entities):\n            if s < i: s = i # Overlapping!  Deal with this better?\n            if e <= s: continue\n            toks.extend(word_tokenize(text[i:s]))\n            toks.append(Tree('NE', text[s:e].split()))\n            i = e\n        toks.extend(word_tokenize(text[i:]))\n        yield toks\n\n    elif fmt == 'multiclass':\n        i = 0\n        toks = Tree('S', [])\n        for (s,e,typ) in sorted(entities):\n            if s < i: s = i # Overlapping!  Deal with this better?\n            if e <= s: continue\n            toks.extend(word_tokenize(text[i:s]))\n            toks.append(Tree(typ, text[s:e].split()))\n            i = e\n        toks.extend(word_tokenize(text[i:]))\n        yield toks\n\n    else:\n        raise ValueError('bad fmt value')\n\ndef cmp_chunks(correct, guessed):\n    correct = NEChunkParser._parse_to_tagged(correct)\n    guessed = NEChunkParser._parse_to_tagged(guessed)\n    ellipsis = False\n    for (w, ct), (w, gt) in zip(correct, guessed):\n        if ct == gt == 'O':\n            if not ellipsis:\n                print(\"  {:15} {:15} {2}\".format(ct, gt, w))\n                print('  {:15} {:15} {2}'.format('...', '...', '...'))\n                ellipsis = True\n        else:\n            ellipsis = False\n            print(\"  {:15} {:15} {2}\".format(ct, gt, w))\n\ndef build_model(fmt='binary'):\n    print('Loading training data...')\n    train_paths = [find('corpora/ace_data/ace.dev'),\n                   find('corpora/ace_data/ace.heldout'),\n                   find('corpora/ace_data/bbn.dev'),\n                   find('corpora/ace_data/muc.dev')]\n    train_trees = load_ace_data(train_paths, fmt)\n    train_data = [postag_tree(t) for t in train_trees]\n    print('Training...')\n    cp = NEChunkParser(train_data)\n    del train_data\n\n    print('Loading eval data...')\n    eval_paths = [find('corpora/ace_data/ace.eval')]\n    eval_trees = load_ace_data(eval_paths, fmt)\n    eval_data = [postag_tree(t) for t in eval_trees]\n\n    print('Evaluating...')\n    chunkscore = ChunkScore()\n    for i, correct in enumerate(eval_data):\n        guess = cp.parse(correct.leaves())\n        chunkscore.score(correct, guess)\n        if i < 3: cmp_chunks(correct, guess)\n    print(chunkscore)\n\n    outfilename = '/tmp/ne_chunker_{0}.pickle'.format(fmt)\n    print('Saving chunker to {0}...'.format(outfilename))\n\n    with open(outfilename, 'wb') as outfile:\n        pickle.dump(cp, outfile, -1)\n\n    return cp\n\n\nif __name__ == '__main__':\n    from nltk.chunk.named_entity import build_model\n\n    build_model('binary')\n    build_model('multiclass')\n"], "nltk\\chunk\\regexp": [".py", "from __future__ import print_function, unicode_literals\nfrom __future__ import division\n\nimport re\n\nfrom six import string_types\n\nfrom nltk.tree import Tree\nfrom nltk.chunk.api import ChunkParserI\nfrom nltk.compat import python_2_unicode_compatible, unicode_repr\n\n\n@python_2_unicode_compatible\nclass ChunkString(object):\n    CHUNK_TAG_CHAR = r'[^\\{\\}<>]'\n    CHUNK_TAG = r'(<%s+?>)' % CHUNK_TAG_CHAR\n\n    IN_CHUNK_PATTERN = r'(?=[^\\{]*\\})'\n    IN_CHINK_PATTERN = r'(?=[^\\}]*(\\{|$))'\n\n    _CHUNK = r'(\\{%s+?\\})+?' % CHUNK_TAG\n    _CHINK = r'(%s+?)+?' % CHUNK_TAG\n    _VALID = re.compile(r'^(\\{?%s\\}?)*?$' % CHUNK_TAG)\n    _BRACKETS = re.compile('[^\\{\\}]+')\n    _BALANCED_BRACKETS = re.compile(r'(\\{\\})*$')\n\n    def __init__(self, chunk_struct, debug_level=1):\n        self._root_label = chunk_struct.label()\n        self._pieces = chunk_struct[:]\n        tags = [self._tag(tok) for tok in self._pieces]\n        self._str = '<' + '><'.join(tags) + '>'\n        self._debug = debug_level\n\n    def _tag(self, tok):\n        if isinstance(tok, tuple):\n            return tok[1]\n        elif isinstance(tok, Tree):\n            return tok.label()\n        else:\n            raise ValueError('chunk structures must contain tagged '\n                             'tokens or trees')\n\n    def _verify(self, s, verify_tags):\n        if not ChunkString._VALID.match(s):\n            raise ValueError('Transformation generated invalid '\n                             'chunkstring:\\n  %s' % s)\n\n        brackets = ChunkString._BRACKETS.sub('', s)\n        for i in range(1 + len(brackets) // 5000):\n            substr = brackets[i*5000:i*5000+5000]\n            if not ChunkString._BALANCED_BRACKETS.match(substr):\n                raise ValueError('Transformation generated invalid '\n                                 'chunkstring:\\n  %s' % s)\n\n        if verify_tags<=0: return\n\n        tags1 = (re.split(r'[\\{\\}<>]+', s))[1:-1]\n        tags2 = [self._tag(piece) for piece in self._pieces]\n        if tags1 != tags2:\n            raise ValueError('Transformation generated invalid '\n                             'chunkstring: tag changed')\n\n    def to_chunkstruct(self, chunk_label='CHUNK'):\n        if self._debug > 0: self._verify(self._str, 1)\n\n        pieces = []\n        index = 0\n        piece_in_chunk = 0\n        for piece in re.split('[{}]', self._str):\n\n            length = piece.count('<')\n            subsequence = self._pieces[index:index+length]\n\n            if piece_in_chunk:\n                pieces.append(Tree(chunk_label, subsequence))\n            else:\n                pieces += subsequence\n\n            index += length\n            piece_in_chunk = not piece_in_chunk\n\n        return Tree(self._root_label, pieces)\n\n    def xform(self, regexp, repl):\n        s = re.sub(regexp, repl, self._str)\n\n        s = re.sub('\\{\\}', '', s)\n\n        if self._debug > 1: self._verify(s, self._debug-2)\n\n        self._str = s\n\n    def __repr__(self):\n        return '<ChunkString: %s>' % unicode_repr(self._str)\n\n    def __str__(self):\n        str = re.sub(r'>(?!\\})', r'> ', self._str)\n        str = re.sub(r'([^\\{])<', r'\\1 <', str)\n        if str[0] == '<': str = ' ' + str\n        return str\n\n\n@python_2_unicode_compatible\nclass RegexpChunkRule(object):\n    def __init__(self, regexp, repl, descr):\n        if isinstance(regexp, string_types):\n            regexp = re.compile(regexp)\n        self._repl = repl\n        self._descr = descr\n        self._regexp = regexp\n\n    def apply(self, chunkstr):\n        chunkstr.xform(self._regexp, self._repl)\n\n    def descr(self):\n        return self._descr\n\n    def __repr__(self):\n        return ('<RegexpChunkRule: '+unicode_repr(self._regexp.pattern)+\n                '->'+unicode_repr(self._repl)+'>')\n\n    @staticmethod\n    def fromstring(s):\n        m = re.match(r'(?P<rule>(\\\\.|[^#])*)(?P<comment>#.*)?', s)\n        rule = m.group('rule').strip()\n        comment = (m.group('comment') or '')[1:].strip()\n\n        try:\n            if not rule:\n                raise ValueError('Empty chunk pattern')\n            if rule[0] == '{' and rule[-1] == '}':\n                return ChunkRule(rule[1:-1], comment)\n            elif rule[0] == '}' and rule[-1] == '{':\n                return ChinkRule(rule[1:-1], comment)\n            elif '}{' in rule:\n                left, right = rule.split('}{')\n                return SplitRule(left, right, comment)\n            elif '{}' in rule:\n                left, right = rule.split('{}')\n                return MergeRule(left, right, comment)\n            elif re.match('[^{}]*{[^{}]*}[^{}]*', rule):\n                left, chunk, right = re.split('[{}]', rule)\n                return ChunkRuleWithContext(left, chunk, right, comment)\n            else:\n                raise ValueError('Illegal chunk pattern: %s' % rule)\n        except (ValueError, re.error):\n            raise ValueError('Illegal chunk pattern: %s' % rule)\n\n\n@python_2_unicode_compatible\nclass ChunkRule(RegexpChunkRule):\n    def __init__(self, tag_pattern, descr):\n\n        self._pattern = tag_pattern\n        regexp = re.compile('(?P<chunk>%s)%s' %\n                            (tag_pattern2re_pattern(tag_pattern),\n                             ChunkString.IN_CHINK_PATTERN))\n        RegexpChunkRule.__init__(self, regexp, '{\\g<chunk>}', descr)\n\n    def __repr__(self):\n        return '<ChunkRule: '+unicode_repr(self._pattern)+'>'\n\n@python_2_unicode_compatible\nclass ChinkRule(RegexpChunkRule):\n    def __init__(self, tag_pattern, descr):\n        self._pattern = tag_pattern\n        regexp = re.compile('(?P<chink>%s)%s' %\n                            (tag_pattern2re_pattern(tag_pattern),\n                             ChunkString.IN_CHUNK_PATTERN))\n        RegexpChunkRule.__init__(self, regexp, '}\\g<chink>{', descr)\n\n    def __repr__(self):\n        return '<ChinkRule: '+unicode_repr(self._pattern)+'>'\n\n\n@python_2_unicode_compatible\nclass UnChunkRule(RegexpChunkRule):\n    def __init__(self, tag_pattern, descr):\n        self._pattern = tag_pattern\n        regexp = re.compile('\\{(?P<chunk>%s)\\}' %\n                            tag_pattern2re_pattern(tag_pattern))\n        RegexpChunkRule.__init__(self, regexp, '\\g<chunk>', descr)\n\n    def __repr__(self):\n        return '<UnChunkRule: '+unicode_repr(self._pattern)+'>'\n\n\n@python_2_unicode_compatible\nclass MergeRule(RegexpChunkRule):\n    def __init__(self, left_tag_pattern, right_tag_pattern, descr):\n        re.compile(tag_pattern2re_pattern(left_tag_pattern))\n        re.compile(tag_pattern2re_pattern(right_tag_pattern))\n\n        self._left_tag_pattern = left_tag_pattern\n        self._right_tag_pattern = right_tag_pattern\n        regexp = re.compile('(?P<left>%s)}{(?=%s)' %\n                            (tag_pattern2re_pattern(left_tag_pattern),\n                             tag_pattern2re_pattern(right_tag_pattern)))\n        RegexpChunkRule.__init__(self, regexp, '\\g<left>', descr)\n\n    def __repr__(self):\n        return ('<MergeRule: '+unicode_repr(self._left_tag_pattern)+', '+\n                unicode_repr(self._right_tag_pattern)+'>')\n\n\n@python_2_unicode_compatible\nclass SplitRule(RegexpChunkRule):\n    def __init__(self, left_tag_pattern, right_tag_pattern, descr):\n        re.compile(tag_pattern2re_pattern(left_tag_pattern))\n        re.compile(tag_pattern2re_pattern(right_tag_pattern))\n\n        self._left_tag_pattern = left_tag_pattern\n        self._right_tag_pattern = right_tag_pattern\n        regexp = re.compile('(?P<left>%s)(?=%s)' %\n                            (tag_pattern2re_pattern(left_tag_pattern),\n                             tag_pattern2re_pattern(right_tag_pattern)))\n        RegexpChunkRule.__init__(self, regexp, r'\\g<left>}{', descr)\n\n    def __repr__(self):\n        return ('<SplitRule: '+unicode_repr(self._left_tag_pattern)+', '+\n                unicode_repr(self._right_tag_pattern)+'>')\n\n\n@python_2_unicode_compatible\nclass ExpandLeftRule(RegexpChunkRule):\n    def __init__(self, left_tag_pattern, right_tag_pattern, descr):\n        re.compile(tag_pattern2re_pattern(left_tag_pattern))\n        re.compile(tag_pattern2re_pattern(right_tag_pattern))\n\n        self._left_tag_pattern = left_tag_pattern\n        self._right_tag_pattern = right_tag_pattern\n        regexp = re.compile('(?P<left>%s)\\{(?P<right>%s)' %\n                            (tag_pattern2re_pattern(left_tag_pattern),\n                             tag_pattern2re_pattern(right_tag_pattern)))\n        RegexpChunkRule.__init__(self, regexp, '{\\g<left>\\g<right>', descr)\n\n    def __repr__(self):\n        return ('<ExpandLeftRule: '+unicode_repr(self._left_tag_pattern)+', '+\n                unicode_repr(self._right_tag_pattern)+'>')\n\n\n@python_2_unicode_compatible\nclass ExpandRightRule(RegexpChunkRule):\n    def __init__(self, left_tag_pattern, right_tag_pattern, descr):\n        re.compile(tag_pattern2re_pattern(left_tag_pattern))\n        re.compile(tag_pattern2re_pattern(right_tag_pattern))\n\n        self._left_tag_pattern = left_tag_pattern\n        self._right_tag_pattern = right_tag_pattern\n        regexp = re.compile('(?P<left>%s)\\}(?P<right>%s)' %\n                            (tag_pattern2re_pattern(left_tag_pattern),\n                             tag_pattern2re_pattern(right_tag_pattern)))\n        RegexpChunkRule.__init__(self, regexp, '\\g<left>\\g<right>}', descr)\n\n    def __repr__(self):\n        return ('<ExpandRightRule: '+unicode_repr(self._left_tag_pattern)+', '+\n                unicode_repr(self._right_tag_pattern)+'>')\n\n\n@python_2_unicode_compatible\nclass ChunkRuleWithContext(RegexpChunkRule):\n    def __init__(self, left_context_tag_pattern, chunk_tag_pattern,\n                 right_context_tag_pattern, descr):\n        re.compile(tag_pattern2re_pattern(left_context_tag_pattern))\n        re.compile(tag_pattern2re_pattern(chunk_tag_pattern))\n        re.compile(tag_pattern2re_pattern(right_context_tag_pattern))\n\n        self._left_context_tag_pattern = left_context_tag_pattern\n        self._chunk_tag_pattern = chunk_tag_pattern\n        self._right_context_tag_pattern = right_context_tag_pattern\n        regexp = re.compile('(?P<left>%s)(?P<chunk>%s)(?P<right>%s)%s' %\n                            (tag_pattern2re_pattern(left_context_tag_pattern),\n                             tag_pattern2re_pattern(chunk_tag_pattern),\n                             tag_pattern2re_pattern(right_context_tag_pattern),\n                             ChunkString.IN_CHINK_PATTERN))\n        replacement = r'\\g<left>{\\g<chunk>}\\g<right>'\n        RegexpChunkRule.__init__(self, regexp, replacement, descr)\n\n    def __repr__(self):\n        return '<ChunkRuleWithContext:  %r, %r, %r>' % (\n            self._left_context_tag_pattern, self._chunk_tag_pattern,\n            self._right_context_tag_pattern)\n\n\nCHUNK_TAG_PATTERN = re.compile(r'^((%s|<%s>)*)$' %\n                                ('([^\\{\\}<>]|\\{\\d+,?\\}|\\{\\d*,\\d+\\})+',\n                                 '[^\\{\\}<>]+'))\n\n\n\n\n\ndef tag_pattern2re_pattern(tag_pattern):\n    tag_pattern = re.sub(r'\\s', '', tag_pattern)\n    tag_pattern = re.sub(r'<', '(<(', tag_pattern)\n    tag_pattern = re.sub(r'>', ')>)', tag_pattern)\n\n    if not CHUNK_TAG_PATTERN.match(tag_pattern):\n        raise ValueError('Bad tag pattern: %r' % tag_pattern)\n\n    def reverse_str(str):\n        lst = list(str)\n        lst.reverse()\n        return ''.join(lst)\n    tc_rev = reverse_str(ChunkString.CHUNK_TAG_CHAR)\n    reversed = reverse_str(tag_pattern)\n    reversed = re.sub(r'\\.(?!\\\\(\\\\\\\\)*($|[^\\\\]))', tc_rev, reversed)\n    tag_pattern = reverse_str(reversed)\n\n    return tag_pattern\n\n\n\n@python_2_unicode_compatible\nclass RegexpChunkParser(ChunkParserI):\n    def __init__(self, rules, chunk_label='NP', root_label='S', trace=0):\n        self._rules = rules\n        self._trace = trace\n        self._chunk_label = chunk_label\n        self._root_label = root_label\n\n    def _trace_apply(self, chunkstr, verbose):\n        print('# Input:')\n        print(chunkstr)\n        for rule in self._rules:\n            rule.apply(chunkstr)\n            if verbose:\n                print('#', rule.descr()+' ('+unicode_repr(rule)+'):')\n            else:\n                print('#', rule.descr()+':')\n            print(chunkstr)\n\n    def _notrace_apply(self, chunkstr):\n\n        for rule in self._rules:\n            rule.apply(chunkstr)\n\n    def parse(self, chunk_struct, trace=None):\n        if len(chunk_struct) == 0:\n            print('Warning: parsing empty text')\n            return Tree(self._root_label, [])\n\n        try:\n            chunk_struct.label()\n        except AttributeError:\n            chunk_struct = Tree(self._root_label, chunk_struct)\n\n        if trace is None: trace = self._trace\n\n        chunkstr = ChunkString(chunk_struct)\n\n        if trace:\n            verbose = (trace>1)\n            self._trace_apply(chunkstr, verbose)\n        else:\n            self._notrace_apply(chunkstr)\n\n        return chunkstr.to_chunkstruct(self._chunk_label)\n\n    def rules(self):\n        return self._rules\n\n    def __repr__(self):\n        return \"<RegexpChunkParser with %d rules>\" % len(self._rules)\n\n    def __str__(self):\n        s = \"RegexpChunkParser with %d rules:\\n\" % len(self._rules)\n        margin = 0\n        for rule in self._rules:\n            margin = max(margin, len(rule.descr()))\n        if margin < 35:\n            format = \"    %\" + repr(-(margin+3)) + \"s%s\\n\"\n        else:\n            format = \"    %s\\n      %s\\n\"\n        for rule in self._rules:\n            s += format % (rule.descr(), unicode_repr(rule))\n        return s[:-1]\n\n\n@python_2_unicode_compatible\nclass RegexpParser(ChunkParserI):\n    def __init__(self, grammar, root_label='S', loop=1, trace=0):\n        self._trace = trace\n        self._stages = []\n        self._grammar = grammar\n        self._loop = loop\n\n        if isinstance(grammar, string_types):\n            self._read_grammar(grammar, root_label, trace)\n        else:\n            type_err = ('Expected string or list of RegexpChunkParsers '\n                        'for the grammar.')\n            try: grammar = list(grammar)\n            except: raise TypeError(type_err)\n            for elt in grammar:\n                if not isinstance(elt, RegexpChunkParser):\n                    raise TypeError(type_err)\n            self._stages = grammar\n\n    def _read_grammar(self, grammar, root_label, trace):\n        rules = []\n        lhs = None\n        for line in grammar.split('\\n'):\n            line = line.strip()\n\n            m = re.match('(?P<nonterminal>(\\\\.|[^:])*)(:(?P<rule>.*))', line)\n            if m:\n                self._add_stage(rules, lhs, root_label, trace)\n                lhs = m.group('nonterminal').strip()\n                rules = []\n                line = m.group('rule').strip()\n\n            if line=='' or line.startswith('#'): continue\n\n            rules.append(RegexpChunkRule.fromstring(line))\n\n        self._add_stage(rules, lhs, root_label, trace)\n\n    def _add_stage(self, rules, lhs, root_label, trace):\n        if rules != []:\n            if not lhs:\n                raise ValueError('Expected stage marker (eg NP:)')\n            parser = RegexpChunkParser(rules, chunk_label=lhs,\n                                       root_label=root_label, trace=trace)\n            self._stages.append(parser)\n\n    def parse(self, chunk_struct, trace=None):\n        if trace is None: trace = self._trace\n        for i in range(self._loop):\n            for parser in self._stages:\n                chunk_struct = parser.parse(chunk_struct, trace=trace)\n        return chunk_struct\n\n    def __repr__(self):\n        return \"<chunk.RegexpParser with %d stages>\" % len(self._stages)\n\n    def __str__(self):\n        s = \"chunk.RegexpParser with %d stages:\\n\" % len(self._stages)\n        margin = 0\n        for parser in self._stages:\n            s += \"%s\\n\" % parser\n        return s[:-1]\n\n\ndef demo_eval(chunkparser, text):\n    from nltk import chunk\n    from nltk.tree import Tree\n\n    chunkscore = chunk.ChunkScore()\n\n    for sentence in text.split('\\n'):\n        print(sentence)\n        sentence = sentence.strip()\n        if not sentence: continue\n        gold = chunk.tagstr2tree(sentence)\n        tokens = gold.leaves()\n        test = chunkparser.parse(Tree('S', tokens), trace=1)\n        chunkscore.score(gold, test)\n        print()\n\n    print('/'+('='*75)+'\\\\')\n    print('Scoring', chunkparser)\n    print(('-'*77))\n    print('Precision: %5.1f%%' % (chunkscore.precision()*100), ' '*4, end=' ')\n    print('Recall: %5.1f%%' % (chunkscore.recall()*100), ' '*6, end=' ')\n    print('F-Measure: %5.1f%%' % (chunkscore.f_measure()*100))\n\n\n    if chunkscore.missed():\n        print('Missed:')\n        missed = chunkscore.missed()\n        for chunk in missed[:10]:\n            print('  ', ' '.join(map(str,chunk)))\n        if len(chunkscore.missed()) > 10:\n            print('  ...')\n\n    if chunkscore.incorrect():\n        print('Incorrect:')\n        incorrect = chunkscore.incorrect()\n        for chunk in incorrect[:10]:\n            print('  ', ' '.join(map(str,chunk)))\n        if len(chunkscore.incorrect()) > 10:\n            print('  ...')\n\n    print('\\\\'+('='*75)+'/')\n    print()\n\ndef demo():\n\n    from nltk import chunk, Tree\n\n    text = \"\"\"\\\n    [ the/DT little/JJ cat/NN ] sat/VBD on/IN [ the/DT mat/NN ] ./.\n    [ John/NNP ] saw/VBD [the/DT cats/NNS] [the/DT dog/NN] chased/VBD ./.\n    [ John/NNP ] thinks/VBZ [ Mary/NN ] saw/VBD [ the/DT cat/NN ] sit/VB on/IN [ the/DT mat/NN ]./.\n    \"\"\"\n\n    print('*'*75)\n    print('Evaluation text:')\n    print(text)\n    print('*'*75)\n    print()\n\n    grammar = r\"\"\"\n    NP:                   # NP stage\n      {<DT>?<JJ>*<NN>}    # chunk determiners, adjectives and nouns\n      {<NNP>+}            # chunk proper nouns\n    \"\"\"\n    cp = chunk.RegexpParser(grammar)\n    demo_eval(cp, text)\n\n    grammar = r\"\"\"\n    NP:\n      {<.*>}              # start by chunking each tag\n      }<[\\.VI].*>+{       # unchunk any verbs, prepositions or periods\n      <DT|JJ>{}<NN.*>     # merge det/adj with nouns\n    \"\"\"\n    cp = chunk.RegexpParser(grammar)\n    demo_eval(cp, text)\n\n    grammar = r\"\"\"\n    NP: {<DT>?<JJ>*<NN>}    # chunk determiners, adjectives and nouns\n    VP: {<TO>?<VB.*>}       # VP = verb words\n    \"\"\"\n    cp = chunk.RegexpParser(grammar)\n    demo_eval(cp, text)\n\n    grammar = r\"\"\"\n    NP: {<.*>*}             # start by chunking everything\n        }<[\\.VI].*>+{       # chink any verbs, prepositions or periods\n        <.*>}{<DT>          # separate on determiners\n    PP: {<IN><NP>}          # PP = preposition + noun phrase\n    VP: {<VB.*><NP|PP>*}    # VP = verb words + NPs and PPs\n    \"\"\"\n    cp = chunk.RegexpParser(grammar)\n    demo_eval(cp, text)\n\n\n    from nltk.corpus import conll2000\n\n    print()\n    print(\"Demonstration of empty grammar:\")\n\n    cp = chunk.RegexpParser(\"\")\n    print(chunk.accuracy(cp, conll2000.chunked_sents('test.txt',\n                                                     chunk_types=('NP',))))\n\n    print()\n    print(\"Demonstration of accuracy evaluation using CoNLL tags:\")\n\n    grammar = r\"\"\"\n    NP:\n      {<.*>}              # start by chunking each tag\n      }<[\\.VI].*>+{       # unchunk any verbs, prepositions or periods\n      <DT|JJ>{}<NN.*>     # merge det/adj with nouns\n    \"\"\"\n    cp = chunk.RegexpParser(grammar)\n    print(chunk.accuracy(cp, conll2000.chunked_sents('test.txt')[:5]))\n\n    print()\n    print(\"Demonstration of tagged token input\")\n\n    grammar = r\"\"\"\n    NP: {<.*>*}             # start by chunking everything\n        }<[\\.VI].*>+{       # chink any verbs, prepositions or periods\n        <.*>}{<DT>          # separate on determiners\n    PP: {<IN><NP>}          # PP = preposition + noun phrase\n    VP: {<VB.*><NP|PP>*}    # VP = verb words + NPs and PPs\n    \"\"\"\n    cp = chunk.RegexpParser(grammar)\n    print(cp.parse([(\"the\",\"DT\"), (\"little\",\"JJ\"), (\"cat\", \"NN\"),\n                    (\"sat\", \"VBD\"), (\"on\", \"IN\"), (\"the\", \"DT\"),\n                    (\"mat\", \"NN\"), (\".\", \".\")]))\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\chunk\\util": [".py", "from __future__ import print_function, unicode_literals, division\n\nimport re\n\nfrom nltk.tree import Tree\nfrom nltk.tag.mapping import map_tag\nfrom nltk.tag.util import str2tuple\nfrom nltk.compat import python_2_unicode_compatible\n\n\nfrom nltk.metrics import accuracy as _accuracy\ndef accuracy(chunker, gold):\n\n    gold_tags = []\n    test_tags = []\n    for gold_tree in gold:\n        test_tree = chunker.parse(gold_tree.flatten())\n        gold_tags += tree2conlltags(gold_tree)\n        test_tags += tree2conlltags(test_tree)\n\n    return _accuracy(gold_tags, test_tags)\n\n\nclass ChunkScore(object):\n    def __init__(self, **kwargs):\n        self._correct = set()\n        self._guessed = set()\n        self._tp = set()\n        self._fp = set()\n        self._fn = set()\n        self._max_tp = kwargs.get('max_tp_examples', 100)\n        self._max_fp = kwargs.get('max_fp_examples', 100)\n        self._max_fn = kwargs.get('max_fn_examples', 100)\n        self._chunk_label = kwargs.get('chunk_label', '.*')\n        self._tp_num = 0\n        self._fp_num = 0\n        self._fn_num = 0\n        self._count = 0\n        self._tags_correct = 0.0\n        self._tags_total = 0.0\n\n        self._measuresNeedUpdate = False\n\n    def _updateMeasures(self):\n        if (self._measuresNeedUpdate):\n           self._tp = self._guessed & self._correct\n           self._fn = self._correct - self._guessed\n           self._fp = self._guessed - self._correct\n           self._tp_num = len(self._tp)\n           self._fp_num = len(self._fp)\n           self._fn_num = len(self._fn)\n           self._measuresNeedUpdate = False\n\n    def score(self, correct, guessed):\n        self._correct |= _chunksets(correct, self._count, self._chunk_label)\n        self._guessed |= _chunksets(guessed, self._count, self._chunk_label)\n        self._count += 1\n        self._measuresNeedUpdate = True\n        try:\n            correct_tags = tree2conlltags(correct)\n            guessed_tags = tree2conlltags(guessed)\n        except ValueError:\n            correct_tags = guessed_tags = ()\n        self._tags_total += len(correct_tags)\n        self._tags_correct += sum(1 for (t,g) in zip(guessed_tags,\n                                                     correct_tags)\n                                  if t==g)\n\n    def accuracy(self):\n        if self._tags_total == 0: return 1\n        return self._tags_correct/self._tags_total\n\n    def precision(self):\n        self._updateMeasures()\n        div = self._tp_num + self._fp_num\n        if div == 0: return 0\n        else: return self._tp_num / div\n\n    def recall(self):\n        self._updateMeasures()\n        div = self._tp_num + self._fn_num\n        if div == 0: return 0\n        else: return self._tp_num / div\n\n    def f_measure(self, alpha=0.5):\n        self._updateMeasures()\n        p = self.precision()\n        r = self.recall()\n        if p == 0 or r == 0:    # what if alpha is 0 or 1?\n            return 0\n        return 1/(alpha/p + (1-alpha)/r)\n\n    def missed(self):\n        self._updateMeasures()\n        chunks = list(self._fn)\n        return [c[1] for c in chunks]  # discard position information\n\n    def incorrect(self):\n        self._updateMeasures()\n        chunks = list(self._fp)\n        return [c[1] for c in chunks]  # discard position information\n\n    def correct(self):\n        chunks = list(self._correct)\n        return [c[1] for c in chunks]  # discard position information\n\n    def guessed(self):\n        chunks = list(self._guessed)\n        return [c[1] for c in chunks]  # discard position information\n\n    def __len__(self):\n        self._updateMeasures()\n        return self._tp_num + self._fn_num\n\n    def __repr__(self):\n        return '<ChunkScoring of '+repr(len(self))+' chunks>'\n\n    def __str__(self):\n        return (\"ChunkParse score:\\n\" +\n                (\"    IOB Accuracy: {:5.1f}%%\\n\".format(self.accuracy()*100)) +\n                (\"    Precision:    {:5.1f}%%\\n\".format(self.precision()*100)) +\n                (\"    Recall:       {:5.1f}%%\\n\".format(self.recall()*100))+\n                (\"    F-Measure:    {:5.1f}%%\".format(self.f_measure()*100)))\n\ndef _chunksets(t, count, chunk_label):\n    pos = 0\n    chunks = []\n    for child in t:\n        if isinstance(child, Tree):\n            if re.match(chunk_label, child.label()):\n                chunks.append(((count, pos), child.freeze()))\n            pos += len(child.leaves())\n        else:\n            pos += 1\n    return set(chunks)\n\n\ndef tagstr2tree(s, chunk_label=\"NP\", root_label=\"S\", sep='/',\n                source_tagset=None, target_tagset=None):\n\n    WORD_OR_BRACKET = re.compile(r'\\[|\\]|[^\\[\\]\\s]+')\n\n    stack = [Tree(root_label, [])]\n    for match in WORD_OR_BRACKET.finditer(s):\n        text = match.group()\n        if text[0] == '[':\n            if len(stack) != 1:\n                raise ValueError('Unexpected [ at char {:d}'.format(match.start()))\n            chunk = Tree(chunk_label, [])\n            stack[-1].append(chunk)\n            stack.append(chunk)\n        elif text[0] == ']':\n            if len(stack) != 2:\n                raise ValueError('Unexpected ] at char {:d}'.format(match.start()))\n            stack.pop()\n        else:\n            if sep is None:\n                stack[-1].append(text)\n            else:\n                word, tag = str2tuple(text, sep)\n                if source_tagset and target_tagset:\n                    tag = map_tag(source_tagset, target_tagset, tag)\n                stack[-1].append((word, tag))\n\n    if len(stack) != 1:\n        raise ValueError('Expected ] at char {:d}'.format(len(s)))\n    return stack[0]\n\n\n_LINE_RE = re.compile('(\\S+)\\s+(\\S+)\\s+([IOB])-?(\\S+)?')\ndef conllstr2tree(s, chunk_types=('NP', 'PP', 'VP'), root_label=\"S\"):\n\n    stack = [Tree(root_label, [])]\n\n    for lineno, line in enumerate(s.split('\\n')):\n        if not line.strip(): continue\n\n        match = _LINE_RE.match(line)\n        if match is None:\n            raise ValueError('Error on line {:d}'.format(lineno))\n        (word, tag, state, chunk_type) = match.groups()\n\n        if (chunk_types is not None and\n            chunk_type not in chunk_types):\n            state = 'O'\n\n        mismatch_I = state == 'I' and chunk_type != stack[-1].label()\n        if state in 'BO' or mismatch_I:\n            if len(stack) == 2: stack.pop()\n\n        if state == 'B' or mismatch_I:\n            chunk = Tree(chunk_type, [])\n            stack[-1].append(chunk)\n            stack.append(chunk)\n\n        stack[-1].append((word, tag))\n\n    return stack[0]\n\ndef tree2conlltags(t):\n\n    tags = []\n    for child in t:\n        try:\n            category = child.label()\n            prefix = \"B-\"\n            for contents in child:\n                if isinstance(contents, Tree):\n                    raise ValueError(\"Tree is too deeply nested to be printed in CoNLL format\")\n                tags.append((contents[0], contents[1], prefix+category))\n                prefix = \"I-\"\n        except AttributeError:\n            tags.append((child[0], child[1], \"O\"))\n    return tags\n\ndef conlltags2tree(sentence, chunk_types=('NP','PP','VP'),\n                   root_label='S', strict=False):\n    tree = Tree(root_label, [])\n    for (word, postag, chunktag) in sentence:\n        if chunktag is None:\n            if strict:\n                raise ValueError(\"Bad conll tag sequence\")\n            else:\n                tree.append((word,postag))\n        elif chunktag.startswith('B-'):\n            tree.append(Tree(chunktag[2:], [(word,postag)]))\n        elif chunktag.startswith('I-'):\n            if (len(tree)==0 or not isinstance(tree[-1], Tree) or\n                tree[-1].label() != chunktag[2:]):\n                if strict:\n                    raise ValueError(\"Bad conll tag sequence\")\n                else:\n                    tree.append(Tree(chunktag[2:], [(word,postag)]))\n            else:\n                tree[-1].append((word,postag))\n        elif chunktag == 'O':\n            tree.append((word,postag))\n        else:\n            raise ValueError(\"Bad conll tag {0!r}\".format(chunktag))\n    return tree\n\ndef tree2conllstr(t):\n    lines = [\" \".join(token) for token in tree2conlltags(t)]\n    return '\\n'.join(lines)\n\n\n_IEER_DOC_RE = re.compile(r'<DOC>\\s*'\n                          r'(<DOCNO>\\s*(?P<docno>.+?)\\s*</DOCNO>\\s*)?'\n                          r'(<DOCTYPE>\\s*(?P<doctype>.+?)\\s*</DOCTYPE>\\s*)?'\n                          r'(<DATE_TIME>\\s*(?P<date_time>.+?)\\s*</DATE_TIME>\\s*)?'\n                          r'<BODY>\\s*'\n                          r'(<HEADLINE>\\s*(?P<headline>.+?)\\s*</HEADLINE>\\s*)?'\n                          r'<TEXT>(?P<text>.*?)</TEXT>\\s*'\n                          r'</BODY>\\s*</DOC>\\s*', re.DOTALL)\n\n_IEER_TYPE_RE = re.compile('<b_\\w+\\s+[^>]*?type=\"(?P<type>\\w+)\"')\n\ndef _ieer_read_text(s, root_label):\n    stack = [Tree(root_label, [])]\n    if s is None:\n        return []\n    for piece_m in re.finditer('<[^>]+>|[^\\s<]+', s):\n        piece = piece_m.group()\n        try:\n            if piece.startswith('<b_'):\n                m = _IEER_TYPE_RE.match(piece)\n                if m is None: print('XXXX', piece)\n                chunk = Tree(m.group('type'), [])\n                stack[-1].append(chunk)\n                stack.append(chunk)\n            elif piece.startswith('<e_'):\n                stack.pop()\n            else:\n                stack[-1].append(piece)\n        except (IndexError, ValueError):\n            raise ValueError('Bad IEER string (error at character {:d})'.format \\\n                             (piece_m.start()))\n    if len(stack) != 1:\n        raise ValueError('Bad IEER string')\n    return stack[0]\n\ndef ieerstr2tree(s, chunk_types = ['LOCATION', 'ORGANIZATION', 'PERSON', 'DURATION',\n               'DATE', 'CARDINAL', 'PERCENT', 'MONEY', 'MEASURE'], root_label=\"S\"):\n\n    m = _IEER_DOC_RE.match(s)\n    if m:\n        return {\n            'text': _ieer_read_text(m.group('text'), root_label),\n            'docno': m.group('docno'),\n            'doctype': m.group('doctype'),\n            'date_time': m.group('date_time'),\n            'headline': _ieer_read_text(m.group('headline'), root_label),\n            }\n    else:\n        return _ieer_read_text(s, root_label)\n\n\ndef demo():\n\n    s = \"[ Pierre/NNP Vinken/NNP ] ,/, [ 61/CD years/NNS ] old/JJ ,/, will/MD join/VB [ the/DT board/NN ] ./.\"\n    import nltk\n    t = nltk.chunk.tagstr2tree(s, chunk_label='NP')\n    t.pprint()\n    print()\n\n    s = \"\"\"\nThese DT B-NP\nresearch NN I-NP\nprotocols NNS I-NP\noffer VBP B-VP\nto TO B-PP\nthe DT B-NP\npatient NN I-NP\nnot RB O\nonly RB O\nthe DT B-NP\nvery RB I-NP\nbest JJS I-NP\ntherapy NN I-NP\nwhich WDT B-NP\nwe PRP B-NP\nhave VBP B-VP\nestablished VBN I-VP\ntoday NN B-NP\nbut CC B-NP\nalso RB I-NP\nthe DT B-NP\nhope NN I-NP\nof IN B-PP\nsomething NN B-NP\nstill RB B-ADJP\nbetter JJR I-ADJP\n. . O\n\"\"\"\n\n    conll_tree = conllstr2tree(s, chunk_types=('NP', 'PP'))\n    conll_tree.pprint()\n\n    print(\"CoNLL output:\")\n    print(nltk.chunk.tree2conllstr(conll_tree))\n    print()\n\n\nif __name__ == '__main__':\n    demo()\n\n"], "nltk\\chunk\\__init__": [".py", "\n\nfrom nltk.data import load\n\nfrom nltk.chunk.api import ChunkParserI\nfrom nltk.chunk.util import (ChunkScore, accuracy, tagstr2tree, conllstr2tree,\n                             conlltags2tree, tree2conlltags, tree2conllstr, tree2conlltags,\n                             ieerstr2tree)\nfrom nltk.chunk.regexp import RegexpChunkParser, RegexpParser\n\n_BINARY_NE_CHUNKER = 'chunkers/maxent_ne_chunker/english_ace_binary.pickle'\n_MULTICLASS_NE_CHUNKER = 'chunkers/maxent_ne_chunker/english_ace_multiclass.pickle'\n\ndef ne_chunk(tagged_tokens, binary=False):\n    if binary:\n        chunker_pickle = _BINARY_NE_CHUNKER\n    else:\n        chunker_pickle = _MULTICLASS_NE_CHUNKER\n    chunker = load(chunker_pickle)\n    return chunker.parse(tagged_tokens)\n\ndef ne_chunk_sents(tagged_sentences, binary=False):\n    if binary:\n        chunker_pickle = _BINARY_NE_CHUNKER\n    else:\n        chunker_pickle = _MULTICLASS_NE_CHUNKER\n    chunker = load(chunker_pickle)\n    return chunker.parse_sents(tagged_sentences)\n\n", 1], "nltk\\classify\\api": [".py", "\nfrom nltk.internals import overridden\n\n\nclass ClassifierI(object):\n    def labels(self):\n        raise NotImplementedError()\n\n    def classify(self, featureset):\n        if overridden(self.classify_many):\n            return self.classify_many([featureset])[0]\n        else:\n            raise NotImplementedError()\n\n    def prob_classify(self, featureset):\n        if overridden(self.prob_classify_many):\n            return self.prob_classify_many([featureset])[0]\n        else:\n            raise NotImplementedError()\n\n    def classify_many(self, featuresets):\n        return [self.classify(fs) for fs in featuresets]\n\n    def prob_classify_many(self, featuresets):\n        return [self.prob_classify(fs) for fs in featuresets]\n\n\nclass MultiClassifierI(object):\n    def labels(self):\n        raise NotImplementedError()\n\n    def classify(self, featureset):\n        if overridden(self.classify_many):\n            return self.classify_many([featureset])[0]\n        else:\n            raise NotImplementedError()\n\n    def prob_classify(self, featureset):\n        if overridden(self.prob_classify_many):\n            return self.prob_classify_many([featureset])[0]\n        else:\n            raise NotImplementedError()\n\n    def classify_many(self, featuresets):\n        return [self.classify(fs) for fs in featuresets]\n\n    def prob_classify_many(self, featuresets):\n        return [self.prob_classify(fs) for fs in featuresets]\n\n\n\n\n\n\n\n"], "nltk\\classify\\decisiontree": [".py", "\nfrom __future__ import print_function, unicode_literals, division\n\nfrom collections import defaultdict\n\nfrom nltk.probability import FreqDist, MLEProbDist, entropy\nfrom nltk.classify.api import ClassifierI\nfrom nltk.compat import python_2_unicode_compatible\n\n@python_2_unicode_compatible\nclass DecisionTreeClassifier(ClassifierI):\n    def __init__(self, label, feature_name=None, decisions=None, default=None):\n        self._label = label\n        self._fname = feature_name\n        self._decisions = decisions\n        self._default = default\n\n    def labels(self):\n        labels = [self._label]\n        if self._decisions is not None:\n            for dt in self._decisions.values():\n                labels.extend(dt.labels())\n        if self._default is not None:\n            labels.extend(self._default.labels())\n        return list(set(labels))\n\n    def classify(self, featureset):\n        if self._fname is None:\n            return self._label\n\n        fval = featureset.get(self._fname)\n        if fval in self._decisions:\n            return self._decisions[fval].classify(featureset)\n        elif self._default is not None:\n            return self._default.classify(featureset)\n        else:\n            return self._label\n\n    def error(self, labeled_featuresets):\n        errors = 0\n        for featureset, label in labeled_featuresets:\n            if self.classify(featureset) != label:\n                errors += 1\n        return errors/len(labeled_featuresets)\n\n    def pretty_format(self, width=70, prefix='', depth=4):\n        if self._fname is None:\n            n = width-len(prefix)-15\n            return '{0}{1} {2}\\n'.format(prefix, '.'*n, self._label)\n        s = ''\n        for i, (fval, result) in enumerate(sorted(self._decisions.items())):\n            hdr = '{0}{1}={2}? '.format(prefix, self._fname, fval)\n            n = width-15-len(hdr)\n            s += '{0}{1} {2}\\n'.format(hdr, '.'*(n), result._label)\n            if result._fname is not None and depth>1:\n                s += result.pretty_format(width, prefix+'  ', depth-1)\n        if self._default is not None:\n            n = width-len(prefix)-21\n            s += '{0}else: {1} {2}\\n'.format(prefix, '.'*n, self._default._label)\n            if self._default._fname is not None and depth>1:\n                s += self._default.pretty_format(width, prefix+'  ', depth-1)\n        return s\n\n    def pseudocode(self, prefix='', depth=4):\n        if self._fname is None:\n            return \"{0}return {1!r}\\n\".format(prefix, self._label)\n        s = ''\n        for (fval, result) in sorted(self._decisions.items()):\n            s += '{0}if {1} == {2!r}: '.format(prefix, self._fname, fval)\n            if result._fname is not None and depth>1:\n                s += '\\n'+result.pseudocode(prefix+'  ', depth-1)\n            else:\n                s += 'return {0!r}\\n'.format(result._label)\n        if self._default is not None:\n            if len(self._decisions) == 1:\n                s += '{0}if {1} != {2!r}: '.format(prefix, self._fname,\n                                         list(self._decisions.keys())[0])\n            else:\n                s += '{0}else: '.format(prefix)\n            if self._default._fname is not None and depth>1:\n                s += '\\n'+self._default.pseudocode(prefix+'  ', depth-1)\n            else:\n                s += 'return {0!r}\\n'.format(self._default._label)\n        return s\n\n    def __str__(self):\n        return self.pretty_format()\n\n    @staticmethod\n    def train(labeled_featuresets, entropy_cutoff=0.05, depth_cutoff=100,\n              support_cutoff=10, binary=False, feature_values=None,\n              verbose=False):\n        feature_names = set()\n        for featureset, label in labeled_featuresets:\n            for fname in featureset:\n                feature_names.add(fname)\n\n        if feature_values is None and binary:\n            feature_values = defaultdict(set)\n            for featureset, label in labeled_featuresets:\n                for fname, fval in featureset.items():\n                    feature_values[fname].add(fval)\n\n        if not binary:\n            tree = DecisionTreeClassifier.best_stump(\n                feature_names, labeled_featuresets, verbose)\n        else:\n            tree = DecisionTreeClassifier.best_binary_stump(\n                feature_names, labeled_featuresets, feature_values, verbose)\n\n        tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n                    support_cutoff, binary, feature_values, verbose)\n\n        return tree\n\n    @staticmethod\n    def leaf(labeled_featuresets):\n        label = FreqDist(label for (featureset, label)\n                         in labeled_featuresets).max()\n        return DecisionTreeClassifier(label)\n\n    @staticmethod\n    def stump(feature_name, labeled_featuresets):\n        label = FreqDist(label for (featureset, label)\n                         in labeled_featuresets).max()\n\n        freqs = defaultdict(FreqDist) # freq(label|value)\n        for featureset, label in labeled_featuresets:\n            feature_value = featureset.get(feature_name)\n            freqs[feature_value][label] += 1\n\n        decisions = dict((val, DecisionTreeClassifier(freqs[val].max()))\n                         for val in freqs)\n        return DecisionTreeClassifier(label, feature_name, decisions)\n\n    def refine(self, labeled_featuresets, entropy_cutoff, depth_cutoff,\n               support_cutoff, binary=False, feature_values=None,\n               verbose=False):\n        if len(labeled_featuresets) <= support_cutoff: return\n        if self._fname is None: return\n        if depth_cutoff <= 0: return\n        for fval in self._decisions:\n            fval_featuresets = [(featureset, label) for (featureset, label)\n                                in labeled_featuresets\n                                if featureset.get(self._fname) == fval]\n\n            label_freqs = FreqDist(label for (featureset, label)\n                                   in fval_featuresets)\n            if entropy(MLEProbDist(label_freqs)) > entropy_cutoff:\n                self._decisions[fval] = DecisionTreeClassifier.train(\n                    fval_featuresets, entropy_cutoff, depth_cutoff,\n                    support_cutoff, binary, feature_values, verbose)\n        if self._default is not None:\n            default_featuresets = [(featureset, label) for (featureset, label)\n                                   in labeled_featuresets\n                                   if featureset.get(self._fname) not in\n                                   self._decisions]\n            label_freqs = FreqDist(label for (featureset, label)\n                                   in default_featuresets)\n            if entropy(MLEProbDist(label_freqs)) > entropy_cutoff:\n                self._default = DecisionTreeClassifier.train(\n                    default_featuresets, entropy_cutoff, depth_cutoff,\n                    support_cutoff, binary, feature_values, verbose)\n\n    @staticmethod\n    def best_stump(feature_names, labeled_featuresets, verbose=False):\n        best_stump = DecisionTreeClassifier.leaf(labeled_featuresets)\n        best_error = best_stump.error(labeled_featuresets)\n        for fname in feature_names:\n            stump = DecisionTreeClassifier.stump(fname, labeled_featuresets)\n            stump_error = stump.error(labeled_featuresets)\n            if stump_error < best_error:\n                best_error = stump_error\n                best_stump = stump\n        if verbose:\n            print(('best stump for {:6d} toks uses {:20} err={:6.4f}'.format \\\n                   (len(labeled_featuresets), best_stump._fname, best_error)))\n        return best_stump\n\n    @staticmethod\n    def binary_stump(feature_name, feature_value, labeled_featuresets):\n        label = FreqDist(label for (featureset, label)\n                         in labeled_featuresets).max()\n\n        pos_fdist = FreqDist()\n        neg_fdist = FreqDist()\n        for featureset, label in labeled_featuresets:\n            if featureset.get(feature_name) == feature_value:\n                pos_fdist[label] += 1\n            else:\n                neg_fdist[label] += 1\n\n\n        decisions = {}\n        default = label\n        if pos_fdist.N() > 0:\n            decisions = {feature_value: DecisionTreeClassifier(pos_fdist.max())}\n        if neg_fdist.N() > 0:\n            default = DecisionTreeClassifier(neg_fdist.max())\n\n        return DecisionTreeClassifier(label, feature_name, decisions, default)\n\n    @staticmethod\n    def best_binary_stump(feature_names, labeled_featuresets, feature_values,\n                          verbose=False):\n        best_stump = DecisionTreeClassifier.leaf(labeled_featuresets)\n        best_error = best_stump.error(labeled_featuresets)\n        for fname in feature_names:\n            for fval in feature_values[fname]:\n                stump = DecisionTreeClassifier.binary_stump(\n                    fname, fval, labeled_featuresets)\n                stump_error = stump.error(labeled_featuresets)\n                if stump_error < best_error:\n                    best_error = stump_error\n                    best_stump = stump\n        if verbose:\n            if best_stump._decisions:\n                descr = '{0}={1}'.format(best_stump._fname,\n                                         list(best_stump._decisions.keys())[0])\n            else:\n                descr = '(default)'\n            print(('best stump for {:6d} toks uses {:20} err={:6.4f}'.format \\\n                   (len(labeled_featuresets), descr, best_error)))\n        return best_stump\n\n\ndef f(x):\n    return DecisionTreeClassifier.train(x, binary=True, verbose=True)\n\ndef demo():\n    from nltk.classify.util import names_demo, binary_names_demo_features\n    classifier = names_demo(f, #DecisionTreeClassifier.train,\n                            binary_names_demo_features)\n    print(classifier.pp(depth=7))\n    print(classifier.pseudocode(depth=7))\n\nif __name__ == '__main__':\n    demo()\n\n"], "nltk\\classify\\maxent": [".py", "\nfrom __future__ import print_function, unicode_literals\n\ntry:\n    import numpy\nexcept ImportError:\n    pass\n\nimport tempfile\nimport os\nimport re\nfrom collections import defaultdict\n\nfrom six import integer_types\n\nfrom nltk import compat\nfrom nltk.data import gzip_open_unicode\nfrom nltk.util import OrderedDict\nfrom nltk.probability import DictionaryProbDist\n\nfrom nltk.classify.api import ClassifierI\nfrom nltk.classify.util import CutoffChecker, accuracy, log_likelihood\nfrom nltk.classify.megam import (call_megam,\n                                 write_megam_file, parse_megam_weights)\nfrom nltk.classify.tadm import call_tadm, write_tadm_file, parse_tadm_weights\n\n__docformat__ = 'epytext en'\n\n\n@compat.python_2_unicode_compatible\nclass MaxentClassifier(ClassifierI):\n    def __init__(self, encoding, weights, logarithmic=True):\n        self._encoding = encoding\n        self._weights = weights\n        self._logarithmic = logarithmic\n        assert encoding.length() == len(weights)\n\n    def labels(self):\n        return self._encoding.labels()\n\n    def set_weights(self, new_weights):\n        self._weights = new_weights\n        assert self._encoding.length() == len(new_weights)\n\n    def weights(self):\n        return self._weights\n\n    def classify(self, featureset):\n        return self.prob_classify(featureset).max()\n\n    def prob_classify(self, featureset):\n        prob_dict = {}\n        for label in self._encoding.labels():\n            feature_vector = self._encoding.encode(featureset, label)\n\n            if self._logarithmic:\n                total = 0.0\n                for (f_id, f_val) in feature_vector:\n                    total += self._weights[f_id] * f_val\n                prob_dict[label] = total\n\n            else:\n                prod = 1.0\n                for (f_id, f_val) in feature_vector:\n                    prod *= self._weights[f_id] ** f_val\n                prob_dict[label] = prod\n\n        return DictionaryProbDist(prob_dict, log=self._logarithmic,\n                                  normalize=True)\n\n    def explain(self, featureset, columns=4):\n        descr_width = 50\n        TEMPLATE = '  %-'+str(descr_width-2)+'s%s%8.3f'\n\n        pdist = self.prob_classify(featureset)\n        labels = sorted(pdist.samples(), key=pdist.prob, reverse=True)\n        labels = labels[:columns]\n        print('  Feature'.ljust(descr_width)+''.join(\n            '%8s' % ((\"%s\" % l)[:7]) for l in labels))\n        print('  '+'-'*(descr_width-2+8*len(labels)))\n        sums = defaultdict(int)\n        for i, label in enumerate(labels):\n            feature_vector = self._encoding.encode(featureset, label)\n            feature_vector.sort(key=lambda fid__: abs(self._weights[fid__[0]]),\n                                reverse=True)\n            for (f_id, f_val) in feature_vector:\n                if self._logarithmic:\n                    score = self._weights[f_id] * f_val\n                else: score = self._weights[f_id] ** f_val\n                descr = self._encoding.describe(f_id)\n                descr = descr.split(' and label is ')[0] # hack\n                descr += ' (%s)' % f_val                 # hack\n                if len(descr) > 47:\n                    descr = descr[:44]+'...'\n                print(TEMPLATE % (descr, i*8*' ', score))\n                sums[label] += score\n        print('  '+'-'*(descr_width-1+8*len(labels)))\n        print('  TOTAL:'.ljust(descr_width)+''.join(\n            '%8.3f' % sums[l] for l in labels))\n        print('  PROBS:'.ljust(descr_width)+''.join(\n            '%8.3f' % pdist.prob(l) for l in labels))\n\n    def most_informative_features(self, n=10):\n        if hasattr(self, '_most_informative_features'):\n            return self._most_informative_features[:n]\n        else:\n            self._most_informative_features = sorted(list(range(len(self._weights))),\n                                                key=lambda fid: abs(self._weights[fid]),\n                                                reverse=True)\n            return self._most_informative_features[:n]\n\n    def show_most_informative_features(self, n=10, show='all'):\n        fids = self.most_informative_features(None)\n        if show == 'pos':\n            fids = [fid for fid in fids if self._weights[fid] > 0]\n        elif show == 'neg':\n            fids = [fid for fid in fids if self._weights[fid] < 0]\n        for fid in fids[:n]:\n            print('%8.3f %s' % (self._weights[fid],\n                                self._encoding.describe(fid)))\n\n    def __repr__(self):\n        return ('<ConditionalExponentialClassifier: %d labels, %d features>' %\n                (len(self._encoding.labels()), self._encoding.length()))\n\n    ALGORITHMS = ['GIS', 'IIS', 'MEGAM', 'TADM']\n\n    @classmethod\n    def train(cls, train_toks, algorithm=None, trace=3, encoding=None,\n              labels=None, gaussian_prior_sigma=0, **cutoffs):\n        if algorithm is None:\n            algorithm = 'iis'\n        for key in cutoffs:\n            if key not in ('max_iter', 'min_ll', 'min_lldelta',\n                           'max_acc', 'min_accdelta', 'count_cutoff',\n                           'norm', 'explicit', 'bernoulli'):\n                raise TypeError('Unexpected keyword arg %r' % key)\n        algorithm = algorithm.lower()\n        if algorithm == 'iis':\n            return train_maxent_classifier_with_iis(\n                train_toks, trace, encoding, labels, **cutoffs)\n        elif algorithm == 'gis':\n            return train_maxent_classifier_with_gis(\n                train_toks, trace, encoding, labels, **cutoffs)\n        elif algorithm == 'megam':\n            return train_maxent_classifier_with_megam(\n                train_toks, trace, encoding, labels,\n                gaussian_prior_sigma, **cutoffs)\n        elif algorithm == 'tadm':\n            kwargs = cutoffs\n            kwargs['trace'] = trace\n            kwargs['encoding'] = encoding\n            kwargs['labels'] = labels\n            kwargs['gaussian_prior_sigma'] = gaussian_prior_sigma\n            return TadmMaxentClassifier.train(train_toks, **kwargs)\n        else:\n            raise ValueError('Unknown algorithm %s' % algorithm)\n\n\nConditionalExponentialClassifier = MaxentClassifier\n\n\n\nclass MaxentFeatureEncodingI(object):\n    def encode(self, featureset, label):\n        raise NotImplementedError()\n\n    def length(self):\n        raise NotImplementedError()\n\n    def labels(self):\n        raise NotImplementedError()\n\n    def describe(self, fid):\n        raise NotImplementedError()\n\n    def train(cls, train_toks):\n        raise NotImplementedError()\n\nclass FunctionBackedMaxentFeatureEncoding(MaxentFeatureEncodingI):\n    def __init__(self, func, length, labels):\n        self._length = length\n        self._func = func\n        self._labels = labels\n\n    def encode(self, featureset, label):\n        return self._func(featureset, label)\n\n    def length(self):\n        return self._length\n\n    def labels(self):\n        return self._labels\n\n    def describe(self, fid):\n        return 'no description available'\n\nclass BinaryMaxentFeatureEncoding(MaxentFeatureEncodingI):\n    def __init__(self, labels, mapping, unseen_features=False,\n                 alwayson_features=False):\n        if set(mapping.values()) != set(range(len(mapping))):\n            raise ValueError('Mapping values must be exactly the '\n                             'set of integers from 0...len(mapping)')\n\n        self._labels = list(labels)\n\n        self._mapping = mapping\n\n        self._length = len(mapping)\n\n        self._alwayson = None\n\n        self._unseen = None\n\n        if alwayson_features:\n            self._alwayson = dict((label, i+self._length)\n                                  for (i, label) in enumerate(labels))\n            self._length += len(self._alwayson)\n\n        if unseen_features:\n            fnames = set(fname for (fname, fval, label) in mapping)\n            self._unseen = dict((fname, i+self._length)\n                                for (i, fname) in enumerate(fnames))\n            self._length += len(fnames)\n\n    def encode(self, featureset, label):\n        encoding = []\n\n        for fname, fval in featureset.items():\n            if (fname, fval, label) in self._mapping:\n                encoding.append((self._mapping[fname, fval, label], 1))\n\n            elif self._unseen:\n                for label2 in self._labels:\n                    if (fname, fval, label2) in self._mapping:\n                        break # we've seen this fname/fval combo\n                else:\n                    if fname in self._unseen:\n                        encoding.append((self._unseen[fname], 1))\n\n        if self._alwayson and label in self._alwayson:\n            encoding.append((self._alwayson[label], 1))\n\n        return encoding\n\n    def describe(self, f_id):\n        if not isinstance(f_id, integer_types):\n            raise TypeError('describe() expected an int')\n        try:\n            self._inv_mapping\n        except AttributeError:\n            self._inv_mapping = [-1]*len(self._mapping)\n            for (info, i) in self._mapping.items():\n                self._inv_mapping[i] = info\n\n        if f_id < len(self._mapping):\n            (fname, fval, label) = self._inv_mapping[f_id]\n            return '%s==%r and label is %r' % (fname, fval, label)\n        elif self._alwayson and f_id in self._alwayson.values():\n            for (label, f_id2) in self._alwayson.items():\n                if f_id == f_id2:\n                    return 'label is %r' % label\n        elif self._unseen and f_id in self._unseen.values():\n            for (fname, f_id2) in self._unseen.items():\n                if f_id == f_id2:\n                    return '%s is unseen' % fname\n        else:\n            raise ValueError('Bad feature id')\n\n    def labels(self):\n        return self._labels\n\n    def length(self):\n        return self._length\n\n    @classmethod\n    def train(cls, train_toks, count_cutoff=0, labels=None, **options):\n        mapping = {}              # maps (fname, fval, label) -> fid\n        seen_labels = set()       # The set of labels we've encountered\n        count = defaultdict(int)  # maps (fname, fval) -> count\n\n        for (tok, label) in train_toks:\n            if labels and label not in labels:\n                raise ValueError('Unexpected label %s' % label)\n            seen_labels.add(label)\n\n            for (fname, fval) in tok.items():\n\n                count[fname, fval] += 1\n                if count[fname, fval] >= count_cutoff:\n                    if (fname, fval, label) not in mapping:\n                        mapping[fname, fval, label] = len(mapping)\n\n        if labels is None:\n            labels = seen_labels\n        return cls(labels, mapping, **options)\n\nclass GISEncoding(BinaryMaxentFeatureEncoding):\n    def __init__(self, labels, mapping, unseen_features=False,\n                 alwayson_features=False, C=None):\n        BinaryMaxentFeatureEncoding.__init__(\n            self, labels, mapping, unseen_features, alwayson_features)\n        if C is None:\n            C = len(set(fname for (fname, fval, label) in mapping))+1\n        self._C = C\n\n    @property\n    def C(self):\n    A feature encoding that generates vectors containing integer,\n    float and binary joint-features of the form:\n\n    Binary (for string and boolean features):\n\n    |  joint_feat(fs, l) = { 1 if (fs[fname] == fval) and (l == label)\n    |                      {\n    |                      { 0 otherwise\n\n    Value (for integer and float features):\n\n    |  joint_feat(fs, l) = { fval if     (fs[fname] == type(fval))\n    |                      {         and (l == label)\n    |                      {\n    |                      { not encoded otherwise\n\n    Where ``fname`` is the name of an input-feature, ``fval`` is a value\n    for that input-feature, and ``label`` is a label.\n\n    Typically, these features are constructed based on a training\n    corpus, using the ``train()`` method.\n\n    For string and boolean features [type(fval) not in (int, float)]\n    this method will create one feature for each combination of\n    ``fname``, ``fval``, and ``label`` that occurs at least once in the\n    training corpus.\n\n    For integer and float features [type(fval) in (int, float)] this\n    method will create one feature for each combination of ``fname``\n    and ``label`` that occurs at least once in the training corpus.\n\n    For binary features the ``unseen_features`` parameter can be used\n    to add \"unseen-value features\", which are used whenever an input\n    feature has a value that was not encountered in the training\n    corpus.  These features have the form:\n\n    |  joint_feat(fs, l) = { 1 if is_unseen(fname, fs[fname])\n    |                      {      and l == label\n    |                      {\n    |                      { 0 otherwise\n\n    Where ``is_unseen(fname, fval)`` is true if the encoding does not\n    contain any joint features that are true when ``fs[fname]==fval``.\n\n    The ``alwayson_features`` parameter can be used to add \"always-on\n    features\", which have the form:\n\n    |  joint_feat(fs, l) = { 1 if (l == label)\n    |                      {\n    |                      { 0 otherwise\n\n    These always-on features allow the maxent model to directly model\n    the prior probabilities of each label.\n    \"\"\"\n    def __init__(self, labels, mapping, unseen_features=False,\n                 alwayson_features=False):\n        \"\"\"\n        :param labels: A list of the \\\"known labels\\\" for this encoding.\n\n        :param mapping: A dictionary mapping from ``(fname,fval,label)``\n            tuples to corresponding joint-feature indexes.  These\n            indexes must be the set of integers from 0...len(mapping).\n            If ``mapping[fname,fval,label]=id``, then\n            ``self.encode({..., fname:fval, ...``, label)[id]} is 1;\n            otherwise, it is 0.\n\n        :param unseen_features: If true, then include unseen value\n           features in the generated joint-feature vectors.\n\n        :param alwayson_features: If true, then include always-on\n           features in the generated joint-feature vectors.\n        \"\"\"\n        if set(mapping.values()) != set(range(len(mapping))):\n            raise ValueError('Mapping values must be exactly the '\n                             'set of integers from 0...len(mapping)')\n\n        self._labels = list(labels)\n\n        self._mapping = mapping\n\n        self._length = len(mapping)\n\n        self._alwayson = None\n\n        self._unseen = None\n\n        if alwayson_features:\n            self._alwayson = dict((label, i+self._length)\n                                  for (i, label) in enumerate(labels))\n            self._length += len(self._alwayson)\n\n        if unseen_features:\n            fnames = set(fname for (fname, fval, label) in mapping)\n            self._unseen = dict((fname, i+self._length)\n                                for (i, fname) in enumerate(fnames))\n            self._length += len(fnames)\n\n    def encode(self, featureset, label):\n        encoding = []\n\n        for fname, fval in featureset.items():\n            if isinstance(fval, (integer_types, float)):\n                if (fname, type(fval), label) in self._mapping:\n                    encoding.append((self._mapping[fname, type(fval),\n                                                   label], fval))\n            else:\n                if (fname, fval, label) in self._mapping:\n                    encoding.append((self._mapping[fname, fval, label], 1))\n\n                elif self._unseen:\n                    for label2 in self._labels:\n                        if (fname, fval, label2) in self._mapping:\n                            break # we've seen this fname/fval combo\n                    else:\n                        if fname in self._unseen:\n                            encoding.append((self._unseen[fname], 1))\n\n\n        if self._alwayson and label in self._alwayson:\n            encoding.append((self._alwayson[label], 1))\n\n        return encoding\n\n    def describe(self, f_id):\n        if not isinstance(f_id, integer_types):\n            raise TypeError('describe() expected an int')\n        try:\n            self._inv_mapping\n        except AttributeError:\n            self._inv_mapping = [-1]*len(self._mapping)\n            for (info, i) in self._mapping.items():\n                self._inv_mapping[i] = info\n\n        if f_id < len(self._mapping):\n            (fname, fval, label) = self._inv_mapping[f_id]\n            return '%s==%r and label is %r' % (fname, fval, label)\n        elif self._alwayson and f_id in self._alwayson.values():\n            for (label, f_id2) in self._alwayson.items():\n                if f_id == f_id2:\n                    return 'label is %r' % label\n        elif self._unseen and f_id in self._unseen.values():\n            for (fname, f_id2) in self._unseen.items():\n                if f_id == f_id2:\n                    return '%s is unseen' % fname\n        else:\n            raise ValueError('Bad feature id')\n\n    def labels(self):\n        return self._labels\n\n    def length(self):\n        return self._length\n\n    @classmethod\n    def train(cls, train_toks, count_cutoff=0, labels=None, **options):\n        \"\"\"\n        Construct and return new feature encoding, based on a given\n        training corpus ``train_toks``.  See the class description\n        ``TypedMaxentFeatureEncoding`` for a description of the\n        joint-features that will be included in this encoding.\n\n        Note: recognized feature values types are (int, float), over\n        types are interpreted as regular binary features.\n\n        :type train_toks: list(tuple(dict, str))\n        :param train_toks: Training data, represented as a list of\n            pairs, the first member of which is a feature dictionary,\n            and the second of which is a classification label.\n\n        :type count_cutoff: int\n        :param count_cutoff: A cutoff value that is used to discard\n            rare joint-features.  If a joint-feature's value is 1\n            fewer than ``count_cutoff`` times in the training corpus,\n            then that joint-feature is not included in the generated\n            encoding.\n\n        :type labels: list\n        :param labels: A list of labels that should be used by the\n            classifier.  If not specified, then the set of labels\n            attested in ``train_toks`` will be used.\n\n        :param options: Extra parameters for the constructor, such as\n            ``unseen_features`` and ``alwayson_features``.\n        \"\"\"\n        mapping = {}              # maps (fname, fval, label) -> fid\n        seen_labels = set()       # The set of labels we've encountered\n        count = defaultdict(int)  # maps (fname, fval) -> count\n\n        for (tok, label) in train_toks:\n            if labels and label not in labels:\n                raise ValueError('Unexpected label %s' % label)\n            seen_labels.add(label)\n\n            for (fname, fval) in tok.items():\n                if type(fval) in (int, float):\n                    fval = type(fval)\n                count[fname, fval] += 1\n                if count[fname, fval] >= count_cutoff:\n                    if (fname, fval, label) not in mapping:\n                        mapping[fname, fval, label] = len(mapping)\n\n        if labels is None:\n            labels = seen_labels\n        return cls(labels, mapping, **options)\n\n\n\n\n\ndef train_maxent_classifier_with_gis(train_toks, trace=3, encoding=None,\n                                     labels=None, **cutoffs):\n    \"\"\"\n    Train a new ``ConditionalExponentialClassifier``, using the given\n    training samples, using the Generalized Iterative Scaling\n    algorithm.  This ``ConditionalExponentialClassifier`` will encode\n    the model that maximizes entropy from all the models that are\n    empirically consistent with ``train_toks``.\n\n    :see: ``train_maxent_classifier()`` for parameter descriptions.\n    \"\"\"\n    cutoffs.setdefault('max_iter', 100)\n    cutoffchecker = CutoffChecker(cutoffs)\n\n    if encoding is None:\n        encoding = GISEncoding.train(train_toks, labels=labels)\n\n    if not hasattr(encoding, 'C'):\n        raise TypeError('The GIS algorithm requires an encoding that '\n                        'defines C (e.g., GISEncoding).')\n\n    Cinv = 1.0/encoding.C\n\n    empirical_fcount = calculate_empirical_fcount(train_toks, encoding)\n\n    unattested = set(numpy.nonzero(empirical_fcount == 0)[0])\n\n    weights = numpy.zeros(len(empirical_fcount), 'd')\n    for fid in unattested:\n        weights[fid] = numpy.NINF\n    classifier = ConditionalExponentialClassifier(encoding, weights)\n\n    log_empirical_fcount = numpy.log2(empirical_fcount)\n    del empirical_fcount\n\n    if trace > 0:\n        print('  ==> Training (%d iterations)' % cutoffs['max_iter'])\n    if trace > 2:\n        print()\n        print('      Iteration    Log Likelihood    Accuracy')\n        print('      ---------------------------------------')\n\n    try:\n        while True:\n            if trace > 2:\n                ll = cutoffchecker.ll or log_likelihood(classifier, train_toks)\n                acc = cutoffchecker.acc or accuracy(classifier, train_toks)\n                iternum = cutoffchecker.iter\n                print('     %9d    %14.5f    %9.3f' % (iternum, ll, acc))\n\n            estimated_fcount = calculate_estimated_fcount(\n                classifier, train_toks, encoding)\n\n            for fid in unattested:\n                estimated_fcount[fid] += 1\n            log_estimated_fcount = numpy.log2(estimated_fcount)\n            del estimated_fcount\n\n            weights = classifier.weights()\n            weights += (log_empirical_fcount - log_estimated_fcount) * Cinv\n            classifier.set_weights(weights)\n\n            if cutoffchecker.check(classifier, train_toks):\n                break\n\n    except KeyboardInterrupt:\n        print('      Training stopped: keyboard interrupt')\n    except:\n        raise\n\n    if trace > 2:\n        ll = log_likelihood(classifier, train_toks)\n        acc = accuracy(classifier, train_toks)\n        print('         Final    %14.5f    %9.3f' % (ll, acc))\n\n    return classifier\n\ndef calculate_empirical_fcount(train_toks, encoding):\n    fcount = numpy.zeros(encoding.length(), 'd')\n\n    for tok, label in train_toks:\n        for (index, val) in encoding.encode(tok, label):\n            fcount[index] += val\n\n    return fcount\n\ndef calculate_estimated_fcount(classifier, train_toks, encoding):\n    fcount = numpy.zeros(encoding.length(), 'd')\n\n    for tok, label in train_toks:\n        pdist = classifier.prob_classify(tok)\n        for label in pdist.samples():\n            prob = pdist.prob(label)\n            for (fid, fval) in encoding.encode(tok, label):\n                fcount[fid] += prob*fval\n\n    return fcount\n\n\n\ndef train_maxent_classifier_with_iis(train_toks, trace=3, encoding=None,\n                                     labels=None, **cutoffs):\n    \"\"\"\n    Train a new ``ConditionalExponentialClassifier``, using the given\n    training samples, using the Improved Iterative Scaling algorithm.\n    This ``ConditionalExponentialClassifier`` will encode the model\n    that maximizes entropy from all the models that are empirically\n    consistent with ``train_toks``.\n\n    :see: ``train_maxent_classifier()`` for parameter descriptions.\n    \"\"\"\n    cutoffs.setdefault('max_iter', 100)\n    cutoffchecker = CutoffChecker(cutoffs)\n\n    if encoding is None:\n        encoding = BinaryMaxentFeatureEncoding.train(train_toks, labels=labels)\n\n    empirical_ffreq = (calculate_empirical_fcount(train_toks, encoding) /\n                       len(train_toks))\n\n    nfmap = calculate_nfmap(train_toks, encoding)\n    nfarray = numpy.array(sorted(nfmap, key=nfmap.__getitem__), 'd')\n    nftranspose = numpy.reshape(nfarray, (len(nfarray), 1))\n\n    unattested = set(numpy.nonzero(empirical_ffreq == 0)[0])\n\n    weights = numpy.zeros(len(empirical_ffreq), 'd')\n    for fid in unattested:\n        weights[fid] = numpy.NINF\n    classifier = ConditionalExponentialClassifier(encoding, weights)\n\n    if trace > 0:\n        print('  ==> Training (%d iterations)' % cutoffs['max_iter'])\n    if trace > 2:\n        print()\n        print('      Iteration    Log Likelihood    Accuracy')\n        print('      ---------------------------------------')\n\n    try:\n        while True:\n            if trace > 2:\n                ll = cutoffchecker.ll or log_likelihood(classifier, train_toks)\n                acc = cutoffchecker.acc or accuracy(classifier, train_toks)\n                iternum = cutoffchecker.iter\n                print('     %9d    %14.5f    %9.3f' % (iternum, ll, acc))\n\n            deltas = calculate_deltas(\n                train_toks, classifier, unattested, empirical_ffreq,\n                nfmap, nfarray, nftranspose, encoding)\n\n            weights = classifier.weights()\n            weights += deltas\n            classifier.set_weights(weights)\n\n            if cutoffchecker.check(classifier, train_toks):\n                break\n\n    except KeyboardInterrupt:\n        print('      Training stopped: keyboard interrupt')\n    except:\n        raise\n\n\n    if trace > 2:\n        ll = log_likelihood(classifier, train_toks)\n        acc = accuracy(classifier, train_toks)\n        print('         Final    %14.5f    %9.3f' % (ll, acc))\n\n    return classifier\n\ndef calculate_nfmap(train_toks, encoding):\n    \"\"\"\n    Construct a map that can be used to compress ``nf`` (which is\n    typically sparse).\n\n    *nf(feature_vector)* is the sum of the feature values for\n    *feature_vector*.\n\n    This represents the number of features that are active for a\n    given labeled text.  This method finds all values of *nf(t)*\n    that are attested for at least one token in the given list of\n    training tokens; and constructs a dictionary mapping these\n    attested values to a continuous range *0...N*.  For example,\n    if the only values of *nf()* that were attested were 3, 5, and\n    7, then ``_nfmap`` might return the dictionary ``{3:0, 5:1, 7:2}``.\n\n    :return: A map that can be used to compress ``nf`` to a dense\n        vector.\n    :rtype: dict(int -> int)\n    \"\"\"\n    nfset = set()\n    for tok, _ in train_toks:\n        for label in encoding.labels():\n            nfset.add(sum(val for (id, val) in encoding.encode(tok, label)))\n    return dict((nf, i) for (i, nf) in enumerate(nfset))\n\ndef calculate_deltas(train_toks, classifier, unattested, ffreq_empirical,\n                     nfmap, nfarray, nftranspose, encoding):\n    \"\"\"\n    Calculate the update values for the classifier weights for\n    this iteration of IIS.  These update weights are the value of\n    ``delta`` that solves the equation::\n\n      ffreq_empirical[i]\n             =\n      SUM[fs,l] (classifier.prob_classify(fs).prob(l) *\n                 feature_vector(fs,l)[i] *\n                 exp(delta[i] * nf(feature_vector(fs,l))))\n\n    Where:\n        - *(fs,l)* is a (featureset, label) tuple from ``train_toks``\n        - *feature_vector(fs,l)* = ``encoding.encode(fs,l)``\n        - *nf(vector)* = ``sum([val for (id,val) in vector])``\n\n    This method uses Newton's method to solve this equation for\n    *delta[i]*.  In particular, it starts with a guess of\n    ``delta[i]`` = 1; and iteratively updates ``delta`` with:\n\n    | delta[i] -= (ffreq_empirical[i] - sum1[i])/(-sum2[i])\n\n    until convergence, where *sum1* and *sum2* are defined as:\n\n    |    sum1[i](delta) = SUM[fs,l] f[i](fs,l,delta)\n    |    sum2[i](delta) = SUM[fs,l] (f[i](fs,l,delta).nf(feature_vector(fs,l)))\n    |    f[i](fs,l,delta) = (classifier.prob_classify(fs).prob(l) .\n    |                        feature_vector(fs,l)[i] .\n    |                        exp(delta[i] . nf(feature_vector(fs,l))))\n\n    Note that *sum1* and *sum2* depend on ``delta``; so they need\n    to be re-computed each iteration.\n\n    The variables ``nfmap``, ``nfarray``, and ``nftranspose`` are\n    used to generate a dense encoding for *nf(ltext)*.  This\n    allows ``_deltas`` to calculate *sum1* and *sum2* using\n    matrices, which yields a significant performance improvement.\n\n    :param train_toks: The set of training tokens.\n    :type train_toks: list(tuple(dict, str))\n    :param classifier: The current classifier.\n    :type classifier: ClassifierI\n    :param ffreq_empirical: An array containing the empirical\n        frequency for each feature.  The *i*\\ th element of this\n        array is the empirical frequency for feature *i*.\n    :type ffreq_empirical: sequence of float\n    :param unattested: An array that is 1 for features that are\n        not attested in the training data; and 0 for features that\n        are attested.  In other words, ``unattested[i]==0`` iff\n        ``ffreq_empirical[i]==0``.\n    :type unattested: sequence of int\n    :param nfmap: A map that can be used to compress ``nf`` to a dense\n        vector.\n    :type nfmap: dict(int -> int)\n    :param nfarray: An array that can be used to uncompress ``nf``\n        from a dense vector.\n    :type nfarray: array(float)\n    :param nftranspose: The transpose of ``nfarray``\n    :type nftranspose: array(float)\n    \"\"\"\n    NEWTON_CONVERGE = 1e-12\n    MAX_NEWTON = 300\n\n    deltas = numpy.ones(encoding.length(), 'd')\n\n    A = numpy.zeros((len(nfmap), encoding.length()), 'd')\n\n    for tok, label in train_toks:\n        dist = classifier.prob_classify(tok)\n\n        for label in encoding.labels():\n            feature_vector = encoding.encode(tok, label)\n            nf = sum(val for (id, val) in feature_vector)\n            for (id, val) in feature_vector:\n                A[nfmap[nf], id] += dist.prob(label) * val\n    A /= len(train_toks)\n\n    for rangenum in range(MAX_NEWTON):\n        nf_delta = numpy.outer(nfarray, deltas)\n        exp_nf_delta = 2 ** nf_delta\n        nf_exp_nf_delta = nftranspose * exp_nf_delta\n        sum1 = numpy.sum(exp_nf_delta * A, axis=0)\n        sum2 = numpy.sum(nf_exp_nf_delta * A, axis=0)\n\n        for fid in unattested:\n            sum2[fid] += 1\n\n        deltas -= (ffreq_empirical - sum1) / -sum2\n\n        n_error = (numpy.sum(abs((ffreq_empirical-sum1)))/\n                   numpy.sum(abs(deltas)))\n        if n_error < NEWTON_CONVERGE:\n            return deltas\n\n    return deltas\n\n\ndef train_maxent_classifier_with_megam(train_toks, trace=3, encoding=None,\n                                       labels=None, gaussian_prior_sigma=0,\n                                       **kwargs):\n    \"\"\"\n    Train a new ``ConditionalExponentialClassifier``, using the given\n    training samples, using the external ``megam`` library.  This\n    ``ConditionalExponentialClassifier`` will encode the model that\n    maximizes entropy from all the models that are empirically\n    consistent with ``train_toks``.\n\n    :see: ``train_maxent_classifier()`` for parameter descriptions.\n    :see: ``nltk.classify.megam``\n    \"\"\"\n\n    explicit = True\n    bernoulli = True\n    if 'explicit' in kwargs:\n        explicit = kwargs['explicit']\n    if 'bernoulli' in kwargs:\n        bernoulli = kwargs['bernoulli']\n\n    if encoding is None:\n        count_cutoff = kwargs.get('count_cutoff', 0)\n        encoding = BinaryMaxentFeatureEncoding.train(train_toks, count_cutoff,\n                                                     labels=labels,\n                                                     alwayson_features=True)\n    elif labels is not None:\n        raise ValueError('Specify encoding or labels, not both')\n\n    try:\n        fd, trainfile_name = tempfile.mkstemp(prefix='nltk-')\n        with open(trainfile_name, 'w') as trainfile:\n            write_megam_file(train_toks, encoding, trainfile,\n                             explicit=explicit, bernoulli=bernoulli)\n        os.close(fd)\n    except (OSError, IOError, ValueError) as e:\n        raise ValueError('Error while creating megam training file: %s' % e)\n\n    options = []\n    options += ['-nobias', '-repeat', '10']\n    if explicit:\n        options += ['-explicit']\n    if not bernoulli:\n        options += ['-fvals']\n    if gaussian_prior_sigma:\n        inv_variance = 1.0 / gaussian_prior_sigma**2\n    else:\n        inv_variance = 0\n    options += ['-lambda', '%.2f' % inv_variance, '-tune']\n    if trace < 3:\n        options += ['-quiet']\n    if 'max_iter' in kwargs:\n        options += ['-maxi', '%s' % kwargs['max_iter']]\n    if 'll_delta' in kwargs:\n        options += ['-dpp', '%s' % abs(kwargs['ll_delta'])]\n    if hasattr(encoding, 'cost'):\n        options += ['-multilabel']  # each possible la\n    options += ['multiclass', trainfile_name]\n    stdout = call_megam(options)\n    try:\n        os.remove(trainfile_name)\n    except (OSError, IOError) as e:\n        print('Warning: unable to delete %s: %s' % (trainfile_name, e))\n\n    weights = parse_megam_weights(stdout, encoding.length(), explicit)\n\n    weights *= numpy.log2(numpy.e)\n\n    return MaxentClassifier(encoding, weights)\n\n\nclass TadmMaxentClassifier(MaxentClassifier):\n    @classmethod\n    def train(cls, train_toks, **kwargs):\n        algorithm = kwargs.get('algorithm', 'tao_lmvm')\n        trace = kwargs.get('trace', 3)\n        encoding = kwargs.get('encoding', None)\n        labels = kwargs.get('labels', None)\n        sigma = kwargs.get('gaussian_prior_sigma', 0)\n        count_cutoff = kwargs.get('count_cutoff', 0)\n        max_iter = kwargs.get('max_iter')\n        ll_delta = kwargs.get('min_lldelta')\n\n        if not encoding:\n            encoding = TadmEventMaxentFeatureEncoding.train(train_toks,\n                                                            count_cutoff,\n                                                            labels=labels)\n\n        trainfile_fd, trainfile_name = \\\n            tempfile.mkstemp(prefix='nltk-tadm-events-', suffix='.gz')\n        weightfile_fd, weightfile_name = \\\n            tempfile.mkstemp(prefix='nltk-tadm-weights-')\n\n        trainfile = gzip_open_unicode(trainfile_name, 'w')\n        write_tadm_file(train_toks, encoding, trainfile)\n        trainfile.close()\n\n        options = []\n        options.extend(['-monitor'])\n        options.extend(['-method', algorithm])\n        if sigma:\n            options.extend(['-l2', '%.6f' % sigma**2])\n        if max_iter:\n            options.extend(['-max_it', '%d' % max_iter])\n        if ll_delta:\n            options.extend(['-fatol', '%.6f' % abs(ll_delta)])\n        options.extend(['-events_in', trainfile_name])\n        options.extend(['-params_out', weightfile_name])\n        if trace < 3:\n            options.extend(['2>&1'])\n        else:\n            options.extend(['-summary'])\n\n        call_tadm(options)\n\n        with open(weightfile_name, 'r') as weightfile:\n            weights = parse_tadm_weights(weightfile)\n\n        os.remove(trainfile_name)\n        os.remove(weightfile_name)\n\n        weights *= numpy.log2(numpy.e)\n\n        return cls(encoding, weights)\n\ndef demo():\n    from nltk.classify.util import names_demo\n    classifier = names_demo(MaxentClassifier.train)\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\classify\\megam": [".py", "\nfrom __future__ import print_function\n\nimport subprocess\n\nfrom six import string_types\n\nfrom nltk import compat\nfrom nltk.internals import find_binary\ntry:\n    import numpy\nexcept ImportError:\n    numpy = None\n\n\n_megam_bin = None\ndef config_megam(bin=None):\n    global _megam_bin\n    _megam_bin = find_binary(\n        'megam', bin,\n        env_vars=['MEGAM'],\n        binary_names=['megam.opt', 'megam', 'megam_686', 'megam_i686.opt'],\n        url='http://www.umiacs.umd.edu/~hal/megam/index.html')\n\n\ndef write_megam_file(train_toks, encoding, stream,\n                     bernoulli=True, explicit=True):\n    labels = encoding.labels()\n    labelnum = dict((label, i) for (i, label) in enumerate(labels))\n\n    for featureset, label in train_toks:\n        if hasattr(encoding, 'cost'):\n            stream.write(':'.join(str(encoding.cost(featureset, label, l))\n                                  for l in labels))\n        else:\n            stream.write('%d' % labelnum[label])\n\n        if not explicit:\n            _write_megam_features(encoding.encode(featureset, label),\n                                  stream, bernoulli)\n\n        else:\n            for l in labels:\n                stream.write(' #')\n                _write_megam_features(encoding.encode(featureset, l),\n                                      stream, bernoulli)\n\n        stream.write('\\n')\n\ndef parse_megam_weights(s, features_count, explicit=True):\n    if numpy is None:\n        raise ValueError('This function requires that numpy be installed')\n    assert explicit, 'non-explicit not supported yet'\n    lines = s.strip().split('\\n')\n    weights = numpy.zeros(features_count, 'd')\n    for line in lines:\n        if line.strip():\n            fid, weight = line.split()\n            weights[int(fid)] = float(weight)\n    return weights\n\ndef _write_megam_features(vector, stream, bernoulli):\n    if not vector:\n        raise ValueError('MEGAM classifier requires the use of an '\n                         'always-on feature.')\n    for (fid, fval) in vector:\n        if bernoulli:\n            if fval == 1:\n                stream.write(' %s' % fid)\n            elif fval != 0:\n                raise ValueError('If bernoulli=True, then all'\n                                 'features must be binary.')\n        else:\n            stream.write(' %s %s' % (fid, fval))\n\ndef call_megam(args):\n    if isinstance(args, string_types):\n        raise TypeError('args should be a list of strings')\n    if _megam_bin is None:\n        config_megam()\n\n    cmd = [_megam_bin] + args\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n    (stdout, stderr) = p.communicate()\n\n    if p.returncode != 0:\n        print()\n        print(stderr)\n        raise OSError('megam command failed!')\n\n    if isinstance(stdout, string_types):\n        return stdout\n    else:\n        return stdout.decode('utf-8')\n"], "nltk\\classify\\naivebayes": [".py", "\nfrom __future__ import print_function, unicode_literals\n\nfrom collections import defaultdict\n\nfrom nltk.probability import FreqDist, DictionaryProbDist, ELEProbDist, sum_logs\nfrom nltk.classify.api import ClassifierI\n\n\nclass NaiveBayesClassifier(ClassifierI):\n    def __init__(self, label_probdist, feature_probdist):\n        self._label_probdist = label_probdist\n        self._feature_probdist = feature_probdist\n        self._labels = list(label_probdist.samples())\n\n    def labels(self):\n        return self._labels\n\n    def classify(self, featureset):\n        return self.prob_classify(featureset).max()\n\n    def prob_classify(self, featureset):\n        featureset = featureset.copy()\n        for fname in list(featureset.keys()):\n            for label in self._labels:\n                if (label, fname) in self._feature_probdist:\n                    break\n            else:\n                del featureset[fname]\n\n        logprob = {}\n        for label in self._labels:\n            logprob[label] = self._label_probdist.logprob(label)\n\n        for label in self._labels:\n            for (fname, fval) in featureset.items():\n                if (label, fname) in self._feature_probdist:\n                    feature_probs = self._feature_probdist[label, fname]\n                    logprob[label] += feature_probs.logprob(fval)\n                else:\n                    logprob[label] += sum_logs([]) # = -INF.\n\n        return DictionaryProbDist(logprob, normalize=True, log=True)\n\n    def show_most_informative_features(self, n=10):\n        cpdist = self._feature_probdist\n        print('Most Informative Features')\n\n        for (fname, fval) in self.most_informative_features(n):\n            def labelprob(l):\n                return cpdist[l, fname].prob(fval)\n\n            labels = sorted([l for l in self._labels\n                             if fval in cpdist[l, fname].samples()],\n                            key=labelprob)\n            if len(labels) == 1:\n                continue\n            l0 = labels[0]\n            l1 = labels[-1]\n            if cpdist[l0, fname].prob(fval) == 0:\n                ratio = 'INF'\n            else:\n                ratio = '%8.1f' % (cpdist[l1, fname].prob(fval) /\n                                   cpdist[l0, fname].prob(fval))\n            print(('%24s = %-14r %6s : %-6s = %s : 1.0' %\n                   (fname, fval, (\"%s\" % l1)[:6], (\"%s\" % l0)[:6], ratio)))\n\n    def most_informative_features(self, n=100):\n        if hasattr(self, '_most_informative_features'):\n            return self._most_informative_features[:n]\n        else:\n            features = set()\n            maxprob = defaultdict(lambda: 0.0)\n            minprob = defaultdict(lambda: 1.0)\n\n            for (label, fname), probdist in self._feature_probdist.items():\n                for fval in probdist.samples():\n                    feature = (fname, fval)\n                    features.add(feature)\n                    p = probdist.prob(fval)\n                    maxprob[feature] = max(p, maxprob[feature])\n                    minprob[feature] = min(p, minprob[feature])\n                    if minprob[feature] == 0:\n                        features.discard(feature)\n\n            self._most_informative_features = sorted(features,\n                                                key=lambda feature_:\n                                                minprob[feature_]/maxprob[feature_])\n        return self._most_informative_features[:n]\n\n    @classmethod\n    def train(cls, labeled_featuresets, estimator=ELEProbDist):\n        label_freqdist = FreqDist()\n        feature_freqdist = defaultdict(FreqDist)\n        feature_values = defaultdict(set)\n        fnames = set()\n\n        for featureset, label in labeled_featuresets:\n            label_freqdist[label] += 1\n            for fname, fval in featureset.items():\n                feature_freqdist[label, fname][fval] += 1\n                feature_values[fname].add(fval)\n                fnames.add(fname)\n\n        for label in label_freqdist:\n            num_samples = label_freqdist[label]\n            for fname in fnames:\n                count = feature_freqdist[label, fname].N()\n                if num_samples - count > 0:\n                    feature_freqdist[label, fname][None] += num_samples - count\n                    feature_values[fname].add(None)\n\n        label_probdist = estimator(label_freqdist)\n\n        feature_probdist = {}\n        for ((label, fname), freqdist) in feature_freqdist.items():\n            probdist = estimator(freqdist, bins=len(feature_values[fname]))\n            feature_probdist[label, fname] = probdist\n\n        return cls(label_probdist, feature_probdist)\n\n\ndef demo():\n    from nltk.classify.util import names_demo\n    classifier = names_demo(NaiveBayesClassifier.train)\n    classifier.show_most_informative_features()\n\nif __name__ == '__main__':\n    demo()\n\n\n"], "nltk\\classify\\positivenaivebayes": [".py", "\n\nfrom collections import defaultdict\n\nfrom nltk.probability import FreqDist, DictionaryProbDist, ELEProbDist\n\nfrom nltk.classify.naivebayes import NaiveBayesClassifier\n\n\nclass PositiveNaiveBayesClassifier(NaiveBayesClassifier):\n    @staticmethod\n    def train(positive_featuresets, unlabeled_featuresets, positive_prob_prior=0.5,\n              estimator=ELEProbDist):\n        positive_feature_freqdist = defaultdict(FreqDist)\n        unlabeled_feature_freqdist = defaultdict(FreqDist)\n        feature_values = defaultdict(set)\n        fnames = set()\n\n        for featureset in positive_featuresets:\n            for fname, fval in featureset.items():\n                positive_feature_freqdist[fname][fval] += 1\n                feature_values[fname].add(fval)\n                fnames.add(fname)\n\n        for featureset in unlabeled_featuresets:\n            for fname, fval in featureset.items():\n                unlabeled_feature_freqdist[fname][fval] += 1\n                feature_values[fname].add(fval)\n                fnames.add(fname)\n\n        num_positive_examples = len(positive_featuresets)\n        for fname in fnames:\n            count = positive_feature_freqdist[fname].N()\n            positive_feature_freqdist[fname][None] += num_positive_examples - count\n            feature_values[fname].add(None)\n\n        num_unlabeled_examples = len(unlabeled_featuresets)\n        for fname in fnames:\n            count = unlabeled_feature_freqdist[fname].N()\n            unlabeled_feature_freqdist[fname][None] += num_unlabeled_examples - count\n            feature_values[fname].add(None)\n\n        negative_prob_prior = 1.0 - positive_prob_prior\n\n        label_probdist = DictionaryProbDist({True: positive_prob_prior,\n                                             False: negative_prob_prior})\n\n        feature_probdist = {}\n        for fname, freqdist in positive_feature_freqdist.items():\n            probdist = estimator(freqdist, bins=len(feature_values[fname]))\n            feature_probdist[True, fname] = probdist\n\n        for fname, freqdist in unlabeled_feature_freqdist.items():\n            global_probdist = estimator(freqdist, bins=len(feature_values[fname]))\n            negative_feature_probs = {}\n            for fval in feature_values[fname]:\n                prob = (global_probdist.prob(fval)\n                        - positive_prob_prior *\n                        feature_probdist[True, fname].prob(fval)) \\\n                        / negative_prob_prior\n                negative_feature_probs[fval] = max(prob, 0.0)\n            feature_probdist[False, fname] = DictionaryProbDist(negative_feature_probs,\n                                                                normalize=True)\n\n        return PositiveNaiveBayesClassifier(label_probdist, feature_probdist)\n\n\ndef demo():\n    from nltk.classify.util import partial_names_demo\n    classifier = partial_names_demo(PositiveNaiveBayesClassifier.train)\n    classifier.show_most_informative_features()\n\n"], "nltk\\classify\\rte_classify": [".py", "\nfrom __future__ import print_function\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.classify.util import accuracy, check_megam_config\nfrom nltk.classify.maxent import MaxentClassifier\n\nclass RTEFeatureExtractor(object):\n    def __init__(self, rtepair, stop=True, use_lemmatize=False):\n        self.stop = stop\n        self.stopwords = set(['a', 'the', 'it', 'they', 'of', 'in', 'to', 'is',\n                              'have', 'are', 'were', 'and', 'very', '.', ','])\n\n        self.negwords = set(['no', 'not', 'never', 'failed', 'rejected',\n                             'denied'])\n        tokenizer = RegexpTokenizer('[\\w.@:/]+|\\w+|\\$[\\d.]+')\n\n        self.text_tokens = tokenizer.tokenize(rtepair.text)\n        self.hyp_tokens = tokenizer.tokenize(rtepair.hyp)\n        self.text_words = set(self.text_tokens)\n        self.hyp_words = set(self.hyp_tokens)\n\n        if use_lemmatize:\n            self.text_words = set(self._lemmatize(token) for token in self.text_tokens)\n            self.hyp_words = set(self._lemmatize(token) for token in self.hyp_tokens)\n\n        if self.stop:\n            self.text_words = self.text_words - self.stopwords\n            self.hyp_words = self.hyp_words - self.stopwords\n\n        self._overlap = self.hyp_words & self.text_words\n        self._hyp_extra = self.hyp_words - self.text_words\n        self._txt_extra = self.text_words - self.hyp_words\n\n\n    def overlap(self, toktype, debug=False):\n        ne_overlap = set(token for token in self._overlap if self._ne(token))\n        if toktype == 'ne':\n            if debug:\n                print(\"ne overlap\", ne_overlap)\n            return ne_overlap\n        elif toktype == 'word':\n            if debug:\n                print(\"word overlap\", self._overlap - ne_overlap)\n            return self._overlap - ne_overlap\n        else:\n            raise ValueError(\"Type not recognized:'%s'\" % toktype)\n\n    def hyp_extra(self, toktype, debug=True):\n        ne_extra = set(token for token in self._hyp_extra if self._ne(token))\n        if toktype == 'ne':\n            return ne_extra\n        elif toktype == 'word':\n            return self._hyp_extra - ne_extra\n        else:\n            raise ValueError(\"Type not recognized: '%s'\" % toktype)\n\n    @staticmethod\n    def _ne(token):\n        if token.istitle() or token.isupper():\n            return True\n        return False\n\n    @staticmethod\n    def _lemmatize(word):\n        lemma = nltk.corpus.wordnet.morphy(word, pos=nltk.corpus.wordnet.VERB)\n        if lemma is not None:\n            return lemma\n        return word\n\n\ndef rte_features(rtepair):\n    extractor = RTEFeatureExtractor(rtepair)\n    features = {}\n    features['alwayson'] = True\n    features['word_overlap'] = len(extractor.overlap('word'))\n    features['word_hyp_extra'] = len(extractor.hyp_extra('word'))\n    features['ne_overlap'] = len(extractor.overlap('ne'))\n    features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))\n    features['neg_txt'] = len(extractor.negwords & extractor.text_words)\n    features['neg_hyp'] = len(extractor.negwords & extractor.hyp_words)\n    return features\n\n\ndef rte_featurize(rte_pairs):\n    return [(rte_features(pair), pair.value) for pair in rte_pairs]\n\n\ndef rte_classifier(algorithm):\n    from nltk.corpus import rte as rte_corpus\n    train_set  = rte_corpus.pairs(['rte1_dev.xml', 'rte2_dev.xml', 'rte3_dev.xml'])\n    test_set = rte_corpus.pairs(['rte1_test.xml', 'rte2_test.xml', 'rte3_test.xml'])\n    featurized_train_set = rte_featurize(train_set)\n    featurized_test_set = rte_featurize(test_set)\n    print('Training classifier...')\n    if algorithm in ['megam', 'BFGS']: # MEGAM based algorithms.\n        check_megam_config()\n        clf = lambda x: MaxentClassifier.train(featurized_train_set, algorithm)\n    elif algorithm in ['GIS', 'IIS']: # Use default GIS/IIS MaxEnt algorithm\n        clf = MaxentClassifier.train(featurized_train_set, algorithm)\n    else:\n        err_msg = str(\"RTEClassifier only supports these algorithms:\\n \"\n                      \"'megam', 'BFGS', 'GIS', 'IIS'.\\n\")\n        raise Exception(err_msg)\n    print('Testing classifier...')\n    acc = accuracy(clf, featurized_test_set)\n    print('Accuracy: %6.4f' % acc)\n    return clf\n"], "nltk\\classify\\scikitlearn": [".py", "from __future__ import print_function, unicode_literals\n\nfrom six.moves import zip\n\nfrom nltk.classify.api import ClassifierI\nfrom nltk.probability import DictionaryProbDist\nfrom nltk import compat\n\ntry:\n    from sklearn.feature_extraction import DictVectorizer\n    from sklearn.preprocessing import LabelEncoder\nexcept ImportError:\n    pass\n\n__all__ = ['SklearnClassifier']\n\n\n@compat.python_2_unicode_compatible\nclass SklearnClassifier(ClassifierI):\n\n    def __init__(self, estimator, dtype=float, sparse=True):\n        self._clf = estimator\n        self._encoder = LabelEncoder()\n        self._vectorizer = DictVectorizer(dtype=dtype, sparse=sparse)\n\n    def __repr__(self):\n        return \"<SklearnClassifier(%r)>\" % self._clf\n\n    def classify_many(self, featuresets):\n        X = self._vectorizer.transform(featuresets)\n        classes = self._encoder.classes_\n        return [classes[i] for i in self._clf.predict(X)]\n\n    def prob_classify_many(self, featuresets):\n        X = self._vectorizer.transform(featuresets)\n        y_proba_list = self._clf.predict_proba(X)\n        return [self._make_probdist(y_proba) for y_proba in y_proba_list]\n\n    def labels(self):\n        return list(self._encoder.classes_)\n\n    def train(self, labeled_featuresets):\n\n        X, y = list(zip(*labeled_featuresets))\n        X = self._vectorizer.fit_transform(X)\n        y = self._encoder.fit_transform(y)\n        self._clf.fit(X, y)\n\n        return self\n\n    def _make_probdist(self, y_proba):\n        classes = self._encoder.classes_\n        return DictionaryProbDist(dict((classes[i], p)\n                                       for i, p in enumerate(y_proba)))\n\n\ndef setup_module(module):\n    from nose import SkipTest\n    try:\n        import sklearn\n    except ImportError:\n        raise SkipTest(\"scikit-learn is not installed\")\n\n\nif __name__ == \"__main__\":\n    from nltk.classify.util import names_demo, names_demo_features\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.naive_bayes import BernoulliNB\n\n    print(\"scikit-learn Naive Bayes:\")\n    names_demo(SklearnClassifier(BernoulliNB(binarize=False)).train,\n               features=names_demo_features)\n\n    print(\"\\n\\nscikit-learn logistic regression:\")\n    names_demo(SklearnClassifier(LogisticRegression(C=1000)).train,\n               features=names_demo_features)\n"], "nltk\\classify\\senna": [".py", "\n\n\nfrom __future__ import unicode_literals\nfrom os import path, sep, environ\nfrom subprocess import Popen, PIPE\nfrom platform import architecture, system\n\nfrom six import text_type\n\nfrom nltk.tag.api import TaggerI\nfrom nltk.compat import python_2_unicode_compatible\n\n_senna_url = 'http://ml.nec-labs.com/senna/'\n\n\n@python_2_unicode_compatible\nclass Senna(TaggerI):\n\n    SUPPORTED_OPERATIONS = ['pos', 'chk', 'ner']\n\n    def __init__(self, senna_path, operations, encoding='utf-8'):\n        self._encoding = encoding\n        self._path = path.normpath(senna_path) + sep\n\n        exe_file_1 = self.executable(self._path)\n        if not path.isfile(exe_file_1):\n            if 'SENNA' in environ:\n                self._path = path.normpath(environ['SENNA']) + sep\n                exe_file_2 = self.executable(self._path)\n                if not path.isfile(exe_file_2):\n                    raise OSError(\"Senna executable expected at %s or %s but not found\" % (exe_file_1,exe_file_2))\n\n        self.operations = operations\n\n\n    def executable(self, base_path):\n        os_name = system()\n        if os_name == 'Linux':\n            bits = architecture()[0]\n            if bits == '64bit':\n                return path.join(base_path, 'senna-linux64')\n            return path.join(base_path, 'senna-linux32')\n        if os_name == 'Windows':\n            return path.join(base_path, 'senna-win32.exe')\n        if os_name == 'Darwin':\n            return path.join(base_path, 'senna-osx')\n        return path.join(base_path, 'senna')\n\n    def _map(self):\n        _map = {}\n        i = 1\n        for operation in Senna.SUPPORTED_OPERATIONS:\n            if operation in self.operations:\n                _map[operation] = i\n                i+= 1\n        return _map\n\n    def tag(self, tokens):\n        return self.tag_sents([tokens])[0]\n\n    def tag_sents(self, sentences):\n        encoding = self._encoding\n\n        if not path.isfile(self.executable(self._path)):\n            raise OSError(\"Senna executable expected at %s but not found\" % self.executable(self._path))\n\n\n        _senna_cmd = [self.executable(self._path), '-path', self._path, '-usrtokens', '-iobtags']\n        _senna_cmd.extend(['-'+op for op in self.operations])\n\n        _input = '\\n'.join((' '.join(x) for x in sentences))+'\\n'\n        if isinstance(_input, text_type) and encoding:\n            _input = _input.encode(encoding)\n\n        p = Popen(_senna_cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n        (stdout, stderr) = p.communicate(input=_input)\n        senna_output = stdout\n\n        if p.returncode != 0:\n            raise RuntimeError('Senna command failed! Details: %s' % stderr)\n\n        if encoding:\n            senna_output = stdout.decode(encoding)\n\n        map_ = self._map()\n        tagged_sentences = [[]]\n        sentence_index = 0\n        token_index = 0\n        for tagged_word in senna_output.strip().split(\"\\n\"):\n            if not tagged_word:\n                tagged_sentences.append([])\n                sentence_index += 1\n                token_index = 0\n                continue\n            tags = tagged_word.split('\\t')\n            result = {}\n            for tag in map_:\n              result[tag] = tags[map_[tag]].strip()\n            try:\n              result['word'] = sentences[sentence_index][token_index]\n            except IndexError:\n              raise IndexError(\n                \"Misalignment error occurred at sentence number %d. Possible reason\"\n                \" is that the sentence size exceeded the maximum size. Check the \"\n                \"documentation of Senna class for more information.\"\n                % sentence_index)\n            tagged_sentences[-1].append(result)\n            token_index += 1\n        return tagged_sentences\n\n\ndef setup_module(module):\n    from nose import SkipTest\n    try:\n        tagger = Senna('/usr/share/senna-v3.0', ['pos', 'chk', 'ner'])\n    except OSError:\n        raise SkipTest(\"Senna executable not found\")\n"], "nltk\\classify\\svm": [".py", "class SvmClassifier(object):\n    def __init__(self, *args, **kwargs):\n        raise NotImplementedError(__doc__)\n"], "nltk\\classify\\tadm": [".py", "from __future__ import print_function, unicode_literals\n\nimport sys\nimport subprocess\n\nfrom six import string_types\n\nfrom nltk.internals import find_binary\ntry:\n    import numpy\nexcept ImportError:\n    pass\n\n_tadm_bin = None\ndef config_tadm(bin=None):\n    global _tadm_bin\n    _tadm_bin = find_binary(\n        'tadm', bin,\n        env_vars=['TADM'],\n        binary_names=['tadm'],\n        url='http://tadm.sf.net')\n\ndef write_tadm_file(train_toks, encoding, stream):\n    labels = encoding.labels()\n    for featureset, label in train_toks:\n        length_line = '%d\\n' % len(labels)\n        stream.write(length_line)\n        for known_label in labels:\n            v = encoding.encode(featureset, known_label)\n            line = '%d %d %s\\n' % (\n                int(label == known_label),\n                len(v),\n                ' '.join('%d %d' % u for u in v)\n            )\n            stream.write(line)\n\ndef parse_tadm_weights(paramfile):\n    weights = []\n    for line in paramfile:\n        weights.append(float(line.strip()))\n    return numpy.array(weights, 'd')\n\ndef call_tadm(args):\n    if isinstance(args, string_types):\n        raise TypeError('args should be a list of strings')\n    if _tadm_bin is None:\n        config_tadm()\n\n    cmd = [_tadm_bin] + args\n    p = subprocess.Popen(cmd, stdout=sys.stdout)\n    (stdout, stderr) = p.communicate()\n\n    if p.returncode != 0:\n        print()\n        print(stderr)\n        raise OSError('tadm command failed!')\n\ndef names_demo():\n    from nltk.classify.util import names_demo\n    from nltk.classify.maxent import TadmMaxentClassifier\n    classifier = names_demo(TadmMaxentClassifier.train)\n\ndef encoding_demo():\n    import sys\n    from nltk.classify.maxent import TadmEventMaxentFeatureEncoding\n    tokens = [({'f0':1, 'f1':1, 'f3':1}, 'A'),\n              ({'f0':1, 'f2':1, 'f4':1}, 'B'),\n              ({'f0':2, 'f2':1, 'f3':1, 'f4':1}, 'A')]\n    encoding = TadmEventMaxentFeatureEncoding.train(tokens)\n    write_tadm_file(tokens, encoding, sys.stdout)\n    print()\n    for i in range(encoding.length()):\n        print('%s --> %d' % (encoding.describe(i), i))\n    print()\n\nif __name__ == '__main__':\n    encoding_demo()\n    names_demo()\n"], "nltk\\classify\\textcat": [".py", "\n\nfrom __future__ import print_function, unicode_literals\n\nfrom nltk.compat import PY3\nfrom nltk.util import trigrams\n\nif PY3:\n    from sys import maxsize\nelse:\n    from sys import maxint\n\ntry:\n    import regex as re\nexcept ImportError:\n    re = None\n\nclass TextCat(object):\n\n    _corpus = None\n    fingerprints = {}\n    _START_CHAR = \"<\"\n    _END_CHAR = \">\"\n    \n    last_distances = {}\n    \n    def __init__(self):\n        if not re:\n            raise EnvironmentError(\"classify.textcat requires the regex module that \"\n                                   \"supports unicode. Try '$ pip install regex' and \"\n                                   \"see https://pypi.python.org/pypi/regex for \"\n                                   \"further details.\")\n\n        from nltk.corpus import crubadan\n        self._corpus = crubadan\n        for lang in self._corpus.langs():\n            self._corpus.lang_freq(lang)\n        \n    def remove_punctuation(self, text):\n        ''' Get rid of punctuation except apostrophes '''\n        return re.sub(r\"[^\\P{P}\\']+\", \"\", text)\n    \n    def profile(self, text):\n        ''' Create FreqDist of trigrams within text '''\n        from nltk import word_tokenize, FreqDist\n\n        clean_text = self.remove_punctuation(text)\n        tokens = word_tokenize(clean_text)\n        \n        fingerprint = FreqDist()\n        for t in tokens:\n            token_trigram_tuples = trigrams(self._START_CHAR + t + self._END_CHAR)\n            token_trigrams = [''.join(tri) for tri in token_trigram_tuples]\n\n            for cur_trigram in token_trigrams:\n                if cur_trigram in fingerprint:\n                    fingerprint[cur_trigram] += 1\n                else:\n                    fingerprint[cur_trigram] = 1\n\n        return fingerprint\n        \n    def calc_dist(self, lang, trigram, text_profile):\n        ''' Calculate the \"out-of-place\" measure between the\n            text and language profile for a single trigram '''\n\n        lang_fd = self._corpus.lang_freq(lang)\n        dist = 0\n\n        if trigram in lang_fd:\n            idx_lang_profile = list(lang_fd.keys()).index(trigram)\n            idx_text = list(text_profile.keys()).index(trigram)\n\n            dist = abs(idx_lang_profile - idx_text) \n        else:\n            if PY3:\n                dist = maxsize\n            else:\n                dist = maxint\n\n        return dist\n        \n    def lang_dists(self, text):\n        ''' Calculate the \"out-of-place\" measure between\n            the text and all languages '''\n        \n        distances = {}\n        profile = self.profile(text)\n        for lang in self._corpus._all_lang_freq.keys():\n            lang_dist = 0\n            for trigram in profile:\n                lang_dist += self.calc_dist(lang, trigram, profile)\n        \n            distances[lang] = lang_dist\n            \n        return distances\n    \n    def guess_language(self, text):\n        ''' Find the language with the min distance\n            to the text and return its ISO 639-3 code '''\n        self.last_distances = self.lang_dists(text)\n        \n        return min(self.last_distances, key=self.last_distances.get)\n\ndef demo():\n    from nltk.corpus import udhr\n\n    langs = ['Kurdish-UTF8', 'Abkhaz-UTF8', 'Farsi_Persian-UTF8',\n             'Hindi-UTF8', 'Hawaiian-UTF8', 'Russian-UTF8', 'Vietnamese-UTF8',\n             'Serbian_Srpski-UTF8','Esperanto-UTF8']\n\n    friendly = {'kmr':'Northern Kurdish',\n                'abk':'Abkhazian',\n                'pes':'Iranian Persian',\n                'hin':'Hindi',\n                'haw':'Hawaiian',\n                'rus':'Russian',\n                'vie':'Vietnamese',\n                'srp':'Serbian',\n                'epo':'Esperanto'}\n        \n    tc = TextCat()\n\n    for cur_lang in langs:\n        raw_sentences = udhr.sents(cur_lang)\n        rows = len(raw_sentences) - 1\n        cols = list(map(len, raw_sentences))\n\n        sample = ''\n          \n        for i in range(0, rows):\n            cur_sent = ''\n            for j in range(0, cols[i]):\n                cur_sent += ' ' + raw_sentences[i][j]\n            \n            sample += cur_sent\n          \n        print('Language snippet: ' + sample[0:140] + '...')\n        guess = tc.guess_language(sample)\n        print('Language detection: %s (%s)' % (guess, friendly[guess]))\n        print('#' * 140)\n\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\classify\\util": [".py", "\nfrom __future__ import print_function, division\n\nimport math\n\nimport nltk.classify.util # for accuracy & log_likelihood\nfrom nltk.util import LazyMap\n\n\ndef apply_features(feature_func, toks, labeled=None):\n    if labeled is None:\n        labeled = toks and isinstance(toks[0], (tuple, list))\n    if labeled:\n        def lazy_func(labeled_token):\n            return (feature_func(labeled_token[0]), labeled_token[1])\n        return LazyMap(lazy_func, toks)\n    else:\n        return LazyMap(feature_func, toks)\n\ndef attested_labels(tokens):\n    return tuple(set(label for (tok, label) in tokens))\n\ndef log_likelihood(classifier, gold):\n    results = classifier.prob_classify_many([fs for (fs, l) in gold])\n    ll = [pdist.prob(l) for ((fs, l), pdist) in zip(gold, results)]\n    return math.log(sum(ll) / len(ll))\n\ndef accuracy(classifier, gold):\n    results = classifier.classify_many([fs for (fs, l) in gold])\n    correct = [l == r for ((fs, l), r) in zip(gold, results)]\n    if correct:\n        return sum(correct) / len(correct)\n    else:\n        return 0\n\nclass CutoffChecker(object):\n    def __init__(self, cutoffs):\n        self.cutoffs = cutoffs.copy()\n        if 'min_ll' in cutoffs:\n            cutoffs['min_ll'] = -abs(cutoffs['min_ll'])\n        if 'min_lldelta' in cutoffs:\n            cutoffs['min_lldelta'] = abs(cutoffs['min_lldelta'])\n        self.ll = None\n        self.acc = None\n        self.iter = 1\n\n    def check(self, classifier, train_toks):\n        cutoffs = self.cutoffs\n        self.iter += 1\n        if 'max_iter' in cutoffs and self.iter >= cutoffs['max_iter']:\n            return True # iteration cutoff.\n\n        new_ll = nltk.classify.util.log_likelihood(classifier, train_toks)\n        if math.isnan(new_ll):\n            return True\n\n        if 'min_ll' in cutoffs or 'min_lldelta' in cutoffs:\n            if 'min_ll' in cutoffs and new_ll >= cutoffs['min_ll']:\n                return True # log likelihood cutoff\n            if ('min_lldelta' in cutoffs and self.ll and\n                ((new_ll - self.ll) <= abs(cutoffs['min_lldelta']))):\n                return True # log likelihood delta cutoff\n            self.ll = new_ll\n\n        if 'max_acc' in cutoffs or 'min_accdelta' in cutoffs:\n            new_acc = nltk.classify.util.log_likelihood(\n                classifier, train_toks)\n            if 'max_acc' in cutoffs and new_acc >= cutoffs['max_acc']:\n                return True # log likelihood cutoff\n            if ('min_accdelta' in cutoffs and self.acc and\n                ((new_acc - self.acc) <= abs(cutoffs['min_accdelta']))):\n                return True # log likelihood delta cutoff\n            self.acc = new_acc\n\n            return False # no cutoff reached.\n\n\ndef names_demo_features(name):\n    features = {}\n    features['alwayson'] = True\n    features['startswith'] = name[0].lower()\n    features['endswith'] = name[-1].lower()\n    for letter in 'abcdefghijklmnopqrstuvwxyz':\n        features['count(%s)' % letter] = name.lower().count(letter)\n        features['has(%s)' % letter] = letter in name.lower()\n    return features\n\ndef binary_names_demo_features(name):\n    features = {}\n    features['alwayson'] = True\n    features['startswith(vowel)'] = name[0].lower() in 'aeiouy'\n    features['endswith(vowel)'] = name[-1].lower() in 'aeiouy'\n    for letter in 'abcdefghijklmnopqrstuvwxyz':\n        features['count(%s)' % letter] = name.lower().count(letter)\n        features['has(%s)' % letter] = letter in name.lower()\n        features['startswith(%s)' % letter] = (letter == name[0].lower())\n        features['endswith(%s)' % letter] = (letter == name[-1].lower())\n    return features\n\ndef names_demo(trainer, features=names_demo_features):\n    from nltk.corpus import names\n    import random\n\n    namelist = ([(name, 'male') for name in names.words('male.txt')] +\n                [(name, 'female') for name in names.words('female.txt')])\n\n    random.seed(123456)\n    random.shuffle(namelist)\n    train = namelist[:5000]\n    test = namelist[5000:5500]\n\n    print('Training classifier...')\n    classifier = trainer( [(features(n), g) for (n, g) in train] )\n\n    print('Testing classifier...')\n    acc = accuracy(classifier, [(features(n), g) for (n, g) in test])\n    print('Accuracy: %6.4f' % acc)\n\n    try:\n        test_featuresets = [features(n) for (n, g) in test]\n        pdists = classifier.prob_classify_many(test_featuresets)\n        ll = [pdist.logprob(gold)\n              for ((name, gold), pdist) in zip(test, pdists)]\n        print('Avg. log likelihood: %6.4f' % (sum(ll) / len(test)))\n        print()\n        print('Unseen Names      P(Male)  P(Female)\\n'+'-'*40)\n        for ((name, gender), pdist) in list(zip(test, pdists))[:5]:\n            if gender == 'male':\n                fmt = '  %-15s *%6.4f   %6.4f'\n            else:\n                fmt = '  %-15s  %6.4f  *%6.4f'\n            print(fmt % (name, pdist.prob('male'), pdist.prob('female')))\n    except NotImplementedError:\n        pass\n\n    return classifier\n\ndef partial_names_demo(trainer, features=names_demo_features):\n    from nltk.corpus import names\n    import random\n\n    male_names = names.words('male.txt')\n    female_names = names.words('female.txt')\n\n    random.seed(654321)\n    random.shuffle(male_names)\n    random.shuffle(female_names)\n\n    positive = map(features, male_names[:2000])\n\n    unlabeled = map(features, male_names[2000:2500] + female_names[:500])\n\n    test = [(name, True) for name in male_names[2500:2750]] \\\n        + [(name, False) for name in female_names[500:750]]\n\n    random.shuffle(test)\n\n    print('Training classifier...')\n    classifier = trainer(positive, unlabeled)\n\n    print('Testing classifier...')\n    acc = accuracy(classifier, [(features(n), m) for (n, m) in test])\n    print('Accuracy: %6.4f' % acc)\n\n    try:\n        test_featuresets = [features(n) for (n, m) in test]\n        pdists = classifier.prob_classify_many(test_featuresets)\n        ll = [pdist.logprob(gold)\n              for ((name, gold), pdist) in zip(test, pdists)]\n        print('Avg. log likelihood: %6.4f' % (sum(ll) / len(test)))\n        print()\n        print('Unseen Names      P(Male)  P(Female)\\n'+'-'*40)\n        for ((name, is_male), pdist) in zip(test, pdists)[:5]:\n            if is_male == True:\n                fmt = '  %-15s *%6.4f   %6.4f'\n            else:\n                fmt = '  %-15s  %6.4f  *%6.4f'\n            print(fmt % (name, pdist.prob(True), pdist.prob(False)))\n    except NotImplementedError:\n        pass\n\n    return classifier\n\n_inst_cache = {}\ndef wsd_demo(trainer, word, features, n=1000):\n    from nltk.corpus import senseval\n    import random\n\n    print('Reading data...')\n    global _inst_cache\n    if word not in _inst_cache:\n        _inst_cache[word] = [(i, i.senses[0]) for i in senseval.instances(word)]\n    instances = _inst_cache[word][:]\n    if n > len(instances):\n        n = len(instances)\n    senses = list(set(l for (i, l) in instances))\n    print('  Senses: ' + ' '.join(senses))\n\n    print('Splitting into test & train...')\n    random.seed(123456)\n    random.shuffle(instances)\n    train = instances[:int(.8*n)]\n    test = instances[int(.8*n):n]\n\n    print('Training classifier...')\n    classifier = trainer([(features(i), l) for (i, l) in train])\n\n    print('Testing classifier...')\n    acc = accuracy(classifier, [(features(i), l) for (i, l) in test])\n    print('Accuracy: %6.4f' % acc)\n\n    try:\n        test_featuresets = [features(i) for (i, n) in test]\n        pdists = classifier.prob_classify_many(test_featuresets)\n        ll = [pdist.logprob(gold)\n              for ((name, gold), pdist) in zip(test, pdists)]\n        print('Avg. log likelihood: %6.4f' % (sum(ll) / len(test)))\n    except NotImplementedError:\n        pass\n\n    return classifier\n\n\n\ndef check_megam_config(self):\n    try:\n        _megam_bin\n    except NameError:\n        err_msg = str(\"Please configure your megam binary first, e.g.\\n\"\n                      \">>> nltk.config_megam('/usr/bin/local/megam')\")\n        raise NameError(err_msg)\n"], "nltk\\classify\\weka": [".py", "\nfrom __future__ import print_function\nimport time\nimport tempfile\nimport os\nimport subprocess\nimport re\nimport zipfile\nfrom sys import stdin\n\nfrom six import integer_types, string_types\n\nfrom nltk.probability import DictionaryProbDist\nfrom nltk.internals import java, config_java\n\nfrom nltk.classify.api import ClassifierI\n\n_weka_classpath = None\n_weka_search = ['.',\n                '/usr/share/weka',\n                '/usr/local/share/weka',\n                '/usr/lib/weka',\n                '/usr/local/lib/weka',]\ndef config_weka(classpath=None):\n    global _weka_classpath\n\n    config_java()\n\n    if classpath is not None:\n        _weka_classpath = classpath\n\n    if _weka_classpath is None:\n        searchpath = _weka_search\n        if 'WEKAHOME' in os.environ:\n            searchpath.insert(0, os.environ['WEKAHOME'])\n\n        for path in searchpath:\n            if os.path.exists(os.path.join(path, 'weka.jar')):\n                _weka_classpath = os.path.join(path, 'weka.jar')\n                version = _check_weka_version(_weka_classpath)\n                if version:\n                    print(('[Found Weka: %s (version %s)]' %\n                           (_weka_classpath, version)))\n                else:\n                    print('[Found Weka: %s]' % _weka_classpath)\n                _check_weka_version(_weka_classpath)\n\n    if _weka_classpath is None:\n        raise LookupError('Unable to find weka.jar!  Use config_weka() '\n                          'or set the WEKAHOME environment variable. '\n                          'For more information about Weka, please see '\n                          'http://www.cs.waikato.ac.nz/ml/weka/')\n\ndef _check_weka_version(jar):\n    try:\n        zf = zipfile.ZipFile(jar)\n    except (SystemExit, KeyboardInterrupt):\n        raise\n    except:\n        return None\n    try:\n        try:\n            return zf.read('weka/core/version.txt')\n        except KeyError:\n            return None\n    finally:\n        zf.close()\n\nclass WekaClassifier(ClassifierI):\n    def __init__(self, formatter, model_filename):\n        self._formatter = formatter\n        self._model = model_filename\n\n    def prob_classify_many(self, featuresets):\n        return self._classify_many(featuresets, ['-p', '0', '-distribution'])\n\n    def classify_many(self, featuresets):\n        return self._classify_many(featuresets, ['-p', '0'])\n\n    def _classify_many(self, featuresets, options):\n        config_weka()\n\n        temp_dir = tempfile.mkdtemp()\n        try:\n            test_filename = os.path.join(temp_dir, 'test.arff')\n            self._formatter.write(test_filename, featuresets)\n\n            cmd = ['weka.classifiers.bayes.NaiveBayes',\n                   '-l', self._model, '-T', test_filename] + options\n            (stdout, stderr) = java(cmd, classpath=_weka_classpath,\n                                    stdout=subprocess.PIPE,\n                                    stderr=subprocess.PIPE)\n\n            if stderr and not stdout:\n                if 'Illegal options: -distribution' in stderr:\n                    raise ValueError('The installed version of weka does '\n                                     'not support probability distribution '\n                                     'output.')\n                else:\n                    raise ValueError('Weka failed to generate output:\\n%s'\n                                     % stderr)\n\n            return self.parse_weka_output(stdout.decode(stdin.encoding).split('\\n'))\n\n        finally:\n            for f in os.listdir(temp_dir):\n                os.remove(os.path.join(temp_dir, f))\n            os.rmdir(temp_dir)\n\n    def parse_weka_distribution(self, s):\n        probs = [float(v) for v in re.split('[*,]+', s) if v.strip()]\n        probs = dict(zip(self._formatter.labels(), probs))\n        return DictionaryProbDist(probs)\n\n    def parse_weka_output(self, lines):\n        for i,line in enumerate(lines):\n            if line.strip().startswith(\"inst#\"):\n                lines = lines[i:]\n                break\n\n        if lines[0].split() == ['inst#', 'actual', 'predicted',\n                                'error', 'prediction']:\n            return [line.split()[2].split(':')[1]\n                    for line in lines[1:] if line.strip()]\n        elif lines[0].split() == ['inst#', 'actual', 'predicted',\n                                  'error', 'distribution']:\n            return [self.parse_weka_distribution(line.split()[-1])\n                    for line in lines[1:] if line.strip()]\n\n        elif re.match(r'^0 \\w+ [01]\\.[0-9]* \\?\\s*$', lines[0]):\n            return [line.split()[1] for line in lines if line.strip()]\n\n        else:\n            for line in lines[:10]:\n                print(line)\n            raise ValueError('Unhandled output format -- your version '\n                             'of weka may not be supported.\\n'\n                             '  Header: %s' % lines[0])\n\n\n\n    _CLASSIFIER_CLASS = {\n        'naivebayes': 'weka.classifiers.bayes.NaiveBayes',\n        'C4.5': 'weka.classifiers.trees.J48',\n        'log_regression': 'weka.classifiers.functions.Logistic',\n        'svm': 'weka.classifiers.functions.SMO',\n        'kstar': 'weka.classifiers.lazy.KStar',\n        'ripper': 'weka.classifiers.rules.JRip',\n        }\n    @classmethod\n    def train(cls, model_filename, featuresets,\n              classifier='naivebayes', options=[], quiet=True):\n        config_weka()\n\n        formatter = ARFF_Formatter.from_train(featuresets)\n\n        temp_dir = tempfile.mkdtemp()\n        try:\n            train_filename = os.path.join(temp_dir, 'train.arff')\n            formatter.write(train_filename, featuresets)\n\n            if classifier in cls._CLASSIFIER_CLASS:\n                javaclass = cls._CLASSIFIER_CLASS[classifier]\n            elif classifier in cls._CLASSIFIER_CLASS.values():\n                javaclass = classifier\n            else:\n                raise ValueError('Unknown classifier %s' % classifier)\n\n            cmd = [javaclass, '-d', model_filename, '-t', train_filename]\n            cmd += list(options)\n            if quiet:\n                stdout = subprocess.PIPE\n            else: stdout = None\n            java(cmd, classpath=_weka_classpath, stdout=stdout)\n\n            return WekaClassifier(formatter, model_filename)\n\n        finally:\n            for f in os.listdir(temp_dir):\n                os.remove(os.path.join(temp_dir, f))\n            os.rmdir(temp_dir)\n\n\nclass ARFF_Formatter:\n\n    def __init__(self, labels, features):\n        self._labels = labels\n        self._features = features\n\n    def format(self, tokens):\n        return self.header_section() + self.data_section(tokens)\n\n    def labels(self):\n        return list(self._labels)\n\n    def write(self, outfile, tokens):\n        if not hasattr(outfile, 'write'):\n            outfile = open(outfile, 'w')\n        outfile.write(self.format(tokens))\n        outfile.close()\n\n    @staticmethod\n    def from_train(tokens):\n        labels = set(label for (tok, label) in tokens)\n\n        features = {}\n        for tok, label in tokens:\n            for (fname, fval) in tok.items():\n                if issubclass(type(fval), bool):\n                    ftype = '{True, False}'\n                elif issubclass(type(fval), (integer_types, float, bool)):\n                    ftype = 'NUMERIC'\n                elif issubclass(type(fval), string_types):\n                    ftype = 'STRING'\n                elif fval is None:\n                    continue # can't tell the type.\n                else:\n                    raise ValueError('Unsupported value type %r' % ftype)\n\n                if features.get(fname, ftype) != ftype:\n                    raise ValueError('Inconsistent type for %s' % fname)\n                features[fname] = ftype\n        features = sorted(features.items())\n\n        return ARFF_Formatter(labels, features)\n\n    def header_section(self):\n        s = ('% Weka ARFF file\\n' +\n             '% Generated automatically by NLTK\\n' +\n             '%% %s\\n\\n' % time.ctime())\n\n        s += '@RELATION rel\\n\\n'\n\n        for fname, ftype in self._features:\n            s += '@ATTRIBUTE %-30r %s\\n' % (fname, ftype)\n\n        s += '@ATTRIBUTE %-30r {%s}\\n' % ('-label-', ','.join(self._labels))\n\n        return s\n\n    def data_section(self, tokens, labeled=None):\n        if labeled is None:\n            labeled = tokens and isinstance(tokens[0], (tuple, list))\n        if not labeled:\n            tokens = [(tok, None) for tok in tokens]\n\n        s = '\\n@DATA\\n'\n        for (tok, label) in tokens:\n            for fname, ftype in self._features:\n                s += '%s,' % self._fmt_arff_val(tok.get(fname))\n            s += '%s\\n' % self._fmt_arff_val(label)\n\n        return s\n\n    def _fmt_arff_val(self, fval):\n        if fval is None:\n            return '?'\n        elif isinstance(fval, (bool, integer_types)):\n            return '%s' % fval\n        elif isinstance(fval, float):\n            return '%r' % fval\n        else:\n            return '%r' % fval\n\n\nif __name__ == '__main__':\n    from nltk.classify.util import names_demo, binary_names_demo_features\n    def make_classifier(featuresets):\n        return WekaClassifier.train('/tmp/name.model', featuresets,\n                                    'C4.5')\n    classifier = names_demo(make_classifier, binary_names_demo_features)\n"], "nltk\\classify\\__init__": [".py", "\n\nfrom nltk.classify.api import ClassifierI, MultiClassifierI\nfrom nltk.classify.megam import config_megam, call_megam\nfrom nltk.classify.weka import WekaClassifier, config_weka\nfrom nltk.classify.naivebayes import NaiveBayesClassifier\nfrom nltk.classify.positivenaivebayes import PositiveNaiveBayesClassifier\nfrom nltk.classify.decisiontree import DecisionTreeClassifier\nfrom nltk.classify.rte_classify import rte_classifier, rte_features, RTEFeatureExtractor\nfrom nltk.classify.util import accuracy, apply_features, log_likelihood\nfrom nltk.classify.scikitlearn import SklearnClassifier\nfrom nltk.classify.maxent import (MaxentClassifier, BinaryMaxentFeatureEncoding,\n                                  TypedMaxentFeatureEncoding,\n                                  ConditionalExponentialClassifier)\nfrom nltk.classify.senna import Senna\nfrom nltk.classify.textcat import TextCat\n", 1], "nltk\\cluster\\api": [".py", "\nfrom abc import ABCMeta, abstractmethod\nfrom six import add_metaclass\n\nfrom nltk.probability import DictionaryProbDist\n\n@add_metaclass(ABCMeta)\nclass ClusterI(object):\n    @abstractmethod\n    def cluster(self, vectors, assign_clusters=False):\n\n    @abstractmethod\n    def classify(self, token):\n\n    def likelihood(self, vector, label):\n        if self.classify(vector) == label:\n            return 1.0\n        else:\n            return 0.0\n\n    def classification_probdist(self, vector):\n        likelihoods = {}\n        sum = 0.0\n        for cluster in self.cluster_names():\n            likelihoods[cluster] = self.likelihood(vector, cluster)\n            sum += likelihoods[cluster]\n        for cluster in self.cluster_names():\n            likelihoods[cluster] /= sum\n        return DictionaryProbDist(likelihoods)\n\n    @abstractmethod\n    def num_clusters(self):\n\n    def cluster_names(self):\n        return list(range(self.num_clusters()))\n\n    def cluster_name(self, index):\n        return index\n"], "nltk\\cluster\\em": [".py", "from __future__ import print_function, unicode_literals\ntry:\n    import numpy\nexcept ImportError:\n    pass\n\nfrom nltk.compat import python_2_unicode_compatible\nfrom nltk.cluster.util import VectorSpaceClusterer\n\n@python_2_unicode_compatible\nclass EMClusterer(VectorSpaceClusterer):\n\n    def __init__(self, initial_means, priors=None, covariance_matrices=None,\n                       conv_threshold=1e-6, bias=0.1, normalise=False,\n                       svd_dimensions=None):\n        VectorSpaceClusterer.__init__(self, normalise, svd_dimensions)\n        self._means = numpy.array(initial_means, numpy.float64)\n        self._num_clusters = len(initial_means)\n        self._conv_threshold = conv_threshold\n        self._covariance_matrices = covariance_matrices\n        self._priors = priors\n        self._bias = bias\n\n    def num_clusters(self):\n        return self._num_clusters\n\n    def cluster_vectorspace(self, vectors, trace=False):\n        assert len(vectors) > 0\n\n        dimensions = len(vectors[0])\n        means = self._means\n        priors = self._priors\n        if not priors:\n            priors = self._priors = numpy.ones(self._num_clusters,\n                                        numpy.float64) / self._num_clusters\n        covariances = self._covariance_matrices\n        if not covariances:\n            covariances = self._covariance_matrices = \\\n                [ numpy.identity(dimensions, numpy.float64)\n                  for i in range(self._num_clusters) ]\n\n        lastl = self._loglikelihood(vectors, priors, means, covariances)\n        converged = False\n\n        while not converged:\n            if trace: print('iteration; loglikelihood', lastl)\n            h = numpy.zeros((len(vectors), self._num_clusters),\n                numpy.float64)\n            for i in range(len(vectors)):\n                for j in range(self._num_clusters):\n                    h[i,j] = priors[j] * self._gaussian(means[j],\n                                               covariances[j], vectors[i])\n                h[i,:] /= sum(h[i,:])\n\n            for j in range(self._num_clusters):\n                covariance_before = covariances[j]\n                new_covariance = numpy.zeros((dimensions, dimensions),\n                            numpy.float64)\n                new_mean = numpy.zeros(dimensions, numpy.float64)\n                sum_hj = 0.0\n                for i in range(len(vectors)):\n                    delta = vectors[i] - means[j]\n                    new_covariance += h[i,j] * \\\n                        numpy.multiply.outer(delta, delta)\n                    sum_hj += h[i,j]\n                    new_mean += h[i,j] * vectors[i]\n                covariances[j] = new_covariance / sum_hj\n                means[j] = new_mean / sum_hj\n                priors[j] = sum_hj / len(vectors)\n\n                covariances[j] += self._bias * \\\n                    numpy.identity(dimensions, numpy.float64)\n\n            l = self._loglikelihood(vectors, priors, means, covariances)\n\n            if abs(lastl - l) < self._conv_threshold:\n                converged = True\n            lastl = l\n\n    def classify_vectorspace(self, vector):\n        best = None\n        for j in range(self._num_clusters):\n            p = self._priors[j] * self._gaussian(self._means[j],\n                                    self._covariance_matrices[j], vector)\n            if not best or p > best[0]:\n                best = (p, j)\n        return best[1]\n\n    def likelihood_vectorspace(self, vector, cluster):\n        cid = self.cluster_names().index(cluster)\n        return self._priors[cluster] * self._gaussian(self._means[cluster],\n                                self._covariance_matrices[cluster], vector)\n\n    def _gaussian(self, mean, cvm, x):\n        m = len(mean)\n        assert cvm.shape == (m, m), \\\n            'bad sized covariance matrix, %s' % str(cvm.shape)\n        try:\n            det = numpy.linalg.det(cvm)\n            inv = numpy.linalg.inv(cvm)\n            a = det ** -0.5 * (2 * numpy.pi) ** (-m / 2.0)\n            dx = x - mean\n            print(dx, inv)\n            b = -0.5 * numpy.dot( numpy.dot(dx, inv), dx)\n            return a * numpy.exp(b)\n        except OverflowError:\n            return 0\n\n    def _loglikelihood(self, vectors, priors, means, covariances):\n        llh = 0.0\n        for vector in vectors:\n            p = 0\n            for j in range(len(priors)):\n                p += priors[j] * \\\n                         self._gaussian(means[j], covariances[j], vector)\n            llh += numpy.log(p)\n        return llh\n\n    def __repr__(self):\n        return '<EMClusterer means=%s>' % list(self._means)\n\ndef demo():\n\n    from nltk import cluster\n\n\n    vectors = [numpy.array(f) for f in [[0.5, 0.5], [1.5, 0.5], [1, 3]]]\n    means = [[4, 2], [4, 2.01]]\n\n    clusterer = cluster.EMClusterer(means, bias=0.1)\n    clusters = clusterer.cluster(vectors, True, trace=True)\n\n    print('Clustered:', vectors)\n    print('As:       ', clusters)\n    print()\n\n    for c in range(2):\n        print('Cluster:', c)\n        print('Prior:  ', clusterer._priors[c])\n        print('Mean:   ', clusterer._means[c])\n        print('Covar:  ', clusterer._covariance_matrices[c])\n        print()\n\n    vector = numpy.array([2, 2])\n    print('classify(%s):' % vector, end=' ')\n    print(clusterer.classify(vector))\n\n    vector = numpy.array([2, 2])\n    print('classification_probdist(%s):' % vector)\n    pdist = clusterer.classification_probdist(vector)\n    for sample in pdist.samples():\n        print('%s => %.0f%%' % (sample,\n                    pdist.prob(sample) *100))\n\n\n\n\n\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\cluster\\gaac": [".py", "from __future__ import print_function, unicode_literals, division\n\ntry:\n    import numpy\nexcept ImportError:\n    pass\n\nfrom nltk.cluster.util import VectorSpaceClusterer, Dendrogram, cosine_distance\nfrom nltk.compat import python_2_unicode_compatible\n\n@python_2_unicode_compatible\nclass GAAClusterer(VectorSpaceClusterer):\n\n    def __init__(self, num_clusters=1, normalise=True, svd_dimensions=None):\n        VectorSpaceClusterer.__init__(self, normalise, svd_dimensions)\n        self._num_clusters = num_clusters\n        self._dendrogram = None\n        self._groups_values = None\n\n    def cluster(self, vectors, assign_clusters=False, trace=False):\n        self._dendrogram = Dendrogram(\n            [numpy.array(vector, numpy.float64) for vector in vectors])\n        return VectorSpaceClusterer.cluster(self, vectors, assign_clusters, trace)\n\n    def cluster_vectorspace(self, vectors, trace=False):\n        N = len(vectors)\n        cluster_len = [1]*N\n        cluster_count = N\n        index_map = numpy.arange(N)\n\n        dims = (N, N)\n        dist = numpy.ones(dims, dtype=numpy.float)*numpy.inf\n        for i in range(N):\n            for j in range(i+1, N):\n                dist[i, j] = cosine_distance(vectors[i], vectors[j])\n\n        while cluster_count > max(self._num_clusters, 1):\n            i, j = numpy.unravel_index(dist.argmin(), dims)\n            if trace:\n                print(\"merging %d and %d\" % (i, j))\n\n            self._merge_similarities(dist, cluster_len, i, j)\n\n            dist[:, j] = numpy.inf\n            dist[j, :] = numpy.inf\n\n            cluster_len[i] = cluster_len[i]+cluster_len[j]\n            self._dendrogram.merge(index_map[i], index_map[j])\n            cluster_count -= 1\n\n            index_map[j+1:] -= 1\n            index_map[j] = N\n\n        self.update_clusters(self._num_clusters)\n\n    def _merge_similarities(self, dist, cluster_len, i, j):\n        i_weight = cluster_len[i]\n        j_weight = cluster_len[j]\n        weight_sum = i_weight+j_weight\n\n        dist[:i, i] = dist[:i, i]*i_weight + dist[:i, j]*j_weight\n        dist[:i, i] /= weight_sum\n        dist[i, i+1:j] = dist[i, i+1:j]*i_weight + dist[i+1:j, j]*j_weight\n        dist[i, j+1:] = dist[i, j+1:]*i_weight + dist[j, j+1:]*j_weight\n        dist[i, i+1:] /= weight_sum\n\n    def update_clusters(self, num_clusters):\n        clusters = self._dendrogram.groups(num_clusters)\n        self._centroids = []\n        for cluster in clusters:\n            assert len(cluster) > 0\n            if self._should_normalise:\n                centroid = self._normalise(cluster[0])\n            else:\n                centroid = numpy.array(cluster[0])\n            for vector in cluster[1:]:\n                if self._should_normalise:\n                    centroid += self._normalise(vector)\n                else:\n                    centroid += vector\n            centroid /= len(cluster)\n            self._centroids.append(centroid)\n        self._num_clusters = len(self._centroids)\n\n    def classify_vectorspace(self, vector):\n        best = None\n        for i in range(self._num_clusters):\n            centroid = self._centroids[i]\n            dist = cosine_distance(vector, centroid)\n            if not best or dist < best[0]:\n                best = (dist, i)\n        return best[1]\n\n    def dendrogram(self):\n        return self._dendrogram\n\n    def num_clusters(self):\n        return self._num_clusters\n\n    def __repr__(self):\n        return '<GroupAverageAgglomerative Clusterer n=%d>' % self._num_clusters\n\ndef demo():\n\n    from nltk.cluster import GAAClusterer\n\n    vectors = [numpy.array(f) for f in [[3, 3], [1, 2], [4, 2], [4, 0], [2, 3], [3, 1]]]\n\n    clusterer = GAAClusterer(4)\n    clusters = clusterer.cluster(vectors, True)\n\n    print('Clusterer:', clusterer)\n    print('Clustered:', vectors)\n    print('As:', clusters)\n    print()\n\n    clusterer.dendrogram().show()\n\n    vector = numpy.array([3, 3])\n    print('classify(%s):' % vector, end=' ')\n    print(clusterer.classify(vector))\n    print()\n\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\cluster\\kmeans": [".py", "from __future__ import print_function, unicode_literals, division\n\nimport copy\nimport random\nimport sys\n\ntry:\n    import numpy\nexcept ImportError:\n    pass\n\n\nfrom nltk.cluster.util import VectorSpaceClusterer\nfrom nltk.compat import python_2_unicode_compatible\n\n\n@python_2_unicode_compatible\nclass KMeansClusterer(VectorSpaceClusterer):\n\n    def __init__(self, num_means, distance, repeats=1,\n                       conv_test=1e-6, initial_means=None,\n                       normalise=False, svd_dimensions=None,\n                       rng=None, avoid_empty_clusters=False):\n\n        VectorSpaceClusterer.__init__(self, normalise, svd_dimensions)\n        self._num_means = num_means\n        self._distance = distance\n        self._max_difference = conv_test\n        assert not initial_means or len(initial_means) == num_means\n        self._means = initial_means\n        assert repeats >= 1\n        assert not (initial_means and repeats > 1)\n        self._repeats = repeats\n        self._rng = (rng if rng else random.Random())\n        self._avoid_empty_clusters = avoid_empty_clusters\n\n    def cluster_vectorspace(self, vectors, trace=False):\n        if self._means and self._repeats > 1:\n            print('Warning: means will be discarded for subsequent trials')\n\n        meanss = []\n        for trial in range(self._repeats):\n            if trace: print('k-means trial', trial)\n            if not self._means or trial > 1:\n                self._means = self._rng.sample(list(vectors), self._num_means)\n            self._cluster_vectorspace(vectors, trace)\n            meanss.append(self._means)\n\n        if len(meanss) > 1:\n            for means in meanss:\n                means.sort(key=sum)\n\n            min_difference = min_means = None\n            for i in range(len(meanss)):\n                d = 0\n                for j in range(len(meanss)):\n                    if i != j:\n                        d += self._sum_distances(meanss[i], meanss[j])\n                if min_difference is None or d < min_difference:\n                    min_difference, min_means = d, meanss[i]\n\n            self._means = min_means\n\n    def _cluster_vectorspace(self, vectors, trace=False):\n        if self._num_means < len(vectors):\n            converged = False\n            while not converged:\n                clusters = [[] for m in range(self._num_means)]\n                for vector in vectors:\n                    index = self.classify_vectorspace(vector)\n                    clusters[index].append(vector)\n\n                if trace: print('iteration')\n\n                new_means = list(map(self._centroid, clusters, self._means))\n\n                difference = self._sum_distances(self._means, new_means)\n                if difference < self._max_difference:\n                    converged = True\n\n                self._means = new_means\n\n    def classify_vectorspace(self, vector):\n        best_distance = best_index = None\n        for index in range(len(self._means)):\n            mean = self._means[index]\n            dist = self._distance(vector, mean)\n            if best_distance is None or dist < best_distance:\n                best_index, best_distance = index, dist\n        return best_index\n\n    def num_clusters(self):\n        if self._means:\n            return len(self._means)\n        else:\n            return self._num_means\n\n    def means(self):\n        return self._means\n\n    def _sum_distances(self, vectors1, vectors2):\n        difference = 0.0\n        for u, v in zip(vectors1, vectors2):\n            difference += self._distance(u, v)\n        return difference\n\n    def _centroid(self, cluster, mean):\n        if self._avoid_empty_clusters:\n            centroid = copy.copy(mean)\n            for vector in cluster:\n                centroid += vector\n            return centroid / (1+len(cluster))\n        else:\n            if not len(cluster):\n                sys.stderr.write('Error: no centroid defined for empty cluster.\\n')\n                sys.stderr.write('Try setting argument \\'avoid_empty_clusters\\' to True\\n')\n                assert(False)\n            centroid = copy.copy(cluster[0])\n            for vector in cluster[1:]:\n                centroid += vector\n            return centroid / len(cluster)\n\n    def __repr__(self):\n        return '<KMeansClusterer means=%s repeats=%d>' % \\\n                    (self._means, self._repeats)\n\n\ndef demo():\n\n    from nltk.cluster import KMeansClusterer, euclidean_distance\n\n    vectors = [numpy.array(f) for f in [[2, 1], [1, 3], [4, 7], [6, 7]]]\n    means = [[4, 3], [5, 5]]\n\n    clusterer = KMeansClusterer(2, euclidean_distance, initial_means=means)\n    clusters = clusterer.cluster(vectors, True, trace=True)\n\n    print('Clustered:', vectors)\n    print('As:', clusters)\n    print('Means:', clusterer.means())\n    print()\n\n    vectors = [numpy.array(f) for f in [[3, 3], [1, 2], [4, 2], [4, 0], [2, 3], [3, 1]]]\n\n\n    clusterer = KMeansClusterer(2, euclidean_distance, repeats=10)\n    clusters = clusterer.cluster(vectors, True)\n    print('Clustered:', vectors)\n    print('As:', clusters)\n    print('Means:', clusterer.means())\n    print()\n\n    vector = numpy.array([3, 3])\n    print('classify(%s):' % vector, end=' ')\n    print(clusterer.classify(vector))\n    print()\n\nif __name__ == '__main__':\n    demo()\n\n"], "nltk\\cluster\\util": [".py", "from __future__ import print_function, unicode_literals, division\nfrom abc import abstractmethod\n\nimport copy\nfrom sys import stdout\nfrom math import sqrt\n\ntry:\n    import numpy\nexcept ImportError:\n    pass\n\nfrom nltk.cluster.api import ClusterI\nfrom nltk.compat import python_2_unicode_compatible\n\n\nclass VectorSpaceClusterer(ClusterI):\n    def __init__(self, normalise=False, svd_dimensions=None):\n        self._Tt = None\n        self._should_normalise = normalise\n        self._svd_dimensions = svd_dimensions\n\n    def cluster(self, vectors, assign_clusters=False, trace=False):\n        assert len(vectors) > 0\n\n        if self._should_normalise:\n            vectors = list(map(self._normalise, vectors))\n\n        if self._svd_dimensions and self._svd_dimensions < len(vectors[0]):\n            [u, d, vt] = numpy.linalg.svd(numpy.transpose(\n                            numpy.array(vectors)))\n            S = d[:self._svd_dimensions] * \\\n                numpy.identity(self._svd_dimensions, numpy.float64)\n            T = u[:, :self._svd_dimensions]\n            Dt = vt[:self._svd_dimensions, :]\n            vectors = numpy.transpose(numpy.dot(S, Dt))\n            self._Tt = numpy.transpose(T)\n\n        self.cluster_vectorspace(vectors, trace)\n\n        if assign_clusters:\n            return [self.classify(vector) for vector in vectors]\n\n    @abstractmethod\n    def cluster_vectorspace(self, vectors, trace):\n\n    def classify(self, vector):\n        if self._should_normalise:\n            vector = self._normalise(vector)\n        if self._Tt is not None:\n            vector = numpy.dot(self._Tt, vector)\n        cluster = self.classify_vectorspace(vector)\n        return self.cluster_name(cluster)\n\n    @abstractmethod\n    def classify_vectorspace(self, vector):\n\n    def likelihood(self, vector, label):\n        if self._should_normalise:\n            vector = self._normalise(vector)\n        if self._Tt is not None:\n            vector = numpy.dot(self._Tt, vector)\n        return self.likelihood_vectorspace(vector, label)\n\n    def likelihood_vectorspace(self, vector, cluster):\n        predicted = self.classify_vectorspace(vector)\n        return (1.0 if cluster == predicted else 0.0)\n\n    def vector(self, vector):\n        if self._should_normalise:\n            vector = self._normalise(vector)\n        if self._Tt is not None:\n            vector = numpy.dot(self._Tt, vector)\n        return vector\n\n    def _normalise(self, vector):\n        return vector / sqrt(numpy.dot(vector, vector))\n\n\ndef euclidean_distance(u, v):\n    diff = u - v\n    return sqrt(numpy.dot(diff, diff))\n\n\ndef cosine_distance(u, v):\n    return 1 - (numpy.dot(u, v) / (\n                sqrt(numpy.dot(u, u)) * sqrt(numpy.dot(v, v))))\n\n\nclass _DendrogramNode(object):\n\n    def __init__(self, value, *children):\n        self._value = value\n        self._children = children\n\n    def leaves(self, values=True):\n        if self._children:\n            leaves = []\n            for child in self._children:\n                leaves.extend(child.leaves(values))\n            return leaves\n        elif values:\n            return [self._value]\n        else:\n            return [self]\n\n    def groups(self, n):\n        queue = [(self._value, self)]\n\n        while len(queue) < n:\n            priority, node = queue.pop()\n            if not node._children:\n                queue.push((priority, node))\n                break\n            for child in node._children:\n                if child._children:\n                    queue.append((child._value, child))\n                else:\n                    queue.append((0, child))\n            queue.sort()\n\n        groups = []\n        for priority, node in queue:\n            groups.append(node.leaves())\n        return groups\n\n    def __lt__(self, comparator):\n        return cosine_distance(self._value, comparator._value) < 0\n\n\n@python_2_unicode_compatible\nclass Dendrogram(object):\n\n    def __init__(self, items=[]):\n        self._items = [_DendrogramNode(item) for item in items]\n        self._original_items = copy.copy(self._items)\n        self._merge = 1\n\n    def merge(self, *indices):\n        assert len(indices) >= 2\n        node = _DendrogramNode(self._merge, *[self._items[i] for i in indices])\n        self._merge += 1\n        self._items[indices[0]] = node\n        for i in indices[1:]:\n            del self._items[i]\n\n    def groups(self, n):\n        if len(self._items) > 1:\n            root = _DendrogramNode(self._merge, *self._items)\n        else:\n            root = self._items[0]\n        return root.groups(n)\n\n    def show(self, leaf_labels=[]):\n\n        JOIN, HLINK, VLINK = '+', '-', '|'\n\n        if len(self._items) > 1:\n            root = _DendrogramNode(self._merge, *self._items)\n        else:\n            root = self._items[0]\n        leaves = self._original_items\n\n        if leaf_labels:\n            last_row = leaf_labels\n        else:\n            last_row = [\"%s\" % leaf._value for leaf in leaves]\n\n        width = max(map(len, last_row)) + 1\n        lhalf = width // 2\n        rhalf = int(width - lhalf - 1)\n\n        def format(centre, left=' ', right=' '):\n            return '%s%s%s' % (lhalf*left, centre, right*rhalf)\n\n        def display(str):\n            stdout.write(str)\n\n        queue = [(root._value, root)]\n        verticals = [format(' ') for leaf in leaves]\n        while queue:\n            priority, node = queue.pop()\n            child_left_leaf = list(map(\n                                lambda c: c.leaves(False)[0], node._children))\n            indices = list(map(leaves.index, child_left_leaf))\n            if child_left_leaf:\n                min_idx = min(indices)\n                max_idx = max(indices)\n            for i in range(len(leaves)):\n                if leaves[i] in child_left_leaf:\n                    if i == min_idx:\n                        display(format(JOIN, ' ', HLINK))\n                    elif i == max_idx:\n                        display(format(JOIN, HLINK, ' '))\n                    else:\n                        display(format(JOIN, HLINK, HLINK))\n                    verticals[i] = format(VLINK)\n                elif min_idx <= i <= max_idx:\n                    display(format(HLINK, HLINK, HLINK))\n                else:\n                    display(verticals[i])\n            display('\\n')\n            for child in node._children:\n                if child._children:\n                    queue.append((child._value, child))\n            queue.sort()\n\n            for vertical in verticals:\n                display(vertical)\n            display('\\n')\n\n        display(''.join(item.center(width) for item in last_row))\n        display('\\n')\n\n    def __repr__(self):\n        if len(self._items) > 1:\n            root = _DendrogramNode(self._merge, *self._items)\n        else:\n            root = self._items[0]\n        leaves = root.leaves(False)\n        return '<Dendrogram with %d leaves>' % len(leaves)\n"], "nltk\\cluster\\__init__": [".py", "\n\nfrom nltk.cluster.util import (VectorSpaceClusterer, Dendrogram,\n                               euclidean_distance, cosine_distance)\nfrom nltk.cluster.kmeans import KMeansClusterer\nfrom nltk.cluster.gaac import GAAClusterer\nfrom nltk.cluster.em import EMClusterer\n", 1], "nltk\\collections": [".py", "from __future__ import print_function, absolute_import\n\nimport locale\nimport re\nimport types\nimport textwrap\nimport pydoc\nimport bisect\nimport os\nfrom itertools import islice, chain, combinations\nfrom functools import total_ordering\nfrom collections import defaultdict, deque, Counter\n\nfrom six import text_type\n\nfrom nltk.internals import slice_bounds, raise_unorderable_types\nfrom nltk.compat import python_2_unicode_compatible\n\n\n\nclass OrderedDict(dict):\n    def __init__(self, data=None, **kwargs):\n        self._keys = self.keys(data, kwargs.get('keys'))\n        self._default_factory = kwargs.get('default_factory')\n        if data is None:\n            dict.__init__(self)\n        else:\n            dict.__init__(self, data)\n\n    def __delitem__(self, key):\n        dict.__delitem__(self, key)\n        self._keys.remove(key)\n\n    def __getitem__(self, key):\n        try:\n            return dict.__getitem__(self, key)\n        except KeyError:\n            return self.__missing__(key)\n\n    def __iter__(self):\n        return (key for key in self.keys())\n\n    def __missing__(self, key):\n        if not self._default_factory and key not in self._keys:\n            raise KeyError()\n        return self._default_factory()\n\n    def __setitem__(self, key, item):\n        dict.__setitem__(self, key, item)\n        if key not in self._keys:\n            self._keys.append(key)\n\n    def clear(self):\n        dict.clear(self)\n        self._keys.clear()\n\n    def copy(self):\n        d = dict.copy(self)\n        d._keys = self._keys\n        return d\n\n    def items(self):\n        return zip(self.keys(), self.values())\n\n    def keys(self, data=None, keys=None):\n        if data:\n            if keys:\n                assert isinstance(keys, list)\n                assert len(data) == len(keys)\n                return keys\n            else:\n                assert isinstance(data, dict) or \\\n                       isinstance(data, OrderedDict) or \\\n                       isinstance(data, list)\n                if isinstance(data, dict) or isinstance(data, OrderedDict):\n                    return data.keys()\n                elif isinstance(data, list):\n                    return [key for (key, value) in data]\n        elif '_keys' in self.__dict__:\n            return self._keys\n        else:\n            return []\n\n    def popitem(self):\n        if not self._keys:\n            raise KeyError()\n\n        key = self._keys.pop()\n        value = self[key]\n        del self[key]\n        return (key, value)\n\n    def setdefault(self, key, failobj=None):\n        dict.setdefault(self, key, failobj)\n        if key not in self._keys:\n            self._keys.append(key)\n\n    def update(self, data):\n        dict.update(self, data)\n        for key in self.keys(data):\n            if key not in self._keys:\n                self._keys.append(key)\n\n    def values(self):\n        return map(self.get, self._keys)\n\n\n@total_ordering\n@python_2_unicode_compatible\nclass AbstractLazySequence(object):\n    def __len__(self):\n        raise NotImplementedError('should be implemented by subclass')\n\n    def iterate_from(self, start):\n        raise NotImplementedError('should be implemented by subclass')\n\n    def __getitem__(self, i):\n        if isinstance(i, slice):\n            start, stop = slice_bounds(self, i)\n            return LazySubsequence(self, start, stop)\n        else:\n            if i < 0: i += len(self)\n            if i < 0: raise IndexError('index out of range')\n            try:\n                return next(self.iterate_from(i))\n            except StopIteration:\n                raise IndexError('index out of range')\n\n    def __iter__(self):\n        Return a string representation for this corpus view that is\n        similar to a list's representation; but if it would be more\n        than 60 characters long, it is truncated.\n        \"\"\"\n        pieces = []\n        length = 5\n        for elt in self:\n            pieces.append(repr(elt))\n            length += len(pieces[-1]) + 2\n            if length > self._MAX_REPR_SIZE and len(pieces) > 2:\n                return '[%s, ...]' % text_type(', ').join(pieces[:-1])\n        return '[%s]' % text_type(', ').join(pieces)\n\n    def __eq__(self, other):\n        return (type(self) == type(other) and list(self) == list(other))\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __lt__(self, other):\n        if type(other) != type(self):\n            raise_unorderable_types(\"<\", self, other)\n        return list(self) < list(other)\n\n    def __hash__(self):\n        \"\"\"\n        :raise ValueError: Corpus view objects are unhashable.\n        \"\"\"\n        raise ValueError('%s objects are unhashable' %\n                         self.__class__.__name__)\n\n\nclass LazySubsequence(AbstractLazySequence):\n    \"\"\"\n    A subsequence produced by slicing a lazy sequence.  This slice\n    keeps a reference to its source sequence, and generates its values\n    by looking them up in the source sequence.\n    \"\"\"\n\n    MIN_SIZE = 100\n    \"\"\"\n    The minimum size for which lazy slices should be created.  If\n    ``LazySubsequence()`` is called with a subsequence that is\n    shorter than ``MIN_SIZE``, then a tuple will be returned instead.\n    \"\"\"\n\n    def __new__(cls, source, start, stop):\n        \"\"\"\n        Construct a new slice from a given underlying sequence.  The\n        ``start`` and ``stop`` indices should be absolute indices --\n        i.e., they should not be negative (for indexing from the back\n        of a list) or greater than the length of ``source``.\n        \"\"\"\n        if stop-start < cls.MIN_SIZE:\n            return list(islice(source.iterate_from(start), stop-start))\n        else:\n            return object.__new__(cls)\n\n    def __init__(self, source, start, stop):\n        self._source = source\n        self._start = start\n        self._stop = stop\n\n    def __len__(self):\n        return self._stop - self._start\n\n    def iterate_from(self, start):\n        return islice(self._source.iterate_from(start+self._start),\n                      max(0, len(self)-start))\n\n\nclass LazyConcatenation(AbstractLazySequence):\n    \"\"\"\n    A lazy sequence formed by concatenating a list of lists.  This\n    underlying list of lists may itself be lazy.  ``LazyConcatenation``\n    maintains an index that it uses to keep track of the relationship\n    between offsets in the concatenated lists and offsets in the\n    sublists.\n    \"\"\"\n    def __init__(self, list_of_lists):\n        self._list = list_of_lists\n        self._offsets = [0]\n\n    def __len__(self):\n        if len(self._offsets) <= len(self._list):\n            for tok in self.iterate_from(self._offsets[-1]): pass\n        return self._offsets[-1]\n\n    def iterate_from(self, start_index):\n        if start_index < self._offsets[-1]:\n            sublist_index = bisect.bisect_right(self._offsets, start_index)-1\n        else:\n            sublist_index = len(self._offsets)-1\n\n        index = self._offsets[sublist_index]\n\n        if isinstance(self._list, AbstractLazySequence):\n            sublist_iter = self._list.iterate_from(sublist_index)\n        else:\n            sublist_iter = islice(self._list, sublist_index, None)\n\n        for sublist in sublist_iter:\n            if sublist_index == (len(self._offsets)-1):\n                assert index+len(sublist) >= self._offsets[-1], (\n                        'offests not monotonic increasing!')\n                self._offsets.append(index+len(sublist))\n            else:\n                assert self._offsets[sublist_index+1] == index+len(sublist), (\n                        'inconsistent list value (num elts)')\n\n            for value in sublist[max(0, start_index-index):]:\n                yield value\n\n            index += len(sublist)\n            sublist_index += 1\n\n\nclass LazyMap(AbstractLazySequence):\n    \"\"\"\n    A lazy sequence whose elements are formed by applying a given\n    function to each element in one or more underlying lists.  The\n    function is applied lazily -- i.e., when you read a value from the\n    list, ``LazyMap`` will calculate that value by applying its\n    function to the underlying lists' value(s).  ``LazyMap`` is\n    essentially a lazy version of the Python primitive function\n    ``map``.  In particular, the following two expressions are\n    equivalent:\n\n        >>> from nltk.collections import LazyMap\n        >>> function = str\n        >>> sequence = [1,2,3]\n        >>> map(function, sequence) # doctest: +SKIP\n        ['1', '2', '3']\n        >>> list(LazyMap(function, sequence))\n        ['1', '2', '3']\n\n    Like the Python ``map`` primitive, if the source lists do not have\n    equal size, then the value None will be supplied for the\n    'missing' elements.\n\n    Lazy maps can be useful for conserving memory, in cases where\n    individual values take up a lot of space.  This is especially true\n    if the underlying list's values are constructed lazily, as is the\n    case with many corpus readers.\n\n    A typical example of a use case for this class is performing\n    feature detection on the tokens in a corpus.  Since featuresets\n    are encoded as dictionaries, which can take up a lot of memory,\n    using a ``LazyMap`` can significantly reduce memory usage when\n    training and running classifiers.\n    \"\"\"\n    def __init__(self, function, *lists, **config):\n        \"\"\"\n        :param function: The function that should be applied to\n            elements of ``lists``.  It should take as many arguments\n            as there are ``lists``.\n        :param lists: The underlying lists.\n        :param cache_size: Determines the size of the cache used\n            by this lazy map.  (default=5)\n        \"\"\"\n        if not lists:\n            raise TypeError('LazyMap requires at least two args')\n\n        self._lists = lists\n        self._func = function\n        self._cache_size = config.get('cache_size', 5)\n        self._cache = ({} if self._cache_size > 0 else None)\n\n        self._all_lazy = sum(isinstance(lst, AbstractLazySequence)\n                             for lst in lists) == len(lists)\n\n    def iterate_from(self, index):\n        if len(self._lists) == 1 and self._all_lazy:\n            for value in self._lists[0].iterate_from(index):\n                yield self._func(value)\n            return\n\n        elif len(self._lists) == 1:\n            while True:\n                try: yield self._func(self._lists[0][index])\n                except IndexError: return\n                index += 1\n\n        elif self._all_lazy:\n            iterators = [lst.iterate_from(index) for lst in self._lists]\n            while True:\n                elements = []\n                for iterator in iterators:\n                    try: elements.append(next(iterator))\n                    except: elements.append(None)\n                if elements == [None] * len(self._lists):\n                    return\n                yield self._func(*elements)\n                index += 1\n\n        else:\n            while True:\n                try: elements = [lst[index] for lst in self._lists]\n                except IndexError:\n                    elements = [None] * len(self._lists)\n                    for i, lst in enumerate(self._lists):\n                        try: elements[i] = lst[index]\n                        except IndexError: pass\n                    if elements == [None] * len(self._lists):\n                        return\n                yield self._func(*elements)\n                index += 1\n\n    def __getitem__(self, index):\n        if isinstance(index, slice):\n            sliced_lists = [lst[index] for lst in self._lists]\n            return LazyMap(self._func, *sliced_lists)\n        else:\n            if index < 0: index += len(self)\n            if index < 0: raise IndexError('index out of range')\n            if self._cache is not None and index in self._cache:\n                return self._cache[index]\n            try: val = next(self.iterate_from(index))\n            except StopIteration:\n                raise IndexError('index out of range')\n            if self._cache is not None:\n                if len(self._cache) > self._cache_size:\n                    self._cache.popitem() # discard random entry\n                self._cache[index] = val\n            return val\n\n    def __len__(self):\n        return max(len(lst) for lst in self._lists)\n\n\nclass LazyZip(LazyMap):\n    \"\"\"\n    A lazy sequence whose elements are tuples, each containing the i-th\n    element from each of the argument sequences.  The returned list is\n    truncated in length to the length of the shortest argument sequence. The\n    tuples are constructed lazily -- i.e., when you read a value from the\n    list, ``LazyZip`` will calculate that value by forming a tuple from\n    the i-th element of each of the argument sequences.\n\n    ``LazyZip`` is essentially a lazy version of the Python primitive function\n    ``zip``.  In particular, an evaluated LazyZip is equivalent to a zip:\n\n        >>> from nltk.collections import LazyZip\n        >>> sequence1, sequence2 = [1, 2, 3], ['a', 'b', 'c']\n        >>> zip(sequence1, sequence2) # doctest: +SKIP\n        [(1, 'a'), (2, 'b'), (3, 'c')]\n        >>> list(LazyZip(sequence1, sequence2))\n        [(1, 'a'), (2, 'b'), (3, 'c')]\n        >>> sequences = [sequence1, sequence2, [6,7,8,9]]\n        >>> list(zip(*sequences)) == list(LazyZip(*sequences))\n        True\n\n    Lazy zips can be useful for conserving memory in cases where the argument\n    sequences are particularly long.\n\n    A typical example of a use case for this class is combining long sequences\n    of gold standard and predicted values in a classification or tagging task\n    in order to calculate accuracy.  By constructing tuples lazily and\n    avoiding the creation of an additional long sequence, memory usage can be\n    significantly reduced.\n    \"\"\"\n    def __init__(self, *lists):\n        \"\"\"\n        :param lists: the underlying lists\n        :type lists: list(list)\n        \"\"\"\n        LazyMap.__init__(self, lambda *elts: elts, *lists)\n\n    def iterate_from(self, index):\n        iterator = LazyMap.iterate_from(self, index)\n        while index < len(self):\n            yield next(iterator)\n            index += 1\n        return\n\n    def __len__(self):\n        return min(len(lst) for lst in self._lists)\n\n\nclass LazyEnumerate(LazyZip):\n    \"\"\"\n    A lazy sequence whose elements are tuples, each ontaining a count (from\n    zero) and a value yielded by underlying sequence.  ``LazyEnumerate`` is\n    useful for obtaining an indexed list. The tuples are constructed lazily\n    -- i.e., when you read a value from the list, ``LazyEnumerate`` will\n    calculate that value by forming a tuple from the count of the i-th\n    element and the i-th element of the underlying sequence.\n\n    ``LazyEnumerate`` is essentially a lazy version of the Python primitive\n    function ``enumerate``.  In particular, the following two expressions are\n    equivalent:\n\n        >>> from nltk.collections import LazyEnumerate\n        >>> sequence = ['first', 'second', 'third']\n        >>> list(enumerate(sequence))\n        [(0, 'first'), (1, 'second'), (2, 'third')]\n        >>> list(LazyEnumerate(sequence))\n        [(0, 'first'), (1, 'second'), (2, 'third')]\n\n    Lazy enumerations can be useful for conserving memory in cases where the\n    argument sequences are particularly long.\n\n    A typical example of a use case for this class is obtaining an indexed\n    list for a long sequence of values.  By constructing tuples lazily and\n    avoiding the creation of an additional long sequence, memory usage can be\n    significantly reduced.\n    \"\"\"\n\n    def __init__(self, lst):\n        \"\"\"\n        :param lst: the underlying list\n        :type lst: list\n        \"\"\"\n        LazyZip.__init__(self, range(len(lst)), lst)\n\nclass LazyIteratorList(AbstractLazySequence):\n    \"\"\"\n    Wraps an iterator, loading its elements on demand\n    and making them subscriptable.\n    __repr__ displays only the first few elements.\n    \"\"\"\n    def __init__(self, it, known_len=None):\n        self._it = it\n        self._len = known_len\n        self._cache = []\n\n    def __len__(self):\n        if self._len:\n            return self._len\n        for x in self.iterate_from(len(self._cache)):\n            pass\n        self._len = len(self._cache)\n        return self._len\n\n    def iterate_from(self, start):\n        while len(self._cache)<start:\n            v = next(self._it)\n            self._cache.append(v)\n        i = start\n        while i<len(self._cache):\n            yield self._cache[i]\n            i += 1\n        while True:\n            v = next(self._it)\n            self._cache.append(v)\n            yield v\n            i += 1\n\n    def __add__(self, other):\n        return type(self)(chain(self, other))\n\n    def __radd__(self, other):\n        return type(self)(chain(other, self))\n\nclass Trie(dict):\n    LEAF = True\n\n    def __init__(self, strings=None):\n        \"\"\"Builds a Trie object, which is built around a ``dict``\n\n        If ``strings`` is provided, it will add the ``strings``, which\n        consist of a ``list`` of ``strings``, to the Trie.\n        Otherwise, it'll construct an empty Trie.\n\n        :param strings: List of strings to insert into the trie\n            (Default is ``None``)\n        :type strings: list(str)\n\n        \"\"\"\n        super(Trie, self).__init__()\n        if strings:\n            for string in strings:\n                self.insert(string)\n\n    def insert(self, string):\n        \"\"\"Inserts ``string`` into the Trie\n\n        :param string: String to insert into the trie\n        :type string: str\n\n        :Example:\n\n        >>> from nltk.collections import Trie\n        >>> trie = Trie([\"abc\", \"def\"])\n        >>> expected = {'a': {'b': {'c': {True: None}}}, \\\n                        'd': {'e': {'f': {True: None}}}}\n        >>> trie == expected\n        True\n\n        \"\"\"\n        if len(string):\n            self[string[0]].insert(string[1:])\n        else:\n            self[Trie.LEAF] = None\n\n    def __missing__(self, key):\n        self[key] = Trie()\n        return self[key]\n\n"], "nltk\\collocations": [".py", "from __future__ import print_function\n\n\nimport itertools as _itertools\nfrom six import iteritems\n\nfrom nltk.probability import FreqDist\nfrom nltk.util import ngrams\nfrom nltk.metrics import ContingencyMeasures, BigramAssocMeasures, TrigramAssocMeasures\nfrom nltk.metrics.spearman import ranks_from_scores, spearman_correlation\n\n\nclass AbstractCollocationFinder(object):\n\n    def __init__(self, word_fd, ngram_fd):\n        self.word_fd = word_fd\n        self.N = word_fd.N()\n        self.ngram_fd = ngram_fd\n\n    @classmethod\n    def _build_new_documents(cls, documents, window_size, pad_left=False, pad_right=False, pad_symbol=None):\n        '''\n        Pad the document with the place holder according to the window_size\n        '''\n        padding = (pad_symbol,) * (window_size - 1)\n        if pad_right:\n            return _itertools.chain.from_iterable(_itertools.chain(doc, padding) for doc in documents)\n        if pad_left:\n            return _itertools.chain.from_iterable(_itertools.chain(padding, doc) for doc in documents)\n\n    @classmethod\n    def from_documents(cls, documents):\n        return cls.from_words(cls._build_new_documents(documents, cls.default_ws, pad_right=True))\n\n    @staticmethod\n    def _ngram_freqdist(words, n):\n        return FreqDist(tuple(words[i:i + n]) for i in range(len(words) - 1))\n\n    def _apply_filter(self, fn=lambda ngram, freq: False):\n        tmp_ngram = FreqDist()\n        for ngram, freq in iteritems(self.ngram_fd):\n            if not fn(ngram, freq):\n                tmp_ngram[ngram] = freq\n        self.ngram_fd = tmp_ngram\n\n    def apply_freq_filter(self, min_freq):\n        self._apply_filter(lambda ng, freq: freq < min_freq)\n\n    def apply_ngram_filter(self, fn):\n        self._apply_filter(lambda ng, f: fn(*ng))\n\n    def apply_word_filter(self, fn):\n        self._apply_filter(lambda ng, f: any(fn(w) for w in ng))\n\n    def _score_ngrams(self, score_fn):\n        for tup in self.ngram_fd:\n            score = self.score_ngram(score_fn, *tup)\n            if score is not None:\n                yield tup, score\n\n    def score_ngrams(self, score_fn):\n        return sorted(self._score_ngrams(score_fn), key=lambda t: (-t[1], t[0]))\n\n    def nbest(self, score_fn, n):\n        return [p for p, s in self.score_ngrams(score_fn)[:n]]\n\n    def above_score(self, score_fn, min_score):\n        for ngram, score in self.score_ngrams(score_fn):\n            if score > min_score:\n                yield ngram\n            else:\n                break\n\n\nclass BigramCollocationFinder(AbstractCollocationFinder):\n    default_ws = 2\n\n    def __init__(self, word_fd, bigram_fd, window_size=2):\n        AbstractCollocationFinder.__init__(self, word_fd, bigram_fd)\n        self.window_size = window_size\n\n    @classmethod\n    def from_words(cls, words, window_size=2):\n        wfd = FreqDist()\n        bfd = FreqDist()\n\n        if window_size < 2:\n            raise ValueError(\"Specify window_size at least 2\")\n\n        for window in ngrams(words, window_size, pad_right=True):\n            w1 = window[0]\n            if w1 is None:\n                continue\n            wfd[w1] += 1\n            for w2 in window[1:]:\n                if w2 is not None:\n                    bfd[(w1, w2)] += 1\n        return cls(wfd, bfd, window_size=window_size)\n\n    def score_ngram(self, score_fn, w1, w2):\n        n_all = self.N\n        n_ii = self.ngram_fd[(w1, w2)] / (self.window_size - 1.0)\n        if not n_ii:\n            return\n        n_ix = self.word_fd[w1]\n        n_xi = self.word_fd[w2]\n        return score_fn(n_ii, (n_ix, n_xi), n_all)\n\n\nclass TrigramCollocationFinder(AbstractCollocationFinder):\n    default_ws = 3\n\n    def __init__(self, word_fd, bigram_fd, wildcard_fd, trigram_fd):\n        AbstractCollocationFinder.__init__(self, word_fd, trigram_fd)\n        self.wildcard_fd = wildcard_fd\n        self.bigram_fd = bigram_fd\n\n    @classmethod\n    def from_words(cls, words, window_size=3):\n        if window_size < 3:\n            raise ValueError(\"Specify window_size at least 3\")\n\n        wfd = FreqDist()\n        wildfd = FreqDist()\n        bfd = FreqDist()\n        tfd = FreqDist()\n        for window in ngrams(words, window_size, pad_right=True):\n            w1 = window[0]\n            if w1 is None:\n                continue\n            for w2, w3 in _itertools.combinations(window[1:], 2):\n                wfd[w1] += 1\n                if w2 is None:\n                    continue\n                bfd[(w1, w2)] += 1\n                if w3 is None:\n                    continue\n                wildfd[(w1, w3)] += 1\n                tfd[(w1, w2, w3)] += 1\n        return cls(wfd, bfd, wildfd, tfd)\n\n    def bigram_finder(self):\n        return BigramCollocationFinder(self.word_fd, self.bigram_fd)\n\n    def score_ngram(self, score_fn, w1, w2, w3):\n        n_all = self.N\n        n_iii = self.ngram_fd[(w1, w2, w3)]\n        if not n_iii:\n            return\n        n_iix = self.bigram_fd[(w1, w2)]\n        n_ixi = self.wildcard_fd[(w1, w3)]\n        n_xii = self.bigram_fd[(w2, w3)]\n        n_ixx = self.word_fd[w1]\n        n_xix = self.word_fd[w2]\n        n_xxi = self.word_fd[w3]\n        return score_fn(n_iii,\n                        (n_iix, n_ixi, n_xii),\n                        (n_ixx, n_xix, n_xxi),\n                        n_all)\n\n\nclass QuadgramCollocationFinder(AbstractCollocationFinder):\n    default_ws = 4\n\n    def __init__(self, word_fd, quadgram_fd, ii, iii, ixi, ixxi, iixi, ixii):\n        AbstractCollocationFinder.__init__(self, word_fd, quadgram_fd)\n        self.iii = iii\n        self.ii = ii\n        self.ixi = ixi\n        self.ixxi = ixxi\n        self.iixi = iixi\n        self.ixii = ixii\n\n    @classmethod\n    def from_words(cls, words, window_size=4):\n        if window_size < 4:\n            raise ValueError(\"Specify window_size at least 4\")\n        ixxx = FreqDist()\n        iiii = FreqDist()\n        ii = FreqDist()\n        iii = FreqDist()\n        ixi = FreqDist()\n        ixxi = FreqDist()\n        iixi = FreqDist()\n        ixii = FreqDist()\n\n        for window in ngrams(words, window_size, pad_right=True):\n            w1 = window[0]\n            if w1 is None:\n                continue\n            for w2, w3, w4 in _itertools.combinations(window[1:], 3):\n                ixxx[w1] += 1\n                if w2 is None:\n                    continue\n                ii[(w1, w2)] += 1\n                if w3 is None:\n                    continue\n                iii[(w1, w2, w3)] += 1\n                ixi[(w1, w3)] += 1\n                if w4 is None:\n                    continue\n                iiii[(w1, w2, w3, w4)] += 1\n                ixxi[(w1, w4)] += 1\n                ixii[(w1, w3, w4)] += 1\n                iixi[(w1, w2, w4)] += 1\n\n        return cls(ixxx, iiii, ii, iii, ixi, ixxi, iixi, ixii)\n\n    def score_ngram(self, score_fn, w1, w2, w3, w4):\n        n_all = self.N\n        n_iiii = self.ngram_fd[(w1, w2, w3, w4)]\n        if not n_iiii:\n            return\n        n_iiix = self.iii[(w1, w2, w3)]\n        n_xiii = self.iii[(w2, w3, w4)]\n        n_iixi = self.iixi[(w1, w2, w4)]\n        n_ixii = self.ixii[(w1, w3, w4)]\n\n        n_iixx = self.ii[(w1, w2)]\n        n_xxii = self.ii[(w3, w4)]\n        n_xiix = self.ii[(w2, w3)]\n        n_ixix = self.ixi[(w1, w3)]\n        n_ixxi = self.ixxi[(w1, w4)]\n        n_xixi = self.ixi[(w2, w4)]\n\n        n_ixxx = self.word_fd[w1]\n        n_xixx = self.word_fd[w2]\n        n_xxix = self.word_fd[w3]\n        n_xxxi = self.word_fd[w4]\n        return score_fn(n_iiii,\n                        (n_iiix, n_iixi, n_ixii, n_xiii),\n                        (n_iixx, n_ixix, n_ixxi, n_xixi, n_xxii, n_xiix),\n                        (n_ixxx, n_xixx, n_xxix, n_xxxi),\n                        n_all)\n\n\ndef demo(scorer=None, compare_scorer=None):\n    from nltk.metrics import BigramAssocMeasures, spearman_correlation, ranks_from_scores\n\n    if scorer is None:\n        scorer = BigramAssocMeasures.likelihood_ratio\n    if compare_scorer is None:\n        compare_scorer = BigramAssocMeasures.raw_freq\n\n    from nltk.corpus import stopwords, webtext\n\n    ignored_words = stopwords.words('english')\n    word_filter = lambda w: len(w) < 3 or w.lower() in ignored_words\n\n    for file in webtext.fileids():\n        words = [word.lower()\n                 for word in webtext.words(file)]\n\n        cf = BigramCollocationFinder.from_words(words)\n        cf.apply_freq_filter(3)\n        cf.apply_word_filter(word_filter)\n\n        corr = spearman_correlation(ranks_from_scores(cf.score_ngrams(scorer)),\n                                    ranks_from_scores(cf.score_ngrams(compare_scorer)))\n        print(file)\n        print('\\t', [' '.join(tup) for tup in cf.nbest(scorer, 15)])\n        print('\\t Correlation to %s: %0.4f' % (compare_scorer.__name__, corr))\n\n\nif __name__ == '__main__':\n    import sys\n    from nltk.metrics import BigramAssocMeasures\n\n    try:\n        scorer = eval('BigramAssocMeasures.' + sys.argv[1])\n    except IndexError:\n        scorer = None\n    try:\n        compare_scorer = eval('BigramAssocMeasures.' + sys.argv[2])\n    except IndexError:\n        compare_scorer = None\n\n    demo(scorer, compare_scorer)\n\n__all__ = ['BigramCollocationFinder',\n           'TrigramCollocationFinder', 'QuadgramCollocationFinder']\n"], "nltk\\compat": [".py", "\nfrom __future__ import absolute_import, print_function\nimport os\nimport sys\nfrom functools import update_wrapper, wraps\nimport fractions\nimport unicodedata\n\nfrom six import string_types, text_type\n\n\nPY3 = sys.version_info[0] == 3\n\nif PY3:\n    def get_im_class(meth):\n        return meth.__self__.__class__\n\n    import io\n    StringIO = io.StringIO\n    BytesIO = io.BytesIO\n\n    from datetime import timezone\n    UTC = timezone.utc\n\n    from tempfile import TemporaryDirectory\n\nelse:\n    def get_im_class(meth):\n        return meth.im_class\n\n    try:\n        from cStringIO import StringIO\n    except ImportError:\n        from StringIO import StringIO\n    BytesIO = StringIO\n\n    from datetime import tzinfo, timedelta\n\n    ZERO = timedelta(0)\n    HOUR = timedelta(hours=1)\n\n    class UTC(tzinfo):\n\n        def utcoffset(self, dt):\n            return ZERO\n\n        def tzname(self, dt):\n            return \"UTC\"\n\n        def dst(self, dt):\n            return ZERO\n\n    UTC = UTC()\n\n    import csv\n    import codecs\n    import cStringIO\n\n    class UnicodeWriter:\n\n        def __init__(self, f, dialect=csv.excel, encoding=\"utf-8\",\n                     errors='replace', **kwds):\n            self.queue = cStringIO.StringIO()\n            self.writer = csv.writer(self.queue, dialect=dialect, **kwds)\n            self.stream = f\n            encoder_cls = codecs.getincrementalencoder(encoding)\n            self.encoder = encoder_cls(errors=errors)\n\n        def encode(self, data):\n            if isinstance(data, string_types):\n                return data.encode(\"utf-8\")\n            else:\n                return data\n\n        def writerow(self, row):\n            self.writer.writerow([self.encode(s) for s in row])\n            data = self.queue.getvalue()\n            data = data.decode(\"utf-8\")\n            data = self.encoder.encode(data, 'replace')\n            self.stream.write(data)\n            self.queue.truncate(0)\n\n    import warnings as _warnings\n    import os as _os\n    from tempfile import mkdtemp\n\n    class TemporaryDirectory(object):\n\n        def __init__(self, suffix=\"\", prefix=\"tmp\", dir=None):\n            self._closed = False\n            self.name = None  # Handle mkdtemp raising an exception\n            self.name = mkdtemp(suffix, prefix, dir)\n\n        def __repr__(self):\n            return \"<{} {!r}>\".format(self.__class__.__name__, self.name)\n\n        def __enter__(self):\n            return self.name\n\n        def cleanup(self, _warn=False):\n            if self.name and not self._closed:\n                try:\n                    self._rmtree(self.name)\n                except (TypeError, AttributeError) as ex:\n                    if \"None\" not in str(ex):\n                        raise\n                    print(\"ERROR: {!r} while cleaning up {!r}\".format(ex,\n                                                                      self),\n                          file=sys.stderr)\n                    return\n                self._closed = True\n                if _warn:\n                    self._warn(\"Implicitly cleaning up {!r}\".format(self),\n                               Warning)\n\n        def __exit__(self, exc, value, tb):\n            self.cleanup()\n\n        def __del__(self):\n            self.cleanup(_warn=True)\n\n        _listdir = staticmethod(_os.listdir)\n        _path_join = staticmethod(_os.path.join)\n        _isdir = staticmethod(_os.path.isdir)\n        _islink = staticmethod(_os.path.islink)\n        _remove = staticmethod(_os.remove)\n        _rmdir = staticmethod(_os.rmdir)\n        _warn = _warnings.warn\n\n        def _rmtree(self, path):\n            for name in self._listdir(path):\n                fullname = self._path_join(path, name)\n                try:\n                    isdir = (self._isdir(fullname) and not\n                             self._islink(fullname))\n                except OSError:\n                    isdir = False\n                if isdir:\n                    self._rmtree(fullname)\n                else:\n                    try:\n                        self._remove(fullname)\n                    except OSError:\n                        pass\n            try:\n                self._rmdir(path)\n            except OSError:\n                pass\n\n\nDATA_UPDATES = [(\"chunkers\", \"maxent_ne_chunker\"),\n                (\"help\", \"tagsets\"),\n                (\"taggers\", \"maxent_treebank_pos_tagger\"),\n                (\"tokenizers\", \"punkt\")]\n\n_PY3_DATA_UPDATES = [os.path.join(*path_list) for path_list in DATA_UPDATES]\n\n\ndef add_py3_data(path):\n    if PY3:\n        for item in _PY3_DATA_UPDATES:\n            if item in str(path) and \"/PY3\" not in str(path):\n                pos = path.index(item) + len(item)\n                if path[pos:pos + 4] == \".zip\":\n                    pos += 4\n                path = path[:pos] + \"/PY3\" + path[pos:]\n                break\n    return path\n\n\ndef py3_data(init_func):\n    def _decorator(*args, **kwargs):\n        args = (args[0], add_py3_data(args[1])) + args[2:]\n        return init_func(*args, **kwargs)\n    return wraps(init_func)(_decorator)\n\n\ndef remove_accents(text):\n\n    if isinstance(text, bytes):\n        text = text.decode('ascii')\n\n    category = unicodedata.category  # this gives a small (~10%) speedup\n    return ''.join(\n        c for c in unicodedata.normalize('NFKD', text) if category(c) != 'Mn'\n    )\n\n\ntry:\n    from unidecode import unidecode as transliterate\nexcept ImportError:\n    try:\n        from text_unidecode import unidecode as transliterate\n    except ImportError:\n        transliterate = remove_accents\n\n\ndef python_2_unicode_compatible(klass):\n\n    if not issubclass(klass, object):\n        raise ValueError(\"This decorator doesn't work for old-style classes\")\n\n\n\n    if not _was_fixed(klass.__str__):\n        klass.__unicode__ = klass.__str__\n        if not PY3:\n            klass.__str__ = _7bit(_transliterated(klass.__unicode__))\n\n    if not _was_fixed(klass.__repr__):\n        klass.unicode_repr = klass.__repr__\n        if not PY3:\n            klass.__repr__ = _7bit(klass.unicode_repr)\n\n    return klass\n\n\ndef unicode_repr(obj):\n    if PY3:\n        return repr(obj)\n\n    if hasattr(obj, 'unicode_repr'):\n        return obj.unicode_repr()\n\n    if isinstance(obj, text_type):\n        return repr(obj)[1:]  # strip \"u\" letter from output\n\n    return repr(obj)\n\n\ndef _transliterated(method):\n    def wrapper(self):\n        return transliterate(method(self))\n\n    update_wrapper(wrapper, method, [\"__name__\", \"__doc__\"])\n    if hasattr(method, \"_nltk_compat_7bit\"):\n        wrapper._nltk_compat_7bit = method._nltk_compat_7bit\n\n    wrapper._nltk_compat_transliterated = True\n    return wrapper\n\n\ndef _7bit(method):\n    def wrapper(self):\n        return method(self).encode('ascii', 'backslashreplace')\n\n    update_wrapper(wrapper, method, [\"__name__\", \"__doc__\"])\n\n    if hasattr(method, \"_nltk_compat_transliterated\"):\n        wrapper._nltk_compat_transliterated = (\n            method._nltk_compat_transliterated\n        )\n\n    wrapper._nltk_compat_7bit = True\n    return wrapper\n\n\ndef _was_fixed(method):\n    return (getattr(method, \"_nltk_compat_7bit\", False) or\n            getattr(method, \"_nltk_compat_transliterated\", False))\n\n\nclass Fraction(fractions.Fraction):\n    def __new__(cls, numerator=0, denominator=None, _normalize=True):\n        cls = super(Fraction, cls).__new__(cls, numerator, denominator)\n        if not _normalize and type(numerator) == int and denominator:\n            cls._numerator = numerator\n            cls._denominator = denominator\n        return cls\n"], "nltk\\corpus\\europarl_raw": [".py", "\nimport re\nfrom nltk.corpus.util import LazyCorpusLoader\nfrom nltk.corpus.reader import *\n\ndanish = LazyCorpusLoader(\n    'europarl_raw/danish', EuroparlCorpusReader, r'ep-.*\\.da', encoding='utf-8')\n\ndutch = LazyCorpusLoader(\n    'europarl_raw/dutch', EuroparlCorpusReader, r'ep-.*\\.nl', encoding='utf-8')\n\nenglish = LazyCorpusLoader(\n    'europarl_raw/english', EuroparlCorpusReader, r'ep-.*\\.en', encoding='utf-8')\n\nfinnish = LazyCorpusLoader(\n    'europarl_raw/finnish', EuroparlCorpusReader, r'ep-.*\\.fi', encoding='utf-8')\n\nfrench = LazyCorpusLoader(\n    'europarl_raw/french', EuroparlCorpusReader, r'ep-.*\\.fr', encoding='utf-8')\n\ngerman = LazyCorpusLoader(\n    'europarl_raw/german', EuroparlCorpusReader, r'ep-.*\\.de', encoding='utf-8')\n\ngreek = LazyCorpusLoader(\n    'europarl_raw/greek', EuroparlCorpusReader, r'ep-.*\\.el', encoding='utf-8')\n\nitalian = LazyCorpusLoader(\n    'europarl_raw/italian', EuroparlCorpusReader, r'ep-.*\\.it', encoding='utf-8')\n\nportuguese = LazyCorpusLoader(\n    'europarl_raw/portuguese', EuroparlCorpusReader, r'ep-.*\\.pt', encoding='utf-8')\n\nspanish = LazyCorpusLoader(\n    'europarl_raw/spanish', EuroparlCorpusReader, r'ep-.*\\.es', encoding='utf-8')\n\nswedish = LazyCorpusLoader(\n    'europarl_raw/swedish', EuroparlCorpusReader, r'ep-.*\\.sv', encoding='utf-8')\n"], "nltk\\corpus\\reader\\aligned": [".py", "\nfrom six import string_types\n\nfrom nltk.tokenize import WhitespaceTokenizer, RegexpTokenizer\nfrom nltk.translate import AlignedSent, Alignment\n\nfrom nltk.corpus.reader.api import CorpusReader\nfrom nltk.corpus.reader.util import StreamBackedCorpusView, concat,\\\n    read_alignedsent_block\n\nclass AlignedCorpusReader(CorpusReader):\n    def __init__(self, root, fileids,\n                 sep='/', word_tokenizer=WhitespaceTokenizer(),\n                 sent_tokenizer=RegexpTokenizer('\\n', gaps=True),\n                 alignedsent_block_reader=read_alignedsent_block,\n                 encoding='latin1'):\n        CorpusReader.__init__(self, root, fileids, encoding)\n        self._sep = sep\n        self._word_tokenizer = word_tokenizer\n        self._sent_tokenizer = sent_tokenizer\n        self._alignedsent_block_reader = alignedsent_block_reader\n\n    def raw(self, fileids=None):\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n    def words(self, fileids=None):\n        return concat([AlignedSentCorpusView(fileid, enc, False, False,\n                                             self._word_tokenizer,\n                                             self._sent_tokenizer,\n                                             self._alignedsent_block_reader)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def sents(self, fileids=None):\n        return concat([AlignedSentCorpusView(fileid, enc, False, True,\n                                             self._word_tokenizer,\n                                             self._sent_tokenizer,\n                                             self._alignedsent_block_reader)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def aligned_sents(self, fileids=None):\n        return concat([AlignedSentCorpusView(fileid, enc, True, True,\n                                             self._word_tokenizer,\n                                             self._sent_tokenizer,\n                                             self._alignedsent_block_reader)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\nclass AlignedSentCorpusView(StreamBackedCorpusView):\n    def __init__(self, corpus_file, encoding, aligned, group_by_sent,\n                 word_tokenizer, sent_tokenizer, alignedsent_block_reader):\n        self._aligned = aligned\n        self._group_by_sent = group_by_sent\n        self._word_tokenizer = word_tokenizer\n        self._sent_tokenizer = sent_tokenizer\n        self._alignedsent_block_reader = alignedsent_block_reader\n        StreamBackedCorpusView.__init__(self, corpus_file, encoding=encoding)\n\n    def read_block(self, stream):\n        block = [self._word_tokenizer.tokenize(sent_str)\n                 for alignedsent_str in self._alignedsent_block_reader(stream)\n                 for sent_str in self._sent_tokenizer.tokenize(alignedsent_str)]\n        if self._aligned:\n            block[2] = Alignment.fromstring(\" \".join(block[2])) # kludge; we shouldn't have tokenized the alignment string\n            block = [AlignedSent(*block)]\n        elif self._group_by_sent:\n            block = [block[0]]\n        else:\n            block = block[0]\n\n        return block\n"], "nltk\\corpus\\reader\\api": [".py", "\nfrom __future__ import unicode_literals\n\nimport os\nimport re\nfrom collections import defaultdict\nfrom itertools import chain\n\nfrom six import string_types\n\nfrom nltk import compat\nfrom nltk.data import PathPointer, FileSystemPathPointer, ZipFilePathPointer\n\nfrom nltk.corpus.reader.util import *\n\n@compat.python_2_unicode_compatible\nclass CorpusReader(object):\n\n    def __init__(self, root, fileids, encoding='utf8', tagset=None):\n        if isinstance(root, string_types) and not isinstance(root, PathPointer):\n            m = re.match('(.*\\.zip)/?(.*)$|', root)\n            zipfile, zipentry = m.groups()\n            if zipfile:\n                root = ZipFilePathPointer(zipfile, zipentry)\n            else:\n                root = FileSystemPathPointer(root)\n        elif not isinstance(root, PathPointer):\n            raise TypeError('CorpusReader: expected a string or a PathPointer')\n\n        if isinstance(fileids, string_types):\n            fileids = find_corpus_fileids(root, fileids)\n\n        self._fileids = fileids\n        Load this corpus (if it has not already been loaded).  This is\n        used by LazyCorpusLoader as a simple method that can be used to\n        make sure a corpus is loaded -- e.g., in case a user wants to\n        do help(some_corpus).\n        \"\"\"\n        pass # no need to actually do anything.\n\n    def readme(self):\n        \"\"\"\n        Return the contents of the corpus README file, if it exists.\n        \"\"\"\n        return self.open(\"README\").read()\n\n    def license(self):\n        \"\"\"\n        Return the contents of the corpus LICENSE file, if it exists.\n        \"\"\"\n        return self.open(\"LICENSE\").read()\n\n    def citation(self):\n        \"\"\"\n        Return the contents of the corpus citation.bib file, if it exists.\n        \"\"\"\n        return self.open(\"citation.bib\").read()\n\n    def fileids(self):\n        \"\"\"\n        Return a list of file identifiers for the fileids that make up\n        this corpus.\n        \"\"\"\n        return self._fileids\n\n    def abspath(self, fileid):\n        \"\"\"\n        Return the absolute path for the given file.\n\n        :type fileid: str\n        :param fileid: The file identifier for the file whose path\n            should be returned.\n        :rtype: PathPointer\n        \"\"\"\n        return self._root.join(fileid)\n\n    def abspaths(self, fileids=None, include_encoding=False,\n                 include_fileid=False):\n        \"\"\"\n        Return a list of the absolute paths for all fileids in this corpus;\n        or for the given list of fileids, if specified.\n\n        :type fileids: None or str or list\n        :param fileids: Specifies the set of fileids for which paths should\n            be returned.  Can be None, for all fileids; a list of\n            file identifiers, for a specified set of fileids; or a single\n            file identifier, for a single file.  Note that the return\n            value is always a list of paths, even if ``fileids`` is a\n            single file identifier.\n\n        :param include_encoding: If true, then return a list of\n            ``(path_pointer, encoding)`` tuples.\n\n        :rtype: list(PathPointer)\n        \"\"\"\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, string_types):\n            fileids = [fileids]\n\n        paths = [self._root.join(f) for f in fileids]\n\n        if include_encoding and include_fileid:\n            return list(zip(paths, [self.encoding(f) for f in fileids], fileids))\n        elif include_fileid:\n            return list(zip(paths, fileids))\n        elif include_encoding:\n            return list(zip(paths, [self.encoding(f) for f in fileids]))\n        else:\n            return paths\n\n    def open(self, file):\n        \"\"\"\n        Return an open stream that can be used to read the given file.\n        If the file's encoding is not None, then the stream will\n        automatically decode the file's contents into unicode.\n\n        :param file: The file identifier of the file to read.\n        \"\"\"\n        encoding = self.encoding(file)\n        stream = self._root.join(file).open(encoding)\n        return stream\n\n    def encoding(self, file):\n        \"\"\"\n        Return the unicode encoding for the given corpus file, if known.\n        If the encoding is unknown, or if the given file should be\n        processed using byte strings (str), then return None.\n        \"\"\"\n        if isinstance(self._encoding, dict):\n            return self._encoding.get(file)\n        else:\n            return self._encoding\n\n    def _get_root(self): return self._root\n    root = property(_get_root, doc=\"\"\"\n        The directory where this corpus is stored.\n\n        :type: PathPointer\"\"\")\n\n\n\nclass CategorizedCorpusReader(object):\n    \"\"\"\n    A mixin class used to aid in the implementation of corpus readers\n    for categorized corpora.  This class defines the method\n    ``categories()``, which returns a list of the categories for the\n    corpus or for a specified set of fileids; and overrides ``fileids()``\n    to take a ``categories`` argument, restricting the set of fileids to\n    be returned.\n\n    Subclasses are expected to:\n\n      - Call ``__init__()`` to set up the mapping.\n\n      - Override all view methods to accept a ``categories`` parameter,\n        which can be used *instead* of the ``fileids`` parameter, to\n        select which fileids should be included in the returned view.\n    \"\"\"\n\n    def __init__(self, kwargs):\n        \"\"\"\n        Initialize this mapping based on keyword arguments, as\n        follows:\n\n          - cat_pattern: A regular expression pattern used to find the\n            category for each file identifier.  The pattern will be\n            applied to each file identifier, and the first matching\n            group will be used as the category label for that file.\n\n          - cat_map: A dictionary, mapping from file identifiers to\n            category labels.\n\n          - cat_file: The name of a file that contains the mapping\n            from file identifiers to categories.  The argument\n            ``cat_delimiter`` can be used to specify a delimiter.\n\n        The corresponding argument will be deleted from ``kwargs``.  If\n        more than one argument is specified, an exception will be\n        raised.\n        \"\"\"\n        self._f2c = None #: file-to-category mapping\n        self._c2f = None #: category-to-file mapping\n\n        self._pattern = None #: regexp specifying the mapping\n        self._map = None #: dict specifying the mapping\n        self._file = None #: fileid of file containing the mapping\n        self._delimiter = None #: delimiter for ``self._file``\n\n        if 'cat_pattern' in kwargs:\n            self._pattern = kwargs['cat_pattern']\n            del kwargs['cat_pattern']\n        elif 'cat_map' in kwargs:\n            self._map = kwargs['cat_map']\n            del kwargs['cat_map']\n        elif 'cat_file' in kwargs:\n            self._file = kwargs['cat_file']\n            del kwargs['cat_file']\n            if 'cat_delimiter' in kwargs:\n                self._delimiter = kwargs['cat_delimiter']\n                del kwargs['cat_delimiter']\n        else:\n            raise ValueError('Expected keyword argument cat_pattern or '\n                             'cat_map or cat_file.')\n\n\n        if ('cat_pattern' in kwargs or 'cat_map' in kwargs or\n            'cat_file' in kwargs):\n            raise ValueError('Specify exactly one of: cat_pattern, '\n                             'cat_map, cat_file.')\n\n    def _init(self):\n        self._f2c = defaultdict(set)\n        self._c2f = defaultdict(set)\n\n        if self._pattern is not None:\n            for file_id in self._fileids:\n                category = re.match(self._pattern, file_id).group(1)\n                self._add(file_id, category)\n\n        elif self._map is not None:\n            for (file_id, categories) in self._map.items():\n                for category in categories:\n                    self._add(file_id, category)\n\n        elif self._file is not None:\n            for line in self.open(self._file).readlines():\n                line = line.strip()\n                file_id, categories = line.split(self._delimiter, 1)\n                if file_id not in self.fileids():\n                    raise ValueError('In category mapping file %s: %s '\n                                     'not found' % (self._file, file_id))\n                for category in categories.split(self._delimiter):\n                    self._add(file_id, category)\n\n    def _add(self, file_id, category):\n        self._f2c[file_id].add(category)\n        self._c2f[category].add(file_id)\n\n    def categories(self, fileids=None):\n        \"\"\"\n        Return a list of the categories that are defined for this corpus,\n        or for the file(s) if it is given.\n        \"\"\"\n        if self._f2c is None:\n            self._init()\n        if fileids is None:\n            return sorted(self._c2f)\n        if isinstance(fileids, string_types):\n            fileids = [fileids]\n        return sorted(set.union(*[self._f2c[d] for d in fileids]))\n\n    def fileids(self, categories=None):\n        \"\"\"\n        Return a list of file identifiers for the files that make up\n        this corpus, or that make up the given category(s) if specified.\n        \"\"\"\n        if categories is None:\n            return super(CategorizedCorpusReader, self).fileids()\n        elif isinstance(categories, string_types):\n            if self._f2c is None:\n                self._init()\n            if categories in self._c2f:\n                return sorted(self._c2f[categories])\n            else:\n                raise ValueError('Category %s not found' % categories)\n        else:\n            if self._f2c is None:\n                self._init()\n            return sorted(set.union(*[self._c2f[c] for c in categories]))\n\n\nclass SyntaxCorpusReader(CorpusReader):\n    \"\"\"\n    An abstract base class for reading corpora consisting of\n    syntactically parsed text.  Subclasses should define:\n\n      - ``__init__``, which specifies the location of the corpus\n        and a method for detecting the sentence blocks in corpus files.\n      - ``_read_block``, which reads a block from the input stream.\n      - ``_word``, which takes a block and returns a list of list of words.\n      - ``_tag``, which takes a block and returns a list of list of tagged\n        words.\n      - ``_parse``, which takes a block and returns a list of parsed\n        sentences.\n    \"\"\"\n    def _parse(self, s):\n        raise NotImplementedError()\n    def _word(self, s):\n        raise NotImplementedError()\n    def _tag(self, s):\n        raise NotImplementedError()\n    def _read_block(self, stream):\n        raise NotImplementedError()\n\n    def raw(self, fileids=None):\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n    def parsed_sents(self, fileids=None):\n        reader = self._read_parsed_sent_block\n        return concat([StreamBackedCorpusView(fileid, reader, encoding=enc)\n                       for fileid, enc in self.abspaths(fileids, True)])\n\n    def tagged_sents(self, fileids=None, tagset=None):\n        def reader(stream):\n            return self._read_tagged_sent_block(stream, tagset)\n        return concat([StreamBackedCorpusView(fileid, reader, encoding=enc)\n                       for fileid, enc in self.abspaths(fileids, True)])\n\n    def sents(self, fileids=None):\n        reader = self._read_sent_block\n        return concat([StreamBackedCorpusView(fileid, reader, encoding=enc)\n                       for fileid, enc in self.abspaths(fileids, True)])\n\n    def tagged_words(self, fileids=None, tagset=None):\n        def reader(stream):\n            return self._read_tagged_word_block(stream, tagset)\n        return concat([StreamBackedCorpusView(fileid, reader, encoding=enc)\n                       for fileid, enc in self.abspaths(fileids, True)])\n\n    def words(self, fileids=None):\n        return concat([StreamBackedCorpusView(fileid,\n                                              self._read_word_block,\n                                              encoding=enc)\n                       for fileid, enc in self.abspaths(fileids, True)])\n\n\n    def _read_word_block(self, stream):\n        return list(chain(*self._read_sent_block(stream)))\n\n    def _read_tagged_word_block(self, stream, tagset=None):\n        return list(chain(*self._read_tagged_sent_block(stream, tagset)))\n\n    def _read_sent_block(self, stream):\n        return list(filter(None, [self._word(t) for t in self._read_block(stream)]))\n\n    def _read_tagged_sent_block(self, stream, tagset=None):\n        return list(filter(None, [self._tag(t, tagset)\n                             for t in self._read_block(stream)]))\n\n    def _read_parsed_sent_block(self, stream):\n        return list(filter(None, [self._parse(t) for t in self._read_block(stream)]))\n\n"], "nltk\\corpus\\reader\\bnc": [".py", "\n\nfrom nltk.corpus.reader.util import concat\nfrom nltk.corpus.reader.xmldocs import XMLCorpusReader, XMLCorpusView, ElementTree\n\n\nclass BNCCorpusReader(XMLCorpusReader):\n\n    def __init__(self, root, fileids, lazy=True):\n        XMLCorpusReader.__init__(self, root, fileids)\n        self._lazy = lazy\n\n    def words(self, fileids=None, strip_space=True, stem=False):\n        return self._views(fileids, False, None, strip_space, stem)\n\n    def tagged_words(self, fileids=None, c5=False, strip_space=True, stem=False):\n        tag = 'c5' if c5 else 'pos'\n        return self._views(fileids, False, tag, strip_space, stem)\n\n    def sents(self, fileids=None, strip_space=True, stem=False):\n        return self._views(fileids, True, None, strip_space, stem)\n\n    def tagged_sents(self, fileids=None, c5=False, strip_space=True, stem=False):\n        tag = 'c5' if c5 else 'pos'\n        return self._views(fileids, sent=True, tag=tag, strip_space=strip_space, stem=stem)\n\n    def _views(self, fileids=None, sent=False, tag=False, strip_space=True, stem=False):\n        f = BNCWordView if self._lazy else self._words\n        return concat([f(fileid, sent, tag, strip_space, stem) for fileid in self.abspaths(fileids)])\n\n    def _words(self, fileid, bracket_sent, tag, strip_space, stem):\n        result = []\n\n        xmldoc = ElementTree.parse(fileid).getroot()\n        for xmlsent in xmldoc.findall('.//s'):\n            sent = []\n            for xmlword in _all_xmlwords_in(xmlsent):\n                word = xmlword.text\n                if not word:\n                    word = \"\"  # fixes issue 337?\n                if strip_space or stem:\n                    word = word.strip()\n                if stem:\n                    word = xmlword.get('hw', word)\n                if tag == 'c5':\n                    word = (word, xmlword.get('c5'))\n                elif tag == 'pos':\n                    word = (word, xmlword.get('pos', xmlword.get('c5')))\n                sent.append(word)\n            if bracket_sent:\n                result.append(BNCSentence(xmlsent.attrib['n'], sent))\n            else:\n                result.extend(sent)\n\n        assert None not in result\n        return result\n\n\ndef _all_xmlwords_in(elt, result=None):\n    if result is None:\n        result = []\n    for child in elt:\n        if child.tag in ('c', 'w'):\n            result.append(child)\n        else:\n            _all_xmlwords_in(child, result)\n    return result\n\n\nclass BNCSentence(list):\n    def __init__(self, num, items):\n        self.num = num\n        list.__init__(self, items)\n\n\nclass BNCWordView(XMLCorpusView):\n\n    tags_to_ignore = set(\n        ['pb', 'gap', 'vocal', 'event', 'unclear', 'shift', 'pause', 'align']\n    )\n\n    def __init__(self, fileid, sent, tag, strip_space, stem):\n        if sent:\n            tagspec = '.*/s'\n        else:\n            tagspec = '.*/s/(.*/)?(c|w)'\n        self._sent = sent\n        self._tag = tag\n        self._strip_space = strip_space\n        self._stem = stem\n\n        self.title = None  #: Title of the document.\n        self.author = None  #: Author of the document.\n        self.editor = None  #: Editor\n        self.resps = None  #: Statement of responsibility\n\n        XMLCorpusView.__init__(self, fileid, tagspec)\n\n        self._open()\n        self.read_block(self._stream, '.*/teiHeader$', self.handle_header)\n        self.close()\n\n        self._tag_context = {0: ()}\n\n    def handle_header(self, elt, context):\n        titles = elt.findall('titleStmt/title')\n        if titles:\n            self.title = '\\n'.join(title.text.strip() for title in titles)\n\n        authors = elt.findall('titleStmt/author')\n        if authors:\n            self.author = '\\n'.join(author.text.strip() for author in authors)\n\n        editors = elt.findall('titleStmt/editor')\n        if editors:\n            self.editor = '\\n'.join(editor.text.strip() for editor in editors)\n\n        resps = elt.findall('titleStmt/respStmt')\n        if resps:\n            self.resps = '\\n\\n'.join(\n                '\\n'.join(\n                    resp_elt.text.strip() for resp_elt in resp\n                ) for resp in resps\n            )\n\n    def handle_elt(self, elt, context):\n        if self._sent:\n            return self.handle_sent(elt)\n        else:\n            return self.handle_word(elt)\n\n    def handle_word(self, elt):\n        word = elt.text\n        if not word:\n            word = \"\"  # fixes issue 337?\n        if self._strip_space or self._stem:\n            word = word.strip()\n        if self._stem:\n            word = elt.get('hw', word)\n        if self._tag == 'c5':\n            word = (word, elt.get('c5'))\n        elif self._tag == 'pos':\n            word = (word, elt.get('pos', elt.get('c5')))\n        return word\n\n    def handle_sent(self, elt):\n        sent = []\n        for child in elt:\n            if child.tag in ('mw', 'hi', 'corr', 'trunc'):\n                sent += [self.handle_word(w) for w in child]\n            elif child.tag in ('w', 'c'):\n                sent.append(self.handle_word(child))\n            elif child.tag not in self.tags_to_ignore:\n                raise ValueError('Unexpected element %s' % child.tag)\n        return BNCSentence(elt.attrib['n'], sent)\n"], "nltk\\corpus\\reader\\bracket_parse": [".py", "\nimport sys\n\nfrom nltk.tree import Tree\nfrom nltk.tag import map_tag\n\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\nSORTTAGWRD = re.compile(r'\\((\\d+) ([^\\s()]+) ([^\\s()]+)\\)') \nTAGWORD = re.compile(r'\\(([^\\s()]+) ([^\\s()]+)\\)')\nWORD = re.compile(r'\\([^\\s()]+ ([^\\s()]+)\\)')\nEMPTY_BRACKETS = re.compile(r'\\s*\\(\\s*\\(')\n\nclass BracketParseCorpusReader(SyntaxCorpusReader):\n    def __init__(self, root, fileids, comment_char=None,\n                 detect_blocks='unindented_paren', encoding='utf8',\n                 tagset=None):\n        CorpusReader.__init__(self, root, fileids, encoding)\n        self._comment_char = comment_char\n        self._detect_blocks = detect_blocks\n        self._tagset = tagset\n\n    def _read_block(self, stream):\n        if self._detect_blocks == 'sexpr':\n            return read_sexpr_block(stream, comment_char=self._comment_char)\n        elif self._detect_blocks == 'blankline':\n            return read_blankline_block(stream)\n        elif self._detect_blocks == 'unindented_paren':\n            toks = read_regexp_block(stream, start_re=r'^\\(')\n            if self._comment_char:\n                toks = [re.sub('(?m)^%s.*'%re.escape(self._comment_char),\n                               '', tok)\n                        for tok in toks]\n            return toks\n        else:\n            assert 0, 'bad block type'\n\n    def _normalize(self, t):\n        if EMPTY_BRACKETS.match(t):\n            t = t.strip()[1:-1]\n        t = re.sub(r\"\\((.)\\)\", r\"(\\1 \\1)\", t)\n        t = re.sub(r\"\\(([^\\s()]+) ([^\\s()]+) [^\\s()]+\\)\", r\"(\\1 \\2)\", t)\n        return t\n\n    def _parse(self, t):\n        try:\n            return Tree.fromstring(self._normalize(t))\n\n        except ValueError as e:\n            sys.stderr.write(\"Bad tree detected; trying to recover...\\n\")\n            if e.args == ('mismatched parens',):\n                for n in range(1, 5):\n                    try:\n                        v = Tree(self._normalize(t+')'*n))\n                        sys.stderr.write(\"  Recovered by adding %d close \"\n                                         \"paren(s)\\n\" % n)\n                        return v\n                    except ValueError: pass\n            sys.stderr.write(\"  Recovered by returning a flat parse.\\n\")\n            return Tree('S', self._tag(t))\n\n    def _tag(self, t, tagset=None):\n        tagged_sent = [(w,p) for (p,w) in TAGWORD.findall(self._normalize(t))]\n        if tagset and tagset != self._tagset:\n            tagged_sent = [(w, map_tag(self._tagset, tagset, p)) for (w,p) in tagged_sent]\n        return tagged_sent\n\n    def _word(self, t):\n        return WORD.findall(self._normalize(t))\n\nclass CategorizedBracketParseCorpusReader(CategorizedCorpusReader,\n                                          BracketParseCorpusReader):\n    def __init__(self, *args, **kwargs):\n        CategorizedCorpusReader.__init__(self, kwargs)\n        BracketParseCorpusReader.__init__(self, *args, **kwargs)\n\n    def _resolve(self, fileids, categories):\n        if fileids is not None and categories is not None:\n            raise ValueError('Specify fileids or categories, not both')\n        if categories is not None:\n            return self.fileids(categories)\n        else:\n            return fileids\n    def raw(self, fileids=None, categories=None):\n        return BracketParseCorpusReader.raw(\n            self, self._resolve(fileids, categories))\n    def words(self, fileids=None, categories=None):\n        return BracketParseCorpusReader.words(\n            self, self._resolve(fileids, categories))\n    def sents(self, fileids=None, categories=None):\n        return BracketParseCorpusReader.sents(\n            self, self._resolve(fileids, categories))\n    def paras(self, fileids=None, categories=None):\n        return BracketParseCorpusReader.paras(\n            self, self._resolve(fileids, categories))\n    def tagged_words(self, fileids=None, categories=None, tagset=None):\n        return BracketParseCorpusReader.tagged_words(\n            self, self._resolve(fileids, categories), tagset)\n    def tagged_sents(self, fileids=None, categories=None, tagset=None):\n        return BracketParseCorpusReader.tagged_sents(\n            self, self._resolve(fileids, categories), tagset)\n    def tagged_paras(self, fileids=None, categories=None, tagset=None):\n        return BracketParseCorpusReader.tagged_paras(\n            self, self._resolve(fileids, categories), tagset)\n    def parsed_words(self, fileids=None, categories=None):\n        return BracketParseCorpusReader.parsed_words(\n            self, self._resolve(fileids, categories))\n    def parsed_sents(self, fileids=None, categories=None):\n        return BracketParseCorpusReader.parsed_sents(\n            self, self._resolve(fileids, categories))\n    def parsed_paras(self, fileids=None, categories=None):\n        return BracketParseCorpusReader.parsed_paras(\n            self, self._resolve(fileids, categories))\n\nclass AlpinoCorpusReader(BracketParseCorpusReader):\n    def __init__(self, root, encoding='ISO-8859-1', tagset=None):\n        BracketParseCorpusReader.__init__(self, root, 'alpino\\.xml',\n                                 detect_blocks='blankline',\n                                 encoding=encoding,\n                                 tagset=tagset)\n\n    def _normalize(self, t, ordered = False):\n        if t[:10] != \"<alpino_ds\":\n            return \"\"\n        t = re.sub(r'  <node .*? cat=\"(\\w+)\".*>', r\"(\\1\", t)\n        if ordered:\n            t = re.sub(r'  <node. *?begin=\"(\\d+)\".*? pos=\"(\\w+)\".*? word=\"([^\"]+)\".*?/>', r\"(\\1 \\2 \\3)\", t)\n        else: \n            t = re.sub(r'  <node .*?pos=\"(\\w+)\".*? word=\"([^\"]+)\".*?/>', r\"(\\1 \\2)\", t)\n        t = re.sub(r\"  </node>\", r\")\", t)\n        t = re.sub(r\"<sentence>.*</sentence>\", r\"\", t)\n        t = re.sub(r\"</?alpino_ds.*>\", r\"\", t)\n        return t\n\n    def _tag(self, t, tagset=None):\n        tagged_sent = [(int(o), w, p) for (o,p,w) in SORTTAGWRD.findall(self._normalize(t, ordered = True))]\n        tagged_sent.sort()\n        if tagset and tagset != self._tagset:\n            tagged_sent = [(w, map_tag(self._tagset, tagset, p)) for (o,w,p) in tagged_sent]\n        else:\n            tagged_sent = [(w,p) for (o,w,p) in tagged_sent]\n        return tagged_sent\n\n    def _word(self, t):\n        tagged_sent = self._tag(t)\n        return [w for (w,p) in tagged_sent]      \n\n"], "nltk\\corpus\\reader\\categorized_sents": [".py", "\nfrom six import string_types\n\nfrom nltk.corpus.reader.api import *\nfrom nltk.tokenize import *\n\nclass CategorizedSentencesCorpusReader(CategorizedCorpusReader, CorpusReader):\n\n    CorpusView = StreamBackedCorpusView\n\n    def __init__(self, root, fileids, word_tokenizer=WhitespaceTokenizer(),\n                 sent_tokenizer=None, encoding='utf8', **kwargs):\n\n        CorpusReader.__init__(self, root, fileids, encoding)\n        CategorizedCorpusReader.__init__(self, kwargs)\n        self._word_tokenizer = word_tokenizer\n        self._sent_tokenizer = sent_tokenizer\n\n    def _resolve(self, fileids, categories):\n        if fileids is not None and categories is not None:\n            raise ValueError('Specify fileids or categories, not both')\n        if categories is not None:\n            return self.fileids(categories)\n        else:\n            return fileids\n\n    def raw(self, fileids=None, categories=None):\n        fileids = self._resolve(fileids, categories)\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, string_types):\n            fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n    def readme(self):\n        return self.open(\"README\").read()\n\n    def sents(self, fileids=None, categories=None):\n        fileids = self._resolve(fileids, categories)\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, string_types):\n            fileids = [fileids]\n        return concat([self.CorpusView(path, self._read_sent_block, encoding=enc)\n            for (path, enc, fileid) in self.abspaths(fileids, True, True)])\n\n    def words(self, fileids=None, categories=None):\n        fileids = self._resolve(fileids, categories)\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, string_types):\n            fileids = [fileids]\n        return concat([self.CorpusView(path, self._read_word_block, encoding=enc)\n            for (path, enc, fileid) in self.abspaths(fileids, True, True)])\n\n    def _read_sent_block(self, stream):\n        sents = []\n        for i in range(20): # Read 20 lines at a time.\n            line = stream.readline()\n            if not line:\n                continue\n            if self._sent_tokenizer:\n                sents.extend([self._word_tokenizer.tokenize(sent)\n                              for sent in self._sent_tokenizer.tokenize(line)])\n            else:\n                sents.append(self._word_tokenizer.tokenize(line))\n        return sents\n\n    def _read_word_block(self, stream):\n        words = []\n        for sent in self._read_sent_block(stream):\n            words.extend(sent)\n        return words\n"], "nltk\\corpus\\reader\\chasen": [".py", "\nfrom __future__ import print_function\n\nimport sys\n\nfrom six import string_types\n\nfrom nltk.corpus.reader import util\n\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\nclass ChasenCorpusReader(CorpusReader):\n\n    def __init__(self, root, fileids, encoding='utf8', sent_splitter=None):\n        self._sent_splitter = sent_splitter\n        CorpusReader.__init__(self, root, fileids, encoding)\n\n    def raw(self, fileids=None):\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n    def words(self, fileids=None):\n        return concat([ChasenCorpusView(fileid, enc,\n                                        False, False, False, self._sent_splitter)\n            for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def tagged_words(self, fileids=None):\n        return concat([ChasenCorpusView(fileid, enc,\n                                        True, False, False, self._sent_splitter)\n            for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def sents(self, fileids=None):\n        return concat([ChasenCorpusView(fileid, enc,\n                                        False, True, False, self._sent_splitter)\n            for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def tagged_sents(self, fileids=None):\n        return concat([ChasenCorpusView(fileid, enc,\n                                        True, True, False, self._sent_splitter)\n            for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def paras(self, fileids=None):\n        return concat([ChasenCorpusView(fileid, enc,\n                                        False, True, True, self._sent_splitter)\n            for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def tagged_paras(self, fileids=None):\n        return concat([ChasenCorpusView(fileid, enc,\n                                        True, True, True, self._sent_splitter)\n            for (fileid, enc) in self.abspaths(fileids, True)])\n\n\nclass ChasenCorpusView(StreamBackedCorpusView):\n\n    def __init__(self, corpus_file, encoding,\n                 tagged, group_by_sent, group_by_para, sent_splitter=None):\n        self._tagged = tagged\n        self._group_by_sent = group_by_sent\n        self._group_by_para = group_by_para\n        self._sent_splitter = sent_splitter\n        StreamBackedCorpusView.__init__(self, corpus_file, encoding=encoding)\n\n\n    def read_block(self, stream):\n        block = []\n        for para_str in read_regexp_block(stream, r\".\", r\"^EOS\\n\"):\n\n            para = []\n\n            sent = []\n            for line in para_str.splitlines():\n\n                _eos = line.strip() == 'EOS'\n                _cells = line.split('\\t')\n                w = (_cells[0], '\\t'.join(_cells[1:]))\n                if not _eos: sent.append(w)\n\n                if _eos or (self._sent_splitter and self._sent_splitter(w)):\n                    if not self._tagged:\n                        sent = [w for (w,t) in sent]\n                    if self._group_by_sent:\n                        para.append(sent)\n                    else:\n                        para.extend(sent)\n                    sent = []\n\n            if len(sent)>0:\n                if not self._tagged:\n                    sent = [w for (w,t) in sent]\n\n                if self._group_by_sent:\n                    para.append(sent)\n                else:\n                    para.extend(sent)\n\n            if self._group_by_para:\n                block.append(para)\n            else:\n                block.extend(para)\n\n        return block\n\ndef demo():\n\n    import nltk\n    from nltk.corpus.util import LazyCorpusLoader\n\n    jeita = LazyCorpusLoader(\n        'jeita', ChasenCorpusReader, r'.*chasen', encoding='utf-8')\n    print('/'.join( jeita.words()[22100:22140] ))\n\n\n    print('\\nEOS\\n'.join('\\n'.join(\"%s/%s\" % (w[0],w[1].split('\\t')[2]) for w in sent)\n                          for sent in jeita.tagged_sents()[2170:2173]))\n\ndef test():\n\n    from nltk.corpus.util import LazyCorpusLoader\n\n    jeita = LazyCorpusLoader(\n        'jeita', ChasenCorpusReader, r'.*chasen', encoding='utf-8')\n\n    assert isinstance(jeita.tagged_words()[0][1], string_types)\n\nif __name__ == '__main__':\n    demo()\n    test()\n"], "nltk\\corpus\\reader\\childes": [".py", "\n\nfrom __future__ import print_function, division\n\n__docformat__ = 'epytext en'\n\nimport re\nfrom collections import defaultdict\nfrom six import string_types\n\nfrom nltk.util import flatten, LazyMap, LazyConcatenation\n\nfrom nltk.corpus.reader.util import concat\nfrom nltk.corpus.reader.xmldocs import XMLCorpusReader, ElementTree\n\nNS = 'http://www.talkbank.org/ns/talkbank'\n\nclass CHILDESCorpusReader(XMLCorpusReader):\n    def __init__(self, root, fileids, lazy=True):\n        XMLCorpusReader.__init__(self, root, fileids)\n        self._lazy = lazy\n\n    def words(self, fileids=None, speaker='ALL', stem=False,\n            relation=False, strip_space=True, replace=False):\n        sent=None\n        pos=False\n        if not self._lazy:\n            return [self._get_words(fileid, speaker, sent, stem, relation,\n                pos, strip_space, replace) for fileid in self.abspaths(fileids)]\n\n        get_words = lambda fileid: self._get_words(fileid, speaker, sent, stem, relation,\n            pos, strip_space, replace)\n        return LazyConcatenation(LazyMap(get_words, self.abspaths(fileids)))\n\n    def tagged_words(self, fileids=None, speaker='ALL', stem=False,\n            relation=False, strip_space=True, replace=False):\n        sent=None\n        pos=True\n        if not self._lazy:\n            return [self._get_words(fileid, speaker, sent, stem, relation,\n                pos, strip_space, replace) for fileid in self.abspaths(fileids)]\n\n        get_words = lambda fileid: self._get_words(fileid, speaker, sent, stem, relation,\n            pos, strip_space, replace)\n        return LazyConcatenation(LazyMap(get_words, self.abspaths(fileids)))\n\n    def sents(self, fileids=None, speaker='ALL', stem=False,\n            relation=None, strip_space=True, replace=False):\n        sent=True\n        pos=False\n        if not self._lazy:\n            return [self._get_words(fileid, speaker, sent, stem, relation,\n                pos, strip_space, replace) for fileid in self.abspaths(fileids)]\n\n        get_words = lambda fileid: self._get_words(fileid, speaker, sent, stem, relation,\n            pos, strip_space, replace)\n        return LazyConcatenation(LazyMap(get_words, self.abspaths(fileids)))\n\n    def tagged_sents(self, fileids=None, speaker='ALL', stem=False,\n            relation=None, strip_space=True, replace=False):\n        sent=True\n        pos=True\n        if not self._lazy:\n            return [self._get_words(fileid, speaker, sent, stem, relation,\n                pos, strip_space, replace) for fileid in self.abspaths(fileids)]\n\n        get_words = lambda fileid: self._get_words(fileid, speaker, sent, stem, relation,\n            pos, strip_space, replace)\n        return LazyConcatenation(LazyMap(get_words, self.abspaths(fileids)))\n\n    def corpus(self, fileids=None):\n        if not self._lazy:\n            return [self._get_corpus(fileid) for fileid in self.abspaths(fileids)]\n        return LazyMap(self._get_corpus, self.abspaths(fileids))\n\n    def _get_corpus(self, fileid):\n        results = dict()\n        xmldoc = ElementTree.parse(fileid).getroot()\n        for key, value in xmldoc.items():\n            results[key] = value\n        return results\n\n    def participants(self, fileids=None):\n        if not self._lazy:\n            return [self._get_participants(fileid) for fileid in self.abspaths(fileids)]\n        return LazyMap(self._get_participants, self.abspaths(fileids))\n\n    def _get_participants(self, fileid):\n        def dictOfDicts():\n            return defaultdict(dictOfDicts)\n\n        xmldoc = ElementTree.parse(fileid).getroot()\n        pat = dictOfDicts()\n        for participant in xmldoc.findall('.//{%s}Participants/{%s}participant'\n                                          % (NS,NS)):\n            for (key,value) in participant.items():\n                pat[participant.get('id')][key] = value\n        return pat\n\n    def age(self, fileids=None, speaker='CHI', month=False):\n        if not self._lazy:\n            return [self._get_age(fileid, speaker, month)\n                for fileid in self.abspaths(fileids)]\n        get_age = lambda fileid: self._get_age(fileid, speaker, month)\n        return LazyMap(get_age, self.abspaths(fileids))\n\n    def _get_age(self, fileid, speaker, month):\n        xmldoc = ElementTree.parse(fileid).getroot()\n        for pat in xmldoc.findall('.//{%s}Participants/{%s}participant'\n                                  % (NS,NS)):\n            try:\n                if pat.get('id') == speaker:\n                    age = pat.get('age')\n                    if month:\n                        age = self.convert_age(age)\n                    return age\n            except (TypeError, AttributeError) as e:\n                return None\n\n    def convert_age(self, age_year):\n        \"Caclculate age in months from a string in CHILDES format\"\n        m = re.match(\"P(\\d+)Y(\\d+)M?(\\d?\\d?)D?\",age_year)\n        age_month = int(m.group(1))*12 + int(m.group(2))\n        try:\n            if int(m.group(3)) > 15:\n                age_month += 1\n        except ValueError as e:\n            pass\n        return age_month\n\n    def MLU(self, fileids=None, speaker='CHI'):\n        if not self._lazy:\n            return [self._getMLU(fileid, speaker=speaker)\n                for fileid in self.abspaths(fileids)]\n        get_MLU = lambda fileid: self._getMLU(fileid, speaker=speaker)\n        return LazyMap(get_MLU, self.abspaths(fileids))\n\n    def _getMLU(self, fileid, speaker):\n        sents = self._get_words(fileid, speaker=speaker, sent=True, stem=True,\n                    relation=False, pos=True, strip_space=True, replace=True)\n        results = []\n        lastSent = []\n        numFillers = 0\n        sentDiscount = 0\n        for sent in sents:\n            posList = [pos for (word,pos) in sent]\n            if any(pos == 'unk' for pos in posList):\n                next\n            elif sent == []:\n                next\n            elif sent == lastSent:\n                next\n            else:\n                results.append([word for (word,pos) in sent])\n                if len(set(['co',None]).intersection(posList)) > 0:\n                    numFillers += posList.count('co')\n                    numFillers += posList.count(None)\n                    sentDiscount += 1\n            lastSent = sent\n        try:\n            thisWordList = flatten(results)\n            numWords = len(flatten([word.split('-')\n                                          for word in thisWordList])) - numFillers\n            numSents = len(results) - sentDiscount\n            mlu = numWords/numSents\n        except ZeroDivisionError:\n            mlu = 0\n        return mlu\n\n    def _get_words(self, fileid, speaker, sent, stem, relation, pos,\n            strip_space, replace):\n        if isinstance(speaker, string_types) and speaker != 'ALL':  # ensure we have a list of speakers\n            speaker = [ speaker ]\n        xmldoc = ElementTree.parse(fileid).getroot()\n        results = []\n        for xmlsent in xmldoc.findall('.//{%s}u' % NS):\n            sents = []\n            if speaker == 'ALL' or xmlsent.get('who') in speaker:\n                for xmlword in xmlsent.findall('.//{%s}w' % NS):\n                    infl = None ; suffixStem = None; suffixTag = None\n                    if replace and xmlsent.find('.//{%s}w/{%s}replacement'\n                                                % (NS,NS)):\n                        xmlword = xmlsent.find('.//{%s}w/{%s}replacement/{%s}w'\n                                               % (NS,NS,NS))\n                    elif replace and xmlsent.find('.//{%s}w/{%s}wk' % (NS,NS)):\n                        xmlword = xmlsent.find('.//{%s}w/{%s}wk' % (NS,NS))\n                    if xmlword.text:\n                        word = xmlword.text\n                    else:\n                        word = ''\n                    if strip_space:\n                        word = word.strip()\n                    if relation or stem:\n                        try:\n                            xmlstem = xmlword.find('.//{%s}stem' % NS)\n                            word = xmlstem.text\n                        except AttributeError as e:\n                            pass\n                        try:\n                            xmlinfl = xmlword.find('.//{%s}mor/{%s}mw/{%s}mk'\n                                                   % (NS,NS,NS))\n                            word += '-' + xmlinfl.text\n                        except:\n                            pass\n                        try:\n                            xmlsuffix = xmlword.find('.//{%s}mor/{%s}mor-post/{%s}mw/{%s}stem'\n                                                     % (NS,NS,NS,NS))\n                            suffixStem = xmlsuffix.text\n                        except AttributeError:\n                            suffixStem = \"\"\n                        if suffixStem:\n                            word += \"~\"+suffixStem\n                    if relation or pos:\n                        try:\n                            xmlpos = xmlword.findall(\".//{%s}c\" % NS)\n                            xmlpos2 = xmlword.findall(\".//{%s}s\" % NS)\n                            if xmlpos2 != []:\n                                tag = xmlpos[0].text+\":\"+xmlpos2[0].text\n                            else:\n                                tag = xmlpos[0].text\n                        except (AttributeError,IndexError) as e:\n                            tag = \"\"\n                        try:\n                            xmlsuffixpos = xmlword.findall('.//{%s}mor/{%s}mor-post/{%s}mw/{%s}pos/{%s}c'\n                                                     % (NS,NS,NS,NS,NS))\n                            xmlsuffixpos2 = xmlword.findall('.//{%s}mor/{%s}mor-post/{%s}mw/{%s}pos/{%s}s'\n                                                     % (NS,NS,NS,NS,NS))\n                            if xmlsuffixpos2:\n                                suffixTag = xmlsuffixpos[0].text+\":\"+xmlsuffixpos2[0].text\n                            else:\n                                suffixTag = xmlsuffixpos[0].text\n                        except:\n                            pass\n                        if suffixTag:\n                            tag += \"~\"+suffixTag\n                        word = (word, tag)\n                    if relation == True:\n                        for xmlstem_rel in xmlword.findall('.//{%s}mor/{%s}gra'\n                                                           % (NS,NS)):\n                            if not xmlstem_rel.get('type') == 'grt':\n                                word = (word[0], word[1],\n                                        xmlstem_rel.get('index')\n                                        + \"|\" + xmlstem_rel.get('head')\n                                        + \"|\" + xmlstem_rel.get('relation'))\n                            else:\n                                word = (word[0], word[1], word[2],\n                                        word[0], word[1],\n                                        xmlstem_rel.get('index')\n                                        + \"|\" + xmlstem_rel.get('head')\n                                        + \"|\" + xmlstem_rel.get('relation'))\n                        try:\n                            for xmlpost_rel in xmlword.findall('.//{%s}mor/{%s}mor-post/{%s}gra'\n                                                               % (NS,NS,NS)):\n                                if not xmlpost_rel.get('type') == 'grt':\n                                    suffixStem = (suffixStem[0],\n                                                  suffixStem[1],\n                                                  xmlpost_rel.get('index')\n                                                  + \"|\" + xmlpost_rel.get('head')\n                                                  + \"|\" + xmlpost_rel.get('relation'))\n                                else:\n                                    suffixStem = (suffixStem[0], suffixStem[1],\n                                                  suffixStem[2], suffixStem[0],\n                                                  suffixStem[1],\n                                                  xmlpost_rel.get('index')\n                                                  + \"|\" + xmlpost_rel.get('head')\n                                                  + \"|\" + xmlpost_rel.get('relation'))\n                        except:\n                            pass\n                    sents.append(word)\n                if sent or relation:\n                    results.append(sents)\n                else:\n                    results.extend(sents)\n        return LazyMap(lambda x: x, results)\n\n\n\n    childes_url_base = r'http://childes.psy.cmu.edu/browser/index.php?url='\n\n\n    def webview_file(self, fileid, urlbase=None):\n\n        import webbrowser, re\n\n        if urlbase:\n            path = urlbase+\"/\"+fileid\n        else:\n            full = self.root + \"/\" + fileid\n            full = re.sub(r'\\\\', '/', full)\n            if '/childes/' in full.lower():\n                path = re.findall(r'(?i)/childes(?:/data-xml)?/(.*)\\.xml', full)[0]\n            elif 'eng-usa' in full.lower():\n                path = 'Eng-USA/' + re.findall(r'/(?i)Eng-USA/(.*)\\.xml', full)[0]\n            else:\n                path = fileid\n\n        if path.endswith('.xml'):\n            path = path[:-4]\n\n        if not path.endswith('.cha'):\n            path = path+'.cha'\n\n        url = self.childes_url_base + path\n\n        webbrowser.open_new_tab(url)\n        print(\"Opening in browser:\", url)\n\n\n\ndef demo(corpus_root=None):\n    if not corpus_root:\n        from nltk.data import find\n        corpus_root = find('corpora/childes/data-xml/Eng-USA/')\n\n    try:\n        childes = CHILDESCorpusReader(corpus_root, '.*.xml')\n        for file in childes.fileids()[:5]:\n            corpus = ''\n            corpus_id = ''\n            for (key,value) in childes.corpus(file)[0].items():\n                if key == \"Corpus\": corpus = value\n                if key == \"Id\": corpus_id = value\n            print('Reading', corpus,corpus_id,' .....')\n            print(\"words:\", childes.words(file)[:7],\"...\")\n            print(\"words with replaced words:\", childes.words(file, replace=True)[:7],\" ...\")\n            print(\"words with pos tags:\", childes.tagged_words(file)[:7],\" ...\")\n            print(\"words (only MOT):\", childes.words(file, speaker='MOT')[:7], \"...\")\n            print(\"words (only CHI):\", childes.words(file, speaker='CHI')[:7], \"...\")\n            print(\"stemmed words:\", childes.words(file, stem=True)[:7],\" ...\")\n            print(\"words with relations and pos-tag:\", childes.words(file, relation=True)[:5],\" ...\")\n            print(\"sentence:\", childes.sents(file)[:2],\" ...\")\n            for (participant, values) in childes.participants(file)[0].items():\n                    for (key, value) in values.items():\n                        print(\"\\tparticipant\", participant, key, \":\", value)\n            print(\"num of sent:\", len(childes.sents(file)))\n            print(\"num of morphemes:\", len(childes.words(file, stem=True)))\n            print(\"age:\", childes.age(file))\n            print(\"age in month:\", childes.age(file, month=True))\n            print(\"MLU:\", childes.MLU(file))\n            print()\n\n    except LookupError as e:\n        print(\"\"\"The CHILDES corpus, or the parts you need, should be manually\n        downloaded from http://childes.psy.cmu.edu/data-xml/ and saved at\n        [NLTK_Data_Dir]/corpora/childes/\n            Alternately, you can call the demo with the path to a portion of the CHILDES corpus, e.g.:\n        demo('/path/to/childes/data-xml/Eng-USA/\")\n        \"\"\")\n\n\nif __name__ == \"__main__\":\n    demo()\n"], "nltk\\corpus\\reader\\chunked": [".py", "\n\nimport os.path, codecs\n\nfrom six import string_types\n\nimport nltk\nfrom nltk.corpus.reader.bracket_parse import BracketParseCorpusReader\nfrom nltk.tree import Tree\nfrom nltk.tokenize import *\nfrom nltk.chunk import tagstr2tree\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\nclass ChunkedCorpusReader(CorpusReader):\n    def __init__(self, root, fileids, extension='',\n                 str2chunktree=tagstr2tree,\n                 sent_tokenizer=RegexpTokenizer('\\n', gaps=True),\n                 para_block_reader=read_blankline_block,\n                 encoding='utf8', tagset=None):\n        CorpusReader.__init__(self, root, fileids, encoding)\n        self._cv_args = (str2chunktree, sent_tokenizer, para_block_reader, tagset)\n        :return: the given file(s) as a single string.\n        :rtype: str\n        \"\"\"\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n    def words(self, fileids=None):\n        \"\"\"\n        :return: the given file(s) as a list of words\n            and punctuation symbols.\n        :rtype: list(str)\n        \"\"\"\n        return concat([ChunkedCorpusView(f, enc, 0, 0, 0, 0, *self._cv_args)\n                       for (f, enc) in self.abspaths(fileids, True)])\n\n    def sents(self, fileids=None):\n        \"\"\"\n        :return: the given file(s) as a list of\n            sentences or utterances, each encoded as a list of word\n            strings.\n        :rtype: list(list(str))\n        \"\"\"\n        return concat([ChunkedCorpusView(f, enc, 0, 1, 0, 0, *self._cv_args)\n                       for (f, enc) in self.abspaths(fileids, True)])\n\n    def paras(self, fileids=None):\n        \"\"\"\n        :return: the given file(s) as a list of\n            paragraphs, each encoded as a list of sentences, which are\n            in turn encoded as lists of word strings.\n        :rtype: list(list(list(str)))\n        \"\"\"\n        return concat([ChunkedCorpusView(f, enc, 0, 1, 1, 0, *self._cv_args)\n                       for (f, enc) in self.abspaths(fileids, True)])\n\n    def tagged_words(self, fileids=None, tagset=None):\n        \"\"\"\n        :return: the given file(s) as a list of tagged\n            words and punctuation symbols, encoded as tuples\n            ``(word,tag)``.\n        :rtype: list(tuple(str,str))\n        \"\"\"\n        return concat([ChunkedCorpusView(f, enc, 1, 0, 0, 0, *self._cv_args, target_tagset=tagset)\n                       for (f, enc) in self.abspaths(fileids, True)])\n\n    def tagged_sents(self, fileids=None, tagset=None):\n        \"\"\"\n        :return: the given file(s) as a list of\n            sentences, each encoded as a list of ``(word,tag)`` tuples.\n\n        :rtype: list(list(tuple(str,str)))\n        \"\"\"\n        return concat([ChunkedCorpusView(f, enc, 1, 1, 0, 0, *self._cv_args, target_tagset=tagset)\n                       for (f, enc) in self.abspaths(fileids, True)])\n\n    def tagged_paras(self, fileids=None, tagset=None):\n        \"\"\"\n        :return: the given file(s) as a list of\n            paragraphs, each encoded as a list of sentences, which are\n            in turn encoded as lists of ``(word,tag)`` tuples.\n        :rtype: list(list(list(tuple(str,str))))\n        \"\"\"\n        return concat([ChunkedCorpusView(f, enc, 1, 1, 1, 0, *self._cv_args, target_tagset=tagset)\n                       for (f, enc) in self.abspaths(fileids, True)])\n\n    def chunked_words(self, fileids=None, tagset=None):\n        \"\"\"\n        :return: the given file(s) as a list of tagged\n            words and chunks.  Words are encoded as ``(word, tag)``\n            tuples (if the corpus has tags) or word strings (if the\n            corpus has no tags).  Chunks are encoded as depth-one\n            trees over ``(word,tag)`` tuples or word strings.\n        :rtype: list(tuple(str,str) and Tree)\n        \"\"\"\n        return concat([ChunkedCorpusView(f, enc, 1, 0, 0, 1, *self._cv_args, target_tagset=tagset)\n                       for (f, enc) in self.abspaths(fileids, True)])\n\n    def chunked_sents(self, fileids=None, tagset=None):\n        \"\"\"\n        :return: the given file(s) as a list of\n            sentences, each encoded as a shallow Tree.  The leaves\n            of these trees are encoded as ``(word, tag)`` tuples (if\n            the corpus has tags) or word strings (if the corpus has no\n            tags).\n        :rtype: list(Tree)\n        \"\"\"\n        return concat([ChunkedCorpusView(f, enc, 1, 1, 0, 1, *self._cv_args, target_tagset=tagset)\n                       for (f, enc) in self.abspaths(fileids, True)])\n\n    def chunked_paras(self, fileids=None, tagset=None):\n        \"\"\"\n        :return: the given file(s) as a list of\n            paragraphs, each encoded as a list of sentences, which are\n            in turn encoded as a shallow Tree.  The leaves of these\n            trees are encoded as ``(word, tag)`` tuples (if the corpus\n            has tags) or word strings (if the corpus has no tags).\n        :rtype: list(list(Tree))\n        \"\"\"\n        return concat([ChunkedCorpusView(f, enc, 1, 1, 1, 1, *self._cv_args, target_tagset=tagset)\n                       for (f, enc) in self.abspaths(fileids, True)])\n\n    def _read_block(self, stream):\n        return [tagstr2tree(t) for t in read_blankline_block(stream)]\n\nclass ChunkedCorpusView(StreamBackedCorpusView):\n    def __init__(self, fileid, encoding, tagged, group_by_sent,\n                 group_by_para, chunked, str2chunktree, sent_tokenizer,\n                 para_block_reader, source_tagset=None, target_tagset=None):\n        StreamBackedCorpusView.__init__(self, fileid, encoding=encoding)\n        self._tagged = tagged\n        self._group_by_sent = group_by_sent\n        self._group_by_para = group_by_para\n        self._chunked = chunked\n        self._str2chunktree = str2chunktree\n        self._sent_tokenizer = sent_tokenizer\n        self._para_block_reader = para_block_reader\n        self._source_tagset = source_tagset\n        self._target_tagset = target_tagset\n\n    def read_block(self, stream):\n        block = []\n        for para_str in self._para_block_reader(stream):\n            para = []\n            for sent_str in self._sent_tokenizer.tokenize(para_str):\n                sent = self._str2chunktree(sent_str, source_tagset=self._source_tagset,\n                                           target_tagset=self._target_tagset)\n\n                if not self._tagged:\n                    sent = self._untag(sent)\n\n                if not self._chunked:\n                    sent = sent.leaves()\n\n                if self._group_by_sent:\n                    para.append(sent)\n                else:\n                    para.extend(sent)\n\n            if self._group_by_para:\n                block.append(para)\n            else:\n                block.extend(para)\n\n        return block\n\n    def _untag(self, tree):\n        for i, child in enumerate(tree):\n            if isinstance(child, Tree):\n                self._untag(child)\n            elif isinstance(child, tuple):\n                tree[i] = child[0]\n            else:\n                raise ValueError('expected child to be Tree or tuple')\n        return tree\n"], "nltk\\corpus\\reader\\cmudict": [".py", "\n\nimport codecs\n\nfrom six import string_types\n\nfrom nltk import compat\nfrom nltk.util import Index\n\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\nclass CMUDictCorpusReader(CorpusReader):\n    def entries(self):\n        return concat([StreamBackedCorpusView(fileid, read_cmudict_block,\n                                              encoding=enc)\n                       for fileid, enc in self.abspaths(None, True)])\n\n    def raw(self):\n        fileids = self._fileids\n        if isinstance(fileids, string_types):\n            fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n    def words(self):\n        return [word.lower() for (word, _) in self.entries()]\n\n    def dict(self):\n        return dict(Index(self.entries()))\n\ndef read_cmudict_block(stream):\n    entries = []\n    while len(entries) < 100: # Read 100 at a time.\n        line = stream.readline()\n        if line == '': return entries # end of file.\n        pieces = line.split()\n        entries.append( (pieces[0].lower(), pieces[2:]) )\n    return entries\n"], "nltk\\corpus\\reader\\comparative_sents": [".py", "\nimport re\n\nfrom six import string_types\n\nfrom nltk.corpus.reader.api import *\nfrom nltk.tokenize import *\n\nSTARS = re.compile(r'^\\*+$')\nCOMPARISON = re.compile(r'<cs-[1234]>')\nCLOSE_COMPARISON = re.compile(r'</cs-[1234]>')\nGRAD_COMPARISON = re.compile(r'<cs-[123]>')\nNON_GRAD_COMPARISON = re.compile(r'<cs-4>')\nENTITIES_FEATS = re.compile(r\"(\\d)_((?:[\\.\\w\\s/-](?!\\d_))+)\")\nKEYWORD = re.compile(r'\\((?!.*\\()(.*)\\)$')\n\nclass Comparison(object):\n    def __init__(self, text=None, comp_type=None, entity_1=None, entity_2=None,\n                 feature=None, keyword=None):\n        self.text = text\n        self.comp_type = comp_type\n        self.entity_1 = entity_1\n        self.entity_2 = entity_2\n        self.feature = feature\n        self.keyword = keyword\n\n    def __repr__(self):\n        return (\"Comparison(text=\\\"{}\\\", comp_type={}, entity_1=\\\"{}\\\", entity_2=\\\"{}\\\", \"\n                \"feature=\\\"{}\\\", keyword=\\\"{}\\\")\").format(self.text, self.comp_type,\n                self.entity_1, self.entity_2, self.feature, self.keyword)\n\nclass ComparativeSentencesCorpusReader(CorpusReader):\n    CorpusView = StreamBackedCorpusView\n\n    def __init__(self, root, fileids, word_tokenizer=WhitespaceTokenizer(),\n                 sent_tokenizer=None, encoding='utf8'):\n\n        CorpusReader.__init__(self, root, fileids, encoding)\n        self._word_tokenizer = word_tokenizer\n        self._sent_tokenizer = sent_tokenizer\n\n    def comparisons(self, fileids=None):\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, string_types):\n            fileids = [fileids]\n        return concat([self.CorpusView(path, self._read_comparison_block, encoding=enc)\n            for (path, enc, fileid) in self.abspaths(fileids, True, True)])\n\n    def keywords(self, fileids=None):\n        all_keywords = concat([self.CorpusView(path, self._read_keyword_block, encoding=enc)\n                       for (path, enc, fileid)\n                       in self.abspaths(fileids, True, True)])\n\n        keywords_set = set([keyword.lower() for keyword in all_keywords if keyword])\n        return keywords_set\n\n    def keywords_readme(self):\n        keywords = []\n        raw_text = self.open(\"listOfkeywords.txt\").read()\n        for line in raw_text.split(\"\\n\"):\n            if not line or line.startswith(\"//\"):\n                continue\n            keywords.append(line.strip())\n        return keywords\n\n    def raw(self, fileids=None):\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, string_types):\n            fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n    def readme(self):\n        return self.open(\"README.txt\").read()\n\n    def sents(self, fileids=None):\n        return concat([self.CorpusView(path, self._read_sent_block, encoding=enc)\n            for (path, enc, fileid) in self.abspaths(fileids, True, True)])\n\n    def words(self, fileids=None):\n        return concat([self.CorpusView(path, self._read_word_block, encoding=enc)\n                       for (path, enc, fileid)\n                       in self.abspaths(fileids, True, True)])\n\n    def _read_comparison_block(self, stream):\n        while True:\n            line = stream.readline()\n            if not line:\n                return [] # end of file.\n            comparison_tags = re.findall(COMPARISON, line)\n            if comparison_tags:\n                grad_comparisons = re.findall(GRAD_COMPARISON, line)\n                non_grad_comparisons = re.findall(NON_GRAD_COMPARISON, line)\n                comparison_text = stream.readline().strip()\n                if self._word_tokenizer:\n                    comparison_text = self._word_tokenizer.tokenize(comparison_text)\n                stream.readline()\n                comparison_bundle = []\n                if grad_comparisons:\n                    for comp in grad_comparisons:\n                        comp_type = int(re.match(r'<cs-(\\d)>', comp).group(1))\n                        comparison = Comparison(text=comparison_text, comp_type=comp_type)\n                        line = stream.readline()\n                        entities_feats = ENTITIES_FEATS.findall(line)\n                        if entities_feats:\n                            for (code, entity_feat) in entities_feats:\n                                if code == '1':\n                                    comparison.entity_1 = entity_feat.strip()\n                                elif code == '2':\n                                    comparison.entity_2 = entity_feat.strip()\n                                elif code == '3':\n                                    comparison.feature = entity_feat.strip()\n                        keyword = KEYWORD.findall(line)\n                        if keyword:\n                            comparison.keyword = keyword[0]\n                        comparison_bundle.append(comparison)\n                if non_grad_comparisons:\n                    for comp in non_grad_comparisons:\n                        comp_type = int(re.match(r'<cs-(\\d)>', comp).group(1))\n                        comparison = Comparison(text=comparison_text, comp_type=comp_type)\n                        comparison_bundle.append(comparison)\n                return comparison_bundle\n\n    def _read_keyword_block(self, stream):\n        keywords = []\n        for comparison in self._read_comparison_block(stream):\n            keywords.append(comparison.keyword)\n        return keywords\n\n    def _read_sent_block(self, stream):\n        while True:\n            line = stream.readline()\n            if re.match(STARS, line):\n                while True:\n                    line = stream.readline()\n                    if re.match(STARS, line):\n                        break\n                continue\n            if not re.findall(COMPARISON, line) and not ENTITIES_FEATS.findall(line) \\\n            and not re.findall(CLOSE_COMPARISON, line):\n                if self._sent_tokenizer:\n                    return [self._word_tokenizer.tokenize(sent)\n                        for sent in self._sent_tokenizer.tokenize(line)]\n                else:\n                    return [self._word_tokenizer.tokenize(line)]\n\n    def _read_word_block(self, stream):\n        words = []\n        for sent in self._read_sent_block(stream):\n            words.extend(sent)\n        return words\n"], "nltk\\corpus\\reader\\conll": [".py", "\n\nfrom __future__ import unicode_literals\n\nimport os\nimport codecs\nimport textwrap\n\nfrom six import string_types\n\nfrom nltk import compat\nfrom nltk.tree import Tree\nfrom nltk.util import LazyMap, LazyConcatenation\nfrom nltk.tag import map_tag\n\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\nclass ConllCorpusReader(CorpusReader):\n\n\n    WORDS = 'words'   #: column type for words\n    POS = 'pos'       #: column type for part-of-speech tags\n    TREE = 'tree'     #: column type for parse trees\n    CHUNK = 'chunk'   #: column type for chunk structures\n    NE = 'ne'         #: column type for named entities\n    SRL = 'srl'       #: column type for semantic role labels\n    IGNORE = 'ignore' #: column type for column that should be ignored\n\n    COLUMN_TYPES = (WORDS, POS, TREE, CHUNK, NE, SRL, IGNORE)\n\n\n    def __init__(self, root, fileids, columntypes,\n                 chunk_types=None, root_label='S', pos_in_tree=False,\n                 srl_includes_roleset=True, encoding='utf8',\n                 tree_class=Tree, tagset=None, separator=None):\n        for columntype in columntypes:\n            if columntype not in self.COLUMN_TYPES:\n                raise ValueError('Bad column type %r' % columntype)\n        if isinstance(chunk_types, string_types):\n            chunk_types = [chunk_types]\n        self._chunk_types = chunk_types\n        self._colmap = dict((c,i) for (i,c) in enumerate(columntypes))\n        self._pos_in_tree = pos_in_tree\n        self._root_label = root_label # for chunks\n        self._srl_includes_roleset = srl_includes_roleset\n        self._tree_class = tree_class\n        CorpusReader.__init__(self, root, fileids, encoding)\n        self._tagset = tagset\n        self.sep = separator\n\n\n    def raw(self, fileids=None):\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n    def words(self, fileids=None):\n        self._require(self.WORDS)\n        return LazyConcatenation(LazyMap(self._get_words, self._grids(fileids)))\n\n    def sents(self, fileids=None):\n        self._require(self.WORDS)\n        return LazyMap(self._get_words, self._grids(fileids))\n\n    def tagged_words(self, fileids=None, tagset=None):\n        self._require(self.WORDS, self.POS)\n        def get_tagged_words(grid):\n            return self._get_tagged_words(grid, tagset)\n        return LazyConcatenation(LazyMap(get_tagged_words,\n                                         self._grids(fileids)))\n\n    def tagged_sents(self, fileids=None, tagset=None):\n        self._require(self.WORDS, self.POS)\n        def get_tagged_words(grid):\n            return self._get_tagged_words(grid, tagset)\n        return LazyMap(get_tagged_words, self._grids(fileids))\n\n    def chunked_words(self, fileids=None, chunk_types=None,\n                      tagset=None):\n        self._require(self.WORDS, self.POS, self.CHUNK)\n        if chunk_types is None: chunk_types = self._chunk_types\n        def get_chunked_words(grid): # capture chunk_types as local var\n            return self._get_chunked_words(grid, chunk_types, tagset)\n        return LazyConcatenation(LazyMap(get_chunked_words,\n                                         self._grids(fileids)))\n\n    def chunked_sents(self, fileids=None, chunk_types=None,\n                      tagset=None):\n        self._require(self.WORDS, self.POS, self.CHUNK)\n        if chunk_types is None: chunk_types = self._chunk_types\n        def get_chunked_words(grid): # capture chunk_types as local var\n            return self._get_chunked_words(grid, chunk_types, tagset)\n        return LazyMap(get_chunked_words, self._grids(fileids))\n\n    def parsed_sents(self, fileids=None, pos_in_tree=None, tagset=None):\n        self._require(self.WORDS, self.POS, self.TREE)\n        if pos_in_tree is None: pos_in_tree = self._pos_in_tree\n        def get_parsed_sent(grid): # capture pos_in_tree as local var\n            return self._get_parsed_sent(grid, pos_in_tree, tagset)\n        return LazyMap(get_parsed_sent, self._grids(fileids))\n\n    def srl_spans(self, fileids=None):\n        self._require(self.SRL)\n        return LazyMap(self._get_srl_spans, self._grids(fileids))\n\n    def srl_instances(self, fileids=None, pos_in_tree=None, flatten=True):\n        self._require(self.WORDS, self.POS, self.TREE, self.SRL)\n        if pos_in_tree is None: pos_in_tree = self._pos_in_tree\n        def get_srl_instances(grid): # capture pos_in_tree as local var\n            return self._get_srl_instances(grid, pos_in_tree)\n        result = LazyMap(get_srl_instances, self._grids(fileids))\n        if flatten: result = LazyConcatenation(result)\n        return result\n\n    def iob_words(self, fileids=None, tagset=None):\n        self._require(self.WORDS, self.POS, self.CHUNK)\n        def get_iob_words(grid):\n            return self._get_iob_words(grid, tagset)\n        return LazyConcatenation(LazyMap(get_iob_words, self._grids(fileids)))\n\n    def iob_sents(self, fileids=None, tagset=None):\n        self._require(self.WORDS, self.POS, self.CHUNK)\n        def get_iob_words(grid):\n            return self._get_iob_words(grid, tagset)\n        return LazyMap(get_iob_words, self._grids(fileids))\n\n\n    def _grids(self, fileids=None):\n        return concat([StreamBackedCorpusView(fileid, self._read_grid_block,\n                                              encoding=enc)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def _read_grid_block(self, stream):\n        grids = []\n        for block in read_blankline_block(stream):\n            block = block.strip()\n            if not block: continue\n\n            grid = [line.split(self.sep) for line in block.split('\\n')]\n\n            if grid[0][self._colmap.get('words', 0)] == '-DOCSTART-':\n                del grid[0]\n\n            for row in grid:\n                if len(row) != len(grid[0]):\n                    raise ValueError('Inconsistent number of columns:\\n%s'\n                                     % block)\n            grids.append(grid)\n        return grids\n\n\n    def _get_words(self, grid):\n        return self._get_column(grid, self._colmap['words'])\n\n    def _get_tagged_words(self, grid, tagset=None):\n        pos_tags = self._get_column(grid, self._colmap['pos'])\n        if tagset and tagset != self._tagset:\n            pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]\n        return list(zip(self._get_column(grid, self._colmap['words']), pos_tags))\n\n    def _get_iob_words(self, grid, tagset=None):\n        pos_tags = self._get_column(grid, self._colmap['pos'])\n        if tagset and tagset != self._tagset:\n            pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]\n        return list(zip(self._get_column(grid, self._colmap['words']), pos_tags,\n                   self._get_column(grid, self._colmap['chunk'])))\n\n    def _get_chunked_words(self, grid, chunk_types, tagset=None):\n        words = self._get_column(grid, self._colmap['words'])\n        pos_tags = self._get_column(grid, self._colmap['pos'])\n        if tagset and tagset != self._tagset:\n            pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]\n        chunk_tags = self._get_column(grid, self._colmap['chunk'])\n\n        stack = [Tree(self._root_label, [])]\n\n        for (word, pos_tag, chunk_tag) in zip(words, pos_tags, chunk_tags):\n            if chunk_tag == 'O':\n                state, chunk_type = 'O', ''\n            else:\n                (state, chunk_type) = chunk_tag.split('-')\n            if chunk_types is not None and chunk_type not in chunk_types:\n                state = 'O'\n            if state == 'I' and chunk_type != stack[-1].label():\n                state = 'B'\n            if state in 'BO' and len(stack) == 2:\n                stack.pop()\n            if state == 'B':\n                new_chunk = Tree(chunk_type, [])\n                stack[-1].append(new_chunk)\n                stack.append(new_chunk)\n            stack[-1].append((word, pos_tag))\n\n        return stack[0]\n\n    def _get_parsed_sent(self, grid, pos_in_tree, tagset=None):\n        words = self._get_column(grid, self._colmap['words'])\n        pos_tags = self._get_column(grid, self._colmap['pos'])\n        if tagset and tagset != self._tagset:\n            pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]\n        parse_tags = self._get_column(grid, self._colmap['tree'])\n\n        treestr = ''\n        for (word, pos_tag, parse_tag) in zip(words, pos_tags, parse_tags):\n            if word == '(': word = '-LRB-'\n            if word == ')': word = '-RRB-'\n            if pos_tag == '(': pos_tag = '-LRB-'\n            if pos_tag == ')': pos_tag = '-RRB-'\n            (left, right) = parse_tag.split('*')\n            right = right.count(')')*')' # only keep ')'.\n            treestr += '%s (%s %s) %s' % (left, pos_tag, word, right)\n        try:\n            tree = self._tree_class.fromstring(treestr)\n        except (ValueError, IndexError):\n            tree = self._tree_class.fromstring('(%s %s)' %\n                                          (self._root_label, treestr))\n\n        if not pos_in_tree:\n            for subtree in tree.subtrees():\n                for i, child in enumerate(subtree):\n                    if (isinstance(child, Tree) and len(child)==1 and\n                        isinstance(child[0], string_types)):\n                        subtree[i] = (child[0], child.label())\n\n        return tree\n\n    def _get_srl_spans(self, grid):\n        if self._srl_includes_roleset:\n            predicates = self._get_column(grid, self._colmap['srl']+1)\n            start_col = self._colmap['srl']+2\n        else:\n            predicates = self._get_column(grid, self._colmap['srl'])\n            start_col = self._colmap['srl']+1\n\n        num_preds = len([p for p in predicates if p != '-'])\n\n        spanlists = []\n        for i in range(num_preds):\n            col = self._get_column(grid, start_col+i)\n            spanlist = []\n            stack = []\n            for wordnum, srl_tag in enumerate(col):\n                (left, right) = srl_tag.split('*')\n                for tag in left.split('('):\n                    if tag:\n                        stack.append((tag, wordnum))\n                for i in range(right.count(')')):\n                    (tag, start) = stack.pop()\n                    spanlist.append( ((start, wordnum+1), tag) )\n            spanlists.append(spanlist)\n\n        return spanlists\n\n    def _get_srl_instances(self, grid, pos_in_tree):\n        tree = self._get_parsed_sent(grid, pos_in_tree)\n        spanlists = self._get_srl_spans(grid)\n        if self._srl_includes_roleset:\n            predicates = self._get_column(grid, self._colmap['srl']+1)\n            rolesets = self._get_column(grid, self._colmap['srl'])\n        else:\n            predicates = self._get_column(grid, self._colmap['srl'])\n            rolesets = [None] * len(predicates)\n\n        instances = ConllSRLInstanceList(tree)\n        for wordnum, predicate in enumerate(predicates):\n            if predicate == '-': continue\n            for spanlist in spanlists:\n                for (start, end), tag in spanlist:\n                    if wordnum in range(start,end) and tag in ('V', 'C-V'):\n                        break\n                else: continue\n                break\n            else:\n                raise ValueError('No srl column found for %r' % predicate)\n            instances.append(ConllSRLInstance(tree, wordnum, predicate,\n                                              rolesets[wordnum], spanlist))\n\n        return instances\n\n\n    def _require(self, *columntypes):\n        for columntype in columntypes:\n            if columntype not in self._colmap:\n                raise ValueError('This corpus does not contain a %s '\n                                 'column.' % columntype)\n\n    @staticmethod\n    def _get_column(grid, column_index):\n        return [grid[i][column_index] for i in range(len(grid))]\n\n\n@compat.python_2_unicode_compatible\nclass ConllSRLInstance(object):\n\n    def __init__(self, tree, verb_head, verb_stem, roleset, tagged_spans):\n        self.verb = []\n    Set of instances for a single sentence\n    \"\"\"\n    def __init__(self, tree, instances=()):\n        self.tree = tree\n        list.__init__(self, instances)\n\n    def __str__(self):\n        return self.pprint()\n\n    def pprint(self, include_tree=False):\n        for inst in self:\n            if inst.tree != self.tree:\n                raise ValueError('Tree mismatch!')\n\n        if include_tree:\n            words = self.tree.leaves()\n            pos = [None] * len(words)\n            synt = ['*'] * len(words)\n            self._tree2conll(self.tree, 0, words, pos, synt)\n\n        s = ''\n        for i in range(len(words)):\n            if include_tree:\n                s += '%-20s ' % words[i]\n                s += '%-8s ' % pos[i]\n                s += '%15s*%-8s ' % tuple(synt[i].split('*'))\n\n            for inst in self:\n                if i == inst.verb_head:\n                    s += '%-20s ' % inst.verb_stem\n                    break\n            else:\n                s += '%-20s ' % '-'\n            for inst in self:\n                argstr = '*'\n                for (start, end), argid in inst.tagged_spans:\n                    if i==start: argstr = '(%s%s' % (argid, argstr)\n                    if i==(end-1): argstr += ')'\n                s += '%-12s ' % argstr\n            s += '\\n'\n        return s\n\n    def _tree2conll(self, tree, wordnum, words, pos, synt):\n        assert isinstance(tree, Tree)\n        if len(tree) == 1 and isinstance(tree[0], string_types):\n            pos[wordnum] = tree.label()\n            assert words[wordnum] == tree[0]\n            return wordnum+1\n        elif len(tree) == 1 and isinstance(tree[0], tuple):\n            assert len(tree[0]) == 2\n            pos[wordnum], pos[wordnum] = tree[0]\n            return wordnum+1\n        else:\n            synt[wordnum] = '(%s%s' % (tree.label(), synt[wordnum])\n            for child in tree:\n                wordnum = self._tree2conll(child, wordnum, words,\n                                                  pos, synt)\n            synt[wordnum-1] += ')'\n            return wordnum\n\nclass ConllChunkCorpusReader(ConllCorpusReader):\n    \"\"\"\n    A ConllCorpusReader whose data file contains three columns: words,\n    pos, and chunk.\n    \"\"\"\n    def __init__(self, root, fileids, chunk_types, encoding='utf8',\n                 tagset=None, separator=None):\n        ConllCorpusReader.__init__(\n            self, root, fileids, ('words', 'pos', 'chunk'),\n            chunk_types=chunk_types, encoding=encoding,\n            tagset=tagset, separator=separator)\n"], "nltk\\corpus\\reader\\crubadan": [".py", "\n\nfrom __future__ import print_function, unicode_literals\n\nimport re\nfrom nltk.compat import PY3\nfrom os import path\nfrom nltk.corpus.reader import CorpusReader\nfrom nltk.probability import FreqDist\nfrom nltk.data import ZipFilePathPointer\n\nclass CrubadanCorpusReader(CorpusReader):\n    \n    _LANG_MAPPER_FILE = 'table.txt'\n    _all_lang_freq = {}\n    \n    def __init__(self, root, fileids, encoding='utf8', tagset=None):\n        super(CrubadanCorpusReader, self).__init__(root, fileids, encoding='utf8')\n        self._lang_mapping_data = []\n        self._load_lang_mapping_data()\n        \n    def lang_freq(self, lang):\n        ''' Return n-gram FreqDist for a specific language\n            given ISO 639-3 language code '''\n        \n        if lang not in self._all_lang_freq:\n            self._all_lang_freq[lang] = self._load_lang_ngrams(lang)\n\n        return self._all_lang_freq[lang]\n    \n    def langs(self):\n        ''' Return a list of supported languages as ISO 639-3 codes '''\n        return [row[1] for row in self._lang_mapping_data]\n            \n    def iso_to_crubadan(self, lang):\n        ''' Return internal Crubadan code based on ISO 639-3 code '''\n        for i in self._lang_mapping_data:\n            if i[1].lower() == lang.lower():\n                return i[0]\n    \n    def crubadan_to_iso(self, lang):\n        ''' Return ISO 639-3 code given internal Crubadan code '''\n        for i in self._lang_mapping_data:\n            if i[0].lower() == lang.lower():\n                return i[1]\n    \n    def _load_lang_mapping_data(self):\n        ''' Load language mappings between codes and description from table.txt '''\n        if isinstance(self.root, ZipFilePathPointer):\n            raise RuntimeError(\"Please install the 'crubadan' corpus first, use nltk.download()\")\n        \n        mapper_file = path.join(self.root, self._LANG_MAPPER_FILE)\n        if self._LANG_MAPPER_FILE not in self.fileids():\n            raise RuntimeError(\"Could not find language mapper file: \" + mapper_file)\n\n        if PY3:\n            raw = open(mapper_file, 'r', encoding='utf-8').read().strip()\n        else:\n            raw = open(mapper_file, 'rU').read().decode('utf-8').strip()\n\n        self._lang_mapping_data = [row.split('\\t') for row in raw.split('\\n')]\n        \n    def _load_lang_ngrams(self, lang):\n        ''' Load single n-gram language file given the ISO 639-3 language code\n            and return its FreqDist '''\n\n        if lang not in self.langs():\n            raise RuntimeError(\"Unsupported language.\")\n\n        crubadan_code = self.iso_to_crubadan(lang)\n        ngram_file = path.join(self.root, crubadan_code + '-3grams.txt')\n\n        if not path.isfile(ngram_file):\n            raise RuntimeError(\"No N-gram file found for requested language.\")\n\n        counts = FreqDist()\n        if PY3:\n            f = open(ngram_file, 'r', encoding='utf-8')\n        else:\n            f = open(ngram_file, 'rU')\n\n        for line in f:\n            if PY3:\n                data = line.split(' ')\n            else:\n                data = line.decode('utf8').split(' ')\n\n            ngram = data[1].strip('\\n')\n            freq = int(data[0])\n            \n            counts[ngram] = freq\n            \n        return counts\n        \n"], "nltk\\corpus\\reader\\dependency": [".py", "\nimport codecs\n\nfrom nltk.parse import DependencyGraph\nfrom nltk.tokenize import *\n\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\nclass DependencyCorpusReader(SyntaxCorpusReader):\n\n    def __init__(self, root, fileids, encoding='utf8',\n                 word_tokenizer=TabTokenizer(),\n                 sent_tokenizer=RegexpTokenizer('\\n', gaps=True),\n                 para_block_reader=read_blankline_block):\n\n        CorpusReader.__init__(self, root, fileids, encoding)\n\n\n    def raw(self, fileids=None):\n        result = []\n        for fileid, encoding in self.abspaths(fileids, include_encoding=True):\n            if isinstance(fileid, PathPointer):\n                result.append(fileid.open(encoding=encoding).read())\n            else:\n                with codecs.open(fileid, \"r\", encoding) as fp:\n                    result.append(fp.read())\n        return concat(result)\n\n    def words(self, fileids=None):\n        return concat([DependencyCorpusView(fileid, False, False, False, encoding=enc)\n                       for fileid, enc in self.abspaths(fileids, include_encoding=True)])\n\n    def tagged_words(self, fileids=None):\n        return concat([DependencyCorpusView(fileid, True, False, False, encoding=enc)\n                       for fileid, enc in self.abspaths(fileids, include_encoding=True)])\n\n    def sents(self, fileids=None):\n        return concat([DependencyCorpusView(fileid, False, True, False, encoding=enc)\n                       for fileid, enc in self.abspaths(fileids, include_encoding=True)])\n\n    def tagged_sents(self, fileids=None):\n            return concat([DependencyCorpusView(fileid, True, True, False, encoding=enc)\n                           for fileid, enc in self.abspaths(fileids, include_encoding=True)])\n\n    def parsed_sents(self, fileids=None):\n        sents=concat([DependencyCorpusView(fileid, False, True, True, encoding=enc)\n                      for fileid, enc in self.abspaths(fileids, include_encoding=True)])\n        return [DependencyGraph(sent) for sent in sents]\n\n\nclass DependencyCorpusView(StreamBackedCorpusView):\n    _DOCSTART = '-DOCSTART- -DOCSTART- O\\n' #dokumentu hasiera definitzen da\n\n    def __init__(self, corpus_file, tagged, group_by_sent, dependencies,\n                 chunk_types=None, encoding='utf8'):\n        self._tagged = tagged\n        self._dependencies = dependencies\n        self._group_by_sent = group_by_sent\n        self._chunk_types = chunk_types\n        StreamBackedCorpusView.__init__(self, corpus_file, encoding=encoding)\n\n    def read_block(self, stream):\n        sent = read_blankline_block(stream)[0].strip()\n        if sent.startswith(self._DOCSTART):\n            sent = sent[len(self._DOCSTART):].lstrip()\n\n        if not self._dependencies:\n            lines = [line.split('\\t') for line in sent.split('\\n')]\n            if len(lines[0]) == 3 or len(lines[0]) == 4:\n                sent = [(line[0], line[1]) for line in lines]\n            elif len(lines[0]) == 10:\n                sent = [(line[1], line[4]) for line in lines]\n            else:\n                raise ValueError('Unexpected number of fields in dependency tree file')\n\n            if not self._tagged:\n                sent = [word for (word, tag) in sent]\n\n        if self._group_by_sent:\n            return [sent]\n        else:\n            return list(sent)\n"], "nltk\\corpus\\reader\\framenet": [".py", "\n\nfrom __future__ import print_function, unicode_literals\n\nimport os, sys\nimport re\nimport textwrap\nimport itertools\nimport types\n\nfrom six import string_types, text_type\nfrom six.moves import zip_longest\n\nfrom collections import defaultdict, OrderedDict\nfrom operator import itemgetter\nfrom pprint import pprint, pformat\nfrom nltk.internals import ElementWrapper\nfrom nltk.corpus.reader import XMLCorpusReader, XMLCorpusView\nfrom nltk.compat import python_2_unicode_compatible\nfrom nltk.util import AbstractLazySequence, LazyConcatenation, LazyMap, LazyIteratorList\n\n__docformat__ = 'epytext en'\n\ndef mimic_wrap(lines, wrap_at=65, **kwargs):\n    l0 = textwrap.fill(lines[0], wrap_at, drop_whitespace=False).split('\\n')\n    yield l0\n\n    def _(line):\n        il0 = 0\n        while line and il0<len(l0)-1:\n            yield line[:len(l0[il0])]\n            line = line[len(l0[il0]):]\n            il0 += 1\n        if line: # Remaining stuff on this line past the end of the mimicked line.\n            for ln in textwrap.fill(line, wrap_at, drop_whitespace=False).split('\\n'):\n                yield ln\n\n    for l in lines[1:]:\n        yield list(_(l))\n\ndef _pretty_longstring(defstr, prefix='', wrap_at=65):\n\n\n    outstr = \"\"\n    for line in textwrap.fill(defstr, wrap_at).split('\\n'):\n        outstr += prefix + line + '\\n'\n    return outstr\n\ndef _pretty_any(obj):\n\n\n    outstr = \"\"\n    for k in obj:\n        if isinstance(obj[k], string_types) and len(obj[k]) > 65:\n            outstr += \"[{0}]\\n\".format(k)\n            outstr += \"{0}\".format(_pretty_longstring(obj[k], prefix='  '))\n            outstr += '\\n'\n        else:\n            outstr += \"[{0}] {1}\\n\".format(k, obj[k])\n\n    return outstr\n\ndef _pretty_semtype(st):\n\n\n    semkeys = st.keys()\n    if len(semkeys) == 1: return \"<None>\"\n\n    outstr = \"\"\n    outstr += \"semantic type ({0.ID}): {0.name}\\n\".format(st)\n    if 'abbrev' in semkeys:\n        outstr += \"[abbrev] {0}\\n\".format(st.abbrev)\n    if 'definition' in semkeys:\n        outstr += \"[definition]\\n\"\n        outstr += _pretty_longstring(st.definition,'  ')\n    outstr += \"[rootType] {0}({1})\\n\".format(st.rootType.name, st.rootType.ID)\n    if st.superType is None:\n        outstr += \"[superType] <None>\\n\"\n    else:\n        outstr += \"[superType] {0}({1})\\n\".format(st.superType.name, st.superType.ID)\n    outstr += \"[subTypes] {0} subtypes\\n\".format(len(st.subTypes))\n    outstr += \"  \" + \", \".join('{0}({1})'.format(x.name, x.ID) for x in st.subTypes) + '\\n'*(len(st.subTypes)>0)\n    return outstr\n\ndef _pretty_frame_relation_type(freltyp):\n\n    outstr = \"<frame relation type ({0.ID}): {0.superFrameName} -- {0.name} -> {0.subFrameName}>\".format(freltyp)\n    return outstr\n\ndef _pretty_frame_relation(frel):\n\n    outstr = \"<{0.type.superFrameName}={0.superFrameName} -- {0.type.name} -> {0.type.subFrameName}={0.subFrameName}>\".format(frel)\n    return outstr\n\ndef _pretty_fe_relation(ferel):\n\n    outstr = \"<{0.type.superFrameName}={0.frameRelation.superFrameName}.{0.superFEName} -- {0.type.name} -> {0.type.subFrameName}={0.frameRelation.subFrameName}.{0.subFEName}>\".format(ferel)\n    return outstr\n\ndef _pretty_lu(lu):\n\n\n    lukeys = lu.keys()\n    outstr = \"\"\n    outstr += \"lexical unit ({0.ID}): {0.name}\\n\\n\".format(lu)\n    if 'definition' in lukeys:\n        outstr += \"[definition]\\n\"\n        outstr += _pretty_longstring(lu.definition,'  ')\n    if 'frame' in lukeys:\n        outstr += \"\\n[frame] {0}({1})\\n\".format(lu.frame.name,lu.frame.ID)\n    if 'incorporatedFE' in lukeys:\n        outstr += \"\\n[incorporatedFE] {0}\\n\".format(lu.incorporatedFE)\n    if 'POS' in lukeys:\n        outstr += \"\\n[POS] {0}\\n\".format(lu.POS)\n    if 'status' in lukeys:\n        outstr += \"\\n[status] {0}\\n\".format(lu.status)\n    if 'totalAnnotated' in lukeys:\n        outstr += \"\\n[totalAnnotated] {0} annotated examples\\n\".format(lu.totalAnnotated)\n    if 'lexemes' in lukeys:\n        outstr += \"\\n[lexemes] {0}\\n\".format(' '.join('{0}/{1}'.format(lex.name,lex.POS) for lex in lu.lexemes))\n    if 'semTypes' in lukeys:\n        outstr += \"\\n[semTypes] {0} semantic types\\n\".format(len(lu.semTypes))\n        outstr += \"  \"*(len(lu.semTypes)>0) + \", \".join('{0}({1})'.format(x.name, x.ID) for x in lu.semTypes) + '\\n'*(len(lu.semTypes)>0)\n    if 'URL' in lukeys:\n        outstr += \"\\n[URL] {0}\\n\".format(lu.URL)\n    if 'subCorpus' in lukeys:\n        subc = [x.name for x in lu.subCorpus]\n        outstr += \"\\n[subCorpus] {0} subcorpora\\n\".format(len(lu.subCorpus))\n        for line in textwrap.fill(\", \".join(sorted(subc)), 60).split('\\n'):\n            outstr += \"  {0}\\n\".format(line)\n    if 'exemplars' in lukeys:\n        outstr += \"\\n[exemplars] {0} sentences across all subcorpora\\n\".format(len(lu.exemplars))\n\n    return outstr\n\ndef _pretty_exemplars(exemplars, lu):\n\n    outstr = \"\"\n    outstr += \"exemplar sentences for {0.name} in {0.frame.name}:\\n\\n\".format(lu)\n    for i,sent in enumerate(exemplars):\n        outstr += \"[{0}] {1}\\n\".format(i, sent.text)\n    outstr += \"\\n\"\n    return outstr\n\ndef _pretty_fulltext_sentences(sents):\n\n    outstr = \"\"\n    outstr += \"full-text document ({0.ID}) {0.name}:\\n\\n\".format(sents)\n    outstr += \"[corpid] {0.corpid}\\n[corpname] {0.corpname}\\n[description] {0.description}\\n[URL] {0.URL}\\n\\n\".format(sents)\n    outstr += \"[sentence]\\n\".format(sents)\n    for i,sent in enumerate(sents.sentence):\n        outstr += \"[{0}] {1}\\n\".format(i, sent.text)\n    outstr += \"\\n\"\n    return outstr\n\ndef _pretty_fulltext_sentence(sent):\n\n    outstr = \"\"\n    outstr += \"full-text sentence ({0.ID}) in {1}:\\n\\n\".format(sent, sent.doc.get('name',sent.doc.description))\n    outstr += \"\\n[POS] {0} tags\\n\".format(len(sent.POS))\n    outstr += \"\\n[POS_tagset] {0}\\n\\n\".format(sent.POS_tagset)\n    outstr += \"[text] + [annotationSet]\\n\\n\"\n    outstr += sent._ascii() # -> _annotation_ascii()\n    outstr += \"\\n\"\n    return outstr\n\ndef _pretty_pos(aset):\n\n    outstr = \"\"\n    outstr += \"POS annotation set ({0.ID}) {0.POS_tagset} in sentence {0.sent.ID}:\\n\\n\".format(aset)\n\n    overt = sorted(aset.POS)\n\n    sent = aset.sent\n    s0 = sent.text\n    s1 = ''\n    s2 = ''\n    i = 0\n    adjust = 0\n    for j,k,lbl in overt:\n        assert j>=i,('Overlapping targets?',(j,k,lbl))\n        s1 += ' '*(j-i) + '-'*(k-j)\n        if len(lbl)>(k-j):\n            amt = len(lbl)-(k-j)\n            s0 = s0[:k+adjust]+ '~'*amt + s0[k+adjust:] # '~' to prevent line wrapping\n            s1 = s1[:k+adjust]+ ' '*amt + s1[k+adjust:]\n            adjust += amt\n        s2 += ' '*(j-i) + lbl.ljust(k-j)\n        i = k\n\n    long_lines = [s0, s1, s2]\n\n    outstr += '\\n\\n'.join(map('\\n'.join, zip_longest(*mimic_wrap(long_lines), fillvalue=' '))).replace('~',' ')\n    outstr += \"\\n\"\n    return outstr\n\ndef _pretty_annotation(sent, aset_level=False):\n\n    sentkeys = sent.keys()\n    outstr = \"annotation set\" if aset_level else \"exemplar sentence\"\n    outstr += \" ({0.ID}):\\n\".format(sent)\n    if aset_level: # TODO: any UNANN exemplars?\n        outstr += \"\\n[status] {0}\\n\".format(sent.status)\n    for k in ('corpID', 'docID', 'paragNo', 'sentNo', 'aPos'):\n        if k in sentkeys:\n            outstr += \"[{0}] {1}\\n\".format(k, sent[k])\n    outstr += \"\\n[LU] ({0.ID}) {0.name} in {0.frame.name}\\n\".format(sent.LU) if sent.LU else '\\n[LU] Not found!'\n    outstr += \"\\n[frame] ({0.ID}) {0.name}\\n\".format(sent.frame)    # redundant with above, but .frame is convenient\n    if not aset_level:\n        outstr += \"\\n[annotationSet] {0} annotation sets\\n\".format(len(sent.annotationSet))\n        outstr += \"\\n[POS] {0} tags\\n\".format(len(sent.POS))\n        outstr += \"\\n[POS_tagset] {0}\\n\".format(sent.POS_tagset)\n    outstr += \"\\n[GF] {0} relation{1}\\n\".format(len(sent.GF), \"s\" if len(sent.GF)!=1 else \"\")\n    outstr += \"\\n[PT] {0} phrase{1}\\n\".format(len(sent.PT), \"s\" if len(sent.PT)!=1 else \"\")\n    for lyr in ('NER', 'WSL', 'Other', 'Sent'):\n        if lyr in sent and sent[lyr]:\n            outstr += \"\\n[{0}] {1} entr{2}\\n\".format(lyr, len(sent[lyr]), \"ies\" if len(sent[lyr])!=1 else \"y\")\n    outstr += \"\\n[text] + [Target] + [FE]\"\n    for lyr in ('Verb', 'Noun', 'Adj', 'Adv', 'Prep', 'Scon', 'Art'):\n        if lyr in sent and sent[lyr]:\n            outstr += \" + [{0}]\".format(lyr)\n    if 'FE2' in sentkeys:\n        outstr += \" + [FE2]\"\n        if 'FE3' in sentkeys:\n            outstr += \" + [FE3]\"\n    outstr += \"\\n\\n\"\n    outstr += sent._ascii() # -> _annotation_ascii()\n    outstr += \"\\n\"\n\n    return outstr\n\ndef _annotation_ascii(sent):\n    '''\n    Given a sentence or FE annotation set, construct the width-limited string showing\n    an ASCII visualization of the sentence's annotations, calling either\n    _annotation_ascii_frames() or _annotation_ascii_FEs() as appropriate.\n    This will be attached as a method to appropriate AttrDict instances\n    and called in the full pretty-printing of the instance.\n    '''\n    if sent._type=='fulltext_sentence' or ('annotationSet' in sent and len(sent.annotationSet)>2):\n        return _annotation_ascii_frames(sent)\n    else:   # an FE annotation set, or an LU sentence with 1 target\n        return _annotation_ascii_FEs(sent)\n\ndef _annotation_ascii_frames(sent):\n    '''\n    ASCII string rendering of the sentence along with its targets and frame names.\n    Called for all full-text sentences, as well as the few LU sentences with multiple\n    targets (e.g., fn.lu(6412).exemplars[82] has two want.v targets).\n    Line-wrapped to limit the display width.\n    '''\n    overt = []\n    for a,aset in enumerate(sent.annotationSet[1:]):\n        for j,k in aset.Target:\n            indexS = \"[{0}]\".format(a+1)\n            if aset.status=='UNANN' or aset.LU.status=='Problem':\n                indexS += \" \"\n                if aset.status=='UNANN':\n                    indexS += \"!\" # warning indicator that there is a frame annotation but no FE annotation\n                if aset.LU.status=='Problem':\n                    indexS += \"?\" # warning indicator that there is a missing LU definition (because the LU has Problem status)\n            overt.append((j,k,aset.LU.frame.name,indexS))\n    overt = sorted(overt)\n\n    duplicates = set()\n    for o,(j,k,fname,asetIndex) in enumerate(overt):\n        if o>0 and j<=overt[o-1][1]:\n            if overt[o-1][:2]==(j,k) and overt[o-1][2]==fname:    # same target, same frame\n                combinedIndex = overt[o-1][3] + asetIndex    # e.g., '[1][2]', '[1]! [2]'\n                combinedIndex = combinedIndex.replace(' !', '! ').replace(' ?', '? ')\n                overt[o-1] = overt[o-1][:3]+(combinedIndex,)\n                duplicates.add(o)\n            else:   # different frames, same or overlapping targets\n                s = sent.text\n                for j,k,fname,asetIndex in overt:\n                    s += '\\n' + asetIndex + ' ' + sent.text[j:k] + ' :: ' + fname\n                s += '\\n(Unable to display sentence with targets marked inline due to overlap)'\n                return s\n    for o in reversed(sorted(duplicates)):\n        del overt[o]\n\n    s0 = sent.text\n    s1 = ''\n    s11 = ''\n    s2 = ''\n    i = 0\n    adjust = 0\n    fAbbrevs = OrderedDict()\n    for j,k,fname,asetIndex in overt:\n        if not j>=i:\n            assert j>=i,('Overlapping targets?'+(' UNANN' if any(aset.status=='UNANN' for aset in sent.annotationSet[1:]) else ''),(j,k,asetIndex))\n        s1 += ' '*(j-i) + '*'*(k-j)\n        short = fname[:k-j]\n        if (k-j)<len(fname):\n            r = 0\n            while short in fAbbrevs:\n                if fAbbrevs[short]==fname:\n                    break\n                r += 1\n                short = fname[:k-j-1] + str(r)\n            else:   # short not in fAbbrevs\n                fAbbrevs[short] = fname\n        s11 += ' '*(j-i) + short.ljust(k-j)\n        if len(asetIndex)>(k-j):\n            amt = len(asetIndex)-(k-j)\n            s0 = s0[:k+adjust]+ '~'*amt + s0[k+adjust:] # '~' to prevent line wrapping\n            s1 = s1[:k+adjust]+ ' '*amt + s1[k+adjust:]\n            s11 = s11[:k+adjust]+ ' '*amt + s11[k+adjust:]\n            adjust += amt\n        s2 += ' '*(j-i) + asetIndex.ljust(k-j)\n        i = k\n\n    long_lines = [s0, s1, s11, s2]\n\n    outstr = '\\n\\n'.join(map('\\n'.join, zip_longest(*mimic_wrap(long_lines), fillvalue=' '))).replace('~',' ')\n    outstr += '\\n'\n    if fAbbrevs:\n        outstr += ' ('+', '.join('='.join(pair) for pair in fAbbrevs.items())+')'\n        assert len(fAbbrevs)==len(dict(fAbbrevs)),'Abbreviation clash'\n\n    return outstr\n\ndef _annotation_ascii_FE_layer(overt, ni, feAbbrevs):\n    '''Helper for _annotation_ascii_FEs().'''\n    s1 = ''\n    s2 = ''\n    i = 0\n    for j,k,fename in overt:\n        s1 += ' '*(j-i) + ('^' if fename.islower() else '-')*(k-j)\n        short = fename[:k-j]\n        if len(fename)>len(short):\n            r = 0\n            while short in feAbbrevs:\n                if feAbbrevs[short]==fename:\n                    break\n                r += 1\n                short = fename[:k-j-1] + str(r)\n            else:   # short not in feAbbrevs\n                feAbbrevs[short] = fename\n        s2 += ' '*(j-i) + short.ljust(k-j)\n        i = k\n\n    sNI = ''\n    if ni:\n        sNI += ' ['+', '.join(':'.join(x) for x in sorted(ni.items()))+']'\n    return [s1,s2,sNI]\n\ndef _annotation_ascii_FEs(sent):\n    '''\n    ASCII string rendering of the sentence along with a single target and its FEs.\n    Secondary and tertiary FE layers are included if present.\n    'sent' can be an FE annotation set or an LU sentence with a single target.\n    Line-wrapped to limit the display width.\n    '''\n    feAbbrevs = OrderedDict()\n    posspec = []    # POS-specific layer spans (e.g., Supp[ort], Cop[ula])\n    posspec_separate = False\n    for lyr in ('Verb', 'Noun', 'Adj', 'Adv', 'Prep', 'Scon', 'Art'):\n        if lyr in sent and sent[lyr]:\n            for a,b,lbl in sent[lyr]:\n                if lbl=='X': # skip this, which covers an entire phrase typically containing the target and all its FEs\n                    continue\n                if any(1 for x,y,felbl in sent.FE[0] if x<=a<y or a<=x<b):\n                    posspec_separate = True # show POS-specific layers on a separate line\n                posspec.append((a,b,lbl.lower().replace('-',''))) # lowercase Cop=>cop, Non-Asp=>nonasp, etc. to distinguish from FE names\n    if posspec_separate:\n        POSSPEC = _annotation_ascii_FE_layer(posspec, {}, feAbbrevs)\n    FE1 = _annotation_ascii_FE_layer(sorted(sent.FE[0] + (posspec if not posspec_separate else [])), sent.FE[1], feAbbrevs)\n    FE2 = FE3 = None\n    if 'FE2' in sent:\n        FE2 = _annotation_ascii_FE_layer(sent.FE2[0], sent.FE2[1], feAbbrevs)\n        if 'FE3' in sent:\n            FE3 = _annotation_ascii_FE_layer(sent.FE3[0], sent.FE3[1], feAbbrevs)\n\n    for i,j in sent.Target:\n        FE1span, FE1name, FE1exp = FE1\n        if len(FE1span)<j:\n            FE1span += ' '*(j-len(FE1span))\n        if len(FE1name)<j:\n            FE1name += ' '*(j-len(FE1name))\n            FE1[1] = FE1name\n        FE1[0] = FE1span[:i] + FE1span[i:j].replace(' ','*').replace('-','=') + FE1span[j:]\n    long_lines = [sent.text]\n    if posspec_separate:\n        long_lines.extend(POSSPEC[:2])\n    long_lines.extend([FE1[0], FE1[1]+FE1[2]]) # lines with no length limit\n    if FE2:\n        long_lines.extend([FE2[0], FE2[1]+FE2[2]])\n        if FE3:\n            long_lines.extend([FE3[0], FE3[1]+FE3[2]])\n    long_lines.append('')\n    outstr = '\\n'.join(map('\\n'.join, zip_longest(*mimic_wrap(long_lines), fillvalue=' ')))\n    if feAbbrevs:\n        outstr += '('+', '.join('='.join(pair) for pair in feAbbrevs.items())+')'\n        assert len(feAbbrevs)==len(dict(feAbbrevs)),'Abbreviation clash'\n    outstr += \"\\n\"\n\n    return outstr\n\ndef _pretty_fe(fe):\n\n    fekeys = fe.keys()\n    outstr = \"\"\n    outstr += \"frame element ({0.ID}): {0.name}\\n    of {1.name}({1.ID})\\n\".format(fe, fe.frame)\n    if 'definition' in fekeys:\n        outstr += \"[definition]\\n\"\n        outstr += _pretty_longstring(fe.definition,'  ')\n    if 'abbrev' in fekeys:\n        outstr += \"[abbrev] {0}\\n\".format(fe.abbrev)\n    if 'coreType' in fekeys:\n        outstr += \"[coreType] {0}\\n\".format(fe.coreType)\n    if 'requiresFE' in fekeys:\n        outstr += \"[requiresFE] \"\n        if fe.requiresFE is None:\n            outstr += \"<None>\\n\"\n        else:\n            outstr += \"{0}({1})\\n\".format(fe.requiresFE.name, fe.requiresFE.ID)\n    if 'excludesFE' in fekeys:\n        outstr += \"[excludesFE] \"\n        if fe.excludesFE is None:\n            outstr += \"<None>\\n\"\n        else:\n            outstr += \"{0}({1})\\n\".format(fe.excludesFE.name, fe.excludesFE.ID)\n    if 'semType' in fekeys:\n        outstr += \"[semType] \"\n        if fe.semType is None:\n            outstr += \"<None>\\n\"\n        else:\n            outstr += \"\\n  \" + \"{0}({1})\".format(fe.semType.name, fe.semType.ID) + '\\n'\n\n    return outstr\n\ndef _pretty_frame(frame):\n\n\n    outstr = \"\"\n    outstr += \"frame ({0.ID}): {0.name}\\n\\n\".format(frame)\n    outstr += \"[URL] {0}\\n\\n\".format(frame.URL)\n    outstr += \"[definition]\\n\"\n    outstr += _pretty_longstring(frame.definition, '  ') + '\\n'\n\n    outstr += \"[semTypes] {0} semantic types\\n\".format(len(frame.semTypes))\n    outstr += \"  \"*(len(frame.semTypes)>0) + \", \".join(\"{0}({1})\".format(x.name, x.ID) for x in frame.semTypes) + '\\n'*(len(frame.semTypes)>0)\n\n    outstr += \"\\n[frameRelations] {0} frame relations\\n\".format(len(frame.frameRelations))\n    outstr += '  ' + '\\n  '.join(repr(frel) for frel in frame.frameRelations) + '\\n'\n\n    outstr += \"\\n[lexUnit] {0} lexical units\\n\".format(len(frame.lexUnit))\n    lustrs = []\n    for luName,lu in sorted(frame.lexUnit.items()):\n        tmpstr = '{0} ({1})'.format(luName, lu.ID)\n        lustrs.append(tmpstr)\n    outstr += \"{0}\\n\".format(_pretty_longstring(', '.join(lustrs),prefix='  '))\n\n    outstr += \"\\n[FE] {0} frame elements\\n\".format(len(frame.FE))\n    fes = {}\n    for feName,fe in sorted(frame.FE.items()):\n        try:\n            fes[fe.coreType].append(\"{0} ({1})\".format(feName, fe.ID))\n        except KeyError:\n            fes[fe.coreType] = []\n            fes[fe.coreType].append(\"{0} ({1})\".format(feName, fe.ID))\n    for ct in sorted(fes.keys(), key=lambda ct2: ['Core','Core-Unexpressed','Peripheral','Extra-Thematic'].index(ct2)):\n        outstr += \"{0:>16}: {1}\\n\".format(ct, ', '.join(sorted(fes[ct])))\n\n    outstr += \"\\n[FEcoreSets] {0} frame element core sets\\n\".format(len(frame.FEcoreSets))\n    outstr += \"  \" + '\\n  '.join(\", \".join([x.name for x in coreSet]) for coreSet in frame.FEcoreSets) + '\\n'\n\n    return outstr\n\nclass FramenetError(Exception):\n\n\n@python_2_unicode_compatible\nclass AttrDict(dict):\n\n\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n\n    def __setattr__(self, name, value):\n        self[name] = value\n    def __getattr__(self, name):\n        if name=='_short_repr':\n            return self._short_repr\n        return self[name]\n    def __getitem__(self, name):\n        v = super(AttrDict,self).__getitem__(name)\n        if isinstance(v,Future):\n            return v._data()\n        return v\n\n    def _short_repr(self):\n        if '_type' in self:\n            if self['_type'].endswith('relation'):\n                return self.__repr__()\n            try:\n                return \"<{0} ID={1} name={2}>\".format(self['_type'], self['ID'], self['name'])\n            except KeyError:\n                try:    # no ID--e.g., for _type=lusubcorpus\n                    return \"<{0} name={1}>\".format(self['_type'], self['name'])\n                except KeyError:    # no name--e.g., for _type=lusentence\n                    return \"<{0} ID={1}>\".format(self['_type'], self['ID'])\n        else:\n            return self.__repr__()\n\n    def _str(self):\n        outstr = \"\"\n\n        if not '_type' in self:\n            outstr = _pretty_any(self)\n        elif self['_type'] == 'frame':\n            outstr = _pretty_frame(self)\n        elif self['_type'] == 'fe':\n            outstr = _pretty_fe(self)\n        elif self['_type'] == 'lu':\n            outstr = _pretty_lu(self)\n        elif self['_type'] == 'luexemplars': # list of ALL exemplars for LU\n            outstr = _pretty_exemplars(self, self[0].LU)\n        elif self['_type'] == 'fulltext_annotation': # list of all sentences for full-text doc\n            outstr = _pretty_fulltext_sentences(self)\n        elif self['_type'] == 'lusentence':\n            outstr = _pretty_annotation(self)\n        elif self['_type'] == 'fulltext_sentence':\n            outstr = _pretty_fulltext_sentence(self)\n        elif self['_type'] in ('luannotationset', 'fulltext_annotationset'):\n            outstr = _pretty_annotation(self, aset_level=True)\n        elif self['_type'] == 'posannotationset':\n            outstr = _pretty_pos(self)\n        elif self['_type'] == 'semtype':\n            outstr = _pretty_semtype(self)\n        elif self['_type'] == 'framerelationtype':\n            outstr = _pretty_frame_relation_type(self)\n        elif self['_type'] == 'framerelation':\n            outstr = _pretty_frame_relation(self)\n        elif self['_type'] == 'ferelation':\n            outstr = _pretty_fe_relation(self)\n        else:\n            outstr = _pretty_any(self)\n\n        return outstr\n\n    def __str__(self):\n        return self._str()\n    def __repr__(self):\n        return self.__str__()\n\n@python_2_unicode_compatible\nclass SpecialList(list):\n    def __init__(self, typ, *args, **kwargs):\n        super(SpecialList,self).__init__(*args, **kwargs)\n        self._type = typ\n\n    def _str(self):\n        outstr = \"\"\n\n        assert self._type\n        if len(self)==0:\n            outstr = \"[]\"\n        elif self._type == 'luexemplars': # list of ALL exemplars for LU\n            outstr = _pretty_exemplars(self, self[0].LU)\n        else:\n            assert False,self._type\n        return outstr\n\n    def __str__(self):\n        return self._str()\n    def __repr__(self):\n        return self.__str__()\n\nclass Future(object):\n    def __init__(self, loader, *args, **kwargs):\n        super (Future, self).__init__(*args, **kwargs)\n        self._loader = loader\n        self._d = None\n    def _data(self):\n        if callable(self._loader):\n            self._d = self._loader()\n            self._loader = None # the data is now cached\n        return self._d\n\n    def __nonzero__(self):\n        return bool(self._data())\n    def __len__(self):\n        return len(self._data())\n\n    def __setitem__(self, key, value):\n        return self._data ().__setitem__(key, value)\n    def __getitem__(self, key):\n        return self._data ().__getitem__(key)\n    def __getattr__(self, key):\n        return self._data().__getattr__(key)\n\n    def __str__(self):\n        return self._data().__str__()\n    def __repr__(self):\n        return self._data().__repr__()\n\n@python_2_unicode_compatible\nclass PrettyDict(AttrDict):\n    def __init__(self, *args, **kwargs):\n        _BREAK_LINES = kwargs.pop('breakLines', False)\n        super(PrettyDict, self).__init__(*args, **kwargs)\n        dict.__setattr__(self, '_BREAK_LINES', _BREAK_LINES)\n    def __repr__(self):\n        parts = []\n        for k,v in sorted(self.items()):\n            kv = repr(k)+': '\n            try:\n                kv += v._short_repr()\n            except AttributeError:\n                kv += repr(v)\n            parts.append(kv)\n        return '{'+(',\\n ' if self._BREAK_LINES else ', ').join(parts)+'}'\n\n@python_2_unicode_compatible\nclass PrettyList(list):\n    def __init__(self, *args, **kwargs):\n        self._MAX_REPR_SIZE = kwargs.pop('maxReprSize', 60)\n        self._BREAK_LINES = kwargs.pop('breakLines', False)\n        super(PrettyList, self).__init__(*args, **kwargs)\n    def __repr__(self):\n        pieces = []\n        length = 5\n\n        for elt in self:\n            pieces.append(elt._short_repr()) # key difference from inherited version: call to _short_repr()\n            length += len(pieces[-1]) + 2\n            if self._MAX_REPR_SIZE and length > self._MAX_REPR_SIZE and len(pieces) > 2:\n                return \"[%s, ...]\" % text_type(',\\n ' if self._BREAK_LINES else ', ').join(pieces[:-1])\n        return \"[%s]\" % text_type(',\\n ' if self._BREAK_LINES else ', ').join(pieces)\n\n@python_2_unicode_compatible\nclass PrettyLazyMap(LazyMap):\n    _MAX_REPR_SIZE = 60\n    def __repr__(self):\n        pieces = []\n        length = 5\n        for elt in self:\n            pieces.append(elt._short_repr()) # key difference from inherited version: call to _short_repr()\n            length += len(pieces[-1]) + 2\n            if length > self._MAX_REPR_SIZE and len(pieces) > 2:\n                return \"[%s, ...]\" % text_type(', ').join(pieces[:-1])\n        return \"[%s]\" % text_type(', ').join(pieces)\n\n@python_2_unicode_compatible\nclass PrettyLazyIteratorList(LazyIteratorList):\n    _MAX_REPR_SIZE = 60\n    def __repr__(self):\n        pieces = []\n        length = 5\n        for elt in self:\n            pieces.append(elt._short_repr()) # key difference from inherited version: call to _short_repr()\n            length += len(pieces[-1]) + 2\n            if length > self._MAX_REPR_SIZE and len(pieces) > 2:\n                return \"[%s, ...]\" % text_type(', ').join(pieces[:-1])\n        return \"[%s]\" % text_type(', ').join(pieces)\n\n@python_2_unicode_compatible\nclass PrettyLazyConcatenation(LazyConcatenation):\n    _MAX_REPR_SIZE = 60\n    def __repr__(self):\n        pieces = []\n        length = 5\n        for elt in self:\n            pieces.append(elt._short_repr()) # key difference from inherited version: call to _short_repr()\n            length += len(pieces[-1]) + 2\n            if length > self._MAX_REPR_SIZE and len(pieces) > 2:\n                return \"[%s, ...]\" % text_type(', ').join(pieces[:-1])\n        return \"[%s]\" % text_type(', ').join(pieces)\n\n    def __add__(self, other):\n        return PrettyLazyIteratorList(itertools.chain(self, other))\n\n    def __radd__(self, other):\n        return PrettyLazyIteratorList(itertools.chain(other, self))\n\n\nclass FramenetCorpusReader(XMLCorpusReader):\n\n    _bad_statuses = ['Problem']\n\n    _warnings = False\n\n    def warnings(self, v):\n        self._warnings = v\n\n    def __init__(self, root, fileids):\n        XMLCorpusReader.__init__(self, root, fileids)\n\n        self._frame_dir = \"frame\"\n        self._lu_dir = \"lu\"\n        self._fulltext_dir = \"fulltext\"\n\n        self._fnweb_url = \"https://framenet2.icsi.berkeley.edu/fnReports/data\"\n\n        self._frame_idx = None\n        self._cached_frames = {}    # name -> ID\n        self._lu_idx = None\n        self._fulltext_idx = None\n        self._semtypes = None\n        self._freltyp_idx = None    # frame relation types (Inheritance, Using, etc.)\n        self._frel_idx = None   # frame-to-frame relation instances\n        self._ferel_idx = None  # FE-to-FE relation instances\n        self._frel_f_idx = None # frame-to-frame relations associated with each frame\n\n    def help(self, attrname=None):\n\n        if attrname is not None:\n            return help(self.__getattribute__(attrname))\n\n\n\n        msg = \"\"\"\nCitation: Nathan Schneider and Chuck Wooters (2017),\n\"The NLTK FrameNet API: Designing for Discoverability with a Rich Linguistic Resource\".\nProceedings of EMNLP: System Demonstrations. https://arxiv.org/abs/1703.07438\n\nUse the following methods to access data in FrameNet.\nProvide a method name to `help()` for more information.\n\nFRAMES\n======\n\nframe() to look up a frame by its exact name or ID\nframes() to get frames matching a name pattern\nframes_by_lemma() to get frames containing an LU matching a name pattern\nframe_ids_and_names() to get a mapping from frame IDs to names\n\nFRAME ELEMENTS\n==============\n\nfes() to get frame elements (a.k.a. roles) matching a name pattern, optionally constrained\n  by a frame name pattern\n\nLEXICAL UNITS\n=============\n\nlu() to look up an LU by its ID\nlus() to get lexical units matching a name pattern, optionally constrained by frame\nlu_ids_and_names() to get a mapping from LU IDs to names\n\nRELATIONS\n=========\n\nframe_relation_types() to get the different kinds of frame-to-frame relations\n  (Inheritance, Subframe, Using, etc.).\nframe_relations() to get the relation instances, optionally constrained by\n  frame(s) or relation type\nfe_relations() to get the frame element pairs belonging to a frame-to-frame relation\n\nSEMANTIC TYPES\n==============\n\nsemtypes() to get the different kinds of semantic types that can be applied to\n  FEs, LUs, and entire frames\nsemtype() to look up a particular semtype by name, ID, or abbreviation\nsemtype_inherits() to check whether two semantic types have a subtype-supertype\n  relationship in the semtype hierarchy\npropagate_semtypes() to apply inference rules that distribute semtypes over relations\n  between FEs\n\nANNOTATIONS\n===========\n\nannotations() to get annotation sets, in which a token in a sentence is annotated\n  with a lexical unit in a frame, along with its frame elements and their syntactic properties;\n  can be constrained by LU name pattern and limited to lexicographic exemplars or full-text.\n  Sentences of full-text annotation can have multiple annotation sets.\nsents() to get annotated sentences illustrating one or more lexical units\nexemplars() to get sentences of lexicographic annotation, most of which have\n  just 1 annotation set; can be constrained by LU name pattern, frame, and overt FE(s)\ndoc() to look up a document of full-text annotation by its ID\ndocs() to get documents of full-text annotation that match a name pattern\ndocs_metadata() to get metadata about all full-text documents without loading them\nft_sents() to iterate over sentences of full-text annotation\n\nUTILITIES\n=========\n\nbuildindexes() loads metadata about all frames, LUs, etc. into memory to avoid\n  delay when one is accessed for the first time. It does not load annotations.\nreadme() gives the text of the FrameNet README file\nwarnings(True) to display corpus consistency warnings when loading data\n        \"\"\"\n        print(msg)\n\n    def _buildframeindex(self):\n        if not self._frel_idx:\n            self._buildrelationindex()  # always load frame relations before frames,\n        self._frame_idx = {}\n        for f in XMLCorpusView(self.abspath(\"frameIndex.xml\"),\n                               'frameIndex/frame', self._handle_elt):\n            self._frame_idx[f['ID']] = f\n\n    def _buildcorpusindex(self):\n        self._fulltext_idx = {}\n        for doclist in XMLCorpusView(self.abspath(\"fulltextIndex.xml\"),\n                                     'fulltextIndex/corpus',\n                                     self._handle_fulltextindex_elt):\n            for doc in doclist:\n                self._fulltext_idx[doc.ID] = doc\n\n    def _buildluindex(self):\n        self._lu_idx = {}\n        for lu in XMLCorpusView(self.abspath(\"luIndex.xml\"),\n                                'luIndex/lu', self._handle_elt):\n            self._lu_idx[lu['ID']] = lu # populate with LU index entries. if any of these\n\n    def _buildrelationindex(self):\n        freltypes = PrettyList(x for x in XMLCorpusView(self.abspath(\"frRelation.xml\"),\n                                            'frameRelations/frameRelationType',\n                                            self._handle_framerelationtype_elt))\n        self._freltyp_idx = {}\n        self._frel_idx = {}\n        self._frel_f_idx = defaultdict(set)\n        self._ferel_idx = {}\n\n        for freltyp in freltypes:\n            self._freltyp_idx[freltyp.ID] = freltyp\n            for frel in freltyp.frameRelations:\n                supF = frel.superFrame = frel[freltyp.superFrameName] = Future((lambda fID: lambda: self.frame_by_id(fID))(frel.supID))\n                subF = frel.subFrame = frel[freltyp.subFrameName] = Future((lambda fID: lambda: self.frame_by_id(fID))(frel.subID))\n                self._frel_idx[frel.ID] = frel\n                self._frel_f_idx[frel.supID].add(frel.ID)\n                self._frel_f_idx[frel.subID].add(frel.ID)\n                for ferel in frel.feRelations:\n                    ferel.superFrame = supF\n                    ferel.subFrame = subF\n                    ferel.superFE = Future((lambda fer: lambda: fer.superFrame.FE[fer.superFEName])(ferel))\n                    ferel.subFE = Future((lambda fer: lambda: fer.subFrame.FE[fer.subFEName])(ferel))\n                    self._ferel_idx[ferel.ID] = ferel\n\n    def _warn(self, *message, **kwargs):\n        if self._warnings:\n            kwargs.setdefault('file', sys.stderr)\n            print(*message, **kwargs)\n\n    def readme(self):\n        \"\"\"\n        Return the contents of the corpus README.txt (or README) file.\n        \"\"\"\n        try:\n            return self.open(\"README.txt\").read()\n        except IOError:\n            return self.open(\"README\").read()\n\n    def buildindexes(self):\n        \"\"\"\n        Build the internal indexes to make look-ups faster.\n        \"\"\"\n        self._buildframeindex()\n        self._buildluindex()\n        self._buildcorpusindex()\n        self._buildrelationindex()\n\n    def doc(self, fn_docid):\n        \"\"\"\n        Returns the annotated document whose id number is\n        ``fn_docid``. This id number can be obtained by calling the\n        Documents() function.\n\n        The dict that is returned from this function will contain the\n        following keys:\n\n        - '_type'      : 'fulltextannotation'\n        - 'sentence'   : a list of sentences in the document\n           - Each item in the list is a dict containing the following keys:\n              - 'ID'    : the ID number of the sentence\n              - '_type' : 'sentence'\n              - 'text'  : the text of the sentence\n              - 'paragNo' : the paragraph number\n              - 'sentNo'  : the sentence number\n              - 'docID'   : the document ID number\n              - 'corpID'  : the corpus ID number\n              - 'aPos'    : the annotation position\n              - 'annotationSet' : a list of annotation layers for the sentence\n                 - Each item in the list is a dict containing the following keys:\n                    - 'ID'       : the ID number of the annotation set\n                    - '_type'    : 'annotationset'\n                    - 'status'   : either 'MANUAL' or 'UNANN'\n                    - 'luName'   : (only if status is 'MANUAL')\n                    - 'luID'     : (only if status is 'MANUAL')\n                    - 'frameID'  : (only if status is 'MANUAL')\n                    - 'frameName': (only if status is 'MANUAL')\n                    - 'layer' : a list of labels for the layer\n                       - Each item in the layer is a dict containing the\n                         following keys:\n                          - '_type': 'layer'\n                          - 'rank'\n                          - 'name'\n                          - 'label' : a list of labels in the layer\n                             - Each item is a dict containing the following keys:\n                                - 'start'\n                                - 'end'\n                                - 'name'\n                                - 'feID' (optional)\n\n        :param fn_docid: The Framenet id number of the document\n        :type fn_docid: int\n        :return: Information about the annotated document\n        :rtype: dict\n        \"\"\"\n        try:\n            xmlfname = self._fulltext_idx[fn_docid].filename\n        except TypeError:  # happens when self._fulltext_idx == None\n            self._buildcorpusindex()\n            xmlfname = self._fulltext_idx[fn_docid].filename\n        except KeyError:  # probably means that fn_docid was not in the index\n            raise FramenetError(\"Unknown document id: {0}\".format(fn_docid))\n\n        locpath = os.path.join(\n            \"{0}\".format(self._root), self._fulltext_dir, xmlfname)\n\n        elt = XMLCorpusView(locpath, 'fullTextAnnotation')[0]\n        info = self._handle_fulltextannotation_elt(elt)\n        for k,v in self._fulltext_idx[fn_docid].items():\n            info[k] = v\n        return info\n\n    def frame_by_id(self, fn_fid, ignorekeys=[]):\n        \"\"\"\n        Get the details for the specified Frame using the frame's id\n        number.\n\n        Usage examples:\n\n        >>> from nltk.corpus import framenet as fn\n        >>> f = fn.frame_by_id(256)\n        >>> f.ID\n        256\n        >>> f.name\n        'Medical_specialties'\n        >>> f.definition\n        \"This frame includes words that name ...\"\n\n        :param fn_fid: The Framenet id number of the frame\n        :type fn_fid: int\n        :param ignorekeys: The keys to ignore. These keys will not be\n            included in the output. (optional)\n        :type ignorekeys: list(str)\n        :return: Information about a frame\n        :rtype: dict\n\n        Also see the ``frame()`` function for details about what is\n        contained in the dict that is returned.\n        \"\"\"\n\n        try:\n            fentry = self._frame_idx[fn_fid]\n            if '_type' in fentry:\n                return fentry   # full frame object is cached\n            name = fentry['name']\n        except TypeError:\n            self._buildframeindex()\n            name = self._frame_idx[fn_fid]['name']\n        except KeyError:\n            raise FramenetError('Unknown frame id: {0}'.format(fn_fid))\n\n        return self.frame_by_name(name, ignorekeys, check_cache=False)\n\n    def frame_by_name(self, fn_fname, ignorekeys=[], check_cache=True):\n        \"\"\"\n        Get the details for the specified Frame using the frame's name.\n\n        Usage examples:\n\n        >>> from nltk.corpus import framenet as fn\n        >>> f = fn.frame_by_name('Medical_specialties')\n        >>> f.ID\n        256\n        >>> f.name\n        'Medical_specialties'\n        >>> f.definition\n        \"This frame includes words that name ...\"\n\n        :param fn_fname: The name of the frame\n        :type fn_fname: str\n        :param ignorekeys: The keys to ignore. These keys will not be\n            included in the output. (optional)\n        :type ignorekeys: list(str)\n        :return: Information about a frame\n        :rtype: dict\n\n        Also see the ``frame()`` function for details about what is\n        contained in the dict that is returned.\n        \"\"\"\n\n        if check_cache and fn_fname in self._cached_frames:\n            return self._frame_idx[self._cached_frames[fn_fname]]\n        elif not self._frame_idx:\n            self._buildframeindex()\n\n        locpath = os.path.join(\n            \"{0}\".format(self._root), self._frame_dir, fn_fname + \".xml\")\n        try:\n            elt = XMLCorpusView(locpath, 'frame')[0]\n        except IOError:\n            raise FramenetError('Unknown frame: {0}'.format(fn_fname))\n\n        fentry = self._handle_frame_elt(elt, ignorekeys)\n        assert fentry\n\n        fentry.URL = self._fnweb_url + '/' + self._frame_dir + '/' + fn_fname + '.xml'\n\n        for st in fentry.semTypes:\n            if st.rootType.name=='Lexical_type':\n                for lu in fentry.lexUnit.values():\n                    if not any(x is st for x in lu.semTypes):  # identity containment check\n                        lu.semTypes.append(st)\n\n\n        self._frame_idx[fentry.ID] = fentry\n        self._cached_frames[fentry.name] = fentry.ID\n        '''\n        for luName,luinfo in fentry.lexUnit.items():\n            fentry.lexUnit[luName] = (lambda luID: Future(lambda: self.lu(luID)))(luinfo.ID)\n        '''\n        return fentry\n\n    def frame(self, fn_fid_or_fname, ignorekeys=[]):\n        \"\"\"\n        Get the details for the specified Frame using the frame's name\n        or id number.\n\n        Usage examples:\n\n        >>> from nltk.corpus import framenet as fn\n        >>> f = fn.frame(256)\n        >>> f.name\n        'Medical_specialties'\n        >>> f = fn.frame('Medical_specialties')\n        >>> f.ID\n        256\n        >>> # ensure non-ASCII character in definition doesn't trigger an encoding error:\n        >>> fn.frame('Imposing_obligation')\n        frame (1494): Imposing_obligation...\n\n        The dict that is returned from this function will contain the\n        following information about the Frame:\n\n        - 'name'       : the name of the Frame (e.g. 'Birth', 'Apply_heat', etc.)\n        - 'definition' : textual definition of the Frame\n        - 'ID'         : the internal ID number of the Frame\n        - 'semTypes'   : a list of semantic types for this frame\n           - Each item in the list is a dict containing the following keys:\n              - 'name' : can be used with the semtype() function\n              - 'ID'   : can be used with the semtype() function\n\n        - 'lexUnit'    : a dict containing all of the LUs for this frame.\n                         The keys in this dict are the names of the LUs and\n                         the value for each key is itself a dict containing\n                         info about the LU (see the lu() function for more info.)\n\n        - 'FE' : a dict containing the Frame Elements that are part of this frame\n                 The keys in this dict are the names of the FEs (e.g. 'Body_system')\n                 and the values are dicts containing the following keys\n              - 'definition' : The definition of the FE\n              - 'name'       : The name of the FE e.g. 'Body_system'\n              - 'ID'         : The id number\n              - '_type'      : 'fe'\n              - 'abbrev'     : Abbreviation e.g. 'bod'\n              - 'coreType'   : one of \"Core\", \"Peripheral\", or \"Extra-Thematic\"\n              - 'semType'    : if not None, a dict with the following two keys:\n                 - 'name' : name of the semantic type. can be used with\n                            the semtype() function\n                 - 'ID'   : id number of the semantic type. can be used with\n                            the semtype() function\n              - 'requiresFE' : if not None, a dict with the following two keys:\n                 - 'name' : the name of another FE in this frame\n                 - 'ID'   : the id of the other FE in this frame\n              - 'excludesFE' : if not None, a dict with the following two keys:\n                 - 'name' : the name of another FE in this frame\n                 - 'ID'   : the id of the other FE in this frame\n\n        - 'frameRelation'      : a list of objects describing frame relations\n        - 'FEcoreSets'  : a list of Frame Element core sets for this frame\n           - Each item in the list is a list of FE objects\n\n        :param fn_fid_or_fname: The Framenet name or id number of the frame\n        :type fn_fid_or_fname: int or str\n        :param ignorekeys: The keys to ignore. These keys will not be\n            included in the output. (optional)\n        :type ignorekeys: list(str)\n        :return: Information about a frame\n        :rtype: dict\n        \"\"\"\n\n        if isinstance(fn_fid_or_fname, string_types):\n            f = self.frame_by_name(fn_fid_or_fname, ignorekeys)\n        else:\n            f = self.frame_by_id(fn_fid_or_fname, ignorekeys)\n\n        return f\n\n    def frames_by_lemma(self, pat):\n        \"\"\"\n        Returns a list of all frames that contain LUs in which the\n        ``name`` attribute of the LU matchs the given regular expression\n        ``pat``. Note that LU names are composed of \"lemma.POS\", where\n        the \"lemma\" part can be made up of either a single lexeme\n        (e.g. 'run') or multiple lexemes (e.g. 'a little').\n\n        Note: if you are going to be doing a lot of this type of\n        searching, you'd want to build an index that maps from lemmas to\n        frames because each time frames_by_lemma() is called, it has to\n        search through ALL of the frame XML files in the db.\n\n        >>> from nltk.corpus import framenet as fn\n        >>> from nltk.corpus.reader.framenet import PrettyList\n        >>> PrettyList(sorted(fn.frames_by_lemma(r'(?i)a little'), key=itemgetter('ID'))) # doctest: +ELLIPSIS\n        [<frame ID=189 name=Quanti...>, <frame ID=2001 name=Degree>]\n\n        :return: A list of frame objects.\n        :rtype: list(AttrDict)\n        \"\"\"\n        return PrettyList(f for f in self.frames() if any(re.search(pat, luName) for luName in f.lexUnit))\n\n    def lu_basic(self, fn_luid):\n        \"\"\"\n        Returns basic information about the LU whose id is\n        ``fn_luid``. This is basically just a wrapper around the\n        ``lu()`` function with \"subCorpus\" info excluded.\n\n        >>> from nltk.corpus import framenet as fn\n        >>> lu = PrettyDict(fn.lu_basic(256), breakLines=True)\n        >>> # ellipses account for differences between FN 1.5 and 1.7\n        >>> lu # doctest: +ELLIPSIS\n        {'ID': 256,\n         'POS': 'V',\n         'URL': u'https://framenet2.icsi.berkeley.edu/fnReports/data/lu/lu256.xml',\n         '_type': 'lu',\n         'cBy': ...,\n         'cDate': '02/08/2001 01:27:50 PST Thu',\n         'definition': 'COD: be aware of beforehand; predict.',\n         'definitionMarkup': 'COD: be aware of beforehand; predict.',\n         'frame': <frame ID=26 name=Expectation>,\n         'lemmaID': 15082,\n         'lexemes': [{'POS': 'V', 'breakBefore': 'false', 'headword': 'false', 'name': 'foresee', 'order': 1}],\n         'name': 'foresee.v',\n         'semTypes': [],\n         'sentenceCount': {'annotated': ..., 'total': ...},\n         'status': 'FN1_Sent'}\n\n        :param fn_luid: The id number of the desired LU\n        :type fn_luid: int\n        :return: Basic information about the lexical unit\n        :rtype: dict\n        \"\"\"\n        return self.lu(fn_luid, ignorekeys=['subCorpus', 'exemplars'])\n\n    def lu(self, fn_luid, ignorekeys=[], luName=None, frameID=None, frameName=None):\n        \"\"\"\n        Access a lexical unit by its ID. luName, frameID, and frameName are used\n        only in the event that the LU does not have a file in the database\n        (which is the case for LUs with \"Problem\" status); in this case,\n        a placeholder LU is created which just contains its name, ID, and frame.\n\n\n        Usage examples:\n\n        >>> from nltk.corpus import framenet as fn\n        >>> fn.lu(256).name\n        'foresee.v'\n        >>> fn.lu(256).definition\n        'COD: be aware of beforehand; predict.'\n        >>> fn.lu(256).frame.name\n        'Expectation'\n        >>> pprint(list(map(PrettyDict, fn.lu(256).lexemes)))\n        [{'POS': 'V', 'breakBefore': 'false', 'headword': 'false', 'name': 'foresee', 'order': 1}]\n\n        >>> fn.lu(227).exemplars[23]\n        exemplar sentence (352962):\n        [sentNo] 0\n        [aPos] 59699508\n        <BLANKLINE>\n        [LU] (227) guess.v in Coming_to_believe\n        <BLANKLINE>\n        [frame] (23) Coming_to_believe\n        <BLANKLINE>\n        [annotationSet] 2 annotation sets\n        <BLANKLINE>\n        [POS] 18 tags\n        <BLANKLINE>\n        [POS_tagset] BNC\n        <BLANKLINE>\n        [GF] 3 relations\n        <BLANKLINE>\n        [PT] 3 phrases\n        <BLANKLINE>\n        [Other] 1 entry\n        <BLANKLINE>\n        [text] + [Target] + [FE]\n        <BLANKLINE>\n        When he was inside the house , Culley noticed the characteristic\n                                                      ------------------\n                                                      Content\n        <BLANKLINE>\n        he would n't have guessed at .\n        --                ******* --\n        Co                        C1 [Evidence:INI]\n         (Co=Cognizer, C1=Content)\n        <BLANKLINE>\n        <BLANKLINE>\n\n        The dict that is returned from this function will contain most of the\n        following information about the LU. Note that some LUs do not contain\n        all of these pieces of information - particularly 'totalAnnotated' and\n        'incorporatedFE' may be missing in some LUs:\n\n        - 'name'       : the name of the LU (e.g. 'merger.n')\n        - 'definition' : textual definition of the LU\n        - 'ID'         : the internal ID number of the LU\n        - '_type'      : 'lu'\n        - 'status'     : e.g. 'Created'\n        - 'frame'      : Frame that this LU belongs to\n        - 'POS'        : the part of speech of this LU (e.g. 'N')\n        - 'totalAnnotated' : total number of examples annotated with this LU\n        - 'incorporatedFE' : FE that incorporates this LU (e.g. 'Ailment')\n        - 'sentenceCount'  : a dict with the following two keys:\n                 - 'annotated': number of sentences annotated with this LU\n                 - 'total'    : total number of sentences with this LU\n\n        - 'lexemes'  : a list of dicts describing the lemma of this LU.\n           Each dict in the list contains these keys:\n           - 'POS'     : part of speech e.g. 'N'\n           - 'name'    : either single-lexeme e.g. 'merger' or\n                         multi-lexeme e.g. 'a little'\n           - 'order': the order of the lexeme in the lemma (starting from 1)\n           - 'headword': a boolean ('true' or 'false')\n           - 'breakBefore': Can this lexeme be separated from the previous lexeme?\n                Consider: \"take over.v\" as in:\n                         Germany took over the Netherlands in 2 days.\n                         Germany took the Netherlands over in 2 days.\n                In this case, 'breakBefore' would be \"true\" for the lexeme\n                \"over\". Contrast this with \"take after.v\" as in:\n                         Mary takes after her grandmother.\n                        *Mary takes her grandmother after.\n                In this case, 'breakBefore' would be \"false\" for the lexeme \"after\"\n\n        - 'lemmaID'    : Can be used to connect lemmas in different LUs\n        - 'semTypes'   : a list of semantic type objects for this LU\n        - 'subCorpus'  : a list of subcorpora\n           - Each item in the list is a dict containing the following keys:\n              - 'name' :\n              - 'sentence' : a list of sentences in the subcorpus\n                 - each item in the list is a dict with the following keys:\n                    - 'ID':\n                    - 'sentNo':\n                    - 'text': the text of the sentence\n                    - 'aPos':\n                    - 'annotationSet': a list of annotation sets\n                       - each item in the list is a dict with the following keys:\n                          - 'ID':\n                          - 'status':\n                          - 'layer': a list of layers\n                             - each layer is a dict containing the following keys:\n                                - 'name': layer name (e.g. 'BNC')\n                                - 'rank':\n                                - 'label': a list of labels for the layer\n                                   - each label is a dict containing the following keys:\n                                      - 'start': start pos of label in sentence 'text' (0-based)\n                                      - 'end': end pos of label in sentence 'text' (0-based)\n                                      - 'name': name of label (e.g. 'NN1')\n\n        Under the hood, this implementation looks up the lexical unit information\n        in the *frame* definition file. That file does not contain\n        corpus annotations, so the LU files will be accessed on demand if those are\n        needed. In principle, valence patterns could be loaded here too,\n        though these are not currently supported.\n\n        :param fn_luid: The id number of the lexical unit\n        :type fn_luid: int\n        :param ignorekeys: The keys to ignore. These keys will not be\n            included in the output. (optional)\n        :type ignorekeys: list(str)\n        :return: All information about the lexical unit\n        :rtype: dict\n        \"\"\"\n        if not self._lu_idx:\n            self._buildluindex()\n        OOV = object()\n        luinfo = self._lu_idx.get(fn_luid, OOV)\n        if luinfo is OOV:\n            self._warn('LU ID not found: {0} ({1}) in {2} ({3})'.format(luName, fn_luid, frameName, frameID))\n            luinfo = AttrDict({'_type': 'lu', 'ID': fn_luid, 'name': luName,\n                               'frameID': frameID, 'status': 'Problem'})\n            f = self.frame_by_id(luinfo.frameID)\n            assert f.name==frameName,(f.name,frameName)\n            luinfo['frame'] = f\n            self._lu_idx[fn_luid] = luinfo\n        elif '_type' not in luinfo:\n            f = self.frame_by_id(luinfo.frameID)\n            luinfo = self._lu_idx[fn_luid]\n        if ignorekeys:\n            return AttrDict(dict((k, v) for k, v in luinfo.items() if k not in ignorekeys))\n\n        return luinfo\n\n    def _lu_file(self, lu, ignorekeys=[]):\n        \"\"\"\n        Augment the LU information that was loaded from the frame file\n        with additional information from the LU file.\n        \"\"\"\n        fn_luid = lu.ID\n\n        fname = \"lu{0}.xml\".format(fn_luid)\n        locpath = os.path.join(\"{0}\".format(self._root), self._lu_dir, fname)\n        if not self._lu_idx:\n            self._buildluindex()\n\n        try:\n            elt = XMLCorpusView(locpath, 'lexUnit')[0]\n        except IOError:\n            raise FramenetError('Unknown LU id: {0}'.format(fn_luid))\n\n        lu2 = self._handle_lexunit_elt(elt, ignorekeys)\n        lu.URL = self._fnweb_url + '/' + self._lu_dir + '/' + fname\n        lu.subCorpus = lu2.subCorpus\n        lu.exemplars = SpecialList('luexemplars',\n                                   [sent for subc in lu.subCorpus for sent in subc.sentence])\n        for sent in lu.exemplars:\n            sent['LU'] = lu\n            sent['frame'] = lu.frame\n            for aset in sent.annotationSet:\n                aset['LU'] = lu\n                aset['frame'] = lu.frame\n\n        return lu\n\n    def _loadsemtypes(self):\n        self._semtypes = AttrDict()\n        semtypeXML = [x for x in XMLCorpusView(self.abspath(\"semTypes.xml\"),\n                                             'semTypes/semType',\n                                             self._handle_semtype_elt)]\n        for st in semtypeXML:\n            n = st['name']\n            a = st['abbrev']\n            i = st['ID']\n            self._semtypes[n] = i\n            self._semtypes[a] = i\n            self._semtypes[i] = st\n        roots = []\n        for st in self.semtypes():\n            if st.superType:\n                st.superType = self.semtype(st.superType.supID)\n                st.superType.subTypes.append(st)\n            else:\n                if st not in roots: roots.append(st)\n                st.rootType = st\n        queue = list(roots)\n        assert queue\n        while queue:\n            st = queue.pop(0)\n            for child in st.subTypes:\n                child.rootType = st.rootType\n                queue.append(child)\n\n    def propagate_semtypes(self):\n        \"\"\"\n        Apply inference rules to distribute semtypes over relations between FEs.\n        For FrameNet 1.5, this results in 1011 semtypes being propagated.\n        (Not done by default because it requires loading all frame files,\n        which takes several seconds. If this needed to be fast, it could be rewritten\n        to traverse the neighboring relations on demand for each FE semtype.)\n\n        >>> from nltk.corpus import framenet as fn\n        >>> x = sum(1 for f in fn.frames() for fe in f.FE.values() if fe.semType)\n        >>> fn.propagate_semtypes()\n        >>> y = sum(1 for f in fn.frames() for fe in f.FE.values() if fe.semType)\n        >>> y-x > 1000\n        True\n        \"\"\"\n        if not self._semtypes:\n            self._loadsemtypes()\n        if not self._ferel_idx:\n            self._buildrelationindex()\n        changed = True\n        i = 0\n        nPropagations = 0\n        while changed:\n            i += 1\n            changed = False\n            for ferel in self.fe_relations():\n                superST = ferel.superFE.semType\n                subST = ferel.subFE.semType\n                try:\n                    if superST and superST is not subST:\n                        assert subST is None or self.semtype_inherits(subST, superST),(superST.name,ferel,subST.name)\n                        if subST is None:\n                            ferel.subFE.semType = subST = superST\n                            changed = True\n                            nPropagations += 1\n                    if ferel.type.name in ['Perspective_on', 'Subframe', 'Precedes'] and subST \\\n                        and subST is not superST:\n                        assert superST is None,(superST.name,ferel,subST.name)\n                        ferel.superFE.semType = superST = subST\n                        changed = True\n                        nPropagations += 1\n                except AssertionError as ex:\n                    continue\n\n    def semtype(self, key):\n        \"\"\"\n        >>> from nltk.corpus import framenet as fn\n        >>> fn.semtype(233).name\n        'Temperature'\n        >>> fn.semtype(233).abbrev\n        'Temp'\n        >>> fn.semtype('Temperature').ID\n        233\n\n        :param key: The name, abbreviation, or id number of the semantic type\n        :type key: string or int\n        :return: Information about a semantic type\n        :rtype: dict\n        \"\"\"\n        if isinstance(key, int):\n            stid = key\n        else:\n            try:\n                stid = self._semtypes[key]\n            except TypeError:\n                self._loadsemtypes()\n                stid = self._semtypes[key]\n\n        try:\n            st = self._semtypes[stid]\n        except TypeError:\n            self._loadsemtypes()\n            st = self._semtypes[stid]\n\n        return st\n\n    def semtype_inherits(self, st, superST):\n        if not isinstance(st, dict):\n            st = self.semtype(st)\n        if not isinstance(superST, dict):\n            superST = self.semtype(superST)\n        par = st.superType\n        while par:\n            if par is superST:\n                return True\n            par = par.superType\n        return False\n\n    def frames(self, name=None):\n        \"\"\"\n        Obtain details for a specific frame.\n\n        >>> from nltk.corpus import framenet as fn\n        >>> len(fn.frames()) in (1019, 1221)    # FN 1.5 and 1.7, resp.\n        True\n        >>> x = PrettyList(fn.frames(r'(?i)crim'), maxReprSize=0, breakLines=True)\n        >>> x.sort(key=itemgetter('ID'))\n        >>> x\n        [<frame ID=200 name=Criminal_process>,\n         <frame ID=500 name=Criminal_investigation>,\n         <frame ID=692 name=Crime_scenario>,\n         <frame ID=700 name=Committing_crime>]\n\n        A brief intro to Frames (excerpted from \"FrameNet II: Extended\n        Theory and Practice\" by Ruppenhofer et. al., 2010):\n\n        A Frame is a script-like conceptual structure that describes a\n        particular type of situation, object, or event along with the\n        participants and props that are needed for that Frame. For\n        example, the \"Apply_heat\" frame describes a common situation\n        involving a Cook, some Food, and a Heating_Instrument, and is\n        evoked by words such as bake, blanch, boil, broil, brown,\n        simmer, steam, etc.\n\n        We call the roles of a Frame \"frame elements\" (FEs) and the\n        frame-evoking words are called \"lexical units\" (LUs).\n\n        FrameNet includes relations between Frames. Several types of\n        relations are defined, of which the most important are:\n\n           - Inheritance: An IS-A relation. The child frame is a subtype\n             of the parent frame, and each FE in the parent is bound to\n             a corresponding FE in the child. An example is the\n             \"Revenge\" frame which inherits from the\n             \"Rewards_and_punishments\" frame.\n\n           - Using: The child frame presupposes the parent frame as\n             background, e.g the \"Speed\" frame \"uses\" (or presupposes)\n             the \"Motion\" frame; however, not all parent FEs need to be\n             bound to child FEs.\n\n           - Subframe: The child frame is a subevent of a complex event\n             represented by the parent, e.g. the \"Criminal_process\" frame\n             has subframes of \"Arrest\", \"Arraignment\", \"Trial\", and\n             \"Sentencing\".\n\n           - Perspective_on: The child frame provides a particular\n             perspective on an un-perspectivized parent frame. A pair of\n             examples consists of the \"Hiring\" and \"Get_a_job\" frames,\n             which perspectivize the \"Employment_start\" frame from the\n             Employer's and the Employee's point of view, respectively.\n\n        :param name: A regular expression pattern used to match against\n            Frame names. If 'name' is None, then a list of all\n            Framenet Frames will be returned.\n        :type name: str\n        :return: A list of matching Frames (or all Frames).\n        :rtype: list(AttrDict)\n        \"\"\"\n        try:\n            fIDs = list(self._frame_idx.keys())\n        except AttributeError:\n            self._buildframeindex()\n            fIDs = list(self._frame_idx.keys())\n\n        if name is not None:\n            return PrettyList(self.frame(fID) for fID,finfo in self.frame_ids_and_names(name).items())\n        else:\n            return PrettyLazyMap(self.frame, fIDs)\n\n    def frame_ids_and_names(self, name=None):\n        \"\"\"\n        Uses the frame index, which is much faster than looking up each frame definition\n        if only the names and IDs are needed.\n        \"\"\"\n        if not self._frame_idx:\n            self._buildframeindex()\n        return dict((fID, finfo.name) for fID,finfo in self._frame_idx.items() if name is None or re.search(name, finfo.name) is not None)\n\n    def fes(self, name=None, frame=None):\n        '''\n        Lists frame element objects. If 'name' is provided, this is treated as\n        a case-insensitive regular expression to filter by frame name.\n        (Case-insensitivity is because casing of frame element names is not always\n        consistent across frames.) Specify 'frame' to filter by a frame name pattern,\n        ID, or object.\n\n        >>> from nltk.corpus import framenet as fn\n        >>> fn.fes('Noise_maker')\n        [<fe ID=6043 name=Noise_maker>]\n        >>> sorted([(fe.frame.name,fe.name) for fe in fn.fes('sound')])\n        [('Cause_to_make_noise', 'Sound_maker'), ('Make_noise', 'Sound'),\n         ('Make_noise', 'Sound_source'), ('Sound_movement', 'Location_of_sound_source'),\n         ('Sound_movement', 'Sound'), ('Sound_movement', 'Sound_source'),\n         ('Sounds', 'Component_sound'), ('Sounds', 'Location_of_sound_source'),\n         ('Sounds', 'Sound_source'), ('Vocalizations', 'Location_of_sound_source'),\n         ('Vocalizations', 'Sound_source')]\n        >>> sorted([(fe.frame.name,fe.name) for fe in fn.fes('sound',r'(?i)make_noise')])\n        [('Cause_to_make_noise', 'Sound_maker'),\n         ('Make_noise', 'Sound'),\n         ('Make_noise', 'Sound_source')]\n        >>> sorted(set(fe.name for fe in fn.fes('^sound')))\n        ['Sound', 'Sound_maker', 'Sound_source']\n        >>> len(fn.fes('^sound$'))\n        2\n\n        :param name: A regular expression pattern used to match against\n            frame element names. If 'name' is None, then a list of all\n            frame elements will be returned.\n        :type name: str\n        :return: A list of matching frame elements\n        :rtype: list(AttrDict)\n        '''\n        if frame is not None:\n            if isinstance(frame, int):\n                frames = [self.frame(frame)]\n            elif isinstance(frame, string_types):\n                frames = self.frames(frame)\n            else:\n                frames = [frame]\n        else:\n            frames = self.frames()\n\n        return PrettyList(fe for f in frames for fename,fe in f.FE.items() if name is None or re.search(name, fename, re.I))\n\n    def lus(self, name=None, frame=None):\n        \"\"\"\n        Obtain details for lexical units.\n        Optionally restrict by lexical unit name pattern, and/or to a certain frame\n        or frames whose name matches a pattern.\n\n        >>> from nltk.corpus import framenet as fn\n        >>> len(fn.lus()) in (11829, 13572) # FN 1.5 and 1.7, resp.\n        True\n        >>> PrettyList(sorted(fn.lus(r'(?i)a little'), key=itemgetter('ID')), maxReprSize=0, breakLines=True)\n        [<lu ID=14733 name=a little.n>,\n         <lu ID=14743 name=a little.adv>,\n         <lu ID=14744 name=a little bit.adv>]\n        >>> PrettyList(sorted(fn.lus(r'interest', r'(?i)stimulus'), key=itemgetter('ID')))\n        [<lu ID=14894 name=interested.a>, <lu ID=14920 name=interesting.a>]\n\n        A brief intro to Lexical Units (excerpted from \"FrameNet II:\n        Extended Theory and Practice\" by Ruppenhofer et. al., 2010):\n\n        A lexical unit (LU) is a pairing of a word with a meaning. For\n        example, the \"Apply_heat\" Frame describes a common situation\n        involving a Cook, some Food, and a Heating Instrument, and is\n        _evoked_ by words such as bake, blanch, boil, broil, brown,\n        simmer, steam, etc. These frame-evoking words are the LUs in the\n        Apply_heat frame. Each sense of a polysemous word is a different\n        LU.\n\n        We have used the word \"word\" in talking about LUs. The reality\n        is actually rather complex. When we say that the word \"bake\" is\n        polysemous, we mean that the lemma \"bake.v\" (which has the\n        word-forms \"bake\", \"bakes\", \"baked\", and \"baking\") is linked to\n        three different frames:\n\n           - Apply_heat: \"Michelle baked the potatoes for 45 minutes.\"\n\n           - Cooking_creation: \"Michelle baked her mother a cake for her birthday.\"\n\n           - Absorb_heat: \"The potatoes have to bake for more than 30 minutes.\"\n\n        These constitute three different LUs, with different\n        definitions.\n\n        Multiword expressions such as \"given name\" and hyphenated words\n        like \"shut-eye\" can also be LUs. Idiomatic phrases such as\n        \"middle of nowhere\" and \"give the slip (to)\" are also defined as\n        LUs in the appropriate frames (\"Isolated_places\" and \"Evading\",\n        respectively), and their internal structure is not analyzed.\n\n        Framenet provides multiple annotated examples of each sense of a\n        word (i.e. each LU).  Moreover, the set of examples\n        (approximately 20 per LU) illustrates all of the combinatorial\n        possibilities of the lexical unit.\n\n        Each LU is linked to a Frame, and hence to the other words which\n        evoke that Frame. This makes the FrameNet database similar to a\n        thesaurus, grouping together semantically similar words.\n\n        In the simplest case, frame-evoking words are verbs such as\n        \"fried\" in:\n\n           \"Matilde fried the catfish in a heavy iron skillet.\"\n\n        Sometimes event nouns may evoke a Frame. For example,\n        \"reduction\" evokes \"Cause_change_of_scalar_position\" in:\n\n           \"...the reduction of debt levels to $665 million from $2.6 billion.\"\n\n        Adjectives may also evoke a Frame. For example, \"asleep\" may\n        evoke the \"Sleep\" frame as in:\n\n           \"They were asleep for hours.\"\n\n        Many common nouns, such as artifacts like \"hat\" or \"tower\",\n        typically serve as dependents rather than clearly evoking their\n        own frames.\n\n        :param name: A regular expression pattern used to search the LU\n            names. Note that LU names take the form of a dotted\n            string (e.g. \"run.v\" or \"a little.adv\") in which a\n            lemma preceeds the \".\" and a POS follows the\n            dot. The lemma may be composed of a single lexeme\n            (e.g. \"run\") or of multiple lexemes (e.g. \"a\n            little\"). If 'name' is not given, then all LUs will\n            be returned.\n\n            The valid POSes are:\n\n                   v    - verb\n                   n    - noun\n                   a    - adjective\n                   adv  - adverb\n                   prep - preposition\n                   num  - numbers\n                   intj - interjection\n                   art  - article\n                   c    - conjunction\n                   scon - subordinating conjunction\n\n        :type name: str\n        :type frame: str or int or frame\n        :return: A list of selected (or all) lexical units\n        :rtype: list of LU objects (dicts). See the lu() function for info\n          about the specifics of LU objects.\n\n        \"\"\"\n        if not self._lu_idx:\n            self._buildluindex()\n\n\n\n        if name is not None:    # match LUs, then restrict by frame\n            result = PrettyList(self.lu(luID) for luID,luName in self.lu_ids_and_names(name).items())\n            if frame is not None:\n                if isinstance(frame, int):\n                    frameIDs = {frame}\n                elif isinstance(frame, string_types):\n                    frameIDs = {f.ID for f in self.frames(frame)}\n                else:\n                    frameIDs = {frame.ID}\n                result = PrettyList(lu for lu in result if lu.frame.ID in frameIDs)\n        elif frame is not None: # all LUs in matching frames\n            if isinstance(frame, int):\n                frames = [self.frame(frame)]\n            elif isinstance(frame, string_types):\n                frames = self.frames(frame)\n            else:\n                frames = [frame]\n            result = PrettyLazyIteratorList(iter(LazyConcatenation(list(f.lexUnit.values()) for f in frames)))\n        else:   # all LUs\n            luIDs = [luID for luID,lu in self._lu_idx.items() if lu.status not in self._bad_statuses]\n            result = PrettyLazyMap(self.lu, luIDs)\n        return result\n\n    def lu_ids_and_names(self, name=None):\n        \"\"\"\n        Uses the LU index, which is much faster than looking up each LU definition\n        if only the names and IDs are needed.\n        \"\"\"\n        if not self._lu_idx:\n            self._buildluindex()\n        return {luID: luinfo.name for luID,luinfo in self._lu_idx.items()\n                if luinfo.status not in self._bad_statuses\n                    and (name is None or re.search(name, luinfo.name) is not None)}\n\n    def docs_metadata(self, name=None):\n        \"\"\"\n        Return an index of the annotated documents in Framenet.\n\n        Details for a specific annotated document can be obtained using this\n        class's doc() function and pass it the value of the 'ID' field.\n\n        >>> from nltk.corpus import framenet as fn\n        >>> len(fn.docs()) in (78, 107) # FN 1.5 and 1.7, resp.\n        True\n        >>> set([x.corpname for x in fn.docs_metadata()])>=set(['ANC', 'KBEval', \\\n                    'LUCorpus-v0.3', 'Miscellaneous', 'NTI', 'PropBank'])\n        True\n\n        :param name: A regular expression pattern used to search the\n            file name of each annotated document. The document's\n            file name contains the name of the corpus that the\n            document is from, followed by two underscores \"__\"\n            followed by the document name. So, for example, the\n            file name \"LUCorpus-v0.3__20000410_nyt-NEW.xml\" is\n            from the corpus named \"LUCorpus-v0.3\" and the\n            document name is \"20000410_nyt-NEW.xml\".\n        :type name: str\n        :return: A list of selected (or all) annotated documents\n        :rtype: list of dicts, where each dict object contains the following\n                keys:\n\n                - 'name'\n                - 'ID'\n                - 'corpid'\n                - 'corpname'\n                - 'description'\n                - 'filename'\n        \"\"\"\n        try:\n            ftlist = PrettyList(self._fulltext_idx.values())\n        except AttributeError:\n            self._buildcorpusindex()\n            ftlist = PrettyList(self._fulltext_idx.values())\n\n        if name is None:\n            return ftlist\n        else:\n            return PrettyList(x for x in ftlist if re.search(name, x['filename']) is not None)\n\n    def docs(self, name=None):\n        \"\"\"\n        Return a list of the annotated full-text documents in FrameNet,\n        optionally filtered by a regex to be matched against the document name.\n        \"\"\"\n        return PrettyLazyMap((lambda x: self.doc(x.ID)), self.docs_metadata(name))\n\n    def sents(self, exemplars=True, full_text=True):\n        \"\"\"\n        Annotated sentences matching the specified criteria.\n        \"\"\"\n        if exemplars:\n            if full_text:\n                return self.exemplars() + self.ft_sents()\n            else:\n                return self.exemplars()\n        elif full_text:\n            return self.ft_sents()\n\n    def annotations(self, luNamePattern=None, exemplars=True, full_text=True):\n        \"\"\"\n        Frame annotation sets matching the specified criteria.\n        \"\"\"\n\n        if exemplars:\n            epart = PrettyLazyIteratorList(sent.frameAnnotation for sent in self.exemplars(luNamePattern))\n        else:\n            epart = []\n\n        if full_text:\n            if luNamePattern is not None:\n                matchedLUIDs = set(self.lu_ids_and_names(luNamePattern).keys())\n            ftpart = PrettyLazyIteratorList(aset for sent in self.ft_sents() for aset in sent.annotationSet[1:] if luNamePattern is None or aset.get('luID','CXN_ASET') in matchedLUIDs)\n        else:\n            ftpart = []\n\n        if exemplars:\n            if full_text:\n                return epart + ftpart\n            else:\n                return epart\n        elif full_text:\n            return ftpart\n\n    def exemplars(self, luNamePattern=None, frame=None, fe=None, fe2=None):\n        \"\"\"\n        Lexicographic exemplar sentences, optionally filtered by LU name and/or 1-2 FEs that\n        are realized overtly. 'frame' may be a name pattern, frame ID, or frame instance.\n        'fe' may be a name pattern or FE instance; if specified, 'fe2' may also\n        be specified to retrieve sentences with both overt FEs (in either order).\n        \"\"\"\n        if fe is None and fe2 is not None:\n            raise FramenetError('exemplars(..., fe=None, fe2=<value>) is not allowed')\n        elif fe is not None and fe2 is not None:\n            if not isinstance(fe2, string_types):\n                if isinstance(fe, string_types):\n                    fe, fe2 = fe2, fe\n                elif fe.frame is not fe2.frame: # ensure frames match\n                    raise FramenetError('exemplars() call with inconsistent `fe` and `fe2` specification (frames must match)')\n        if frame is None and fe is not None and not isinstance(fe, string_types):\n            frame = fe.frame\n\n\n        lusByFrame = defaultdict(list)   # frame name -> matching LUs, if luNamePattern is specified\n        if frame is not None or luNamePattern is not None:\n            if frame is None or isinstance(frame, string_types):\n                if luNamePattern is not None:\n                    frames = set()\n                    for lu in self.lus(luNamePattern, frame=frame):\n                        frames.add(lu.frame.ID)\n                        lusByFrame[lu.frame.name].append(lu)\n                    frames = LazyMap(self.frame, list(frames))\n                else:\n                    frames = self.frames(frame)\n            else:\n                if isinstance(frame,int):\n                    frames = [self.frame(frame)]\n                else:   # frame object\n                    frames = [frame]\n\n                if luNamePattern is not None:\n                    lusByFrame = {frame.name: self.lus(luNamePattern, frame=frame)}\n\n            if fe is not None:  # narrow to frames that define this FE\n                if isinstance(fe, string_types):\n                    frames = PrettyLazyIteratorList(f for f in frames if fe in f.FE or any(re.search(fe, ffe, re.I) for ffe in f.FE.keys()))\n                else:\n                    if fe.frame not in frames:\n                        raise FramenetError('exemplars() call with inconsistent `frame` and `fe` specification')\n                    frames = [fe.frame]\n\n                if fe2 is not None: # narrow to frames that ALSO define this FE\n                    if isinstance(fe2, string_types):\n                        frames = PrettyLazyIteratorList(f for f in frames if fe2 in f.FE or any(re.search(fe2, ffe, re.I) for ffe in f.FE.keys()))\n        else:   # frame, luNamePattern are None. fe, fe2 are None or strings\n            if fe is not None:\n                frames = {ffe.frame.ID for ffe in self.fes(fe)}\n                if fe2 is not None:\n                    frames2 = {ffe.frame.ID for ffe in self.fes(fe2)}\n                    frames = frames & frames2\n                frames = LazyMap(self.frame, list(frames))\n            else:\n                frames = self.frames()\n\n\n        def _matching_exs():\n            for f in frames:\n                fes = fes2 = None   # FEs of interest\n                if fe is not None:\n                    fes = {ffe for ffe in f.FE.keys() if re.search(fe, ffe, re.I)} if isinstance(fe, string_types) else {fe.name}\n                    if fe2 is not None:\n                        fes2 = {ffe for ffe in f.FE.keys() if re.search(fe2, ffe, re.I)} if isinstance(fe2, string_types) else {fe2.name}\n\n                for lu in lusByFrame[f.name] if luNamePattern is not None else f.lexUnit.values():\n                    for ex in lu.exemplars:\n                        if (fes is None or self._exemplar_of_fes(ex, fes)) and (fes2 is None or self._exemplar_of_fes(ex, fes2)):\n                            yield ex\n\n        return PrettyLazyIteratorList(_matching_exs())\n\n    def _exemplar_of_fes(self, ex, fes=None):\n        \"\"\"\n        Given an exemplar sentence and a set of FE names, return the subset of FE names\n        that are realized overtly in the sentence on the FE, FE2, or FE3 layer.\n\n        If 'fes' is None, returns all overt FE names.\n        \"\"\"\n        overtNames = set(list(zip(*ex.FE[0]))[2]) if ex.FE[0] else set()\n        if 'FE2' in ex:\n            overtNames |= set(list(zip(*ex.FE2[0]))[2]) if ex.FE2[0] else set()\n            if 'FE3' in ex:\n                overtNames |= set(list(zip(*ex.FE3[0]))[2]) if ex.FE3[0] else set()\n        return overtNames & fes if fes is not None else overtNames\n\n    def ft_sents(self, docNamePattern=None):\n        \"\"\"\n        Full-text annotation sentences, optionally filtered by document name.\n        \"\"\"\n        return PrettyLazyIteratorList(sent for d in self.docs(docNamePattern) for sent in d.sentence)\n\n\n    def frame_relation_types(self):\n        \"\"\"\n        Obtain a list of frame relation types.\n\n        >>> from nltk.corpus import framenet as fn\n        >>> frts = sorted(fn.frame_relation_types(), key=itemgetter('ID'))\n        >>> isinstance(frts, list)\n        True\n        >>> len(frts) in (9, 10)    # FN 1.5 and 1.7, resp.\n        True\n        >>> PrettyDict(frts[0], breakLines=True)\n        {'ID': 1,\n         '_type': 'framerelationtype',\n         'frameRelations': [<Parent=Event -- Inheritance -> Child=Change_of_consistency>, <Parent=Event -- Inheritance -> Child=Rotting>, ...],\n         'name': 'Inheritance',\n         'subFrameName': 'Child',\n         'superFrameName': 'Parent'}\n\n        :return: A list of all of the frame relation types in framenet\n        :rtype: list(dict)\n        \"\"\"\n        if not self._freltyp_idx:\n            self._buildrelationindex()\n        return self._freltyp_idx.values()\n\n    def frame_relations(self, frame=None, frame2=None, type=None):\n        \"\"\"\n        :param frame: (optional) frame object, name, or ID; only relations involving\n        this frame will be returned\n        :param frame2: (optional; 'frame' must be a different frame) only show relations\n        between the two specified frames, in either direction\n        :param type: (optional) frame relation type (name or object); show only relations\n        of this type\n        :type frame: int or str or AttrDict\n        :return: A list of all of the frame relations in framenet\n        :rtype: list(dict)\n\n        >>> from nltk.corpus import framenet as fn\n        >>> frels = fn.frame_relations()\n        >>> isinstance(frels, list)\n        True\n        >>> len(frels) in (1676, 2070)  # FN 1.5 and 1.7, resp.\n        True\n        >>> PrettyList(fn.frame_relations('Cooking_creation'), maxReprSize=0, breakLines=True)\n        [<Parent=Intentionally_create -- Inheritance -> Child=Cooking_creation>,\n         <Parent=Apply_heat -- Using -> Child=Cooking_creation>,\n         <MainEntry=Apply_heat -- See_also -> ReferringEntry=Cooking_creation>]\n        >>> PrettyList(fn.frame_relations(274), breakLines=True)\n        [<Parent=Avoiding -- Inheritance -> Child=Dodging>,\n         <Parent=Avoiding -- Inheritance -> Child=Evading>, ...]\n        >>> PrettyList(fn.frame_relations(fn.frame('Cooking_creation')), breakLines=True)\n        [<Parent=Intentionally_create -- Inheritance -> Child=Cooking_creation>,\n         <Parent=Apply_heat -- Using -> Child=Cooking_creation>, ...]\n        >>> PrettyList(fn.frame_relations('Cooking_creation', type='Inheritance'))\n        [<Parent=Intentionally_create -- Inheritance -> Child=Cooking_creation>]\n        >>> PrettyList(fn.frame_relations('Cooking_creation', 'Apply_heat'), breakLines=True)\n        [<Parent=Apply_heat -- Using -> Child=Cooking_creation>,\n        <MainEntry=Apply_heat -- See_also -> ReferringEntry=Cooking_creation>]\n        \"\"\"\n        relation_type = type\n\n        if not self._frel_idx:\n            self._buildrelationindex()\n\n        rels = None\n\n        if relation_type is not None:\n            if not isinstance(relation_type, dict):\n                type = [rt for rt in self.frame_relation_types() if rt.name==type][0]\n                assert isinstance(type,dict)\n\n        if frame is not None:\n            if isinstance(frame,dict) and 'frameRelations' in frame:\n                rels = PrettyList(frame.frameRelations)\n            else:\n                if not isinstance(frame, int):\n                    if isinstance(frame, dict):\n                        frame = frame.ID\n                    else:\n                        frame = self.frame_by_name(frame).ID\n                rels = [self._frel_idx[frelID] for frelID in self._frel_f_idx[frame]]\n\n            if type is not None:\n                rels = [rel for rel in rels if rel.type is type]\n        elif type is not None:\n            rels = type.frameRelations\n        else:\n            rels = self._frel_idx.values()\n\n        if frame2 is not None:\n            if frame is None:\n                raise FramenetError(\"frame_relations(frame=None, frame2=<value>) is not allowed\")\n            if not isinstance(frame2, int):\n                if isinstance(frame2, dict):\n                    frame2 = frame2.ID\n                else:\n                    frame2 = self.frame_by_name(frame2).ID\n            if frame==frame2:\n                raise FramenetError(\"The two frame arguments to frame_relations() must be different frames\")\n            rels = [rel for rel in rels if rel.superFrame.ID==frame2 or rel.subFrame.ID==frame2]\n\n        return PrettyList(sorted(rels,\n                key=lambda frel: (frel.type.ID, frel.superFrameName, frel.subFrameName)))\n\n    def fe_relations(self):\n        \"\"\"\n        Obtain a list of frame element relations.\n\n        >>> from nltk.corpus import framenet as fn\n        >>> ferels = fn.fe_relations()\n        >>> isinstance(ferels, list)\n        True\n        >>> len(ferels) in (10020, 12393)   # FN 1.5 and 1.7, resp.\n        True\n        >>> PrettyDict(ferels[0], breakLines=True)\n        {'ID': 14642,\n        '_type': 'ferelation',\n        'frameRelation': <Parent=Abounding_with -- Inheritance -> Child=Lively_place>,\n        'subFE': <fe ID=11370 name=Degree>,\n        'subFEName': 'Degree',\n        'subFrame': <frame ID=1904 name=Lively_place>,\n        'subID': 11370,\n        'supID': 2271,\n        'superFE': <fe ID=2271 name=Degree>,\n        'superFEName': 'Degree',\n        'superFrame': <frame ID=262 name=Abounding_with>,\n        'type': <framerelationtype ID=1 name=Inheritance>}\n\n        :return: A list of all of the frame element relations in framenet\n        :rtype: list(dict)\n        \"\"\"\n        if not self._ferel_idx:\n            self._buildrelationindex()\n        return PrettyList(sorted(self._ferel_idx.values(),\n                key=lambda ferel: (ferel.type.ID, ferel.frameRelation.superFrameName,\n                    ferel.superFEName, ferel.frameRelation.subFrameName, ferel.subFEName)))\n\n    def semtypes(self):\n        \"\"\"\n        Obtain a list of semantic types.\n\n        >>> from nltk.corpus import framenet as fn\n        >>> stypes = fn.semtypes()\n        >>> len(stypes) in (73, 109) # FN 1.5 and 1.7, resp.\n        True\n        >>> sorted(stypes[0].keys())\n        ['ID', '_type', 'abbrev', 'definition', 'definitionMarkup', 'name', 'rootType', 'subTypes', 'superType']\n\n        :return: A list of all of the semantic types in framenet\n        :rtype: list(dict)\n        \"\"\"\n        if not self._semtypes:\n            self._loadsemtypes()\n        return PrettyList(self._semtypes[i] for i in self._semtypes if isinstance(i, int))\n\n    def _load_xml_attributes(self, d, elt):\n        \"\"\"\n        Extracts a subset of the attributes from the given element and\n        returns them in a dictionary.\n\n        :param d: A dictionary in which to store the attributes.\n        :type d: dict\n        :param elt: An ElementTree Element\n        :type elt: Element\n        :return: Returns the input dict ``d`` possibly including attributes from ``elt``\n        :rtype: dict\n        \"\"\"\n\n        d = type(d)(d)\n\n        try:\n            attr_dict = elt.attrib\n        except AttributeError:\n            return d\n\n        if attr_dict is None:\n            return d\n\n        ignore_attrs = [ #'cBy', 'cDate', 'mDate', # <-- annotation metadata that could be of interest\n                        'xsi', 'schemaLocation', 'xmlns', 'bgColor', 'fgColor']\n\n        for attr in attr_dict:\n\n            if any(attr.endswith(x) for x in ignore_attrs):\n                continue\n\n            val = attr_dict[attr]\n            if val.isdigit():\n                d[attr] = int(val)\n            else:\n                d[attr] = val\n\n        return d\n\n    def _strip_tags(self, data):\n        \"\"\"\n        Gets rid of all tags and newline characters from the given input\n\n        :return: A cleaned-up version of the input string\n        :rtype: str\n        \"\"\"\n\n        try:\n            '''\n            m = re.search(r'\\w[<][^/]|[<][/][^>]+[>](s\\w|[a-rt-z0-9])', data)\n            if m:\n                print('Markup boundary:', data[max(0,m.start(0)-10):m.end(0)+10].replace('\\n',' '), file=sys.stderr)\n            '''\n\n            data = data.replace('<t>', '')\n            data = data.replace('</t>', '')\n            data = re.sub('<fex name=\"[^\"]+\">', '', data)\n            data = data.replace('</fex>', '')\n            data = data.replace('<fen>', '')\n            data = data.replace('</fen>', '')\n            data = data.replace('<m>', '')\n            data = data.replace('</m>', '')\n            data = data.replace('<ment>', '')\n            data = data.replace('</ment>', '')\n            data = data.replace('<ex>', \"'\")\n            data = data.replace('</ex>', \"'\")\n            data = data.replace('<gov>', '')\n            data = data.replace('</gov>', '')\n            data = data.replace('<x>', '')\n            data = data.replace('</x>', '')\n\n            data = data.replace('<def-root>', '')\n            data = data.replace('</def-root>', '')\n\n            data = data.replace('\\n', ' ')\n        except AttributeError:\n            pass\n\n        return data\n\n    def _handle_elt(self, elt, tagspec=None):\n        return self._load_xml_attributes(AttrDict(), elt)\n\n    def _handle_fulltextindex_elt(self, elt, tagspec=None):\n        \"\"\"\n        Extracts corpus/document info from the fulltextIndex.xml file.\n\n        Note that this function \"flattens\" the information contained\n        in each of the \"corpus\" elements, so that each \"document\"\n        element will contain attributes for the corpus and\n        corpusid. Also, each of the \"document\" items will contain a\n        new attribute called \"filename\" that is the base file name of\n        the xml file for the document in the \"fulltext\" subdir of the\n        Framenet corpus.\n        \"\"\"\n        ftinfo = self._load_xml_attributes(AttrDict(), elt)\n        corpname = ftinfo.name\n        corpid = ftinfo.ID\n        retlist = []\n        for sub in elt:\n            if sub.tag.endswith('document'):\n                doc = self._load_xml_attributes(AttrDict(), sub)\n                if 'name' in doc:\n                    docname = doc.name\n                else:\n                    docname = doc.description\n                doc.filename = \"{0}__{1}.xml\".format(corpname, docname)\n                doc.URL = self._fnweb_url + '/' + self._fulltext_dir + '/' + doc.filename\n                doc.corpname = corpname\n                doc.corpid = corpid\n                retlist.append(doc)\n\n        return retlist\n\n    def _handle_frame_elt(self, elt, ignorekeys=[]):\n        frinfo = self._load_xml_attributes(AttrDict(), elt)\n\n        frinfo['_type'] = 'frame'\n        frinfo['definition'] = \"\"\n        frinfo['definitionMarkup'] = \"\"\n        frinfo['FE'] = PrettyDict()\n        frinfo['FEcoreSets'] = []\n        frinfo['lexUnit'] = PrettyDict()\n        frinfo['semTypes'] = []\n        for k in ignorekeys:\n            if k in frinfo:\n                del frinfo[k]\n\n        for sub in elt:\n            if sub.tag.endswith('definition') and 'definition' not in ignorekeys:\n                frinfo['definitionMarkup'] = sub.text\n                frinfo['definition'] = self._strip_tags(sub.text)\n            elif sub.tag.endswith('FE') and 'FE' not in ignorekeys:\n                feinfo = self._handle_fe_elt(sub)\n                frinfo['FE'][feinfo.name] = feinfo\n                feinfo['frame'] = frinfo    # backpointer\n            elif sub.tag.endswith('FEcoreSet') and 'FEcoreSet' not in ignorekeys:\n                coreset = self._handle_fecoreset_elt(sub)\n                frinfo['FEcoreSets'].append(PrettyList(frinfo['FE'][fe.name] for fe in coreset))\n            elif sub.tag.endswith('lexUnit') and 'lexUnit' not in ignorekeys:\n                luentry = self._handle_framelexunit_elt(sub)\n                if luentry['status'] in self._bad_statuses:\n                    continue\n                luentry['frame'] = frinfo\n                luentry['URL'] = self._fnweb_url + '/' + self._lu_dir + '/' + \"lu{0}.xml\".format(luentry['ID'])\n                luentry['subCorpus'] = Future((lambda lu: lambda: self._lu_file(lu).subCorpus)(luentry))\n                luentry['exemplars'] = Future((lambda lu: lambda: self._lu_file(lu).exemplars)(luentry))\n                frinfo['lexUnit'][luentry.name] = luentry\n                if not self._lu_idx:\n                    self._buildluindex()\n                self._lu_idx[luentry.ID] = luentry\n            elif sub.tag.endswith('semType') and 'semTypes' not in ignorekeys:\n                semtypeinfo = self._load_xml_attributes(AttrDict(), sub)\n                frinfo['semTypes'].append(self.semtype(semtypeinfo.ID))\n\n        frinfo['frameRelations'] = self.frame_relations(frame=frinfo)\n\n        for fe in frinfo.FE.values():\n            if fe.requiresFE:\n                name, ID = fe.requiresFE.name, fe.requiresFE.ID\n                fe.requiresFE = frinfo.FE[name]\n                assert fe.requiresFE.ID==ID\n            if fe.excludesFE:\n                name, ID = fe.excludesFE.name, fe.excludesFE.ID\n                fe.excludesFE = frinfo.FE[name]\n                assert fe.excludesFE.ID==ID\n\n        return frinfo\n\n    def _handle_fecoreset_elt(self, elt):\n        info = self._load_xml_attributes(AttrDict(), elt)\n        tmp = []\n        for sub in elt:\n            tmp.append(self._load_xml_attributes(AttrDict(), sub))\n\n        return tmp\n\n    def _handle_framerelationtype_elt(self, elt, *args):\n        info = self._load_xml_attributes(AttrDict(), elt)\n        info['_type'] = 'framerelationtype'\n        info['frameRelations'] = PrettyList()\n\n        for sub in elt:\n            if sub.tag.endswith('frameRelation'):\n                frel = self._handle_framerelation_elt(sub)\n                frel['type'] = info   # backpointer\n                for ferel in frel.feRelations:\n                    ferel['type'] = info\n                info['frameRelations'].append(frel)\n\n        return info\n\n    def _handle_framerelation_elt(self, elt):\n        info = self._load_xml_attributes(AttrDict(), elt)\n        assert info['superFrameName']!=info['subFrameName'],(elt,info)\n        info['_type'] = 'framerelation'\n        info['feRelations'] = PrettyList()\n\n        for sub in elt:\n            if sub.tag.endswith('FERelation'):\n                ferel = self._handle_elt(sub)\n                ferel['_type'] = 'ferelation'\n                ferel['frameRelation'] = info   # backpointer\n                info['feRelations'].append(ferel)\n\n        return info\n\n    def _handle_fulltextannotation_elt(self, elt):\n        \"\"\"Load full annotation info for a document from its xml\n        file. The main element (fullTextAnnotation) contains a 'header'\n        element (which we ignore here) and a bunch of 'sentence'\n        elements.\"\"\"\n        info = AttrDict()\n        info['_type'] = 'fulltext_annotation'\n        info['sentence'] = []\n\n        for sub in elt:\n            if sub.tag.endswith('header'):\n                continue  # not used\n            elif sub.tag.endswith('sentence'):\n                s = self._handle_fulltext_sentence_elt(sub)\n                s.doc = info\n                info['sentence'].append(s)\n\n        return info\n\n    def _handle_fulltext_sentence_elt(self, elt):\n        \"\"\"Load information from the given 'sentence' element. Each\n        'sentence' element contains a \"text\" and \"annotationSet\" sub\n        elements.\"\"\"\n        info = self._load_xml_attributes(AttrDict(), elt)\n        info['_type'] = \"fulltext_sentence\"\n        info['annotationSet'] = []\n        info['targets'] = []\n        target_spans = set()\n        info['_ascii'] = types.MethodType(_annotation_ascii, info)  # attach a method for this instance\n        info['text'] = \"\"\n\n        for sub in elt:\n            if sub.tag.endswith('text'):\n                info['text'] = self._strip_tags(sub.text)\n            elif sub.tag.endswith('annotationSet'):\n                a = self._handle_fulltextannotationset_elt(sub, is_pos=(len(info['annotationSet'])==0))\n                if 'cxnID' in a: # ignoring construction annotations for now\n                    continue\n                a.sent = info\n                a.text = info.text\n                info['annotationSet'].append(a)\n                if 'Target' in a:\n                    for tspan in a.Target:\n                        if tspan in target_spans:\n                            self._warn('Duplicate target span \"{0}\"'.format(info.text[slice(*tspan)]),\n                                tspan, 'in sentence',info['ID'], info.text)\n                        else:\n                            target_spans.add(tspan)\n                    info['targets'].append((a.Target, a.luName, a.frameName))\n\n        assert info['annotationSet'][0].status=='UNANN'\n        info['POS'] = info['annotationSet'][0].POS\n        info['POS_tagset'] = info['annotationSet'][0].POS_tagset\n        return info\n\n    def _handle_fulltextannotationset_elt(self, elt, is_pos=False):\n        \"\"\"Load information from the given 'annotationSet' element. Each\n        'annotationSet' contains several \"layer\" elements.\"\"\"\n\n        info = self._handle_luannotationset_elt(elt, is_pos=is_pos)\n        if not is_pos:\n            info['_type'] = 'fulltext_annotationset'\n            if 'cxnID' not in info: # ignoring construction annotations for now\n                info['LU'] = self.lu(info.luID, luName=info.luName, frameID=info.frameID, frameName=info.frameName)\n                info['frame'] = info.LU.frame\n        return info\n\n    def _handle_fulltextlayer_elt(self, elt):\n        \"\"\"Load information from the given 'layer' element. Each\n        'layer' contains several \"label\" elements.\"\"\"\n        info = self._load_xml_attributes(AttrDict(), elt)\n        info['_type'] = 'layer'\n        info['label'] = []\n\n        for sub in elt:\n            if sub.tag.endswith('label'):\n                l = self._load_xml_attributes(AttrDict(), sub)\n                info['label'].append(l)\n\n        return info\n\n    def _handle_framelexunit_elt(self, elt):\n        luinfo = AttrDict()\n        luinfo['_type'] = 'lu'\n        luinfo = self._load_xml_attributes(luinfo, elt)\n        luinfo[\"definition\"] = \"\"\n        luinfo[\"definitionMarkup\"] = \"\"\n        luinfo[\"sentenceCount\"] = PrettyDict()\n        luinfo['lexemes'] = PrettyList()   # multiword LUs have multiple lexemes\n        luinfo['semTypes'] = PrettyList()  # an LU can have multiple semtypes\n\n        for sub in elt:\n            if sub.tag.endswith('definition'):\n                luinfo['definitionMarkup'] = sub.text\n                luinfo['definition'] = self._strip_tags(sub.text)\n            elif sub.tag.endswith('sentenceCount'):\n                luinfo['sentenceCount'] = self._load_xml_attributes(\n                    PrettyDict(), sub)\n            elif sub.tag.endswith('lexeme'):\n                lexemeinfo = self._load_xml_attributes(PrettyDict(), sub)\n                if not isinstance(lexemeinfo.name, string_types):\n                    lexemeinfo.name = str(lexemeinfo.name)\n                luinfo['lexemes'].append(lexemeinfo)\n            elif sub.tag.endswith('semType'):\n                semtypeinfo = self._load_xml_attributes(PrettyDict(), sub)\n                luinfo['semTypes'].append(self.semtype(semtypeinfo.ID))\n\n        luinfo['lexemes'].sort(key=lambda x: x.order)\n\n        return luinfo\n\n    def _handle_lexunit_elt(self, elt, ignorekeys):\n        \"\"\"\n        Load full info for a lexical unit from its xml file.\n        This should only be called when accessing corpus annotations\n        (which are not included in frame files).\n        \"\"\"\n        luinfo = self._load_xml_attributes(AttrDict(), elt)\n        luinfo['_type'] = 'lu'\n        luinfo['definition'] = \"\"\n        luinfo['definitionMarkup'] = \"\"\n        luinfo['subCorpus'] = PrettyList()\n        luinfo['lexemes'] = PrettyList()   # multiword LUs have multiple lexemes\n        luinfo['semTypes'] = PrettyList()  # an LU can have multiple semtypes\n        for k in ignorekeys:\n            if k in luinfo:\n                del luinfo[k]\n\n        for sub in elt:\n            if sub.tag.endswith('header'):\n                continue  # not used\n            elif sub.tag.endswith('valences'):\n                continue  # not used\n            elif sub.tag.endswith('definition') and 'definition' not in ignorekeys:\n                luinfo['definitionMarkup'] = sub.text\n                luinfo['definition'] = self._strip_tags(sub.text)\n            elif sub.tag.endswith('subCorpus') and 'subCorpus' not in ignorekeys:\n                sc = self._handle_lusubcorpus_elt(sub)\n                if sc is not None:\n                    luinfo['subCorpus'].append(sc)\n            elif sub.tag.endswith('lexeme') and 'lexeme' not in ignorekeys:\n                luinfo['lexemes'].append(self._load_xml_attributes(PrettyDict(), sub))\n            elif sub.tag.endswith('semType') and 'semType' not in ignorekeys:\n                semtypeinfo = self._load_xml_attributes(AttrDict(), sub)\n                luinfo['semTypes'].append(self.semtype(semtypeinfo.ID))\n\n        return luinfo\n\n    def _handle_lusubcorpus_elt(self, elt):\n        sc = AttrDict()\n        try:\n            sc['name'] = elt.get('name')\n        except AttributeError:\n            return None\n        sc['_type'] = \"lusubcorpus\"\n        sc['sentence'] = []\n\n        for sub in elt:\n            if sub.tag.endswith('sentence'):\n                s = self._handle_lusentence_elt(sub)\n                if s is not None:\n                    sc['sentence'].append(s)\n\n        return sc\n\n    def _handle_lusentence_elt(self, elt):\n        info = self._load_xml_attributes(AttrDict(), elt)\n        info['_type'] = 'lusentence'\n        info['annotationSet'] = []\n        info['_ascii'] = types.MethodType(_annotation_ascii, info)  # attach a method for this instance\n        for sub in elt:\n            if sub.tag.endswith('text'):\n                info['text'] = self._strip_tags(sub.text)\n            elif sub.tag.endswith('annotationSet'):\n                annset = self._handle_luannotationset_elt(sub, is_pos=(len(info['annotationSet'])==0))\n                if annset is not None:\n                    assert annset.status=='UNANN' or 'FE' in annset,annset\n                    if annset.status!='UNANN':\n                        info['frameAnnotation'] = annset\n                    for k in ('Target', 'FE', 'FE2', 'FE3', 'GF', 'PT', 'POS', 'POS_tagset',\n                              'Other', 'Sent', 'Verb', 'Noun', 'Adj', 'Adv', 'Prep', 'Scon', 'Art'):\n                        if k in annset:\n                            info[k] = annset[k]\n                    info['annotationSet'].append(annset)\n                    annset['sent'] = info\n                    annset['text'] = info.text\n        return info\n\n    def _handle_luannotationset_elt(self, elt, is_pos=False):\n        info = self._load_xml_attributes(AttrDict(), elt)\n        info['_type'] = 'posannotationset' if is_pos else 'luannotationset'\n        info['layer'] = []\n        info['_ascii'] = types.MethodType(_annotation_ascii, info)  # attach a method for this instance\n\n        if 'cxnID' in info: # ignoring construction annotations for now.\n            return info\n\n        for sub in elt:\n            if sub.tag.endswith('layer'):\n                l = self._handle_lulayer_elt(sub)\n                if l is not None:\n                    overt = []\n                    ni = {} # null instantiations\n\n                    info['layer'].append(l)\n                    for lbl in l.label:\n                        if 'start' in lbl:\n                            thespan = (lbl.start,lbl.end+1,lbl.name)\n                            if l.name not in ('Sent','Other'):  # 'Sent' and 'Other' layers sometimes contain accidental duplicate spans\n                                assert thespan not in overt,(info.ID,l.name,thespan)\n                            overt.append(thespan)\n                        else: # null instantiation\n                            if lbl.name in ni:\n                                self._warn('FE with multiple NI entries:', lbl.name, ni[lbl.name], lbl.itype)\n                            else:\n                                ni[lbl.name] = lbl.itype\n                    overt = sorted(overt)\n\n                    if l.name=='Target':\n                        if not overt:\n                            self._warn('Skipping empty Target layer in annotation set ID={0}'.format(info.ID))\n                            continue\n                        assert all(lblname=='Target' for i,j,lblname in overt)\n                        if 'Target' in info:\n                            self._warn('Annotation set {0} has multiple Target layers'.format(info.ID))\n                        else:\n                            info['Target'] = [(i,j) for (i,j,_) in overt]\n                    elif l.name=='FE':\n                        if l.rank==1:\n                            assert 'FE' not in info\n                            info['FE'] = (overt, ni)\n                        else:\n                            assert 2<=l.rank<=3,l.rank\n                            k = 'FE'+str(l.rank)\n                            assert k not in info\n                            info[k] = (overt, ni)\n                    elif l.name in ('GF', 'PT'):\n                        assert l.rank==1\n                        info[l.name] = overt\n                    elif l.name in ('BNC', 'PENN'):\n                        assert l.rank==1\n                        info['POS'] = overt\n                        info['POS_tagset'] = l.name\n                    else:\n                        if is_pos:\n                            if l.name not in ('NER', 'WSL'):\n                                self._warn('Unexpected layer in sentence annotationset:', l.name)\n                        else:\n                            if l.name not in ('Sent', 'Verb', 'Noun', 'Adj', 'Adv', 'Prep', 'Scon', 'Art', 'Other'):\n                                self._warn('Unexpected layer in frame annotationset:', l.name)\n                        info[l.name] = overt\n        if not is_pos and 'cxnID' not in info:\n            if 'Target' not in info:\n                self._warn('Missing target in annotation set ID={0}'.format(info.ID))\n            assert 'FE' in info\n            if 'FE3' in info:\n                assert 'FE2' in info\n\n        return info\n\n    def _handle_lulayer_elt(self, elt):\n        layer = self._load_xml_attributes(AttrDict(), elt)\n        layer['_type'] = 'lulayer'\n        layer['label'] = []\n\n        for sub in elt:\n            if sub.tag.endswith('label'):\n                l = self._load_xml_attributes(AttrDict(), sub)\n                if l is not None:\n                    layer['label'].append(l)\n        return layer\n\n    def _handle_fe_elt(self, elt):\n        feinfo = self._load_xml_attributes(AttrDict(), elt)\n        feinfo['_type'] = 'fe'\n        feinfo['definition'] = \"\"\n        feinfo['definitionMarkup'] = \"\"\n        feinfo['semType'] = None\n        feinfo['requiresFE'] = None\n        feinfo['excludesFE'] = None\n        for sub in elt:\n            if sub.tag.endswith('definition'):\n                feinfo['definitionMarkup'] = sub.text\n                feinfo['definition'] = self._strip_tags(sub.text)\n            elif sub.tag.endswith('semType'):\n                stinfo = self._load_xml_attributes(AttrDict(), sub)\n                feinfo['semType'] = self.semtype(stinfo.ID)\n            elif sub.tag.endswith('requiresFE'):\n                feinfo['requiresFE'] = self._load_xml_attributes(AttrDict(), sub)\n            elif sub.tag.endswith('excludesFE'):\n                feinfo['excludesFE'] = self._load_xml_attributes(AttrDict(), sub)\n\n        return feinfo\n\n    def _handle_semtype_elt(self, elt, tagspec=None):\n        semt = self._load_xml_attributes(AttrDict(), elt)\n        semt['_type'] = 'semtype'\n        semt['superType'] = None\n        semt['subTypes'] = PrettyList()\n        for sub in elt:\n            if sub.text is not None:\n                semt['definitionMarkup'] = sub.text\n                semt['definition'] = self._strip_tags(sub.text)\n            else:\n                supertypeinfo = self._load_xml_attributes(AttrDict(), sub)\n                semt['superType'] = supertypeinfo\n\n        return semt\n\n\ndef demo():\n    from nltk.corpus import framenet as fn\n\n    print('Building the indexes...')\n    fn.buildindexes()\n\n    print('Number of Frames:', len(fn.frames()))\n    print('Number of Lexical Units:', len(fn.lus()))\n    print('Number of annotated documents:', len(fn.docs()))\n    print()\n\n    print('getting frames whose name matches the (case insensitive) regex: \"(?i)medical\"')\n    medframes = fn.frames(r'(?i)medical')\n    print(\n        'Found {0} Frames whose name matches \"(?i)medical\":'.format(len(medframes)))\n    print([(f.name, f.ID) for f in medframes])\n\n    tmp_id = medframes[0].ID\n    m_frame = fn.frame(tmp_id)  # reads all info for the frame\n\n    print(\n        '\\nNumber of frame relations for the \"{0}\" ({1}) frame:'.format(m_frame.name,\n                                                                        m_frame.ID),\n        len(m_frame.frameRelations))\n    for fr in m_frame.frameRelations:\n        print('   ', fr)\n\n    print(\n        '\\nNumber of Frame Elements in the \"{0}\" frame:'.format(m_frame.name),\n        len(m_frame.FE))\n    print('   ', [x for x in m_frame.FE])\n\n    print(\n        '\\nThe \"core\" Frame Elements in the \"{0}\" frame:'.format(m_frame.name))\n    print('   ', [x.name for x in m_frame.FE.values() if x.coreType == \"Core\"])\n\n    print('\\nAll Lexical Units that are incorporated in the \"Ailment\" FE:')\n    m_frame = fn.frame(239)\n    ailment_lus = [x for x in m_frame.lexUnit.values() if 'incorporatedFE' in x and x.incorporatedFE == 'Ailment']\n    print('   ', [x.name for x in ailment_lus])\n\n    print('\\nNumber of Lexical Units in the \"{0}\" frame:'.format(m_frame.name),\n          len(m_frame.lexUnit))\n    print('  ', [x.name for x in m_frame.lexUnit.values()][:5], '...')\n\n    tmp_id = m_frame.lexUnit['ailment.n'].ID  # grab the id of the specified LU\n    luinfo = fn.lu_basic(tmp_id)  # get basic info on the LU\n    print('\\nInformation on the LU: {0}'.format(luinfo.name))\n    pprint(luinfo)\n\n    print('\\nNames of all of the corpora used for fulltext annotation:')\n    allcorpora = set([x.corpname for x in fn.docs_metadata()])\n    pprint(list(allcorpora))\n\n    firstcorp = list(allcorpora)[0]\n    firstcorp_docs = fn.docs(firstcorp)\n    print(\n        '\\nNames of the annotated documents in the \"{0}\" corpus:'.format(firstcorp))\n    pprint([x.filename for x in firstcorp_docs])\n\n    print('\\nSearching for all Frames that have a lemma that matches the regexp: \"^run.v$\":')\n    pprint(fn.frames_by_lemma(r'^run.v$'))\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\corpus\\reader\\ieer": [".py", "\nfrom __future__ import unicode_literals\n\nfrom six import string_types\n\nimport nltk\nfrom nltk import compat\nfrom nltk.corpus.reader.api import *\n\ntitles = {\n    'APW_19980314': 'Associated Press Weekly, 14 March 1998',\n    'APW_19980424': 'Associated Press Weekly, 24 April 1998',\n    'APW_19980429': 'Associated Press Weekly, 29 April 1998',\n    'NYT_19980315': 'New York Times, 15 March 1998',\n    'NYT_19980403': 'New York Times, 3 April 1998',\n    'NYT_19980407': 'New York Times, 7 April 1998',\n    }\n\ndocuments = sorted(titles)\n\n@compat.python_2_unicode_compatible\nclass IEERDocument(object):\n    def __init__(self, text, docno=None, doctype=None,\n                 date_time=None, headline=''):\n        self.text = text\n        self.docno = docno\n        self.doctype = doctype\n        self.date_time = date_time\n        self.headline = headline\n\n    def __repr__(self):\n        if self.headline:\n            headline = ' '.join(self.headline.leaves())\n        else:\n            headline = ' '.join([w for w in self.text.leaves()\n                                 if w[:1] != '<'][:12])+'...'\n        if self.docno is not None:\n            return '<IEERDocument %s: %r>' % (self.docno, headline)\n        else:\n            return '<IEERDocument: %r>' % headline\n\nclass IEERCorpusReader(CorpusReader):\n    def raw(self, fileids=None):\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n    def docs(self, fileids=None):\n        return concat([StreamBackedCorpusView(fileid, self._read_block,\n                                              encoding=enc)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def parsed_docs(self, fileids=None):\n        return concat([StreamBackedCorpusView(fileid,\n                                              self._read_parsed_block,\n                                              encoding=enc)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def _read_parsed_block(self,stream):\n        return [self._parse(doc) for doc in self._read_block(stream)\n                if self._parse(doc).docno is not None]\n\n    def _parse(self, doc):\n        val = nltk.chunk.ieerstr2tree(doc, root_label=\"DOCUMENT\")\n        if isinstance(val, dict):\n            return IEERDocument(**val)\n        else:\n            return IEERDocument(val)\n\n    def _read_block(self, stream):\n        out = []\n        while True:\n            line = stream.readline()\n            if not line: break\n            if line.strip() == '<DOC>': break\n        out.append(line)\n        while True:\n            line = stream.readline()\n            if not line: break\n            out.append(line)\n            if line.strip() == '</DOC>': break\n        return ['\\n'.join(out)]\n"], "nltk\\corpus\\reader\\indian": [".py", "\n\nfrom six import string_types\n\nfrom nltk.tag import str2tuple, map_tag\n\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\nclass IndianCorpusReader(CorpusReader):\n    def words(self, fileids=None):\n        return concat([IndianCorpusView(fileid, enc,\n                                        False, False)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def tagged_words(self, fileids=None, tagset=None):\n        if tagset and tagset != self._tagset:\n            tag_mapping_function = lambda t: map_tag(self._tagset, tagset, t)\n        else:\n            tag_mapping_function = None\n        return concat([IndianCorpusView(fileid, enc,\n                                        True, False, tag_mapping_function)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def sents(self, fileids=None):\n        return concat([IndianCorpusView(fileid, enc,\n                                        False, True)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def tagged_sents(self, fileids=None, tagset=None):\n        if tagset and tagset != self._tagset:\n            tag_mapping_function = lambda t: map_tag(self._tagset, tagset, t)\n        else:\n            tag_mapping_function = None\n        return concat([IndianCorpusView(fileid, enc,\n                                        True, True, tag_mapping_function)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def raw(self, fileids=None):\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n\nclass IndianCorpusView(StreamBackedCorpusView):\n    def __init__(self, corpus_file, encoding, tagged,\n                 group_by_sent, tag_mapping_function=None):\n        self._tagged = tagged\n        self._group_by_sent = group_by_sent\n        self._tag_mapping_function = tag_mapping_function\n        StreamBackedCorpusView.__init__(self, corpus_file, encoding=encoding)\n\n    def read_block(self, stream):\n        line = stream.readline()\n        if line.startswith('<'):\n            return []\n        sent = [str2tuple(word, sep='_') for word in line.split()]\n        if self._tag_mapping_function:\n            sent = [(w, self._tag_mapping_function(t)) for (w,t) in sent]\n        if not self._tagged: sent = [w for (w,t) in sent]\n        if self._group_by_sent:\n            return [sent]\n        else:\n            return sent\n"], "nltk\\corpus\\reader\\ipipan": [".py", "\nimport functools\n\nfrom six import string_types\n\nfrom nltk.corpus.reader.util import StreamBackedCorpusView, concat\nfrom nltk.corpus.reader.api import CorpusReader\n\ndef _parse_args(fun):\n    @functools.wraps(fun)\n    def decorator(self, fileids=None, **kwargs):\n        kwargs.pop('tags', None)\n        if not fileids:\n            fileids = self.fileids()\n        return fun(self, fileids, **kwargs)\n    return decorator\n\nclass IPIPANCorpusReader(CorpusReader):\n\n    def __init__(self, root, fileids):\n        CorpusReader.__init__(self, root, fileids, None, None)\n\n    def raw(self, fileids=None):\n        if not fileids:\n            fileids = self.fileids()\n\n        filecontents = []\n        for fileid in self._list_morph_files(fileids):\n            with open(fileid, 'r') as infile:\n                filecontents.append(infile.read())\n        return ''.join(filecontents)\n\n    def channels(self, fileids=None):\n        if not fileids:\n            fileids = self.fileids()\n        return self._parse_header(fileids, 'channel')\n\n    def domains(self, fileids=None):\n        if not fileids:\n            fileids = self.fileids()\n        return self._parse_header(fileids, 'domain')\n\n    def categories(self, fileids=None):\n        if not fileids:\n            fileids = self.fileids()\n        return [self._map_category(cat)\n                for cat in self._parse_header(fileids, 'keyTerm')]\n\n    def fileids(self, channels=None, domains=None, categories=None):\n        if channels is not None and domains is not None and \\\n                categories is not None:\n            raise ValueError('You can specify only one of channels, domains '\n                             'and categories parameter at once')\n        if channels is None and domains is None and \\\n                categories is None:\n            return CorpusReader.fileids(self)\n        if isinstance(channels, string_types):\n            channels = [channels]\n        if isinstance(domains, string_types):\n            domains = [domains]\n        if isinstance(categories, string_types):\n            categories = [categories]\n        if channels:\n            return self._list_morph_files_by('channel', channels)\n        elif domains:\n            return self._list_morph_files_by('domain', domains)\n        else:\n            return self._list_morph_files_by('keyTerm', categories,\n                    map=self._map_category)\n\n    @_parse_args\n    def sents(self, fileids=None, **kwargs):\n        return concat([self._view(fileid,\n            mode=IPIPANCorpusView.SENTS_MODE, tags=False, **kwargs)\n            for fileid in self._list_morph_files(fileids)])\n\n    @_parse_args\n    def paras(self, fileids=None, **kwargs):\n        return concat([self._view(fileid,\n            mode=IPIPANCorpusView.PARAS_MODE, tags=False, **kwargs)\n            for fileid in self._list_morph_files(fileids)])\n\n    @_parse_args\n    def words(self, fileids=None, **kwargs):\n        return concat([self._view(fileid, tags=False, **kwargs)\n            for fileid in self._list_morph_files(fileids)])\n\n    @_parse_args\n    def tagged_sents(self, fileids=None, **kwargs):\n        return concat([self._view(fileid, mode=IPIPANCorpusView.SENTS_MODE,\n            **kwargs)\n            for fileid in self._list_morph_files(fileids)])\n\n    @_parse_args\n    def tagged_paras(self, fileids=None, **kwargs):\n        return concat([self._view(fileid, mode=IPIPANCorpusView.PARAS_MODE,\n            **kwargs)\n            for fileid in self._list_morph_files(fileids)])\n\n    @_parse_args\n    def tagged_words(self, fileids=None, **kwargs):\n        return concat([self._view(fileid, **kwargs)\n            for fileid in self._list_morph_files(fileids)])\n\n    def _list_morph_files(self, fileids):\n        return [f for f in self.abspaths(fileids)]\n\n    def _list_header_files(self, fileids):\n        return [f.replace('morph.xml', 'header.xml')\n                for f in self._list_morph_files(fileids)]\n\n    def _parse_header(self, fileids, tag):\n        values = set()\n        for f in self._list_header_files(fileids):\n            values_list = self._get_tag(f, tag)\n            for v in values_list:\n                values.add(v)\n        return list(values)\n\n    def _list_morph_files_by(self, tag, values, map=None):\n        fileids = self.fileids()\n        ret_fileids = set()\n        for f in fileids:\n            fp = self.abspath(f).replace('morph.xml', 'header.xml')\n            values_list = self._get_tag(fp, tag)\n            for value in values_list:\n                if map is not None:\n                    value = map(value)\n                if value in values:\n                    ret_fileids.add(f)\n        return list(ret_fileids)\n\n    def _get_tag(self, f, tag):\n        tags = []\n        with open(f, 'r') as infile:\n            header = infile.read()\n        tag_end = 0\n        while True:\n            tag_pos = header.find('<'+tag, tag_end)\n            if tag_pos < 0: return tags\n            tag_end = header.find('</'+tag+'>', tag_pos)\n            tags.append(header[tag_pos+len(tag)+2:tag_end])\n\n    def _map_category(self, cat):\n        pos = cat.find('>')\n        if pos == -1:\n            return cat\n        else:\n            return cat[pos+1:]\n\n    def _view(self, filename, **kwargs):\n        tags = kwargs.pop('tags', True)\n        mode = kwargs.pop('mode', 0)\n        simplify_tags = kwargs.pop('simplify_tags', False)\n        one_tag = kwargs.pop('one_tag', True)\n        disamb_only = kwargs.pop('disamb_only', True)\n        append_no_space = kwargs.pop('append_no_space', False)\n        append_space = kwargs.pop('append_space', False)\n        replace_xmlentities = kwargs.pop('replace_xmlentities', True)\n\n        if len(kwargs) > 0:\n            raise ValueError('Unexpected arguments: %s' % kwargs.keys())\n        if not one_tag and not disamb_only:\n            raise ValueError('You cannot specify both one_tag=False and '\n                             'disamb_only=False')\n        if not tags and (simplify_tags or not one_tag or not disamb_only):\n            raise ValueError('You cannot specify simplify_tags, one_tag or '\n                             'disamb_only with functions other than tagged_*')\n\n        return IPIPANCorpusView(filename,\n                 tags=tags, mode=mode, simplify_tags=simplify_tags,\n                 one_tag=one_tag, disamb_only=disamb_only,\n                 append_no_space=append_no_space,\n                 append_space=append_space,\n                 replace_xmlentities=replace_xmlentities\n                 )\n\n\nclass IPIPANCorpusView(StreamBackedCorpusView):\n\n    WORDS_MODE = 0\n    SENTS_MODE = 1\n    PARAS_MODE = 2\n\n    def __init__(self, filename, startpos=0, **kwargs):\n        StreamBackedCorpusView.__init__(self, filename, None, startpos, None)\n        self.in_sentence = False\n        self.position = 0\n\n        self.show_tags = kwargs.pop('tags', True)\n        self.disamb_only = kwargs.pop('disamb_only', True)\n        self.mode = kwargs.pop('mode', IPIPANCorpusView.WORDS_MODE)\n        self.simplify_tags = kwargs.pop('simplify_tags', False)\n        self.one_tag = kwargs.pop('one_tag', True)\n        self.append_no_space = kwargs.pop('append_no_space', False)\n        self.append_space = kwargs.pop('append_space', False)\n        self.replace_xmlentities = kwargs.pop('replace_xmlentities', True)\n\n    def read_block(self, stream):\n        sentence = []\n        sentences = []\n        space = False\n        no_space = False\n\n        tags = set()\n\n        lines = self._read_data(stream)\n\n        while True:\n\n            if len(lines) <= 1:\n                self._seek(stream)\n                lines = self._read_data(stream)\n\n            if lines == ['']:\n                assert not sentences\n                return []\n\n            line = lines.pop()\n            self.position += len(line) + 1\n\n            if line.startswith('<chunk type=\"s\"'):\n                self.in_sentence = True\n            elif line.startswith('<chunk type=\"p\"'):\n                pass\n            elif line.startswith('<tok'):\n                if self.append_space and space and not no_space:\n                    self._append_space(sentence)\n                space = True\n                no_space = False\n                orth = \"\"\n                tags = set()\n            elif line.startswith('</chunk'):\n                if self.in_sentence:\n                    self.in_sentence = False\n                    self._seek(stream)\n                    if self.mode == self.SENTS_MODE:\n                        return [sentence]\n                    elif self.mode == self.WORDS_MODE:\n                        if self.append_space:\n                            self._append_space(sentence)\n                        return sentence\n                    else:\n                        sentences.append(sentence)\n                elif self.mode == self.PARAS_MODE:\n                    self._seek(stream)\n                    return [sentences]\n            elif line.startswith('<orth'):\n                orth = line[6:-7]\n                if self.replace_xmlentities:\n                    orth = orth.replace('&quot;', '\"').replace('&amp;', '&')\n            elif line.startswith('<lex'):\n                if not self.disamb_only or line.find('disamb=') != -1:\n                    tag = line[line.index('<ctag')+6 : line.index('</ctag') ]\n                    tags.add(tag)\n            elif line.startswith('</tok'):\n                if self.show_tags:\n                    if self.simplify_tags:\n                        tags = [t.split(':')[0] for t in tags]\n                    if not self.one_tag or not self.disamb_only:\n                        sentence.append((orth, tuple(tags)))\n                    else:\n                        sentence.append((orth, tags.pop()))\n                else:\n                    sentence.append(orth)\n            elif line.startswith('<ns/>'):\n                if self.append_space:\n                    no_space = True\n                if self.append_no_space:\n                    if self.show_tags:\n                        sentence.append(('', 'no-space'))\n                    else:\n                        sentence.append('')\n            elif line.startswith('</cesAna'):\n                pass\n\n    def _read_data(self, stream):\n        self.position = stream.tell()\n        buff = stream.read(4096)\n        lines = buff.split('\\n')\n        lines.reverse()\n        return lines\n\n    def _seek(self, stream):\n        stream.seek(self.position)\n\n    def _append_space(self, sentence):\n        if self.show_tags:\n            sentence.append((' ', 'space'))\n        else:\n            sentence.append(' ')\n"], "nltk\\corpus\\reader\\knbc": [".py", "\nfrom __future__ import print_function\n\nimport re\nfrom six import string_types\n\nfrom nltk.parse import DependencyGraph\n\nfrom nltk.corpus.reader.util import (\n    FileSystemPathPointer,\n    find_corpus_fileids,\n    read_blankline_block,\n)\nfrom nltk.corpus.reader.api import SyntaxCorpusReader, CorpusReader\n\n_morphs2str_default = lambda morphs: '/'.join(m[0] for m in morphs if m[0] != 'EOS')\n\n\nclass KNBCorpusReader(SyntaxCorpusReader):\n\n    def __init__(self, root, fileids, encoding='utf8', morphs2str=_morphs2str_default):\n        CorpusReader.__init__(self, root, fileids, encoding)\n        self.morphs2str = morphs2str\n\n    def _read_block(self, stream):\n        return read_blankline_block(stream)\n\n    def _word(self, t):\n        res = []\n        for line in t.splitlines():\n            if not re.match(r\"EOS|\\*|\\#|\\+\", line):\n                cells = line.strip().split(\" \")\n                res.append(cells[0])\n\n        return res\n\n    def _tag(self, t, tagset=None):\n        res = []\n        for line in t.splitlines():\n            if not re.match(r\"EOS|\\*|\\#|\\+\", line):\n                cells = line.strip().split(\" \")\n                res.append((cells[0], ' '.join(cells[1:])))\n\n        return res\n\n    def _parse(self, t):\n        dg = DependencyGraph()\n        i = 0\n        for line in t.splitlines():\n            if line[0] in '*+':\n\n                cells = line.strip().split(\" \", 3)\n                m = re.match(r\"([\\-0-9]*)([ADIP])\", cells[1])\n\n                assert m is not None\n\n                node = dg.nodes[i]\n                node.update(\n                    {\n                        'address': i,\n                        'rel': m.group(2),\n                        'word': [],\n                    }\n                )\n\n                dep_parent = int(m.group(1))\n\n                if dep_parent == -1:\n                    dg.root = node\n                else:\n                    dg.nodes[dep_parent]['deps'].append(i)\n\n                i += 1\n            elif line[0] != '#':\n                cells = line.strip().split(\" \")\n                morph = cells[0], ' '.join(cells[1:])\n                dg.nodes[i - 1]['word'].append(morph)\n\n        if self.morphs2str:\n            for node in dg.nodes.values():\n                node['word'] = self.morphs2str(node['word'])\n\n        return dg.tree()\n\n\n\ndef demo():\n\n    import nltk\n    from nltk.corpus.util import LazyCorpusLoader\n\n    root = nltk.data.find('corpora/knbc/corpus1')\n    fileids = [f for f in find_corpus_fileids(FileSystemPathPointer(root), \".*\")\n               if re.search(r\"\\d\\-\\d\\-[\\d]+\\-[\\d]+\", f)]\n\n    def _knbc_fileids_sort(x):\n        cells = x.split('-')\n        return (cells[0], int(cells[1]), int(cells[2]), int(cells[3]))\n\n    knbc = LazyCorpusLoader('knbc/corpus1', KNBCorpusReader,\n                            sorted(fileids, key=_knbc_fileids_sort), encoding='euc-jp')\n\n    print(knbc.fileids()[:10])\n    print(''.join(knbc.words()[:100]))\n\n    print('\\n\\n'.join(str(tree) for tree in knbc.parsed_sents()[:2]))\n\n    knbc.morphs2str = lambda morphs: '/'.join(\n        \"%s(%s)\" % (m[0], m[1].split(' ')[2]) for m in morphs if m[0] != 'EOS'\n    ).encode('utf-8')\n\n    print('\\n\\n'.join('%s' % tree for tree in knbc.parsed_sents()[:2]))\n\n    print(\n        '\\n'.join(\n            ' '.join(\"%s/%s\" % (w[0], w[1].split(' ')[2]) for w in sent)\n            for sent in knbc.tagged_sents()[0:2]\n        )\n    )\n\n\ndef test():\n\n    from nltk.corpus.util import LazyCorpusLoader\n    knbc = LazyCorpusLoader(\n        'knbc/corpus1', KNBCorpusReader, r'.*/KN.*', encoding='euc-jp')\n    assert isinstance(knbc.words()[0], string_types)\n    assert isinstance(knbc.sents()[0][0], string_types)\n    assert isinstance(knbc.tagged_words()[0], tuple)\n    assert isinstance(knbc.tagged_sents()[0][0], tuple)\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\corpus\\reader\\lin": [".py", "from __future__ import print_function\n\nimport re\nfrom collections import defaultdict\nfrom functools import reduce\n\nfrom nltk.corpus.reader import CorpusReader\n\n\nclass LinThesaurusCorpusReader(CorpusReader):\n\n    _key_re = re.compile(r'\\(\"?([^\"]+)\"? \\(desc [0-9.]+\\).+')\n\n    @staticmethod\n    def __defaultdict_factory():\n        ''' Factory for creating defaultdict of defaultdict(dict)s '''\n        return defaultdict(dict)\n\n    def __init__(self, root, badscore=0.0):\n        '''\n        Initialize the thesaurus.\n\n        :param root: root directory containing thesaurus LISP files\n        :type root: C{string}\n        :param badscore: the score to give to words which do not appear in each other's sets of synonyms\n        :type badscore: C{float}\n        '''\n\n        super(LinThesaurusCorpusReader, self).__init__(root, r'sim[A-Z]\\.lsp')\n        self._thesaurus = defaultdict(LinThesaurusCorpusReader.__defaultdict_factory)\n        self._badscore = badscore\n        for path, encoding, fileid in self.abspaths(include_encoding=True, include_fileid=True):\n            with open(path) as lin_file:\n                first = True\n                for line in lin_file:\n                    line = line.strip()\n                    if first:\n                        key = LinThesaurusCorpusReader._key_re.sub(r'\\1', line)\n                        first = False\n                    elif line == '))':\n                        first = True\n                    else:\n                        split_line = line.split('\\t')\n                        if len(split_line) == 2:\n                            ngram, score = split_line\n                            self._thesaurus[fileid][key][ngram.strip('\"')] = float(score)\n\n    def similarity(self, ngram1, ngram2, fileid=None):\n        '''\n        Returns the similarity score for two ngrams.\n\n        :param ngram1: first ngram to compare\n        :type ngram1: C{string}\n        :param ngram2: second ngram to compare\n        :type ngram2: C{string}\n        :param fileid: thesaurus fileid to search in. If None, search all fileids.\n        :type fileid: C{string}\n        :return: If fileid is specified, just the score for the two ngrams; otherwise,\n                 list of tuples of fileids and scores.\n        '''\n        if ngram1 == ngram2:\n            if fileid:\n                return 1.0\n            else:\n                return [(fid, 1.0) for fid in self._fileids]\n        else:\n            if fileid:\n                return self._thesaurus[fileid][ngram1][ngram2] if ngram2 in self._thesaurus[fileid][ngram1] else self._badscore\n            else:\n                return [(fid, (self._thesaurus[fid][ngram1][ngram2] if ngram2 in self._thesaurus[fid][ngram1]\n                                  else self._badscore)) for fid in self._fileids]\n\n    def scored_synonyms(self, ngram, fileid=None):\n        '''\n        Returns a list of scored synonyms (tuples of synonyms and scores) for the current ngram\n\n        :param ngram: ngram to lookup\n        :type ngram: C{string}\n        :param fileid: thesaurus fileid to search in. If None, search all fileids.\n        :type fileid: C{string}\n        :return: If fileid is specified, list of tuples of scores and synonyms; otherwise,\n                 list of tuples of fileids and lists, where inner lists consist of tuples of\n                 scores and synonyms.\n        '''\n        if fileid:\n            return self._thesaurus[fileid][ngram].items()\n        else:\n            return [(fileid, self._thesaurus[fileid][ngram].items()) for fileid in self._fileids]\n\n    def synonyms(self, ngram, fileid=None):\n        '''\n        Returns a list of synonyms for the current ngram.\n\n        :param ngram: ngram to lookup\n        :type ngram: C{string}\n        :param fileid: thesaurus fileid to search in. If None, search all fileids.\n        :type fileid: C{string}\n        :return: If fileid is specified, list of synonyms; otherwise, list of tuples of fileids and\n                 lists, where inner lists contain synonyms.\n        '''\n        if fileid:\n            return self._thesaurus[fileid][ngram].keys()\n        else:\n            return [(fileid, self._thesaurus[fileid][ngram].keys()) for fileid in self._fileids]\n\n    def __contains__(self, ngram):\n        '''\n        Determines whether or not the given ngram is in the thesaurus.\n\n        :param ngram: ngram to lookup\n        :type ngram: C{string}\n        :return: whether the given ngram is in the thesaurus.\n        '''\n        return reduce(lambda accum, fileid: accum or (ngram in self._thesaurus[fileid]), self._fileids, False)\n\n\n\ndef demo():\n    from nltk.corpus import lin_thesaurus as thes\n\n    word1 = \"business\"\n    word2 = \"enterprise\"\n    print(\"Getting synonyms for \" + word1)\n    print(thes.synonyms(word1))\n\n    print(\"Getting scored synonyms for \" + word1)\n    print(thes.scored_synonyms(word1))\n\n    print(\"Getting synonyms from simN.lsp (noun subsection) for \" + word1)\n    print(thes.synonyms(word1, fileid=\"simN.lsp\"))\n\n    print(\"Getting synonyms from simN.lsp (noun subsection) for \" + word1)\n    print(thes.synonyms(word1, fileid=\"simN.lsp\"))\n\n    print(\"Similarity score for %s and %s:\" % (word1, word2))\n    print(thes.similarity(word1, word2))\n\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\corpus\\reader\\mte": [".py", "import os\nimport re\nfrom functools import reduce\n\nfrom six import string_types\n\nfrom nltk.corpus.reader import concat, TaggedCorpusReader\nfrom nltk.corpus.reader.xmldocs import XMLCorpusView\n\n\ndef xpath(root, path, ns):\n    return root.findall(path, ns)\n\nclass MTECorpusView(XMLCorpusView):\n\n    def __init__(self, fileid, tagspec, elt_handler=None):\n        XMLCorpusView.__init__(self, fileid, tagspec, elt_handler)\n\n    def read_block(self, stream, tagspec=None, elt_handler=None):\n        return list(filter(lambda x: x is not None, XMLCorpusView.read_block(self, stream, tagspec, elt_handler)))\n\nclass MTEFileReader:\n    ns = {'tei': 'http://www.tei-c.org/ns/1.0',\n          'xml': 'http://www.w3.org/XML/1998/namespace'}\n    tag_ns = '{http://www.tei-c.org/ns/1.0}'\n    xml_ns = '{http://www.w3.org/XML/1998/namespace}'\n    word_path = \"TEI/text/body/div/div/p/s/(w|c)\"\n    sent_path = \"TEI/text/body/div/div/p/s\"\n    para_path = \"TEI/text/body/div/div/p\"\n\n\n    def __init__(self, file_path):\n        self.__file_path = file_path\n\n    @classmethod\n    def _word_elt(self, elt, context):\n        return elt.text\n\n    @classmethod\n    def _sent_elt(self, elt, context):\n        return [self._word_elt(w, None) for w in xpath(elt, '*', self.ns)]\n\n    @classmethod\n    def _para_elt(self, elt, context):\n        return [self._sent_elt(s, None) for s in xpath(elt, '*', self.ns)]\n\n    @classmethod\n    def _tagged_word_elt(self, elt, context):\n        if ('ana' not in elt.attrib):\n            return (elt.text, '')\n\n        if self.__tags == \"\" and self.__tagset == \"msd\":\n            return (elt.text, elt.attrib['ana'])\n        elif self.__tags == \"\" and self.__tagset == \"universal\":\n            return (elt.text, MTETagConverter.msd_to_universal(elt.attrib['ana']))\n        else:\n            tags = re.compile('^' + re.sub(\"-\", \".\", self.__tags) + '.*$')\n            if (tags.match(elt.attrib['ana'])):\n                if self.__tagset == \"msd\":\n                    return (elt.text, elt.attrib['ana'])\n                else:\n                    return (elt.text, MTETagConverter.msd_to_universal(elt.attrib['ana']))\n            else:\n                return None\n\n    @classmethod\n    def _tagged_sent_elt(self, elt, context):\n        return list(filter(lambda x: x is not None, [self._tagged_word_elt(w, None) for w in xpath(elt, '*', self.ns)]))\n\n    @classmethod\n    def _tagged_para_elt(self, elt, context):\n        return list(filter(lambda x: x is not None, [self._tagged_sent_elt(s, None) for s in xpath(elt, '*', self.ns)]))\n\n    @classmethod\n    def _lemma_word_elt(self, elt, context):\n        if ('lemma' not in elt.attrib):\n            return (elt.text, '')\n        else:\n            return (elt.text, elt.attrib['lemma'])\n\n    @classmethod\n    def _lemma_sent_elt(self, elt, context):\n        return [self._lemma_word_elt(w, None) for w in xpath(elt, '*', self.ns)]\n\n    @classmethod\n    def _lemma_para_elt(self, elt, context):\n        return [self._lemma_sent_elt(s, None) for s in xpath(elt, '*', self.ns)]\n\n    def words(self):\n        return MTECorpusView(self.__file_path, MTEFileReader.word_path, MTEFileReader._word_elt)\n\n    def sents(self):\n        return MTECorpusView(self.__file_path, MTEFileReader.sent_path, MTEFileReader._sent_elt)\n\n    def paras(self):\n        return MTECorpusView(self.__file_path, MTEFileReader.para_path, MTEFileReader._para_elt)\n\n    def lemma_words(self):\n        return MTECorpusView(self.__file_path, MTEFileReader.word_path, MTEFileReader._lemma_word_elt)\n\n    def tagged_words(self, tagset, tags):\n        MTEFileReader.__tagset = tagset\n        MTEFileReader.__tags = tags\n        return MTECorpusView(self.__file_path, MTEFileReader.word_path, MTEFileReader._tagged_word_elt)\n\n    def lemma_sents(self):\n        return MTECorpusView(self.__file_path, MTEFileReader.sent_path, MTEFileReader._lemma_sent_elt)\n\n    def tagged_sents(self, tagset, tags):\n        MTEFileReader.__tagset = tagset\n        MTEFileReader.__tags = tags\n        return MTECorpusView(self.__file_path, MTEFileReader.sent_path, MTEFileReader._tagged_sent_elt)\n\n    def lemma_paras(self):\n        return MTECorpusView(self.__file_path, MTEFileReader.para_path, MTEFileReader._lemma_para_elt)\n\n    def tagged_paras(self, tagset, tags):\n        MTEFileReader.__tagset = tagset\n        MTEFileReader.__tags = tags\n        return MTECorpusView(self.__file_path, MTEFileReader.para_path, MTEFileReader._tagged_para_elt)\n\n\nclass MTETagConverter:\n\n    mapping_msd_universal = {\n        'A': 'ADJ', 'S': 'ADP', 'R': 'ADV', 'C': 'CONJ',\n        'D': 'DET', 'N': 'NOUN', 'M': 'NUM', 'Q': 'PRT',\n        'P': 'PRON', 'V': 'VERB', '.': '.', '-': 'X'}\n\n    @staticmethod\n    def msd_to_universal(tag):\n        indicator = tag[0] if not tag[0] == \"#\" else tag[1]\n\n        if not indicator in MTETagConverter.mapping_msd_universal:\n            indicator = '-'\n\n        return MTETagConverter.mapping_msd_universal[indicator]\n\nclass MTECorpusReader(TaggedCorpusReader):\n\n    def __init__(self, root=None, fileids=None, encoding='utf8'):\n        TaggedCorpusReader.__init__(self, root, fileids, encoding)\n\n    def __fileids(self, fileids):\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        fileids = filter(lambda x : x in self._fileids, fileids)\n        fileids = filter(lambda x : x not in [\"oana-bg.xml\", \"oana-mk.xml\"], fileids)\n        if not fileids:\n            print(\"No valid multext-east file specified\")\n        return fileids\n\n    def readme(self):\n        return self.open(\"00README.txt\").read()\n\n    def raw(self, fileids=None):\n        return reduce([self.open(f).read() for f in self.__fileids(fileids)], [])\n\n    def words(self, fileids=None):\n        return  concat([MTEFileReader(os.path.join(self._root, f)).words() for f in self.__fileids(fileids)])\n\n    def sents(self, fileids=None):\n        return  concat([MTEFileReader(os.path.join(self._root, f)).sents() for f in self.__fileids(fileids)])\n\n    def paras(self, fileids=None):\n        return  concat([MTEFileReader(os.path.join(self._root, f)).paras() for f in self.__fileids(fileids)])\n\n    def lemma_words(self, fileids=None):\n        return  concat([MTEFileReader(os.path.join(self._root, f)).lemma_words() for f in self.__fileids(fileids)])\n\n    def tagged_words(self, fileids=None, tagset=\"msd\", tags=\"\"):\n        if tagset == \"universal\" or tagset == \"msd\":\n            return concat([MTEFileReader(os.path.join(self._root, f)).tagged_words(tagset, tags) for f in self.__fileids(fileids)])\n        else:\n            print(\"Unknown tagset specified.\")\n\n    def lemma_sents(self, fileids=None):\n        return  concat([MTEFileReader(os.path.join(self._root, f)).lemma_sents() for f in self.__fileids(fileids)])\n\n\n    def tagged_sents(self, fileids=None, tagset=\"msd\", tags=\"\"):\n        if tagset == \"universal\" or tagset == \"msd\":\n            return concat([MTEFileReader(os.path.join(self._root, f)).tagged_sents(tagset, tags) for f in self.__fileids(fileids)])\n        else:\n            print(\"Unknown tagset specified.\")\n\n    def lemma_paras(self, fileids=None):\n        return concat([MTEFileReader(os.path.join(self._root, f)).lemma_paras() for f in self.__fileids(fileids)])\n\n    def tagged_paras(self, fileids=None, tagset=\"msd\", tags=\"\"):\n        if tagset == \"universal\" or tagset == \"msd\":\n            return concat([MTEFileReader(os.path.join(self._root, f)).tagged_paras(tagset, tags) for f in self.__fileids(fileids)])\n        else:\n            print(\"Unknown tagset specified.\")\n"], "nltk\\corpus\\reader\\nkjp": [".py", "\nimport functools\nimport os\nimport tempfile\n\nfrom six import string_types\n\nfrom nltk.corpus.reader.util import concat\nfrom nltk.corpus.reader.xmldocs import XMLCorpusReader, XMLCorpusView\nimport re\n\n\ndef _parse_args(fun):\n    @functools.wraps(fun)\n    def decorator(self, fileids=None, **kwargs):\n        if not fileids:\n            fileids = self._paths\n        return fun(self, fileids, **kwargs)\n\n    return decorator\n\n\nclass NKJPCorpusReader(XMLCorpusReader):\n    WORDS_MODE = 0\n    SENTS_MODE = 1\n    HEADER_MODE = 2\n    RAW_MODE = 3\n\n    def __init__(self, root, fileids='.*'):\n        if isinstance(fileids, string_types):\n            XMLCorpusReader.__init__(self, root, fileids + '.*/header.xml')\n        else:\n            XMLCorpusReader.__init__(self, root, [fileid + '/header.xml' for fileid in fileids])\n        self._paths = self.get_paths()\n\n    def get_paths(self):\n        return [os.path.join(str(self._root), f.split(\"header.xml\")[0]) for f in self._fileids]\n\n    def fileids(self):\n        return [f.split(\"header.xml\")[0] for f in self._fileids]\n\n    def _view(self, filename, tags=None, **kwargs):\n        mode = kwargs.pop('mode', NKJPCorpusReader.WORDS_MODE)\n        if mode is NKJPCorpusReader.WORDS_MODE:\n            return NKJPCorpus_Morph_View(filename, tags=tags)\n        elif mode is NKJPCorpusReader.SENTS_MODE:\n            return NKJPCorpus_Segmentation_View(filename, tags=tags)\n        elif mode is NKJPCorpusReader.HEADER_MODE:\n            return NKJPCorpus_Header_View(filename, tags=tags)\n        elif mode is NKJPCorpusReader.RAW_MODE:\n            return NKJPCorpus_Text_View(filename, tags=tags, mode=NKJPCorpus_Text_View.RAW_MODE)\n\n        else:\n            raise NameError('No such mode!')\n\n    def add_root(self, fileid):\n        if self.root in fileid:\n            return fileid\n        return self.root + fileid\n\n    @_parse_args\n    def header(self, fileids=None, **kwargs):\n        return concat([self._view(self.add_root(fileid),\n                                  mode=NKJPCorpusReader.HEADER_MODE, **kwargs).handle_query()\n                       for fileid in fileids])\n\n    @_parse_args\n    def sents(self, fileids=None, **kwargs):\n        return concat([self._view(self.add_root(fileid),\n                                  mode=NKJPCorpusReader.SENTS_MODE, **kwargs).handle_query()\n                       for fileid in fileids])\n\n    @_parse_args\n    def words(self, fileids=None, **kwargs):\n\n        return concat([self._view(self.add_root(fileid),\n                                  mode=NKJPCorpusReader.WORDS_MODE, **kwargs).handle_query()\n                       for fileid in fileids])\n\n    @_parse_args\n    def tagged_words(self, fileids=None, **kwargs):\n        tags = kwargs.pop('tags', [])\n        return concat([self._view(self.add_root(fileid),\n                                  mode=NKJPCorpusReader.WORDS_MODE, tags=tags, **kwargs).handle_query()\n                       for fileid in fileids])\n\n    @_parse_args\n    def raw(self, fileids=None, **kwargs):\n        return concat([self._view(self.add_root(fileid),\n                                  mode=NKJPCorpusReader.RAW_MODE, **kwargs).handle_query()\n                       for fileid in fileids])\n\n\nclass NKJPCorpus_Header_View(XMLCorpusView):\n\n    def __init__(self, filename, **kwargs):\n        self.tagspec = \".*/sourceDesc$\"\n        XMLCorpusView.__init__(self, filename + 'header.xml', self.tagspec)\n\n    def handle_query(self):\n        self._open()\n        header = []\n        while True:\n            segm = XMLCorpusView.read_block(self, self._stream)\n            if len(segm) == 0:\n                break\n            header.extend(segm)\n        self.close()\n        return header\n\n    def handle_elt(self, elt, context):\n        titles = elt.findall('bibl/title')\n        title = []\n        if titles:\n            title = '\\n'.join(title.text.strip() for title in titles)\n\n        authors = elt.findall('bibl/author')\n        author = []\n        if authors:\n            author = '\\n'.join(author.text.strip() for author in authors)\n\n        dates = elt.findall('bibl/date')\n        date = []\n        if dates:\n            date = '\\n'.join(date.text.strip() for date in dates)\n\n        publishers = elt.findall('bibl/publisher')\n        publisher = []\n        if publishers:\n            publisher = '\\n'.join(publisher.text.strip() for publisher in publishers)\n\n        idnos = elt.findall('bibl/idno')\n        idno = []\n        if idnos:\n            idno = '\\n'.join(idno.text.strip() for idno in idnos)\n\n        notes = elt.findall('bibl/note')\n        note = []\n        if notes:\n            note = '\\n'.join(note.text.strip() for note in notes)\n\n        return {'title': title, 'author': author, 'date': date, 'publisher': publisher,\n                'idno': idno, 'note': note}\n\n\nclass XML_Tool():\n    def __init__(self, root, filename):\n        self.read_file = os.path.join(root, filename)\n        self.write_file = tempfile.NamedTemporaryFile(delete=False)\n\n    def build_preprocessed_file(self):\n        try:\n            fr = open(self.read_file, 'r')\n            fw = self.write_file\n            line = ' '\n            while len(line):\n                line = fr.readline()\n                x = re.split(r'nkjp:[^ ]* ', line)  #in all files\n                ret = ' '.join(x)\n                x = re.split('<nkjp:paren>', ret)   #in ann_segmentation.xml\n                ret = ' '.join(x)\n                x = re.split('</nkjp:paren>', ret)  #in ann_segmentation.xml\n                ret = ' '.join(x)\n                x = re.split('<choice>', ret)   #in ann_segmentation.xml\n                ret = ' '.join(x)\n                x = re.split('</choice>', ret)  #in ann_segmentation.xml\n                ret = ' '.join(x)\n                fw.write(ret)\n            fr.close()\n            fw.close()\n            return self.write_file.name\n        except Exception:\n            self.remove_preprocessed_file()\n            raise Exception\n\n    def remove_preprocessed_file(self):\n        os.remove(self.write_file.name)\n        pass\n\n\nclass NKJPCorpus_Segmentation_View(XMLCorpusView):\n\n    def __init__(self, filename, **kwargs):\n        self.tagspec = '.*p/.*s'\n        self.text_view = NKJPCorpus_Text_View(filename, mode=NKJPCorpus_Text_View.SENTS_MODE)\n        self.text_view.handle_query()\n        self.xml_tool = XML_Tool(filename, 'ann_segmentation.xml')\n        XMLCorpusView.__init__(self, self.xml_tool.build_preprocessed_file(), self.tagspec)\n\n    def get_segm_id(self, example_word):\n        return example_word.split('(')[1].split(',')[0]\n\n    def get_sent_beg(self, beg_word):\n        return int(beg_word.split(',')[1])\n\n    def get_sent_end(self, end_word):\n        splitted = end_word.split(')')[0].split(',')\n        return int(splitted[1]) + int(splitted[2])\n\n    def get_sentences(self, sent_segm):\n        id = self.get_segm_id(sent_segm[0])\n        segm = self.text_view.segm_dict[id]    #text segment\n        beg = self.get_sent_beg(sent_segm[0])\n        end = self.get_sent_end(sent_segm[len(sent_segm)-1])\n        return segm[beg:end]\n\n    def remove_choice(self, segm):\n        ret = []\n        prev_txt_end = -1\n        prev_txt_nr = -1\n        for word in segm:\n            txt_nr = self.get_segm_id(word)\n            if self.get_sent_beg(word) > prev_txt_end-1 or prev_txt_nr != txt_nr:\n                ret.append(word)\n                prev_txt_end = self.get_sent_end(word)\n            prev_txt_nr = txt_nr\n\n        return ret\n\n    def handle_query(self):\n        try:\n            self._open()\n            sentences = []\n            while True:\n                sent_segm = XMLCorpusView.read_block(self, self._stream)\n                if len(sent_segm) == 0:\n                    break\n                for segm in sent_segm:\n                    segm = self.remove_choice(segm)\n                    sentences.append(self.get_sentences(segm))\n            self.close()\n            self.xml_tool.remove_preprocessed_file()\n            return sentences\n        except Exception:\n            self.xml_tool.remove_preprocessed_file()\n            raise Exception\n\n    def handle_elt(self, elt, context):\n        ret = []\n        for seg in elt:\n            ret.append(seg.get('corresp'))\n        return ret\n\n\nclass NKJPCorpus_Text_View(XMLCorpusView):\n    SENTS_MODE = 0\n    RAW_MODE = 1\n\n    def __init__(self, filename, **kwargs):\n        self.mode = kwargs.pop('mode', 0)\n        self.tagspec = '.*/div/ab'\n        self.segm_dict = dict()\n        self.xml_tool = XML_Tool(filename, 'text.xml')\n        XMLCorpusView.__init__(self, self.xml_tool.build_preprocessed_file(), self.tagspec)\n\n    def handle_query(self):\n        try:\n            self._open()\n            x = self.read_block(self._stream)\n            self.close()\n            self.xml_tool.remove_preprocessed_file()\n            return x\n        except Exception:\n            self.xml_tool.remove_preprocessed_file()\n            raise Exception\n\n    def read_block(self, stream, tagspec=None, elt_handler=None):\n        txt = []\n        while True:\n            segm = XMLCorpusView.read_block(self, stream)\n            if len(segm) == 0:\n                break\n            for part in segm:\n                txt.append(part)\n\n        return [' '.join([segm for segm in txt])]\n\n    def get_segm_id(self, elt):\n        for attr in elt.attrib:\n            if attr.endswith('id'):\n                return elt.get(attr)\n\n    def handle_elt(self, elt, context):\n        if self.mode is NKJPCorpus_Text_View.SENTS_MODE:\n            self.segm_dict[self.get_segm_id(elt)] = elt.text\n        return elt.text\n\n\nclass NKJPCorpus_Morph_View(XMLCorpusView):\n\n    def __init__(self, filename, **kwargs):\n        self.tags = kwargs.pop('tags', None)\n        self.tagspec = '.*/seg/fs'\n        self.xml_tool = XML_Tool(filename, 'ann_morphosyntax.xml')\n        XMLCorpusView.__init__(self, self.xml_tool.build_preprocessed_file(), self.tagspec)\n\n    def handle_query(self):\n        try:\n            self._open()\n            words = []\n            while True:\n                segm = XMLCorpusView.read_block(self, self._stream)\n                if len(segm) == 0:\n                    break\n                for part in segm:\n                    if part is not None:\n                        words.append(part)\n            self.close()\n            self.xml_tool.remove_preprocessed_file()\n            return words\n        except Exception:\n            self.xml_tool.remove_preprocessed_file()\n            raise Exception\n\n    def handle_elt(self, elt, context):\n        word = ''\n        flag = False\n        is_not_interp = True\n        if self.tags is None:\n            flag = True\n\n        for child in elt:\n\n            if 'name' in child.keys() and child.attrib['name'] == 'orth':\n                for symbol in child:\n                    if symbol.tag == 'string':\n                        word = symbol.text\n            elif 'name' in child.keys() and child.attrib['name'] == 'interps':\n                for symbol in child:\n                    if 'type' in symbol.keys() and symbol.attrib['type'] == 'lex':\n                        for symbol2 in symbol:\n                            if 'name' in symbol2.keys() and symbol2.attrib['name'] == 'ctag':\n                                for symbol3 in symbol2:\n                                    if 'value' in symbol3.keys() and self.tags is not None and symbol3.attrib['value'] in self.tags:\n                                        flag = True\n                                    elif 'value' in symbol3.keys() and symbol3.attrib['value'] == 'interp':\n                                        is_not_interp = False\n        if flag and is_not_interp:\n            return word\n"], "nltk\\corpus\\reader\\nombank": [".py", "\nfrom __future__ import unicode_literals\nfrom xml.etree import ElementTree\nfrom functools import total_ordering\n\nfrom six import string_types\n\nfrom nltk.tree import Tree\nfrom nltk.internals import raise_unorderable_types\nfrom nltk.compat import python_2_unicode_compatible\n\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\nclass NombankCorpusReader(CorpusReader):\n    def __init__(self, root, nomfile, framefiles='',\n                 nounsfile=None, parse_fileid_xform=None,\n                 parse_corpus=None, encoding='utf8'):\n        if isinstance(framefiles, string_types):\n            framefiles = find_corpus_fileids(root, framefiles)\n        framefiles = list(framefiles)\n        CorpusReader.__init__(self, root, [nomfile, nounsfile] + framefiles,\n                              encoding)\n\n        self._nomfile = nomfile\n        self._framefiles = framefiles\n        self._nounsfile = nounsfile\n        self._parse_fileid_xform = parse_fileid_xform\n        self._parse_corpus = parse_corpus\n\n    def raw(self, fileids=None):\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n    def instances(self, baseform=None):\n        kwargs = {}\n        if baseform is not None:\n            kwargs['instance_filter'] = lambda inst: inst.baseform==baseform\n        return StreamBackedCorpusView(self.abspath(self._nomfile),\n                                      lambda stream: self._read_instance_block(stream, **kwargs),\n                                      encoding=self.encoding(self._nomfile))\n\n    def lines(self):\n        return StreamBackedCorpusView(self.abspath(self._nomfile),\n                                      read_line_block,\n                                      encoding=self.encoding(self._nomfile))\n\n    def roleset(self, roleset_id):\n        baseform = roleset_id.split('.')[0]\n        baseform = baseform.replace('perc-sign','%')\n        baseform = baseform.replace('oneslashonezero', '1/10').replace('1/10','1-slash-10')\n        framefile = 'frames/%s.xml' % baseform\n        if framefile not in self._framefiles:\n            raise ValueError('Frameset file for %s not found' %\n                             roleset_id)\n\n        etree = ElementTree.parse(self.abspath(framefile).open()).getroot()\n        for roleset in etree.findall('predicate/roleset'):\n            if roleset.attrib['id'] == roleset_id:\n                return roleset\n        raise ValueError('Roleset %s not found in %s' % (roleset_id, framefile))\n\n    def rolesets(self, baseform=None):\n        if baseform is not None:\n            framefile = 'frames/%s.xml' % baseform\n            if framefile not in self._framefiles:\n                raise ValueError('Frameset file for %s not found' %\n                                 baseform)\n            framefiles = [framefile]\n        else:\n            framefiles = self._framefiles\n\n        rsets = []\n        for framefile in framefiles:\n            etree = ElementTree.parse(self.abspath(framefile).open()).getroot()\n            rsets.append(etree.findall('predicate/roleset'))\n        return LazyConcatenation(rsets)\n\n    def nouns(self):\n        return StreamBackedCorpusView(self.abspath(self._nounsfile),\n                                      read_line_block,\n                                      encoding=self.encoding(self._nounsfile))\n\n    def _read_instance_block(self, stream, instance_filter=lambda inst: True):\n        block = []\n\n        for i in range(100):\n            line = stream.readline().strip()\n            if line:\n                inst = NombankInstance.parse(\n                    line, self._parse_fileid_xform,\n                    self._parse_corpus)\n                if instance_filter(inst):\n                    block.append(inst)\n\n        return block\n\n\n@python_2_unicode_compatible\nclass NombankInstance(object):\n\n    def __init__(self, fileid, sentnum, wordnum, baseform, sensenumber,\n                 predicate, predid, arguments, parse_corpus=None):\n\n        self.fileid = fileid\n    A pointer used by nombank to identify one or more constituents in\n    a parse tree.  ``NombankPointer`` is an abstract base class with\n    three concrete subclasses:\n\n    - ``NombankTreePointer`` is used to point to single constituents.\n    - ``NombankSplitTreePointer`` is used to point to 'split'\n      constituents, which consist of a sequence of two or more\n      ``NombankTreePointer`` pointers.\n    - ``NombankChainTreePointer`` is used to point to entire trace\n      chains in a tree.  It consists of a sequence of pieces, which\n      can be ``NombankTreePointer`` or ``NombankSplitTreePointer`` pointers.\n    \"\"\"\n    def __init__(self):\n        if self.__class__ == NombankPointer:\n            raise NotImplementedError()\n\n@python_2_unicode_compatible\nclass NombankChainTreePointer(NombankPointer):\n    def __init__(self, pieces):\n        self.pieces = pieces\n        \"\"\"A list of the pieces that make up this chain.  Elements may\n           be either ``NombankSplitTreePointer`` or\n           ``NombankTreePointer`` pointers.\"\"\"\n\n    def __str__(self):\n        return '*'.join('%s' % p for p in self.pieces)\n    def __repr__(self):\n        return '<NombankChainTreePointer: %s>' % self\n    def select(self, tree):\n        if tree is None: raise ValueError('Parse tree not avaialable')\n        return Tree('*CHAIN*', [p.select(tree) for p in self.pieces])\n\n@python_2_unicode_compatible\nclass NombankSplitTreePointer(NombankPointer):\n    def __init__(self, pieces):\n        self.pieces = pieces\n        \"\"\"A list of the pieces that make up this chain.  Elements are\n           all ``NombankTreePointer`` pointers.\"\"\"\n\n    def __str__(self):\n        return ','.join('%s' % p for p in self.pieces)\n    def __repr__(self):\n        return '<NombankSplitTreePointer: %s>' % self\n    def select(self, tree):\n        if tree is None: raise ValueError('Parse tree not avaialable')\n        return Tree('*SPLIT*', [p.select(tree) for p in self.pieces])\n\n@total_ordering\n@python_2_unicode_compatible\nclass NombankTreePointer(NombankPointer):\n    \"\"\"\n    wordnum:height*wordnum:height*...\n    wordnum:height,\n\n    \"\"\"\n    def __init__(self, wordnum, height):\n        self.wordnum = wordnum\n        self.height = height\n\n    @staticmethod\n    def parse(s):\n        pieces = s.split('*')\n        if len(pieces) > 1:\n            return NombankChainTreePointer([NombankTreePointer.parse(elt)\n                                              for elt in pieces])\n\n        pieces = s.split(',')\n        if len(pieces) > 1:\n            return NombankSplitTreePointer([NombankTreePointer.parse(elt)\n                                             for elt in pieces])\n\n        pieces = s.split(':')\n        if len(pieces) != 2: raise ValueError('bad nombank pointer %r' % s)\n        return NombankTreePointer(int(pieces[0]), int(pieces[1]))\n\n    def __str__(self):\n        return '%s:%s' % (self.wordnum, self.height)\n\n    def __repr__(self):\n        return 'NombankTreePointer(%d, %d)' % (self.wordnum, self.height)\n\n    def __eq__(self, other):\n        while isinstance(other, (NombankChainTreePointer,\n                                 NombankSplitTreePointer)):\n            other = other.pieces[0]\n\n        if not isinstance(other, NombankTreePointer):\n            return self is other\n\n        return (self.wordnum == other.wordnum and self.height == other.height)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __lt__(self, other):\n        while isinstance(other, (NombankChainTreePointer,\n                                 NombankSplitTreePointer)):\n            other = other.pieces[0]\n\n        if not isinstance(other, NombankTreePointer):\n            return id(self) < id(other)\n\n        return (self.wordnum, -self.height) < (other.wordnum, -other.height)\n\n    def select(self, tree):\n        if tree is None: raise ValueError('Parse tree not avaialable')\n        return tree[self.treepos(tree)]\n\n    def treepos(self, tree):\n        \"\"\"\n        Convert this pointer to a standard 'tree position' pointer,\n        given that it points to the given tree.\n        \"\"\"\n        if tree is None: raise ValueError('Parse tree not avaialable')\n        stack = [tree]\n        treepos = []\n\n        wordnum = 0\n        while True:\n            if isinstance(stack[-1], Tree):\n                if len(treepos) < len(stack):\n                    treepos.append(0)\n                else:\n                    treepos[-1] += 1\n                if treepos[-1] < len(stack[-1]):\n                    stack.append(stack[-1][treepos[-1]])\n                else:\n                    stack.pop()\n                    treepos.pop()\n            else:\n                if wordnum == self.wordnum:\n                    return tuple(treepos[:len(treepos)-self.height-1])\n                else:\n                    wordnum += 1\n                    stack.pop()\n"], "nltk\\corpus\\reader\\nps_chat": [".py", "from __future__ import unicode_literals\n\nimport re\nimport textwrap\n\nfrom nltk.util import LazyConcatenation\nfrom nltk.internals import ElementWrapper\nfrom nltk.tag import map_tag\n\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\nfrom nltk.corpus.reader.xmldocs import *\n\nclass NPSChatCorpusReader(XMLCorpusReader):\n\n    def __init__(self, root, fileids, wrap_etree=False, tagset=None):\n        XMLCorpusReader.__init__(self, root, fileids, wrap_etree)\n        self._tagset = tagset\n\n    def xml_posts(self, fileids=None):\n        if self._wrap_etree:\n            return concat([XMLCorpusView(fileid, 'Session/Posts/Post',\n                                         self._wrap_elt)\n                           for fileid in self.abspaths(fileids)])\n        else:\n            return concat([XMLCorpusView(fileid, 'Session/Posts/Post')\n                           for fileid in self.abspaths(fileids)])\n\n    def posts(self, fileids=None):\n        return concat([XMLCorpusView(fileid, 'Session/Posts/Post/terminals',\n                                     self._elt_to_words)\n                       for fileid in self.abspaths(fileids)])\n\n    def tagged_posts(self, fileids=None, tagset=None):\n        def reader(elt, handler):\n            return self._elt_to_tagged_words(elt, handler, tagset)\n        return concat([XMLCorpusView(fileid, 'Session/Posts/Post/terminals',\n                                     reader)\n                       for fileid in self.abspaths(fileids)])\n\n    def words(self, fileids=None):\n        return LazyConcatenation(self.posts(fileids))\n\n    def tagged_words(self, fileids=None, tagset=None):\n        return LazyConcatenation(self.tagged_posts(fileids, tagset))\n\n    def _wrap_elt(self, elt, handler):\n        return ElementWrapper(elt)\n\n    def _elt_to_words(self, elt, handler):\n        return [self._simplify_username(t.attrib['word'])\n                for t in elt.findall('t')]\n\n    def _elt_to_tagged_words(self, elt, handler, tagset=None):\n        tagged_post = [(self._simplify_username(t.attrib['word']),\n                        t.attrib['pos']) for t in elt.findall('t')]\n        if tagset and tagset != self._tagset:\n            tagged_post = [(w, map_tag(self._tagset, tagset, t)) for (w, t) in tagged_post]\n        return tagged_post\n\n    @staticmethod\n    def _simplify_username(word):\n        if 'User' in word:\n            word = 'U' + word.split('User', 1)[1]\n        elif isinstance(word, bytes):\n            word = word.decode('ascii')\n        return word\n"], "nltk\\corpus\\reader\\opinion_lexicon": [".py", "\nfrom six import string_types\n\nfrom nltk.corpus.reader import WordListCorpusReader\nfrom nltk.corpus.reader.api import *\n\nclass IgnoreReadmeCorpusView(StreamBackedCorpusView):\n    def __init__(self, *args, **kwargs):\n        StreamBackedCorpusView.__init__(self, *args, **kwargs)\n        self._open()\n        read_blankline_block(self._stream)\n        self._filepos = [self._stream.tell()]\n\n\nclass OpinionLexiconCorpusReader(WordListCorpusReader):\n\n    CorpusView = IgnoreReadmeCorpusView\n\n    def words(self, fileids=None):\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        return concat([self.CorpusView(path, self._read_word_block, encoding=enc)\n            for (path, enc, fileid) in self.abspaths(fileids, True, True)])\n\n    def positive(self):\n        return self.words('positive-words.txt')\n\n    def negative(self):\n        return self.words('negative-words.txt')\n\n    def _read_word_block(self, stream):\n        words = []\n        for i in range(20): # Read 20 lines at a time.\n            line = stream.readline()\n            if not line:\n                continue\n            words.append(line.strip())\n        return words\n"], "nltk\\corpus\\reader\\panlex_lite": [".py", "\n\nimport os\nimport sqlite3\n\nfrom nltk.corpus.reader.api import CorpusReader\n\nclass PanLexLiteCorpusReader(CorpusReader):\n    MEANING_Q = \"\"\"\n        SELECT dnx2.mn, dnx2.uq, dnx2.ap, dnx2.ui, ex2.tt, ex2.lv\n        FROM dnx\n        JOIN ex ON (ex.ex = dnx.ex)\n        JOIN dnx dnx2 ON (dnx2.mn = dnx.mn)\n        JOIN ex ex2 ON (ex2.ex = dnx2.ex)\n        WHERE dnx.ex != dnx2.ex AND ex.tt = ? AND ex.lv = ?\n        ORDER BY dnx2.uq DESC\n    \"\"\"\n\n    TRANSLATION_Q = \"\"\"\n        SELECT s.tt, sum(s.uq) AS trq FROM (\n            SELECT ex2.tt, max(dnx.uq) AS uq\n            FROM dnx\n            JOIN ex ON (ex.ex = dnx.ex)\n            JOIN dnx dnx2 ON (dnx2.mn = dnx.mn)\n            JOIN ex ex2 ON (ex2.ex = dnx2.ex)\n            WHERE dnx.ex != dnx2.ex AND ex.lv = ? AND ex.tt = ? AND ex2.lv = ?\n            GROUP BY ex2.tt, dnx.ui\n        ) s\n        GROUP BY s.tt\n        ORDER BY trq DESC, s.tt\n    \"\"\"\n\n    def __init__(self, root):\n        self._c = sqlite3.connect(os.path.join(root, 'db.sqlite')).cursor()\n\n        self._uid_lv = {}\n        self._lv_uid = {}\n\n        for row in self._c.execute('SELECT uid, lv FROM lv'):\n            self._uid_lv[row[0]] = row[1]\n            self._lv_uid[row[1]] = row[0]\n\n    def language_varieties(self, lc=None):\n        \"\"\"\n        Return a list of PanLex language varieties.\n\n        :param lc: ISO 639 alpha-3 code. If specified, filters returned varieties\n            by this code. If unspecified, all varieties are returned.\n        :return: the specified language varieties as a list of tuples. The first\n            element is the language variety's seven-character uniform identifier,\n            and the second element is its default name.\n        :rtype: list(tuple)\n        \"\"\"\n\n        if lc == None:\n            return self._c.execute('SELECT uid, tt FROM lv ORDER BY uid').fetchall()\n        else:\n            return self._c.execute('SELECT uid, tt FROM lv WHERE lc = ? ORDER BY uid', (lc,)).fetchall()\n\n    def meanings(self, expr_uid, expr_tt):\n        \"\"\"\n        Return a list of meanings for an expression.\n\n        :param expr_uid: the expression's language variety, as a seven-character\n            uniform identifier.\n        :param expr_tt: the expression's text.\n        :return: a list of Meaning objects.\n        :rtype: list(Meaning)\n        \"\"\"\n\n        expr_lv = self._uid_lv[expr_uid]\n\n        mn_info = {}\n\n        for i in self._c.execute(self.MEANING_Q, (expr_tt, expr_lv)):\n            mn = i[0]\n            uid = self._lv_uid[i[5]]\n\n            if not mn in mn_info:\n                mn_info[mn] = { 'uq': i[1], 'ap': i[2], 'ui': i[3], 'ex': { expr_uid: [expr_tt] } }\n\n            if not uid in mn_info[mn]['ex']:\n                mn_info[mn]['ex'][uid] = []\n\n            mn_info[mn]['ex'][uid].append(i[4])\n\n        return [ Meaning(mn, mn_info[mn]) for mn in mn_info ]\n\n    def translations(self, from_uid, from_tt, to_uid):\n        \"\"\"\n        Return a list of translations for an expression into a single language\n            variety.\n\n        :param from_uid: the source expression's language variety, as a\n            seven-character uniform identifier.\n        :param from_tt: the source expression's text.\n        :param to_uid: the target language variety, as a seven-character\n            uniform identifier.\n        :return a list of translation tuples. The first element is the expression \n            text and the second element is the translation quality.\n        :rtype: list(tuple)\n        \"\"\"\n\n        from_lv = self._uid_lv[from_uid]\n        to_lv = self._uid_lv[to_uid]\n\n        return self._c.execute(self.TRANSLATION_Q, (from_lv, from_tt, to_lv)).fetchall()\n\nclass Meaning(dict):\n    \"\"\"\n    Represents a single PanLex meaning. A meaning is a translation set derived\n    from a single source.\n    \"\"\"\n\n    def __init__(self, mn, attr):\n        super(Meaning, self).__init__(**attr)\n        self['mn'] = mn\n\n    def id(self):\n        \"\"\"\n        :return: the meaning's id.\n        :rtype: int\n        \"\"\"\n        return self['mn']\n\n    def quality(self):\n        \"\"\"\n        :return: the meaning's source's quality (0=worst, 9=best).\n        :rtype: int\n        \"\"\"\n        return self['uq']\n\n    def source(self):\n        \"\"\"\n        :return: the meaning's source id.\n        :rtype: int\n        \"\"\"\n        return self['ap']\n\n    def source_group(self):\n        \"\"\"\n        :return: the meaning's source group id.\n        :rtype: int\n        \"\"\"\n        return self['ui']\n\n    def expressions(self):\n        \"\"\"\n        :return: the meaning's expressions as a dictionary whose keys are language\n            variety uniform identifiers and whose values are lists of expression\n            texts.\n        :rtype: dict\n        \"\"\"\n        return self['ex']\n"], "nltk\\corpus\\reader\\pl196x": [".py", "\nfrom six import string_types\n\nfrom nltk.corpus.reader.api import *\nfrom nltk.corpus.reader.xmldocs import XMLCorpusReader\n\n\nPARA = re.compile(r'<p(?: [^>]*){0,1}>(.*?)</p>')\nSENT = re.compile(r'<s(?: [^>]*){0,1}>(.*?)</s>')\n\nTAGGEDWORD = re.compile(r'<([wc](?: [^>]*){0,1}>)(.*?)</[wc]>')\nWORD = re.compile(r'<[wc](?: [^>]*){0,1}>(.*?)</[wc]>')\n\nTYPE = re.compile(r'type=\"(.*?)\"')\nANA = re.compile(r'ana=\"(.*?)\"')\n\nTEXTID = re.compile(r'text id=\"(.*?)\"')\n\n\nclass TEICorpusView(StreamBackedCorpusView):\n    def __init__(self, corpus_file,\n                 tagged, group_by_sent, group_by_para,\n                 tagset=None, head_len=0, textids=None):\n\n        self._tagged = tagged\n        self._textids = textids\n\n        self._group_by_sent = group_by_sent\n        self._group_by_para = group_by_para\n        StreamBackedCorpusView.__init__(self, corpus_file, startpos=head_len)\n\n    _pagesize = 4096\n\n    def read_block(self, stream):\n        block = stream.readlines(self._pagesize)\n        block = concat(block)\n        while (block.count('<text id') > block.count('</text>')) \\\n                or block.count('<text id') == 0:\n            tmp = stream.readline()\n            if len(tmp) <= 0:\n                break\n            block += tmp\n\n        block = block.replace('\\n', '')\n\n        textids = TEXTID.findall(block)\n        if self._textids:\n            for tid in textids:\n                if tid not in self._textids:\n                    beg = block.find(tid) - 1\n                    end = block[beg:].find('</text>') + len('</text>')\n                    block = block[:beg] + block[beg + end:]\n\n        output = []\n        for para_str in PARA.findall(block):\n            para = []\n            for sent_str in SENT.findall(para_str):\n                if not self._tagged:\n                    sent = WORD.findall(sent_str)\n                else:\n                    sent = list(\n                        map(self._parse_tag, TAGGEDWORD.findall(sent_str)))\n                if self._group_by_sent:\n                    para.append(sent)\n                else:\n                    para.extend(sent)\n            if self._group_by_para:\n                output.append(para)\n            else:\n                output.extend(para)\n        return output\n\n    def _parse_tag(self, tag_word_tuple):\n        (tag, word) = tag_word_tuple\n        if tag.startswith('w'):\n            tag = ANA.search(tag).group(1)\n        else:  # tag.startswith('c')\n            tag = TYPE.search(tag).group(1)\n        return word, tag\n\n\nclass Pl196xCorpusReader(CategorizedCorpusReader, XMLCorpusReader):\n    head_len = 2770\n\n    def __init__(self, *args, **kwargs):\n        if 'textid_file' in kwargs:\n            self._textids = kwargs['textid_file']\n        else:\n            self._textids = None\n\n        XMLCorpusReader.__init__(self, *args)\n        CategorizedCorpusReader.__init__(self, kwargs)\n\n        self._init_textids()\n\n    def _init_textids(self):\n        self._f2t = defaultdict(list)\n        self._t2f = defaultdict(list)\n        if self._textids is not None:\n            with open(self._textids) as fp:\n                for line in fp:\n                    line = line.strip()\n                    file_id, text_ids = line.split(' ', 1)\n                    if file_id not in self.fileids():\n                        raise ValueError(\n                            'In text_id mapping file %s: %s not found'\n                            % (self._textids, file_id)\n                        )\n                    for text_id in text_ids.split(self._delimiter):\n                        self._add_textids(file_id, text_id)\n\n    def _add_textids(self, file_id, text_id):\n        self._f2t[file_id].append(text_id)\n        self._t2f[text_id].append(file_id)\n\n    def _resolve(self, fileids, categories, textids=None):\n        tmp = None\n        if len(filter(lambda accessor: accessor is None,\n                      (fileids, categories, textids))) != 1:\n\n            raise ValueError('Specify exactly one of: fileids, '\n                             'categories or textids')\n\n        if fileids is not None:\n            return fileids, None\n\n        if categories is not None:\n            return self.fileids(categories), None\n\n        if textids is not None:\n            if isinstance(textids, string_types):\n                textids = [textids]\n            files = sum((self._t2f[t] for t in textids), [])\n            tdict = dict()\n            for f in files:\n                tdict[f] = (set(self._f2t[f]) & set(textids))\n            return files, tdict\n\n    def decode_tag(self, tag):\n        return tag\n\n    def textids(self, fileids=None, categories=None):\n        fileids, _ = self._resolve(fileids, categories)\n        if fileids is None: return sorted(self._t2f)\n\n        if isinstance(fileids, string_types):\n            fileids = [fileids]\n        return sorted(sum((self._f2t[d] for d in fileids), []))\n\n    def words(self, fileids=None, categories=None, textids=None):\n        fileids, textids = self._resolve(fileids, categories, textids)\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, string_types):\n            fileids = [fileids]\n\n        if textids:\n            return concat([TEICorpusView(self.abspath(fileid),\n                                         False, False, False,\n                                         head_len=self.head_len,\n                                         textids=textids[fileid])\n                           for fileid in fileids])\n        else:\n            return concat([TEICorpusView(self.abspath(fileid),\n                                         False, False, False,\n                                         head_len=self.head_len)\n                           for fileid in fileids])\n\n    def sents(self, fileids=None, categories=None, textids=None):\n        fileids, textids = self._resolve(fileids, categories, textids)\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, string_types):\n            fileids = [fileids]\n\n        if textids:\n            return concat([TEICorpusView(self.abspath(fileid),\n                                         False, True, False,\n                                         head_len=self.head_len,\n                                         textids=textids[fileid])\n                           for fileid in fileids])\n        else:\n            return concat([TEICorpusView(self.abspath(fileid),\n                                         False, True, False,\n                                         head_len=self.head_len)\n                           for fileid in fileids])\n\n    def paras(self, fileids=None, categories=None, textids=None):\n        fileids, textids = self._resolve(fileids, categories, textids)\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, string_types):\n            fileids = [fileids]\n\n        if textids:\n            return concat([TEICorpusView(self.abspath(fileid),\n                                         False, True, True,\n                                         head_len=self.head_len,\n                                         textids=textids[fileid])\n                           for fileid in fileids])\n        else:\n            return concat([TEICorpusView(self.abspath(fileid),\n                                         False, True, True,\n                                         head_len=self.head_len)\n                           for fileid in fileids])\n\n    def tagged_words(self, fileids=None, categories=None, textids=None):\n        fileids, textids = self._resolve(fileids, categories, textids)\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, string_types):\n            fileids = [fileids]\n\n        if textids:\n            return concat([TEICorpusView(self.abspath(fileid),\n                                         True, False, False,\n                                         head_len=self.head_len,\n                                         textids=textids[fileid])\n                           for fileid in fileids])\n        else:\n            return concat([TEICorpusView(self.abspath(fileid),\n                                         True, False, False,\n                                         head_len=self.head_len)\n                           for fileid in fileids])\n\n    def tagged_sents(self, fileids=None, categories=None, textids=None):\n        fileids, textids = self._resolve(fileids, categories, textids)\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, string_types):\n            fileids = [fileids]\n\n        if textids:\n            return concat([TEICorpusView(self.abspath(fileid),\n                                         True, True, False,\n                                         head_len=self.head_len,\n                                         textids=textids[fileid])\n                           for fileid in fileids])\n        else:\n            return concat([TEICorpusView(self.abspath(fileid),\n                                         True, True, False,\n                                         head_len=self.head_len)\n                           for fileid in fileids])\n\n    def tagged_paras(self, fileids=None, categories=None, textids=None):\n        fileids, textids = self._resolve(fileids, categories, textids)\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, string_types):\n            fileids = [fileids]\n\n        if textids:\n            return concat([TEICorpusView(self.abspath(fileid),\n                                         True, True, True,\n                                         head_len=self.head_len,\n                                         textids=textids[fileid])\n                           for fileid in fileids])\n        else:\n            return concat([TEICorpusView(self.abspath(fileid),\n                                         True, True, True,\n                                         head_len=self.head_len)\n                           for fileid in fileids])\n\n    def xml(self, fileids=None, categories=None):\n        fileids, _ = self._resolve(fileids, categories)\n        if len(fileids) == 1:\n            return XMLCorpusReader.xml(self, fileids[0])\n        else:\n            raise TypeError('Expected a single file')\n\n    def raw(self, fileids=None, categories=None):\n        fileids, _ = self._resolve(fileids, categories)\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, string_types):\n            fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n"], "nltk\\corpus\\reader\\plaintext": [".py", "\n\nfrom six import string_types\nimport codecs\n\nimport nltk.data\nfrom nltk.tokenize import *\n\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\nclass PlaintextCorpusReader(CorpusReader):\n\n    CorpusView = StreamBackedCorpusView\n        Construct a new plaintext corpus reader for a set of documents\n        located at the given root directory.  Example usage:\n\n            >>> root = '/usr/local/share/nltk_data/corpora/webtext/'\n            >>> reader = PlaintextCorpusReader(root, '.*\\.txt') # doctest: +SKIP\n\n        :param root: The root directory for this corpus.\n        :param fileids: A list or regexp specifying the fileids in this corpus.\n        :param word_tokenizer: Tokenizer for breaking sentences or\n            paragraphs into words.\n        :param sent_tokenizer: Tokenizer for breaking paragraphs\n            into words.\n        :param para_block_reader: The block reader used to divide the\n            corpus into paragraph blocks.\n        \"\"\"\n        CorpusReader.__init__(self, root, fileids, encoding)\n        self._word_tokenizer = word_tokenizer\n        self._sent_tokenizer = sent_tokenizer\n        self._para_block_reader = para_block_reader\n\n    def raw(self, fileids=None):\n        \"\"\"\n        :return: the given file(s) as a single string.\n        :rtype: str\n        \"\"\"\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        raw_texts = []\n        for f in fileids:\n            _fin = self.open(f)\n            raw_texts.append(_fin.read())\n            _fin.close()\n        return concat(raw_texts)\n\n    def words(self, fileids=None):\n        \"\"\"\n        :return: the given file(s) as a list of words\n            and punctuation symbols.\n        :rtype: list(str)\n        \"\"\"\n        return concat([self.CorpusView(path, self._read_word_block, encoding=enc)\n                       for (path, enc, fileid)\n                       in self.abspaths(fileids, True, True)])\n\n    def sents(self, fileids=None):\n        \"\"\"\n        :return: the given file(s) as a list of\n            sentences or utterances, each encoded as a list of word\n            strings.\n        :rtype: list(list(str))\n        \"\"\"\n        if self._sent_tokenizer is None:\n            raise ValueError('No sentence tokenizer for this corpus')\n\n        return concat([self.CorpusView(path, self._read_sent_block, encoding=enc)\n                       for (path, enc, fileid)\n                       in self.abspaths(fileids, True, True)])\n\n    def paras(self, fileids=None):\n        \"\"\"\n        :return: the given file(s) as a list of\n            paragraphs, each encoded as a list of sentences, which are\n            in turn encoded as lists of word strings.\n        :rtype: list(list(list(str)))\n        \"\"\"\n        if self._sent_tokenizer is None:\n            raise ValueError('No sentence tokenizer for this corpus')\n\n        return concat([self.CorpusView(path, self._read_para_block, encoding=enc)\n                       for (path, enc, fileid)\n                       in self.abspaths(fileids, True, True)])\n\n    def _read_word_block(self, stream):\n        words = []\n        for i in range(20): # Read 20 lines at a time.\n            words.extend(self._word_tokenizer.tokenize(stream.readline()))\n        return words\n\n    def _read_sent_block(self, stream):\n        sents = []\n        for para in self._para_block_reader(stream):\n            sents.extend([self._word_tokenizer.tokenize(sent)\n                          for sent in self._sent_tokenizer.tokenize(para)])\n        return sents\n\n    def _read_para_block(self, stream):\n        paras = []\n        for para in self._para_block_reader(stream):\n            paras.append([self._word_tokenizer.tokenize(sent)\n                          for sent in self._sent_tokenizer.tokenize(para)])\n        return paras\n\n\nclass CategorizedPlaintextCorpusReader(CategorizedCorpusReader,\n                                    PlaintextCorpusReader):\n    \"\"\"\n    A reader for plaintext corpora whose documents are divided into\n    categories based on their file identifiers.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize the corpus reader.  Categorization arguments\n        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n        the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n        are passed to the ``PlaintextCorpusReader`` constructor.\n        \"\"\"\n        CategorizedCorpusReader.__init__(self, kwargs)\n        PlaintextCorpusReader.__init__(self, *args, **kwargs)\n\n    def _resolve(self, fileids, categories):\n        if fileids is not None and categories is not None:\n            raise ValueError('Specify fileids or categories, not both')\n        if categories is not None:\n            return self.fileids(categories)\n        else:\n            return fileids\n    def raw(self, fileids=None, categories=None):\n        return PlaintextCorpusReader.raw(\n            self, self._resolve(fileids, categories))\n    def words(self, fileids=None, categories=None):\n        return PlaintextCorpusReader.words(\n            self, self._resolve(fileids, categories))\n    def sents(self, fileids=None, categories=None):\n        return PlaintextCorpusReader.sents(\n            self, self._resolve(fileids, categories))\n    def paras(self, fileids=None, categories=None):\n        return PlaintextCorpusReader.paras(\n            self, self._resolve(fileids, categories))\n\nclass PortugueseCategorizedPlaintextCorpusReader(CategorizedPlaintextCorpusReader):\n    def __init__(self, *args, **kwargs):\n        CategorizedCorpusReader.__init__(self, kwargs)\n        kwargs['sent_tokenizer'] = nltk.data.LazyLoader('tokenizers/punkt/portuguese.pickle')\n        PlaintextCorpusReader.__init__(self, *args, **kwargs)\n\nclass EuroparlCorpusReader(PlaintextCorpusReader):\n\n    \"\"\"\n    Reader for Europarl corpora that consist of plaintext documents.\n    Documents are divided into chapters instead of paragraphs as\n    for regular plaintext documents. Chapters are separated using blank\n    lines. Everything is inherited from ``PlaintextCorpusReader`` except\n    that:\n      - Since the corpus is pre-processed and pre-tokenized, the\n        word tokenizer should just split the line at whitespaces.\n      - For the same reason, the sentence tokenizer should just\n        split the paragraph at line breaks.\n      - There is a new 'chapters()' method that returns chapters instead\n        instead of paragraphs.\n      - The 'paras()' method inherited from PlaintextCorpusReader is\n        made non-functional to remove any confusion between chapters\n        and paragraphs for Europarl.\n    \"\"\"\n\n    def _read_word_block(self, stream):\n        words = []\n        for i in range(20): # Read 20 lines at a time.\n            words.extend(stream.readline().split())\n        return words\n\n    def _read_sent_block(self, stream):\n        sents = []\n        for para in self._para_block_reader(stream):\n            sents.extend([sent.split() for sent in para.splitlines()])\n        return sents\n\n    def _read_para_block(self, stream):\n        paras = []\n        for para in self._para_block_reader(stream):\n            paras.append([sent.split() for sent in para.splitlines()])\n        return paras\n\n    def chapters(self, fileids=None):\n        \"\"\"\n        :return: the given file(s) as a list of\n            chapters, each encoded as a list of sentences, which are\n            in turn encoded as lists of word strings.\n        :rtype: list(list(list(str)))\n        \"\"\"\n        return concat([self.CorpusView(fileid, self._read_para_block,\n                                       encoding=enc)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def paras(self, fileids=None):\n        raise NotImplementedError('The Europarl corpus reader does not support paragraphs. Please use chapters() instead.')\n"], "nltk\\corpus\\reader\\ppattach": [".py", "\nfrom __future__ import unicode_literals\n\nfrom six import string_types\n\nfrom nltk import compat\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\n\n@compat.python_2_unicode_compatible\nclass PPAttachment(object):\n    def __init__(self, sent, verb, noun1, prep, noun2, attachment):\n        self.sent = sent\n        self.verb = verb\n        self.noun1 = noun1\n        self.prep = prep\n        self.noun2 = noun2\n        self.attachment = attachment\n\n    def __repr__(self):\n        return ('PPAttachment(sent=%r, verb=%r, noun1=%r, prep=%r, '\n                'noun2=%r, attachment=%r)' %\n                (self.sent, self.verb, self.noun1, self.prep,\n                 self.noun2, self.attachment))\n\nclass PPAttachmentCorpusReader(CorpusReader):\n    def attachments(self, fileids):\n        return concat([StreamBackedCorpusView(fileid, self._read_obj_block,\n                                              encoding=enc)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def tuples(self, fileids):\n        return concat([StreamBackedCorpusView(fileid, self._read_tuple_block,\n                                              encoding=enc)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def raw(self, fileids=None):\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n    def _read_tuple_block(self, stream):\n        line = stream.readline()\n        if line:\n            return [tuple(line.split())]\n        else:\n            return []\n\n    def _read_obj_block(self, stream):\n        line = stream.readline()\n        if line:\n            return [PPAttachment(*line.split())]\n        else:\n            return []\n"], "nltk\\corpus\\reader\\propbank": [".py", "\nfrom __future__ import unicode_literals\nimport re\nfrom functools import total_ordering\nfrom xml.etree import ElementTree\n\nfrom six import string_types\n\nfrom nltk.tree import Tree\nfrom nltk.internals import raise_unorderable_types\n\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\nclass PropbankCorpusReader(CorpusReader):\n    def __init__(self, root, propfile, framefiles='',\n                 verbsfile=None, parse_fileid_xform=None,\n                 parse_corpus=None, encoding='utf8'):\n        if isinstance(framefiles, string_types):\n            framefiles = find_corpus_fileids(root, framefiles)\n        framefiles = list(framefiles)\n        CorpusReader.__init__(self, root, [propfile, verbsfile] + framefiles,\n                              encoding)\n\n        self._propfile = propfile\n        self._framefiles = framefiles\n        self._verbsfile = verbsfile\n        self._parse_fileid_xform = parse_fileid_xform\n        self._parse_corpus = parse_corpus\n\n    def raw(self, fileids=None):\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, ): fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n    def instances(self, baseform=None):\n        kwargs = {}\n        if baseform is not None:\n            kwargs['instance_filter'] = lambda inst: inst.baseform==baseform\n        return StreamBackedCorpusView(self.abspath(self._propfile),\n                                      lambda stream: self._read_instance_block(stream, **kwargs),\n                                      encoding=self.encoding(self._propfile))\n\n    def lines(self):\n        return StreamBackedCorpusView(self.abspath(self._propfile),\n                                      read_line_block,\n                                      encoding=self.encoding(self._propfile))\n\n    def roleset(self, roleset_id):\n        baseform = roleset_id.split('.')[0]\n        framefile = 'frames/%s.xml' % baseform\n        if framefile not in self._framefiles:\n            raise ValueError('Frameset file for %s not found' %\n                             roleset_id)\n\n        etree = ElementTree.parse(self.abspath(framefile).open()).getroot()\n        for roleset in etree.findall('predicate/roleset'):\n            if roleset.attrib['id'] == roleset_id:\n                return roleset\n        raise ValueError('Roleset %s not found in %s' % (roleset_id, framefile))\n\n    def rolesets(self, baseform=None):\n        if baseform is not None:\n            framefile = 'frames/%s.xml' % baseform\n            if framefile not in self._framefiles:\n                raise ValueError('Frameset file for %s not found' %\n                                 baseform)\n            framefiles = [framefile]\n        else:\n            framefiles = self._framefiles\n\n        rsets = []\n        for framefile in framefiles:\n            etree = ElementTree.parse(self.abspath(framefile).open()).getroot()\n            rsets.append(etree.findall('predicate/roleset'))\n        return LazyConcatenation(rsets)\n\n    def verbs(self):\n        return StreamBackedCorpusView(self.abspath(self._verbsfile),\n                                      read_line_block,\n                                      encoding=self.encoding(self._verbsfile))\n\n    def _read_instance_block(self, stream, instance_filter=lambda inst: True):\n        block = []\n\n        for i in range(100):\n            line = stream.readline().strip()\n            if line:\n                inst = PropbankInstance.parse(\n                    line, self._parse_fileid_xform,\n                    self._parse_corpus)\n                if instance_filter(inst):\n                    block.append(inst)\n\n        return block\n\n\n@compat.python_2_unicode_compatible\nclass PropbankInstance(object):\n\n    def __init__(self, fileid, sentnum, wordnum, tagger, roleset,\n                 inflection, predicate, arguments, parse_corpus=None):\n\n        self.fileid = fileid\n    A pointer used by propbank to identify one or more constituents in\n    a parse tree.  ``PropbankPointer`` is an abstract base class with\n    three concrete subclasses:\n\n      - ``PropbankTreePointer`` is used to point to single constituents.\n      - ``PropbankSplitTreePointer`` is used to point to 'split'\n        constituents, which consist of a sequence of two or more\n        ``PropbankTreePointer`` pointers.\n      - ``PropbankChainTreePointer`` is used to point to entire trace\n        chains in a tree.  It consists of a sequence of pieces, which\n        can be ``PropbankTreePointer`` or ``PropbankSplitTreePointer`` pointers.\n    \"\"\"\n    def __init__(self):\n        if self.__class__ == PropbankPointer:\n            raise NotImplementedError()\n\n@compat.python_2_unicode_compatible\nclass PropbankChainTreePointer(PropbankPointer):\n    def __init__(self, pieces):\n        self.pieces = pieces\n        \"\"\"A list of the pieces that make up this chain.  Elements may\n           be either ``PropbankSplitTreePointer`` or\n           ``PropbankTreePointer`` pointers.\"\"\"\n\n    def __str__(self):\n        return '*'.join('%s' % p for p in self.pieces)\n    def __repr__(self):\n        return '<PropbankChainTreePointer: %s>' % self\n    def select(self, tree):\n        if tree is None: raise ValueError('Parse tree not avaialable')\n        return Tree('*CHAIN*', [p.select(tree) for p in self.pieces])\n\n\n@compat.python_2_unicode_compatible\nclass PropbankSplitTreePointer(PropbankPointer):\n    def __init__(self, pieces):\n        self.pieces = pieces\n        \"\"\"A list of the pieces that make up this chain.  Elements are\n           all ``PropbankTreePointer`` pointers.\"\"\"\n\n    def __str__(self):\n        return ','.join('%s' % p for p in self.pieces)\n    def __repr__(self):\n        return '<PropbankSplitTreePointer: %s>' % self\n    def select(self, tree):\n        if tree is None: raise ValueError('Parse tree not avaialable')\n        return Tree('*SPLIT*', [p.select(tree) for p in self.pieces])\n\n\n@total_ordering\n@compat.python_2_unicode_compatible\nclass PropbankTreePointer(PropbankPointer):\n    \"\"\"\n    wordnum:height*wordnum:height*...\n    wordnum:height,\n\n    \"\"\"\n    def __init__(self, wordnum, height):\n        self.wordnum = wordnum\n        self.height = height\n\n    @staticmethod\n    def parse(s):\n        pieces = s.split('*')\n        if len(pieces) > 1:\n            return PropbankChainTreePointer([PropbankTreePointer.parse(elt)\n                                              for elt in pieces])\n\n        pieces = s.split(',')\n        if len(pieces) > 1:\n            return PropbankSplitTreePointer([PropbankTreePointer.parse(elt)\n                                             for elt in pieces])\n\n        pieces = s.split(':')\n        if len(pieces) != 2: raise ValueError('bad propbank pointer %r' % s)\n        return PropbankTreePointer(int(pieces[0]), int(pieces[1]))\n\n    def __str__(self):\n        return '%s:%s' % (self.wordnum, self.height)\n\n    def __repr__(self):\n        return 'PropbankTreePointer(%d, %d)' % (self.wordnum, self.height)\n\n    def __eq__(self, other):\n        while isinstance(other, (PropbankChainTreePointer,\n                                 PropbankSplitTreePointer)):\n            other = other.pieces[0]\n\n        if not isinstance(other, PropbankTreePointer):\n            return self is other\n\n        return (self.wordnum == other.wordnum and self.height == other.height)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __lt__(self, other):\n        while isinstance(other, (PropbankChainTreePointer,\n                                 PropbankSplitTreePointer)):\n            other = other.pieces[0]\n\n        if not isinstance(other, PropbankTreePointer):\n            return id(self) < id(other)\n\n        return (self.wordnum, -self.height) < (other.wordnum, -other.height)\n\n    def select(self, tree):\n        if tree is None: raise ValueError('Parse tree not avaialable')\n        return tree[self.treepos(tree)]\n\n    def treepos(self, tree):\n        \"\"\"\n        Convert this pointer to a standard 'tree position' pointer,\n        given that it points to the given tree.\n        \"\"\"\n        if tree is None: raise ValueError('Parse tree not avaialable')\n        stack = [tree]\n        treepos = []\n\n        wordnum = 0\n        while True:\n            if isinstance(stack[-1], Tree):\n                if len(treepos) < len(stack):\n                    treepos.append(0)\n                else:\n                    treepos[-1] += 1\n                if treepos[-1] < len(stack[-1]):\n                    stack.append(stack[-1][treepos[-1]])\n                else:\n                    stack.pop()\n                    treepos.pop()\n            else:\n                if wordnum == self.wordnum:\n                    return tuple(treepos[:len(treepos)-self.height-1])\n                else:\n                    wordnum += 1\n                    stack.pop()\n\n@compat.python_2_unicode_compatible\nclass PropbankInflection(object):\n    INFINITIVE = 'i'\n    GERUND = 'g'\n    PARTICIPLE = 'p'\n    FINITE = 'v'\n    FUTURE = 'f'\n    PAST = 'p'\n    PRESENT = 'n'\n    PERFECT = 'p'\n    PROGRESSIVE = 'o'\n    PERFECT_AND_PROGRESSIVE = 'b'\n    THIRD_PERSON = '3'\n    ACTIVE = 'a'\n    PASSIVE = 'p'\n    NONE = '-'\n\n    def __init__(self, form='-', tense='-', aspect='-', person='-', voice='-'):\n        self.form = form\n        self.tense = tense\n        self.aspect = aspect\n        self.person = person\n        self.voice = voice\n\n    def __str__(self):\n        return self.form+self.tense+self.aspect+self.person+self.voice\n\n    def __repr__(self):\n        return '<PropbankInflection: %s>' % self\n\n    _VALIDATE = re.compile(r'[igpv\\-][fpn\\-][pob\\-][3\\-][ap\\-]$')\n\n    @staticmethod\n    def parse(s):\n        if not isinstance(s, string_types):\n            raise TypeError('expected a string')\n        if (len(s) != 5 or\n            not PropbankInflection._VALIDATE.match(s)):\n            raise ValueError('Bad propbank inflection string %r' % s)\n        return PropbankInflection(*s)\n"], "nltk\\corpus\\reader\\pros_cons": [".py", "\nimport re\n\nfrom six import string_types\n\nfrom nltk.corpus.reader.api import *\nfrom nltk.tokenize import *\n\n\nclass ProsConsCorpusReader(CategorizedCorpusReader, CorpusReader):\n    CorpusView = StreamBackedCorpusView\n\n    def __init__(self, root, fileids, word_tokenizer=WordPunctTokenizer(),\n                 encoding='utf8', **kwargs):\n\n        CorpusReader.__init__(self, root, fileids, encoding)\n        CategorizedCorpusReader.__init__(self, kwargs)\n        self._word_tokenizer = word_tokenizer\n\n    def sents(self, fileids=None, categories=None):\n        fileids = self._resolve(fileids, categories)\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, string_types):\n            fileids = [fileids]\n        return concat([self.CorpusView(path, self._read_sent_block, encoding=enc)\n            for (path, enc, fileid) in self.abspaths(fileids, True, True)])\n\n    def words(self, fileids=None, categories=None):\n        fileids = self._resolve(fileids, categories)\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, string_types):\n            fileids = [fileids]\n        return concat([self.CorpusView(path, self._read_word_block, encoding=enc)\n            for (path, enc, fileid) in self.abspaths(fileids, True, True)])\n\n    def _read_sent_block(self, stream):\n        sents = []\n        for i in range(20): # Read 20 lines at a time.\n            line = stream.readline()\n            if not line:\n                continue\n            sent = re.match(r\"^(?!\\n)\\s*<(Pros|Cons)>(.*)</(?:Pros|Cons)>\", line)\n            if sent:\n                sents.append(self._word_tokenizer.tokenize(sent.group(2).strip()))\n        return sents\n\n    def _read_word_block(self, stream):\n        words = []\n        for sent in self._read_sent_block(stream):\n            words.extend(sent)\n        return words\n\n    def _resolve(self, fileids, categories):\n        if fileids is not None and categories is not None:\n            raise ValueError('Specify fileids or categories, not both')\n        if categories is not None:\n            return self.fileids(categories)\n        else:\n            return fileids\n"], "nltk\\corpus\\reader\\reviews": [".py", "\n\nfrom __future__ import division\n\nfrom six import string_types\n\nimport re\n\nfrom nltk.corpus.reader.api import *\nfrom nltk.tokenize import *\n\nTITLE = re.compile(r'^\\[t\\](.*)$') # [t] Title\nFEATURES = re.compile(r'((?:(?:\\w+\\s)+)?\\w+)\\[((?:\\+|\\-)\\d)\\]') # find 'feature' in feature[+3]\nNOTES = re.compile(r'\\[(?!t)(p|u|s|cc|cs)\\]') # find 'p' in camera[+2][p]\nSENT = re.compile(r'##(.*)$') # find tokenized sentence\n\n\n@compat.python_2_unicode_compatible\nclass Review(object):\n    def __init__(self, title=None, review_lines=None):\n        self.title = title\n        if review_lines is None:\n            self.review_lines = []\n        else:\n            self.review_lines = review_lines\n\n    def add_line(self, review_line):\n        assert isinstance(review_line, ReviewLine)\n        self.review_lines.append(review_line)\n\n    def features(self):\n        features = []\n        for review_line in self.review_lines:\n            features.extend(review_line.features)\n        return features\n\n    def sents(self):\n        return [review_line.sent for review_line in self.review_lines]\n\n    def __repr__(self):\n        return 'Review(title=\\\"{}\\\", review_lines={})'.format(self.title, self.review_lines)\n\n\n@compat.python_2_unicode_compatible\nclass ReviewLine(object):\n    def __init__(self, sent, features=None, notes=None):\n        self.sent = sent\n        if features is None:\n            self.features = []\n        else:\n            self.features = features\n\n        if notes is None:\n            self.notes = []\n        else:\n            self.notes = notes\n\n    def __repr__(self):\n        return ('ReviewLine(features={}, notes={}, sent={})'.format(\n            self.features, self.notes, self.sent))\n\n\nclass ReviewsCorpusReader(CorpusReader):\n    CorpusView = StreamBackedCorpusView\n\n    def __init__(self, root, fileids, word_tokenizer=WordPunctTokenizer(),\n                 encoding='utf8'):\n\n        CorpusReader.__init__(self, root, fileids, encoding)\n        self._word_tokenizer = word_tokenizer\n\n    def features(self, fileids=None):\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, string_types):\n            fileids = [fileids]\n        return concat([self.CorpusView(fileid, self._read_features, encoding=enc)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def raw(self, fileids=None):\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, string_types):\n            fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n    def readme(self):\n        return self.open(\"README.txt\").read()\n\n    def reviews(self, fileids=None):\n        if fileids is None:\n            fileids = self._fileids\n        return concat([self.CorpusView(fileid, self._read_review_block, encoding=enc)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def sents(self, fileids=None):\n        return concat([self.CorpusView(path, self._read_sent_block, encoding=enc)\n                       for (path, enc, fileid)\n                       in self.abspaths(fileids, True, True)])\n\n    def words(self, fileids=None):\n        return concat([self.CorpusView(path, self._read_word_block, encoding=enc)\n                       for (path, enc, fileid)\n                       in self.abspaths(fileids, True, True)])\n\n    def _read_features(self, stream):\n        features = []\n        for i in range(20):\n            line = stream.readline()\n            if not line:\n                return features\n            features.extend(re.findall(FEATURES, line))\n        return features\n\n    def _read_review_block(self, stream):\n        while True:\n            line = stream.readline()\n            if not line:\n                return [] # end of file.\n            title_match = re.match(TITLE, line)\n            if title_match:\n                review = Review(title=title_match.group(1).strip()) # We create a new review\n                break\n\n        while True:\n            oldpos = stream.tell()\n            line = stream.readline()\n            if not line:\n                return [review]\n            if re.match(TITLE, line):\n                stream.seek(oldpos)\n                return [review]\n            feats = re.findall(FEATURES, line)\n            notes = re.findall(NOTES, line)\n            sent = re.findall(SENT, line)\n            if sent:\n                sent = self._word_tokenizer.tokenize(sent[0])\n            review_line = ReviewLine(sent=sent, features=feats, notes=notes)\n            review.add_line(review_line)\n\n    def _read_sent_block(self, stream):\n        sents = []\n        for review in self._read_review_block(stream):\n            sents.extend([sent for sent in review.sents()])\n        return sents\n\n    def _read_word_block(self, stream):\n        words = []\n        for i in range(20): # Read 20 lines at a time.\n            line = stream.readline()\n            sent = re.findall(SENT, line)\n            if sent:\n                words.extend(self._word_tokenizer.tokenize(sent[0]))\n        return words\n"], "nltk\\corpus\\reader\\rte": [".py", "\nfrom __future__ import unicode_literals\n\nfrom six import string_types\n\nfrom nltk import compat\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\nfrom nltk.corpus.reader.xmldocs import *\n\n\ndef norm(value_string):\n\n    valdict = {\"TRUE\": 1,\n                     \"FALSE\": 0,\n                     \"YES\": 1,\n                     \"NO\": 0}\n    return valdict[value_string.upper()]\n\n@compat.python_2_unicode_compatible\nclass RTEPair(object):\n    def __init__(self, pair, challenge=None, id=None, text=None, hyp=None,\n             value=None, task=None, length=None):\n        self.challenge =  challenge\n        self.id = pair.attrib[\"id\"]\n        self.gid = \"%s-%s\" % (self.challenge, self.id)\n        self.text = pair[0].text\n        self.hyp = pair[1].text\n\n        if \"value\" in pair.attrib:\n            self.value = norm(pair.attrib[\"value\"])\n        elif \"entailment\" in pair.attrib:\n            self.value = norm(pair.attrib[\"entailment\"])\n        else:\n            self.value = value\n        if \"task\" in pair.attrib:\n            self.task = pair.attrib[\"task\"]\n        else:\n            self.task = task\n        if \"length\" in pair.attrib:\n            self.length = pair.attrib[\"length\"]\n        else:\n            self.length = length\n\n    def __repr__(self):\n        if self.challenge:\n            return '<RTEPair: gid=%s-%s>' % (self.challenge, self.id)\n        else:\n            return '<RTEPair: id=%s>' % self.id\n\n\nclass RTECorpusReader(XMLCorpusReader):\n\n    def _read_etree(self, doc):\n        try:\n            challenge = doc.attrib['challenge']\n        except KeyError:\n            challenge = None\n        return [RTEPair(pair, challenge=challenge)\n                for pair in doc.getiterator(\"pair\")]\n\n\n    def pairs(self, fileids):\n        if isinstance(fileids, string_types): fileids = [fileids]\n        return concat([self._read_etree(self.xml(fileid)) for fileid in fileids])\n"], "nltk\\corpus\\reader\\semcor": [".py", "\nfrom __future__ import absolute_import, unicode_literals\n__docformat__ = 'epytext en'\n\nfrom nltk.corpus.reader.api import *\nfrom nltk.corpus.reader.xmldocs import XMLCorpusReader, XMLCorpusView\nfrom nltk.tree import Tree\n\nclass SemcorCorpusReader(XMLCorpusReader):\n    def __init__(self, root, fileids, wordnet, lazy=True):\n        XMLCorpusReader.__init__(self, root, fileids)\n        self._lazy = lazy\n        self._wordnet = wordnet\n\n    def words(self, fileids=None):\n        return self._items(fileids, 'word', False, False, False)\n\n    def chunks(self, fileids=None):\n        return self._items(fileids, 'chunk', False, False, False)\n\n    def tagged_chunks(self, fileids=None, tag=('pos' or 'sem' or 'both')):\n        return self._items(fileids, 'chunk', False, tag!='sem', tag!='pos')\n\n    def sents(self, fileids=None):\n        return self._items(fileids, 'word', True, False, False)\n\n    def chunk_sents(self, fileids=None):\n        return self._items(fileids, 'chunk', True, False, False)\n\n    def tagged_sents(self, fileids=None, tag=('pos' or 'sem' or 'both')):\n        return self._items(fileids, 'chunk', True, tag!='sem', tag!='pos')\n\n    def _items(self, fileids, unit, bracket_sent, pos_tag, sem_tag):\n        if unit=='word' and not bracket_sent:\n            _ = lambda *args: LazyConcatenation((SemcorWordView if self._lazy else self._words)(*args))\n        else:\n            _ = SemcorWordView if self._lazy else self._words\n        return concat([_(fileid, unit, bracket_sent, pos_tag, sem_tag, self._wordnet)\n                       for fileid in self.abspaths(fileids)])\n\n    def _words(self, fileid, unit, bracket_sent, pos_tag, sem_tag):\n        assert unit in ('token', 'word', 'chunk')\n        result = []\n\n        xmldoc = ElementTree.parse(fileid).getroot()\n        for xmlsent in xmldoc.findall('.//s'):\n            sent = []\n            for xmlword in _all_xmlwords_in(xmlsent):\n                itm = SemcorCorpusReader._word(xmlword, unit, pos_tag, sem_tag, self._wordnet)\n                if unit=='word':\n                    sent.extend(itm)\n                else:\n                    sent.append(itm)\n\n            if bracket_sent:\n                result.append(SemcorSentence(xmlsent.attrib['snum'], sent))\n            else:\n                result.extend(sent)\n\n        assert None not in result\n        return result\n\n    @staticmethod\n    def _word(xmlword, unit, pos_tag, sem_tag, wordnet):\n        tkn = xmlword.text\n        if not tkn:\n            tkn = \"\" # fixes issue 337?\n\n        lemma = xmlword.get('lemma', tkn) # lemma or NE class\n        lexsn = xmlword.get('lexsn') # lex_sense (locator for the lemma's sense)\n        if lexsn is not None:\n            sense_key = lemma + '%' + lexsn\n            wnpos = ('n','v','a','r','s')[int(lexsn.split(':')[0])-1]   # see http://wordnet.princeton.edu/man/senseidx.5WN.html\n        else:\n            sense_key = wnpos = None\n        redef = xmlword.get('rdf', tkn)\t# redefinition--this indicates the lookup string\n        sensenum = xmlword.get('wnsn')  # WordNet sense number\n        isOOVEntity = 'pn' in xmlword.keys()   # a \"personal name\" (NE) not in WordNet\n        pos = xmlword.get('pos')    # part of speech for the whole chunk (None for punctuation)\n\n        if unit=='token':\n            if not pos_tag and not sem_tag:\n                itm = tkn\n            else:\n                itm = (tkn,) + ((pos,) if pos_tag else ()) + ((lemma, wnpos, sensenum, isOOVEntity) if sem_tag else ())\n            return itm\n        else:\n            ww = tkn.split('_') # TODO: case where punctuation intervenes in MWE\n            if unit=='word':\n                return ww\n            else:\n                if sensenum is not None:\n                    try:\n                        sense = wordnet.lemma_from_key(sense_key)   # Lemma object\n                    except Exception:\n                        try:\n                            sense = '%s.%s.%02d' % (lemma, wnpos, int(sensenum))    # e.g.: reach.v.02\n                        except ValueError:\n                            sense = lemma+'.'+wnpos+'.'+sensenum  # e.g. the sense number may be \"2;1\"\n\n                bottom = [Tree(pos, ww)] if pos_tag else ww\n\n                if sem_tag and isOOVEntity:\n                    if sensenum is not None:\n                        return Tree(sense, [Tree('NE', bottom)])\n                    else:\t# 'other' NE\n                        return Tree('NE', bottom)\n                elif sem_tag and sensenum is not None:\n                    return Tree(sense, bottom)\n                elif pos_tag:\n                    return bottom[0]\n                else:\n                    return bottom # chunk as a list\n\ndef _all_xmlwords_in(elt, result=None):\n    if result is None: result = []\n    for child in elt:\n        if child.tag in ('wf', 'punc'): result.append(child)\n        else: _all_xmlwords_in(child, result)\n    return result\n\nclass SemcorSentence(list):\n    def __init__(self, num, items):\n        self.num = num\n        list.__init__(self, items)\n\nclass SemcorWordView(XMLCorpusView):\n    def __init__(self, fileid, unit, bracket_sent, pos_tag, sem_tag, wordnet):\n        if bracket_sent: tagspec = '.*/s'\n        else: tagspec = '.*/s/(punc|wf)'\n\n        self._unit = unit\n        self._sent = bracket_sent\n        self._pos_tag = pos_tag\n        self._sem_tag = sem_tag\n        self._wordnet = wordnet\n\n        XMLCorpusView.__init__(self, fileid, tagspec)\n\n    def handle_elt(self, elt, context):\n        if self._sent: return self.handle_sent(elt)\n        else: return self.handle_word(elt)\n\n    def handle_word(self, elt):\n        return SemcorCorpusReader._word(elt, self._unit, self._pos_tag, self._sem_tag, self._wordnet)\n\n    def handle_sent(self, elt):\n        sent = []\n        for child in elt:\n            if child.tag in ('wf','punc'):\n                itm = self.handle_word(child)\n                if self._unit=='word':\n                    sent.extend(itm)\n                else:\n                    sent.append(itm)\n            else:\n                raise ValueError('Unexpected element %s' % child.tag)\n        return SemcorSentence(elt.attrib['snum'], sent)\n"], "nltk\\corpus\\reader\\senseval": [".py", "\nfrom __future__ import print_function, unicode_literals\n\nfrom six import string_types\n\nimport re\nfrom xml.etree import ElementTree\n\nfrom nltk import compat\nfrom nltk.tokenize import *\n\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\n@compat.python_2_unicode_compatible\nclass SensevalInstance(object):\n    def __init__(self, word, position, context, senses):\n        self.word = word\n        self.senses = tuple(senses)\n        self.position = position\n        self.context = context\n\n    def __repr__(self):\n        return ('SensevalInstance(word=%r, position=%r, '\n                'context=%r, senses=%r)' %\n                (self.word, self.position, self.context, self.senses))\n\n\nclass SensevalCorpusReader(CorpusReader):\n    def instances(self, fileids=None):\n        return concat([SensevalCorpusView(fileid, enc)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def raw(self, fileids=None):\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n    def _entry(self, tree):\n        elts = []\n        for lexelt in tree.findall('lexelt'):\n            for inst in lexelt.findall('instance'):\n                sense = inst[0].attrib['senseid']\n                context = [(w.text, w.attrib['pos'])\n                           for w in inst[1]]\n                elts.append( (sense, context) )\n        return elts\n\n\nclass SensevalCorpusView(StreamBackedCorpusView):\n    def __init__(self, fileid, encoding):\n        StreamBackedCorpusView.__init__(self, fileid, encoding=encoding)\n\n        self._word_tokenizer = WhitespaceTokenizer()\n        self._lexelt_starts = [0] # list of streampos\n        self._lexelts = [None] # list of lexelt names\n\n    def read_block(self, stream):\n        lexelt_num = bisect.bisect_right(self._lexelt_starts, stream.tell())-1\n        lexelt = self._lexelts[lexelt_num]\n\n        instance_lines = []\n        in_instance = False\n        while True:\n            line = stream.readline()\n            if line == '':\n                assert instance_lines == []\n                return []\n\n            if line.lstrip().startswith('<lexelt'):\n                lexelt_num += 1\n                m = re.search('item=(\"[^\"]+\"|\\'[^\\']+\\')', line)\n                assert m is not None # <lexelt> has no 'item=...'\n                lexelt = m.group(1)[1:-1]\n                if lexelt_num < len(self._lexelts):\n                    assert lexelt == self._lexelts[lexelt_num]\n                else:\n                    self._lexelts.append(lexelt)\n                    self._lexelt_starts.append(stream.tell())\n\n            if line.lstrip().startswith('<instance'):\n                assert instance_lines == []\n                in_instance = True\n\n            if in_instance:\n                instance_lines.append(line)\n\n            if line.lstrip().startswith('</instance'):\n                xml_block = '\\n'.join(instance_lines)\n                xml_block = _fixXML(xml_block)\n                inst = ElementTree.fromstring(xml_block)\n                return [self._parse_instance(inst, lexelt)]\n\n    def _parse_instance(self, instance, lexelt):\n        senses = []\n        context = []\n        position = None\n        for child in instance:\n            if child.tag == 'answer':\n                senses.append(child.attrib['senseid'])\n            elif child.tag == 'context':\n                context += self._word_tokenizer.tokenize(child.text)\n                for cword in child:\n                    if cword.tag == 'compound':\n                        cword = cword[0] # is this ok to do?\n\n                    if cword.tag == 'head':\n                        assert position is None, 'head specified twice'\n                        assert cword.text.strip() or len(cword)==1\n                        assert not (cword.text.strip() and len(cword)==1)\n                        position = len(context)\n                        if cword.text.strip():\n                            context.append(cword.text.strip())\n                        elif cword[0].tag == 'wf':\n                            context.append((cword[0].text,\n                                            cword[0].attrib['pos']))\n                            if cword[0].tail:\n                                context += self._word_tokenizer.tokenize(\n                                    cword[0].tail)\n                        else:\n                            assert False, 'expected CDATA or wf in <head>'\n                    elif cword.tag == 'wf':\n                        context.append((cword.text, cword.attrib['pos']))\n                    elif cword.tag == 's':\n                        pass # Sentence boundary marker.\n\n                    else:\n                        print('ACK', cword.tag)\n                        assert False, 'expected CDATA or <wf> or <head>'\n                    if cword.tail:\n                        context += self._word_tokenizer.tokenize(cword.tail)\n            else:\n                assert False, 'unexpected tag %s' % child.tag\n        return SensevalInstance(lexelt, position, context, senses)\n\ndef _fixXML(text):\n    text = re.sub(r'<([~\\^])>', r'\\1', text)\n    text = re.sub(r'(\\s+)\\&(\\s+)', r'\\1&amp;\\2', text)\n    text = re.sub(r'\"\"\"', '\\'\"\\'', text)\n    text = re.sub(r'(<[^<]*snum=)([^\">]+)>', r'\\1\"\\2\"/>', text)\n    text = re.sub(r'<\\&frasl>\\s*<p[^>]*>', 'FRASL', text)\n    text = re.sub(r'<\\&I[^>]*>', '', text)\n    text = re.sub(r'<{([^}]+)}>', r'\\1', text)\n    text = re.sub(r'<(@|/?p)>', r'', text)\n    text = re.sub(r'<&\\w+ \\.>', r'', text)\n    text = re.sub(r'<!DOCTYPE[^>]*>', r'', text)\n    text = re.sub(r'<\\[\\/?[^>]+\\]*>', r'', text)\n    text = re.sub(r'<(\\&\\w+;)>', r'\\1', text)\n    text = re.sub(r'&(?!amp|gt|lt|apos|quot)', r'', text)\n    text = re.sub(r'[ \\t]*([^<>\\s]+?)[ \\t]*<p=\"([^\"]*\"?)\"/>',\n                  r' <wf pos=\"\\2\">\\1</wf>', text)\n    text = re.sub(r'\\s*\"\\s*<p=\\'\"\\'/>', \" <wf pos='\\\"'>\\\"</wf>\", text)\n    return text\n"], "nltk\\corpus\\reader\\sentiwordnet": [".py", "\n\nimport re\nfrom nltk.compat import python_2_unicode_compatible\nfrom nltk.corpus.reader import CorpusReader\n\n@python_2_unicode_compatible\nclass SentiWordNetCorpusReader(CorpusReader):\n    def __init__(self, root, fileids, encoding='utf-8'):\n        super(SentiWordNetCorpusReader, self).__init__(root, fileids,\n                                                  encoding=encoding)\n        if len(self._fileids) != 1:\n            raise ValueError('Exactly one file must be specified')\n        self._db = {}\n        self._parse_src_file()\n\n    def _parse_src_file(self):\n        lines = self.open(self._fileids[0]).read().splitlines()\n        lines = filter((lambda x : not re.search(r\"^\\s*#\", x)), lines)\n        for i, line in enumerate(lines):\n            fields = [field.strip() for field in re.split(r\"\\t+\", line)]\n            try:            \n                pos, offset, pos_score, neg_score, synset_terms, gloss = fields\n            except:\n                raise ValueError('Line %s formatted incorrectly: %s\\n' % (i, line))\n            if pos and offset:\n                offset = int(offset)\n                self._db[(pos, offset)] = (float(pos_score), float(neg_score))\n\n    def senti_synset(self, *vals):        \n        from nltk.corpus import wordnet as wn\n        if tuple(vals) in self._db:\n            pos_score, neg_score = self._db[tuple(vals)]\n            pos, offset = vals\n            if pos == 's':\n                pos = 'a'\n            synset = wn.synset_from_pos_and_offset(pos, offset)\n            return SentiSynset(pos_score, neg_score, synset)\n        else:\n            synset = wn.synset(vals[0])\n            pos = synset.pos()\n            if pos == 's':\n                pos = 'a'\n            offset = synset.offset()\n            if (pos, offset) in self._db:\n                pos_score, neg_score = self._db[(pos, offset)]\n                return SentiSynset(pos_score, neg_score, synset)\n            else:\n                return None\n\n    def senti_synsets(self, string, pos=None):\n        from nltk.corpus import wordnet as wn\n        sentis = []\n        synset_list = wn.synsets(string, pos)\n        for synset in synset_list:\n            sentis.append(self.senti_synset(synset.name()))\n        sentis = filter(lambda x : x, sentis)\n        return sentis\n\n    def all_senti_synsets(self):\n        from nltk.corpus import wordnet as wn\n        for key, fields in self._db.items():\n            pos, offset = key\n            pos_score, neg_score = fields\n            synset = wn.synset_from_pos_and_offset(pos, offset)\n            yield SentiSynset(pos_score, neg_score, synset)\n\n\n@python_2_unicode_compatible\nclass SentiSynset(object):\n    def __init__(self, pos_score, neg_score, synset):\n        self._pos_score = pos_score\n        self._neg_score = neg_score\n        self._obj_score = 1.0 - (self._pos_score + self._neg_score)\n        self.synset = synset\n\n    def pos_score(self):\n        return self._pos_score\n\n    def neg_score(self):\n        return self._neg_score\n\n    def obj_score(self):\n        return self._obj_score\n\n    def __str__(self):\n        s = \"<\"\n        s += self.synset.name() + \": \"\n        s += \"PosScore=%s \" % self._pos_score\n        s += \"NegScore=%s\" % self._neg_score\n        s += \">\"\n        return s\n\n    def __repr__(self):\n        return \"Senti\" + repr(self.synset)\n                    \n"], "nltk\\corpus\\reader\\sinica_treebank": [".py", "\n\nimport os\nimport re\n\nfrom nltk.tree import sinica_parse\nfrom nltk.tag import map_tag\n\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\nIDENTIFIER = re.compile(r'^#\\S+\\s')\nAPPENDIX = re.compile(r'(?<=\\))#.*$')\nTAGWORD = re.compile(r':([^:()|]+):([^:()|]+)')\nWORD = re.compile(r':[^:()|]+:([^:()|]+)')\n\nclass SinicaTreebankCorpusReader(SyntaxCorpusReader):\n    def _read_block(self, stream):\n        sent = stream.readline()\n        sent = IDENTIFIER.sub('', sent)\n        sent = APPENDIX.sub('', sent)\n        return [sent]\n\n    def _parse(self, sent):\n        return sinica_parse(sent)\n\n    def _tag(self, sent, tagset=None):\n        tagged_sent = [(w,t) for (t,w) in TAGWORD.findall(sent)]\n        if tagset and tagset != self._tagset:\n            tagged_sent = [(w, map_tag(self._tagset, tagset, t)) for (w,t) in tagged_sent]\n        return tagged_sent\n\n    def _word(self, sent):\n        return WORD.findall(sent)\n"], "nltk\\corpus\\reader\\string_category": [".py", "\n\nfrom six import string_types\n\nfrom nltk import compat\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\nclass StringCategoryCorpusReader(CorpusReader):\n    def __init__(self, root, fileids, delimiter=' ', encoding='utf8'):\n        CorpusReader.__init__(self, root, fileids, encoding)\n        self._delimiter = delimiter\n\n    def tuples(self, fileids=None):\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        return concat([StreamBackedCorpusView(fileid, self._read_tuple_block,\n                                              encoding=enc)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def raw(self, fileids=None):\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n    def _read_tuple_block(self, stream):\n        line = stream.readline().strip()\n        if line:\n            return [tuple(line.split(self._delimiter, 1))]\n        else:\n            return []\n"], "nltk\\corpus\\reader\\switchboard": [".py", "from __future__ import unicode_literals\nimport re\n\nfrom nltk.tag import str2tuple, map_tag\nfrom nltk import compat\n\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\n\n@compat.python_2_unicode_compatible\nclass SwitchboardTurn(list):\n    def __init__(self, words, speaker, id):\n        list.__init__(self, words)\n        self.speaker = speaker\n        self.id = int(id)\n\n    def __repr__(self):\n        if len(self) == 0:\n            text = ''\n        elif isinstance(self[0], tuple):\n            text = ' '.join('%s/%s' % w for w in self)\n        else:\n            text = ' '.join(self)\n        return '<%s.%s: %r>' % (self.speaker, self.id, text)\n\n\nclass SwitchboardCorpusReader(CorpusReader):\n    _FILES = ['tagged']\n\n    def __init__(self, root, tagset=None):\n        CorpusReader.__init__(self, root, self._FILES)\n        self._tagset = tagset\n\n    def words(self):\n        return StreamBackedCorpusView(self.abspath('tagged'),\n                                      self._words_block_reader)\n\n    def tagged_words(self, tagset=None):\n        def tagged_words_block_reader(stream):\n            return self._tagged_words_block_reader(stream, tagset)\n        return StreamBackedCorpusView(self.abspath('tagged'),\n                                      tagged_words_block_reader)\n\n    def turns(self):\n        return StreamBackedCorpusView(self.abspath('tagged'),\n                                      self._turns_block_reader)\n\n    def tagged_turns(self, tagset=None):\n        def tagged_turns_block_reader(stream):\n            return self._tagged_turns_block_reader(stream, tagset)\n        return StreamBackedCorpusView(self.abspath('tagged'),\n                                      tagged_turns_block_reader)\n\n    def discourses(self):\n        return StreamBackedCorpusView(self.abspath('tagged'),\n                                      self._discourses_block_reader)\n\n    def tagged_discourses(self, tagset=False):\n        def tagged_discourses_block_reader(stream):\n            return self._tagged_discourses_block_reader(stream, tagset)\n        return StreamBackedCorpusView(self.abspath('tagged'),\n                                      tagged_discourses_block_reader)\n\n    def _discourses_block_reader(self, stream):\n        return [[self._parse_utterance(u, include_tag=False)\n                 for b in read_blankline_block(stream)\n                 for u in b.split('\\n') if u.strip()]]\n\n    def _tagged_discourses_block_reader(self, stream, tagset=None):\n        return [[self._parse_utterance(u, include_tag=True,\n                                       tagset=tagset)\n                 for b in read_blankline_block(stream)\n                 for u in b.split('\\n') if u.strip()]]\n\n    def _turns_block_reader(self, stream):\n        return self._discourses_block_reader(stream)[0]\n\n    def _tagged_turns_block_reader(self, stream, tagset=None):\n        return self._tagged_discourses_block_reader(stream, tagset)[0]\n\n    def _words_block_reader(self, stream):\n        return sum(self._discourses_block_reader(stream)[0], [])\n\n    def _tagged_words_block_reader(self, stream, tagset=None):\n        return sum(self._tagged_discourses_block_reader(stream,\n                                                        tagset)[0], [])\n\n    _UTTERANCE_RE = re.compile('(\\w+)\\.(\\d+)\\:\\s*(.*)')\n    _SEP = '/'\n    def _parse_utterance(self, utterance, include_tag, tagset=None):\n        m = self._UTTERANCE_RE.match(utterance)\n        if m is None:\n            raise ValueError('Bad utterance %r' % utterance)\n        speaker, id, text = m.groups()\n        words = [str2tuple(s, self._SEP) for s in text.split()]\n        if not include_tag:\n            words = [w for (w,t) in words]\n        elif tagset and tagset != self._tagset:\n            words = [(w, map_tag(self._tagset, tagset, t)) for (w,t) in words]\n        return SwitchboardTurn(words, speaker, id)\n\n"], "nltk\\corpus\\reader\\tagged": [".py", "\n\nimport os\n\nfrom six import string_types\n\nfrom nltk.tag import str2tuple, map_tag\nfrom nltk.tokenize import *\n\nfrom nltk.corpus.reader.api import *\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.timit import read_timit_block\n\nclass TaggedCorpusReader(CorpusReader):\n    def __init__(self, root, fileids,\n                 sep='/', word_tokenizer=WhitespaceTokenizer(),\n                 sent_tokenizer=RegexpTokenizer('\\n', gaps=True),\n                 para_block_reader=read_blankline_block,\n                 encoding='utf8',\n                 tagset=None):\n        CorpusReader.__init__(self, root, fileids, encoding)\n        self._sep = sep\n        self._word_tokenizer = word_tokenizer\n        self._sent_tokenizer = sent_tokenizer\n        self._para_block_reader = para_block_reader\n        self._tagset = tagset\n\n    def raw(self, fileids=None):\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n    def words(self, fileids=None):\n        return concat([TaggedCorpusView(fileid, enc,\n                                        False, False, False,\n                                        self._sep, self._word_tokenizer,\n                                        self._sent_tokenizer,\n                                        self._para_block_reader,\n                                        None)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def sents(self, fileids=None):\n        return concat([TaggedCorpusView(fileid, enc,\n                                        False, True, False,\n                                        self._sep, self._word_tokenizer,\n                                        self._sent_tokenizer,\n                                        self._para_block_reader,\n                                        None)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def paras(self, fileids=None):\n        return concat([TaggedCorpusView(fileid, enc,\n                                        False, True, True,\n                                        self._sep, self._word_tokenizer,\n                                        self._sent_tokenizer,\n                                        self._para_block_reader,\n                                        None)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def tagged_words(self, fileids=None, tagset=None):\n        if tagset and tagset != self._tagset:\n            tag_mapping_function = lambda t: map_tag(self._tagset, tagset, t)\n        else:\n            tag_mapping_function = None\n        return concat([TaggedCorpusView(fileid, enc,\n                                        True, False, False,\n                                        self._sep, self._word_tokenizer,\n                                        self._sent_tokenizer,\n                                        self._para_block_reader,\n                                        tag_mapping_function)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def tagged_sents(self, fileids=None, tagset=None):\n        if tagset and tagset != self._tagset:\n            tag_mapping_function = lambda t: map_tag(self._tagset, tagset, t)\n        else:\n            tag_mapping_function = None\n        return concat([TaggedCorpusView(fileid, enc,\n                                        True, True, False,\n                                        self._sep, self._word_tokenizer,\n                                        self._sent_tokenizer,\n                                        self._para_block_reader,\n                                        tag_mapping_function)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\n    def tagged_paras(self, fileids=None, tagset=None):\n        if tagset and tagset != self._tagset:\n            tag_mapping_function = lambda t: map_tag(self._tagset, tagset, t)\n        else:\n            tag_mapping_function = None\n        return concat([TaggedCorpusView(fileid, enc,\n                                        True, True, True,\n                                        self._sep, self._word_tokenizer,\n                                        self._sent_tokenizer,\n                                        self._para_block_reader,\n                                        tag_mapping_function)\n                       for (fileid, enc) in self.abspaths(fileids, True)])\n\nclass CategorizedTaggedCorpusReader(CategorizedCorpusReader,\n                                    TaggedCorpusReader):\n    def __init__(self, *args, **kwargs):\n        CategorizedCorpusReader.__init__(self, kwargs)\n        TaggedCorpusReader.__init__(self, *args, **kwargs)\n\n    def _resolve(self, fileids, categories):\n        if fileids is not None and categories is not None:\n            raise ValueError('Specify fileids or categories, not both')\n        if categories is not None:\n            return self.fileids(categories)\n        else:\n            return fileids\n    def raw(self, fileids=None, categories=None):\n        return TaggedCorpusReader.raw(\n            self, self._resolve(fileids, categories))\n    def words(self, fileids=None, categories=None):\n        return TaggedCorpusReader.words(\n            self, self._resolve(fileids, categories))\n    def sents(self, fileids=None, categories=None):\n        return TaggedCorpusReader.sents(\n            self, self._resolve(fileids, categories))\n    def paras(self, fileids=None, categories=None):\n        return TaggedCorpusReader.paras(\n            self, self._resolve(fileids, categories))\n    def tagged_words(self, fileids=None, categories=None, tagset=None):\n        return TaggedCorpusReader.tagged_words(\n            self, self._resolve(fileids, categories), tagset)\n    def tagged_sents(self, fileids=None, categories=None, tagset=None):\n        return TaggedCorpusReader.tagged_sents(\n            self, self._resolve(fileids, categories), tagset)\n    def tagged_paras(self, fileids=None, categories=None, tagset=None):\n        return TaggedCorpusReader.tagged_paras(\n            self, self._resolve(fileids, categories), tagset)\n\nclass TaggedCorpusView(StreamBackedCorpusView):\n    def __init__(self, corpus_file, encoding, tagged, group_by_sent,\n                 group_by_para, sep, word_tokenizer, sent_tokenizer,\n                 para_block_reader, tag_mapping_function=None):\n        self._tagged = tagged\n        self._group_by_sent = group_by_sent\n        self._group_by_para = group_by_para\n        self._sep = sep\n        self._word_tokenizer = word_tokenizer\n        self._sent_tokenizer = sent_tokenizer\n        self._para_block_reader = para_block_reader\n        self._tag_mapping_function = tag_mapping_function\n        StreamBackedCorpusView.__init__(self, corpus_file, encoding=encoding)\n\n    def read_block(self, stream):\n        block = []\n        for para_str in self._para_block_reader(stream):\n            para = []\n            for sent_str in self._sent_tokenizer.tokenize(para_str):\n                sent = [str2tuple(s, self._sep) for s in\n                        self._word_tokenizer.tokenize(sent_str)]\n                if self._tag_mapping_function:\n                    sent = [(w, self._tag_mapping_function(t)) for (w,t) in sent]\n                if not self._tagged:\n                    sent = [w for (w,t) in sent]\n                if self._group_by_sent:\n                    para.append(sent)\n                else:\n                    para.extend(sent)\n            if self._group_by_para:\n                block.append(para)\n            else:\n                block.extend(para)\n        return block\n\nclass MacMorphoCorpusReader(TaggedCorpusReader):\n    def __init__(self, root, fileids, encoding='utf8', tagset=None):\n        TaggedCorpusReader.__init__(\n            self, root, fileids, sep='_',\n            word_tokenizer=LineTokenizer(),\n            sent_tokenizer=RegexpTokenizer('.*\\n'),\n            para_block_reader=self._read_block,\n            encoding=encoding,\n            tagset=tagset)\n\n    def _read_block(self, stream):\n        return read_regexp_block(stream, r'.*', r'.*_\\.')\n\nclass TimitTaggedCorpusReader(TaggedCorpusReader):\n    def __init__(self, *args, **kwargs):\n        TaggedCorpusReader.__init__(\n            self, para_block_reader=read_timit_block, *args, **kwargs)\n\n    def paras(self):\n        raise NotImplementedError('use sents() instead')\n\n    def tagged_paras(self):\n        raise NotImplementedError('use tagged_sents() instead')\n"], "nltk\\corpus\\reader\\timit": [".py", "\nfrom __future__ import print_function, unicode_literals\n\nimport sys\nimport os\nimport re\nimport tempfile\nimport time\n\nfrom six import string_types\n\nfrom nltk import compat\nfrom nltk.tree import Tree\nfrom nltk.internals import import_from_stdlib\n\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\nclass TimitCorpusReader(CorpusReader):\n\n    _FILE_RE = (r'(\\w+-\\w+/\\w+\\.(phn|txt|wav|wrd))|' +\n                r'timitdic\\.txt|spkrinfo\\.txt')\n    _UTTERANCE_RE = r'\\w+-\\w+/\\w+\\.txt'\n\n    def __init__(self, root, encoding='utf8'):\n        if isinstance(encoding, string_types):\n            encoding = [('.*\\.wav', None), ('.*', encoding)]\n\n        CorpusReader.__init__(self, root,\n                              find_corpus_fileids(root, self._FILE_RE),\n                              encoding=encoding)\n\n        self._utterances = [name[:-4] for name in\n                            find_corpus_fileids(root, self._UTTERANCE_RE)]\n        Return a list of file identifiers for the files that make up\n        this corpus.\n\n        :param filetype: If specified, then ``filetype`` indicates that\n            only the files that have the given type should be\n            returned.  Accepted values are: ``txt``, ``wrd``, ``phn``,\n            ``wav``, or ``metadata``,\n        \"\"\"\n        if filetype is None:\n            return CorpusReader.fileids(self)\n        elif filetype in ('txt', 'wrd', 'phn', 'wav'):\n            return ['%s.%s' % (u, filetype) for u in self._utterances]\n        elif filetype == 'metadata':\n            return ['timitdic.txt', 'spkrinfo.txt']\n        else:\n            raise ValueError('Bad value for filetype: %r' % filetype)\n\n    def utteranceids(self, dialect=None, sex=None, spkrid=None,\n                   sent_type=None, sentid=None):\n        \"\"\"\n        :return: A list of the utterance identifiers for all\n        utterances in this corpus, or for the given speaker, dialect\n        region, gender, sentence type, or sentence number, if\n        specified.\n        \"\"\"\n        if isinstance(dialect, string_types): dialect = [dialect]\n        if isinstance(sex, string_types): sex = [sex]\n        if isinstance(spkrid, string_types): spkrid = [spkrid]\n        if isinstance(sent_type, string_types): sent_type = [sent_type]\n        if isinstance(sentid, string_types): sentid = [sentid]\n\n        utterances = self._utterances[:]\n        if dialect is not None:\n            utterances = [u for u in utterances if u[2] in dialect]\n        if sex is not None:\n            utterances = [u for u in utterances if u[4] in sex]\n        if spkrid is not None:\n            utterances = [u for u in utterances if u[:9] in spkrid]\n        if sent_type is not None:\n            utterances = [u for u in utterances if u[11] in sent_type]\n        if sentid is not None:\n            utterances = [u for u in utterances if u[10:] in spkrid]\n        return utterances\n\n    def transcription_dict(self):\n        \"\"\"\n        :return: A dictionary giving the 'standard' transcription for\n        each word.\n        \"\"\"\n        _transcriptions = {}\n        for line in self.open('timitdic.txt'):\n            if not line.strip() or line[0] == ';': continue\n            m = re.match(r'\\s*(\\S+)\\s+/(.*)/\\s*$', line)\n            if not m: raise ValueError('Bad line: %r' % line)\n            _transcriptions[m.group(1)] = m.group(2).split()\n        return _transcriptions\n\n    def spkrid(self, utterance):\n        return utterance.split('/')[0]\n\n    def sentid(self, utterance):\n        return utterance.split('/')[1]\n\n    def utterance(self, spkrid, sentid):\n        return '%s/%s' % (spkrid, sentid)\n\n    def spkrutteranceids(self, speaker):\n        \"\"\"\n        :return: A list of all utterances associated with a given\n        speaker.\n        \"\"\"\n        return [utterance for utterance in self._utterances\n                if utterance.startswith(speaker+'/')]\n\n    def spkrinfo(self, speaker):\n        \"\"\"\n        :return: A dictionary mapping .. something.\n        \"\"\"\n        if speaker in self._utterances:\n            speaker = self.spkrid(speaker)\n\n        if self._speakerinfo is None:\n            self._speakerinfo = {}\n            for line in self.open('spkrinfo.txt'):\n                if not line.strip() or line[0] == ';': continue\n                rec = line.strip().split(None, 9)\n                key = \"dr%s-%s%s\" % (rec[2],rec[1].lower(),rec[0].lower())\n                self._speakerinfo[key] = SpeakerInfo(*rec)\n\n        return self._speakerinfo[speaker]\n\n    def phones(self, utterances=None):\n        return [line.split()[-1]\n                for fileid in self._utterance_fileids(utterances, '.phn')\n                for line in self.open(fileid) if line.strip()]\n\n    def phone_times(self, utterances=None):\n        \"\"\"\n        offset is represented as a number of 16kHz samples!\n        \"\"\"\n        return [(line.split()[2], int(line.split()[0]), int(line.split()[1]))\n                for fileid in self._utterance_fileids(utterances, '.phn')\n                for line in self.open(fileid) if line.strip()]\n\n    def words(self, utterances=None):\n        return [line.split()[-1]\n                for fileid in self._utterance_fileids(utterances, '.wrd')\n                for line in self.open(fileid) if line.strip()]\n\n    def word_times(self, utterances=None):\n        return [(line.split()[2], int(line.split()[0]), int(line.split()[1]))\n                for fileid in self._utterance_fileids(utterances, '.wrd')\n                for line in self.open(fileid) if line.strip()]\n\n    def sents(self, utterances=None):\n        return [[line.split()[-1]\n                 for line in self.open(fileid) if line.strip()]\n                for fileid in self._utterance_fileids(utterances, '.wrd')]\n\n    def sent_times(self, utterances=None):\n        return [(line.split(None,2)[-1].strip(),\n                 int(line.split()[0]), int(line.split()[1]))\n                for fileid in self._utterance_fileids(utterances, '.txt')\n                for line in self.open(fileid) if line.strip()]\n\n    def phone_trees(self, utterances=None):\n        if utterances is None: utterances = self._utterances\n        if isinstance(utterances, string_types): utterances = [utterances]\n\n        trees = []\n        for utterance in utterances:\n            word_times = self.word_times(utterance)\n            phone_times = self.phone_times(utterance)\n            sent_times = self.sent_times(utterance)\n\n            while sent_times:\n                (sent, sent_start, sent_end) = sent_times.pop(0)\n                trees.append(Tree('S', []))\n                while (word_times and phone_times and\n                       phone_times[0][2] <= word_times[0][1]):\n                    trees[-1].append(phone_times.pop(0)[0])\n                while word_times and word_times[0][2] <= sent_end:\n                    (word, word_start, word_end) = word_times.pop(0)\n                    trees[-1].append(Tree(word, []))\n                    while phone_times and phone_times[0][2] <= word_end:\n                        trees[-1][-1].append(phone_times.pop(0)[0])\n                while phone_times and phone_times[0][2] <= sent_end:\n                    trees[-1].append(phone_times.pop(0)[0])\n        return trees\n\n    def wav(self, utterance, start=0, end=None):\n        wave = import_from_stdlib('wave')\n\n        w = wave.open(self.open(utterance+'.wav'), 'rb')\n\n        if end is None:\n            end = w.getnframes()\n\n        w.readframes(start)\n        frames = w.readframes(end-start)\n\n        tf = tempfile.TemporaryFile()\n        out = wave.open(tf, 'w')\n\n        out.setparams(w.getparams())\n        out.writeframes(frames)\n        out.close()\n\n        tf.seek(0)\n        return tf.read()\n\n    def audiodata(self, utterance, start=0, end=None):\n        assert(end is None or end > start)\n        headersize = 44\n        if end is None:\n            data = self.open(utterance+'.wav').read()\n        else:\n            data = self.open(utterance+'.wav').read(headersize+end*2)\n        return data[headersize+start*2:]\n\n    def _utterance_fileids(self, utterances, extension):\n        if utterances is None: utterances = self._utterances\n        if isinstance(utterances, string_types): utterances = [utterances]\n        return ['%s%s' % (u, extension) for u in utterances]\n\n    def play(self, utterance, start=0, end=None):\n        \"\"\"\n        Play the given audio sample.\n\n        :param utterance: The utterance id of the sample to play\n        \"\"\"\n        try:\n            import ossaudiodev\n            try:\n                dsp = ossaudiodev.open('w')\n                dsp.setfmt(ossaudiodev.AFMT_S16_LE)\n                dsp.channels(1)\n                dsp.speed(16000)\n                dsp.write(self.audiodata(utterance, start, end))\n                dsp.close()\n            except IOError as e:\n                print((\"can't acquire the audio device; please \"\n                                     \"activate your audio device.\"), file=sys.stderr)\n                print(\"system error message:\", str(e), file=sys.stderr)\n            return\n        except ImportError:\n            pass\n\n        try:\n            import pygame.mixer, StringIO\n            pygame.mixer.init(16000)\n            f = StringIO.StringIO(self.wav(utterance, start, end))\n            pygame.mixer.Sound(f).play()\n            while pygame.mixer.get_busy():\n                time.sleep(0.01)\n            return\n        except ImportError:\n            pass\n\n        print((\"you must install pygame or ossaudiodev \"\n                             \"for audio playback.\"), file=sys.stderr)\n\n\n@compat.python_2_unicode_compatible\nclass SpeakerInfo(object):\n    def __init__(self, id, sex, dr, use, recdate, birthdate,\n                 ht, race, edu, comments=None):\n        self.id = id\n        self.sex = sex\n        self.dr = dr\n        self.use = use\n        self.recdate = recdate\n        self.birthdate = birthdate\n        self.ht = ht\n        self.race = race\n        self.edu = edu\n        self.comments = comments\n\n    def __repr__(self):\n        attribs = 'id sex dr use recdate birthdate ht race edu comments'\n        args = ['%s=%r' % (attr, getattr(self, attr))\n                for attr in attribs.split()]\n        return 'SpeakerInfo(%s)' % (', '.join(args))\n\n\ndef read_timit_block(stream):\n    \"\"\"\n    Block reader for timit tagged sentences, which are preceded by a sentence\n    number that will be ignored.\n    \"\"\"\n    line = stream.readline()\n    if not line: return []\n    n, sent = line.split(' ', 1)\n    return [sent]\n"], "nltk\\corpus\\reader\\toolbox": [".py", "\n\nimport os\nimport re\nimport codecs\n\nfrom six import string_types\n\nfrom nltk.toolbox import ToolboxData\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\nclass ToolboxCorpusReader(CorpusReader):\n    def xml(self, fileids, key=None):\n        return concat([ToolboxData(path, enc).parse(key=key)\n                       for (path, enc) in self.abspaths(fileids, True)])\n\n    def fields(self, fileids, strip=True, unwrap=True, encoding='utf8',\n               errors='strict', unicode_fields=None):\n        return concat([list(ToolboxData(fileid,enc).fields(\n                             strip, unwrap, encoding, errors, unicode_fields))\n                       for (fileid, enc)\n                       in self.abspaths(fileids, include_encoding=True)])\n\n    def entries(self, fileids, **kwargs):\n        if 'key' in kwargs:\n            key = kwargs['key']\n            del kwargs['key']\n        else:\n            key = 'lx'  # the default key in MDF\n        entries = []\n        for marker, contents in self.fields(fileids, **kwargs):\n            if marker == key:\n                entries.append((contents, []))\n            else:\n                try:\n                    entries[-1][-1].append((marker, contents))\n                except IndexError:\n                    pass\n        return entries\n\n    def words(self, fileids, key='lx'):\n        return [contents for marker, contents in self.fields(fileids) if marker == key]\n\n    def raw(self, fileids):\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n\ndef demo():\n    pass\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\corpus\\reader\\twitter": [".py", "\n\nimport json\nimport os\n\nfrom six import string_types\n\nfrom nltk.tokenize import TweetTokenizer\n\nfrom nltk.corpus.reader.util import StreamBackedCorpusView, concat, ZipFilePathPointer\nfrom nltk.corpus.reader.api import CorpusReader\n\n\nclass TwitterCorpusReader(CorpusReader):\n\n    CorpusView = StreamBackedCorpusView\n\n    def __init__(self, root, fileids=None,\n                 word_tokenizer=TweetTokenizer(),\n                 encoding='utf8'):\n        CorpusReader.__init__(self, root, fileids, encoding)\n\n        for path in self.abspaths(self._fileids):\n            if isinstance(path, ZipFilePathPointer):\n                pass\n            elif os.path.getsize(path) == 0:\n                raise ValueError(\"File {} is empty\".format(path))\n\n        self._word_tokenizer = word_tokenizer\n\n\n\n    def docs(self, fileids=None):\n        return concat([self.CorpusView(path, self._read_tweets, encoding=enc)\n                       for (path, enc, fileid) in self.abspaths(fileids, True, True)])\n\n\n    def strings(self, fileids=None):\n        fulltweets = self.docs(fileids)\n        tweets = []\n        for jsono in fulltweets:\n            try:\n                text = jsono['text']\n                if isinstance(text, bytes):\n                    text = text.decode(self.encoding)\n                tweets.append(text)\n            except KeyError:\n                pass\n        return tweets\n\n\n    def tokenized(self, fileids=None):\n        tweets = self.strings(fileids)\n        tokenizer = self._word_tokenizer\n        return [tokenizer.tokenize(t) for t in tweets]\n\n\n    def raw(self, fileids=None):\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, string_types):\n            fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n\n    def _read_tweets(self, stream):\n        tweets = []\n        for i in range(10):\n            line = stream.readline()\n            if not line:\n                return tweets\n            tweet = json.loads(line)\n            tweets.append(tweet)\n        return tweets\n"], "nltk\\corpus\\reader\\udhr": [".py", "from __future__ import absolute_import, unicode_literals\n\nfrom nltk.corpus.reader.util import find_corpus_fileids\nfrom nltk.corpus.reader.plaintext import PlaintextCorpusReader\n\nclass UdhrCorpusReader(PlaintextCorpusReader):\n\n    ENCODINGS = [\n        ('.*-Latin1$', 'latin-1'),\n        ('.*-Hebrew$', 'hebrew'),\n        ('.*-Arabic$', 'cp1256'),\n        ('Czech_Cesky-UTF8', 'cp1250'), # yeah\n        ('.*-Cyrillic$', 'cyrillic'),\n        ('.*-SJIS$', 'SJIS'),\n        ('.*-GB2312$', 'GB2312'),\n        ('.*-Latin2$', 'ISO-8859-2'),\n        ('.*-Greek$', 'greek'),\n        ('.*-UTF8$', 'utf-8'),\n\n        ('Hungarian_Magyar-Unicode', 'utf-16-le'),\n        ('Amahuaca', 'latin1'),\n        ('Turkish_Turkce-Turkish', 'latin5'),\n        ('Lithuanian_Lietuviskai-Baltic', 'latin4'),\n        ('Japanese_Nihongo-EUC', 'EUC-JP'),\n        ('Japanese_Nihongo-JIS', 'iso2022_jp'),\n        ('Chinese_Mandarin-HZ', 'hz'),\n        ('Abkhaz\\-Cyrillic\\+Abkh', 'cp1251'),\n    ]\n\n    SKIP = set([\n        'Burmese_Myanmar-UTF8',\n        'Japanese_Nihongo-JIS',\n        'Chinese_Mandarin-HZ',\n        'Chinese_Mandarin-UTF8',\n        'Gujarati-UTF8',\n        'Hungarian_Magyar-Unicode',\n        'Lao-UTF8',\n        'Magahi-UTF8',\n        'Marathi-UTF8',\n        'Tamil-UTF8',\n\n        'Vietnamese-VPS',\n        'Vietnamese-VIQR',\n        'Vietnamese-TCVN',\n        'Magahi-Agra',\n        'Bhojpuri-Agra',\n        'Esperanto-T61', # latin3 raises an exception\n\n        'Burmese_Myanmar-WinResearcher',\n        'Armenian-DallakHelv',\n        'Tigrinya_Tigrigna-VG2Main',\n        'Amharic-Afenegus6..60375', # ?\n        'Navaho_Dine-Navajo-Navaho-font',\n\n        'Azeri_Azerbaijani_Cyrillic-Az.Times.Cyr.Normal0117',\n        'Azeri_Azerbaijani_Latin-Az.Times.Lat0117',\n\n        'Czech-Latin2-err',\n        'Russian_Russky-UTF8~',\n    ])\n\n\n    def __init__(self, root='udhr'):\n        fileids = find_corpus_fileids(root, r'(?!README|\\.).*')\n        super(UdhrCorpusReader, self).__init__(\n            root,\n            [fileid for fileid in fileids if fileid not in self.SKIP],\n            encoding=self.ENCODINGS\n        )\n"], "nltk\\corpus\\reader\\util": [".py", "\nimport os\nimport bisect\nimport re\nimport tempfile\nfrom six import string_types, text_type\nfrom functools import reduce\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\n\ntry: from xml.etree import cElementTree as ElementTree\nexcept ImportError: from xml.etree import ElementTree\n\nfrom nltk.tokenize import wordpunct_tokenize\nfrom nltk.internals import slice_bounds\nfrom nltk.data import PathPointer, FileSystemPathPointer, ZipFilePathPointer\nfrom nltk.data import SeekableUnicodeStreamReader\nfrom nltk.util import AbstractLazySequence, LazySubsequence, LazyConcatenation, py25\n\n\nclass StreamBackedCorpusView(AbstractLazySequence):\n    def __init__(self, fileid, block_reader=None, startpos=0,\n                 encoding='utf8'):\n        if block_reader:\n            self.read_block = block_reader\n        self._toknum = [0]\n        self._filepos = [startpos]\n        self._encoding = encoding\n        self._len = None\n\n        self._fileid = fileid\n        self._stream = None\n\n        self._current_toknum = None\n        Read a block from the input stream.\n\n        :return: a block of tokens from the input stream\n        :rtype: list(any)\n        :param stream: an input stream\n        :type stream: stream\n        \"\"\"\n        raise NotImplementedError('Abstract Method')\n\n    def _open(self):\n        \"\"\"\n        Open the file stream associated with this corpus view.  This\n        will be called performed if any value is read from the view\n        while its file stream is closed.\n        \"\"\"\n        if isinstance(self._fileid, PathPointer):\n            self._stream = self._fileid.open(self._encoding)\n        elif self._encoding:\n            self._stream = SeekableUnicodeStreamReader(\n                open(self._fileid, 'rb'), self._encoding)\n        else:\n            self._stream = open(self._fileid, 'rb')\n\n    def close(self):\n        \"\"\"\n        Close the file stream associated with this corpus view.  This\n        can be useful if you are worried about running out of file\n        handles (although the stream should automatically be closed\n        upon garbage collection of the corpus view).  If the corpus\n        view is accessed after it is closed, it will be automatically\n        re-opened.\n        \"\"\"\n        if self._stream is not None:\n            self._stream.close()\n        self._stream = None\n\n    def __len__(self):\n        if self._len is None:\n            for tok in self.iterate_from(self._toknum[-1]): pass\n        return self._len\n\n    def __getitem__(self, i):\n        if isinstance(i, slice):\n            start, stop = slice_bounds(self, i)\n            offset = self._cache[0]\n            if offset <= start and stop <= self._cache[1]:\n                return self._cache[2][start-offset:stop-offset]\n            return LazySubsequence(self, start, stop)\n        else:\n            if i < 0: i += len(self)\n            if i < 0: raise IndexError('index out of range')\n            offset = self._cache[0]\n            if offset <= i < self._cache[1]:\n                return self._cache[2][i-offset]\n            try:\n                return next(self.iterate_from(i))\n            except StopIteration:\n                raise IndexError('index out of range')\n\n    def iterate_from(self, start_tok):\n        if self._cache[0] <= start_tok < self._cache[1]:\n            for tok in self._cache[2][start_tok-self._cache[0]:]:\n                yield tok\n                start_tok += 1\n\n        if start_tok < self._toknum[-1]:\n            block_index = bisect.bisect_right(self._toknum, start_tok)-1\n            toknum = self._toknum[block_index]\n            filepos = self._filepos[block_index]\n        else:\n            block_index = len(self._toknum)-1\n            toknum = self._toknum[-1]\n            filepos = self._filepos[-1]\n\n        if self._stream is None:\n            self._open()\n\n        if self._eofpos == 0:\n            self._len = 0\n\n        while filepos < self._eofpos:\n            self._stream.seek(filepos)\n            self._current_toknum = toknum\n            self._current_blocknum = block_index\n            tokens = self.read_block(self._stream)\n            assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n                'block reader %s() should return list or tuple.' %\n                self.read_block.__name__)\n            num_toks = len(tokens)\n            new_filepos = self._stream.tell()\n            assert new_filepos > filepos, (\n                'block reader %s() should consume at least 1 byte (filepos=%d)' %\n                (self.read_block.__name__, filepos))\n\n            self._cache = (toknum, toknum+num_toks, list(tokens))\n\n            assert toknum <= self._toknum[-1]\n            if num_toks > 0:\n                block_index += 1\n                if toknum == self._toknum[-1]:\n                    assert new_filepos > self._filepos[-1] # monotonic!\n                    self._filepos.append(new_filepos)\n                    self._toknum.append(toknum+num_toks)\n                else:\n                    assert new_filepos == self._filepos[block_index], (\n                        'inconsistent block reader (num chars read)')\n                    assert toknum+num_toks == self._toknum[block_index], (\n                        'inconsistent block reader (num tokens returned)')\n\n            if new_filepos == self._eofpos:\n                self._len = toknum + num_toks\n            for tok in tokens[max(0, start_tok-toknum):]:\n                yield tok\n            assert new_filepos <= self._eofpos\n            if new_filepos == self._eofpos:\n                break\n            toknum += num_toks\n            filepos = new_filepos\n\n        assert self._len is not None\n        self.close()\n\n    def __add__(self, other):\n        return concat([self, other])\n    def __radd__(self, other):\n        return concat([other, self])\n    def __mul__(self, count):\n        return concat([self] * count)\n    def __rmul__(self, count):\n        return concat([self] * count)\n\nclass ConcatenatedCorpusView(AbstractLazySequence):\n    \"\"\"\n    A 'view' of a corpus file that joins together one or more\n    ``StreamBackedCorpusViews<StreamBackedCorpusView>``.  At most\n    one file handle is left open at any time.\n    \"\"\"\n    def __init__(self, corpus_views):\n        self._pieces = corpus_views\n        \"\"\"A list of the corpus subviews that make up this\n        concatenation.\"\"\"\n\n        self._offsets = [0]\n        \"\"\"A list of offsets, indicating the index at which each\n        subview begins.  In particular::\n            offsets[i] = sum([len(p) for p in pieces[:i]])\"\"\"\n\n        self._open_piece = None\n        \"\"\"The most recently accessed corpus subview (or None).\n        Before a new subview is accessed, this subview will be closed.\"\"\"\n\n    def __len__(self):\n        if len(self._offsets) <= len(self._pieces):\n            for tok in self.iterate_from(self._offsets[-1]): pass\n\n        return self._offsets[-1]\n\n    def close(self):\n        for piece in self._pieces:\n            piece.close()\n\n    def iterate_from(self, start_tok):\n        piecenum = bisect.bisect_right(self._offsets, start_tok)-1\n\n        while piecenum < len(self._pieces):\n            offset = self._offsets[piecenum]\n            piece = self._pieces[piecenum]\n\n            if self._open_piece is not piece:\n                if self._open_piece is not None:\n                    self._open_piece.close()\n                self._open_piece = piece\n\n            for tok in piece.iterate_from(max(0, start_tok-offset)):\n                yield tok\n\n            if piecenum+1 == len(self._offsets):\n                self._offsets.append(self._offsets[-1] + len(piece))\n\n            piecenum += 1\n\ndef concat(docs):\n    \"\"\"\n    Concatenate together the contents of multiple documents from a\n    single corpus, using an appropriate concatenation function.  This\n    utility function is used by corpus readers when the user requests\n    more than one document at a time.\n    \"\"\"\n    if len(docs) == 1:\n        return docs[0]\n    if len(docs) == 0:\n        raise ValueError('concat() expects at least one object!')\n\n    types = set(d.__class__ for d in docs)\n\n    if all(isinstance(doc, string_types) for doc in docs):\n        return ''.join(docs)\n\n    for typ in types:\n        if not issubclass(typ, (StreamBackedCorpusView,\n                                ConcatenatedCorpusView)):\n            break\n    else:\n        return ConcatenatedCorpusView(docs)\n\n    for typ in types:\n        if not issubclass(typ, AbstractLazySequence):\n            break\n    else:\n        return LazyConcatenation(docs)\n\n    if len(types) == 1:\n        typ = list(types)[0]\n\n        if issubclass(typ, list):\n            return reduce((lambda a,b:a+b), docs, [])\n\n        if issubclass(typ, tuple):\n            return reduce((lambda a,b:a+b), docs, ())\n\n        if ElementTree.iselement(typ):\n            xmltree = ElementTree.Element('documents')\n            for doc in docs: xmltree.append(doc)\n            return xmltree\n\n    raise ValueError(\"Don't know how to concatenate types: %r\" % types)\n\n\nclass PickleCorpusView(StreamBackedCorpusView):\n    \"\"\"\n    A stream backed corpus view for corpus files that consist of\n    sequences of serialized Python objects (serialized using\n    ``pickle.dump``).  One use case for this class is to store the\n    result of running feature detection on a corpus to disk.  This can\n    be useful when performing feature detection is expensive (so we\n    don't want to repeat it); but the corpus is too large to store in\n    memory.  The following example illustrates this technique:\n\n        >>> from nltk.corpus.reader.util import PickleCorpusView\n        >>> from nltk.util import LazyMap\n        >>> feature_corpus = LazyMap(detect_features, corpus) # doctest: +SKIP\n        >>> PickleCorpusView.write(feature_corpus, some_fileid)  # doctest: +SKIP\n        >>> pcv = PickleCorpusView(some_fileid) # doctest: +SKIP\n    \"\"\"\n    BLOCK_SIZE = 100\n    PROTOCOL = -1\n\n    def __init__(self, fileid, delete_on_gc=False):\n        \"\"\"\n        Create a new corpus view that reads the pickle corpus\n        ``fileid``.\n\n        :param delete_on_gc: If true, then ``fileid`` will be deleted\n            whenever this object gets garbage-collected.\n        \"\"\"\n        self._delete_on_gc = delete_on_gc\n        StreamBackedCorpusView.__init__(self, fileid)\n\n    def read_block(self, stream):\n        result = []\n        for i in range(self.BLOCK_SIZE):\n            try: result.append(pickle.load(stream))\n            except EOFError: break\n        return result\n\n    def __del__(self):\n        \"\"\"\n        If ``delete_on_gc`` was set to true when this\n        ``PickleCorpusView`` was created, then delete the corpus view's\n        fileid.  (This method is called whenever a\n        ``PickledCorpusView`` is garbage-collected.\n        \"\"\"\n        if getattr(self, '_delete_on_gc'):\n            if os.path.exists(self._fileid):\n                try: os.remove(self._fileid)\n                except (OSError, IOError): pass\n        self.__dict__.clear() # make the garbage collector's job easier\n\n    @classmethod\n    def write(cls, sequence, output_file):\n        if isinstance(output_file, string_types):\n            output_file = open(output_file, 'wb')\n        for item in sequence:\n            pickle.dump(item, output_file, cls.PROTOCOL)\n\n    @classmethod\n    def cache_to_tempfile(cls, sequence, delete_on_gc=True):\n        \"\"\"\n        Write the given sequence to a temporary file as a pickle\n        corpus; and then return a ``PickleCorpusView`` view for that\n        temporary corpus file.\n\n        :param delete_on_gc: If true, then the temporary file will be\n            deleted whenever this object gets garbage-collected.\n        \"\"\"\n        try:\n            fd, output_file_name = tempfile.mkstemp('.pcv', 'nltk-')\n            output_file = os.fdopen(fd, 'wb')\n            cls.write(sequence, output_file)\n            output_file.close()\n            return PickleCorpusView(output_file_name, delete_on_gc)\n        except (OSError, IOError) as e:\n            raise ValueError('Error while creating temp file: %s' % e)\n\n\n\n\ndef read_whitespace_block(stream):\n    toks = []\n    for i in range(20): # Read 20 lines at a time.\n        toks.extend(stream.readline().split())\n    return toks\n\ndef read_wordpunct_block(stream):\n    toks = []\n    for i in range(20): # Read 20 lines at a time.\n        toks.extend(wordpunct_tokenize(stream.readline()))\n    return toks\n\ndef read_line_block(stream):\n    toks = []\n    for i in range(20):\n        line = stream.readline()\n        if not line: return toks\n        toks.append(line.rstrip('\\n'))\n    return toks\n\ndef read_blankline_block(stream):\n    s = ''\n    while True:\n        line = stream.readline()\n        if not line:\n            if s: return [s]\n            else: return []\n        elif line and not line.strip():\n            if s: return [s]\n        else:\n            s += line\n\ndef read_alignedsent_block(stream):\n    s = ''\n    while True:\n        line = stream.readline()\n        if line[0] == '=' or line[0] == '\\n' or line[:2] == '\\r\\n':\n            continue\n        if not line:\n            if s: return [s]\n            else: return []\n        else:\n            s += line\n            if re.match('^\\d+-\\d+', line) is not None:\n                return [s]\n\ndef read_regexp_block(stream, start_re, end_re=None):\n    \"\"\"\n    Read a sequence of tokens from a stream, where tokens begin with\n    lines that match ``start_re``.  If ``end_re`` is specified, then\n    tokens end with lines that match ``end_re``; otherwise, tokens end\n    whenever the next line matching ``start_re`` or EOF is found.\n    \"\"\"\n    while True:\n        line = stream.readline()\n        if not line: return [] # end of file.\n        if re.match(start_re, line): break\n\n    lines = [line]\n    while True:\n        oldpos = stream.tell()\n        line = stream.readline()\n        if not line:\n            return [''.join(lines)]\n        if end_re is not None and re.match(end_re, line):\n            return [''.join(lines)]\n        if end_re is None and re.match(start_re, line):\n            stream.seek(oldpos)\n            return [''.join(lines)]\n        lines.append(line)\n\ndef read_sexpr_block(stream, block_size=16384, comment_char=None):\n    \"\"\"\n    Read a sequence of s-expressions from the stream, and leave the\n    stream's file position at the end the last complete s-expression\n    read.  This function will always return at least one s-expression,\n    unless there are no more s-expressions in the file.\n\n    If the file ends in in the middle of an s-expression, then that\n    incomplete s-expression is returned when the end of the file is\n    reached.\n\n    :param block_size: The default block size for reading.  If an\n        s-expression is longer than one block, then more than one\n        block will be read.\n    :param comment_char: A character that marks comments.  Any lines\n        that begin with this character will be stripped out.\n        (If spaces or tabs precede the comment character, then the\n        line will not be stripped.)\n    \"\"\"\n    start = stream.tell()\n    block = stream.read(block_size)\n    encoding = getattr(stream, 'encoding', None)\n    assert encoding is not None or isinstance(block, text_type)\n    if encoding not in (None, 'utf-8'):\n        import warnings\n        warnings.warn('Parsing may fail, depending on the properties '\n                      'of the %s encoding!' % encoding)\n\n    if comment_char:\n        COMMENT = re.compile('(?m)^%s.*$' % re.escape(comment_char))\n    while True:\n        try:\n            if comment_char:\n                block += stream.readline()\n                block = re.sub(COMMENT, _sub_space, block)\n            tokens, offset = _parse_sexpr_block(block)\n            offset = re.compile(r'\\s*').search(block, offset).end()\n\n            if encoding is None:\n                stream.seek(start+offset)\n            else:\n                stream.seek(start+len(block[:offset].encode(encoding)))\n\n            return tokens\n        except ValueError as e:\n            if e.args[0] == 'Block too small':\n                next_block = stream.read(block_size)\n                if next_block:\n                    block += next_block\n                    continue\n                else:\n                    return [block.strip()]\n            else: raise\n\ndef _sub_space(m):\n    \"\"\"Helper function: given a regexp match, return a string of\n    spaces that's the same length as the matched string.\"\"\"\n    return ' '*(m.end()-m.start())\n\ndef _parse_sexpr_block(block):\n    tokens = []\n    start = end = 0\n\n    while end < len(block):\n        m = re.compile(r'\\S').search(block, end)\n        if not m:\n            return tokens, end\n\n        start = m.start()\n\n        if m.group() != '(':\n            m2 = re.compile(r'[\\s(]').search(block, start)\n            if m2:\n                end = m2.start()\n            else:\n                if tokens: return tokens, end\n                raise ValueError('Block too small')\n\n        else:\n            nesting = 0\n            for m in re.compile(r'[()]').finditer(block, start):\n                if m.group()=='(': nesting += 1\n                else: nesting -= 1\n                if nesting == 0:\n                    end = m.end()\n                    break\n            else:\n                if tokens: return tokens, end\n                raise ValueError('Block too small')\n\n        tokens.append(block[start:end])\n\n    return tokens, end\n\n\n\ndef find_corpus_fileids(root, regexp):\n    if not isinstance(root, PathPointer):\n        raise TypeError('find_corpus_fileids: expected a PathPointer')\n    regexp += '$'\n\n    if isinstance(root, ZipFilePathPointer):\n        fileids = [name[len(root.entry):] for name in root.zipfile.namelist()\n                 if not name.endswith('/')]\n        items = [name for name in fileids if re.match(regexp, name)]\n        return sorted(items)\n\n    elif isinstance(root, FileSystemPathPointer):\n        items = []\n        kwargs = {}\n        if not py25():\n            kwargs = {'followlinks': True}\n        for dirname, subdirs, fileids in os.walk(root.path, **kwargs):\n            prefix = ''.join('%s/' % p for p in _path_from(root.path, dirname))\n            items += [prefix+fileid for fileid in fileids\n                      if re.match(regexp, prefix+fileid)]\n            if '.svn' in subdirs: subdirs.remove('.svn')\n        return sorted(items)\n\n    else:\n        raise AssertionError(\"Don't know how to handle %r\" % root)\n\ndef _path_from(parent, child):\n    if os.path.split(parent)[1] == '':\n        parent = os.path.split(parent)[0]\n    path = []\n    while parent != child:\n        child, dirname = os.path.split(child)\n        path.insert(0, dirname)\n        assert os.path.split(child)[0] != child\n    return path\n\n\ndef tagged_treebank_para_block_reader(stream):\n    para = ''\n    while True:\n        line = stream.readline()\n        if re.match('======+\\s*$', line):\n            if para.strip(): return [para]\n        elif line == '':\n            if para.strip(): return [para]\n            else: return []\n        else:\n            para += line\n"], "nltk\\corpus\\reader\\verbnet": [".py", "\nfrom __future__ import unicode_literals\n\nimport re\nimport textwrap\nfrom collections import defaultdict\n\nfrom six import string_types\n\nfrom nltk.corpus.reader.xmldocs import XMLCorpusReader\n\n\nclass VerbnetCorpusReader(XMLCorpusReader):\n\n    def __init__(self, root, fileids, wrap_etree=False):\n        XMLCorpusReader.__init__(self, root, fileids, wrap_etree)\n\n        self._lemma_to_class = defaultdict(list)\n        Return a list of all verb lemmas that appear in any class, or\n        in the ``classid`` if specified.\n        \"\"\"\n        if vnclass is None:\n            return sorted(self._lemma_to_class.keys())\n        else:\n            if isinstance(vnclass, string_types):\n                vnclass = self.vnclass(vnclass)\n            return [member.get('name') for member in\n                    vnclass.findall('MEMBERS/MEMBER')]\n\n    def wordnetids(self, vnclass=None):\n        \"\"\"\n        Return a list of all wordnet identifiers that appear in any\n        class, or in ``classid`` if specified.\n        \"\"\"\n        if vnclass is None:\n            return sorted(self._wordnet_to_class.keys())\n        else:\n            if isinstance(vnclass, string_types):\n                vnclass = self.vnclass(vnclass)\n            return sum([member.get('wn', '').split() for member in\n                        vnclass.findall('MEMBERS/MEMBER')], [])\n\n    def classids(self, lemma=None, wordnetid=None, fileid=None, classid=None):\n        \"\"\"\n        Return a list of the VerbNet class identifiers.  If a file\n        identifier is specified, then return only the VerbNet class\n        identifiers for classes (and subclasses) defined by that file.\n        If a lemma is specified, then return only VerbNet class\n        identifiers for classes that contain that lemma as a member.\n        If a wordnetid is specified, then return only identifiers for\n        classes that contain that wordnetid as a member.  If a classid\n        is specified, then return only identifiers for subclasses of\n        the specified VerbNet class.\n        If nothing is specified, return all classids within VerbNet\n        \"\"\"\n        if fileid is not None:\n            return [c for (c, f) in self._class_to_fileid.items()\n                    if f == fileid]\n        elif lemma is not None:\n            return self._lemma_to_class[lemma]\n        elif wordnetid is not None:\n            return self._wordnet_to_class[wordnetid]\n        elif classid is not None:\n            xmltree = self.vnclass(classid)\n            return [subclass.get('ID') for subclass in\n                    xmltree.findall('SUBCLASSES/VNSUBCLASS')]\n        else:\n            return sorted(self._class_to_fileid.keys())\n\n    def vnclass(self, fileid_or_classid):\n        \"\"\"Returns VerbNet class ElementTree\n        \n        Return an ElementTree containing the xml for the specified\n        VerbNet class.\n\n        :param fileid_or_classid: An identifier specifying which class\n            should be returned.  Can be a file identifier (such as\n            ``'put-9.1.xml'``), or a VerbNet class identifier (such as\n            ``'put-9.1'``) or a short VerbNet class identifier (such as\n            ``'9.1'``).\n        \"\"\"\n        if fileid_or_classid in self._fileids:\n            return self.xml(fileid_or_classid)\n\n        classid = self.longid(fileid_or_classid)\n        if classid in self._class_to_fileid:\n            fileid = self._class_to_fileid[self.longid(classid)]\n            tree = self.xml(fileid)\n            if classid == tree.get('ID'):\n                return tree\n            else:\n                for subclass in tree.findall('.//VNSUBCLASS'):\n                    if classid == subclass.get('ID'):\n                        return subclass\n                else:\n                    assert False  # we saw it during _index()!\n\n        else:\n            raise ValueError('Unknown identifier {}'.format(fileid_or_classid))\n\n    def fileids(self, vnclass_ids=None):\n        \"\"\"\n        Return a list of fileids that make up this corpus.  If\n        ``vnclass_ids`` is specified, then return the fileids that make\n        up the specified VerbNet class(es).\n        \"\"\"\n        if vnclass_ids is None:\n            return self._fileids\n        elif isinstance(vnclass_ids, string_types):\n            return [self._class_to_fileid[self.longid(vnclass_ids)]]\n        else:\n            return [self._class_to_fileid[self.longid(vnclass_id)]\n                    for vnclass_id in vnclass_ids]\n\n    def frames(self, vnclass):\n        \"\"\"Given a VerbNet class, this method returns VerbNet frames\n        \n        The members returned are:\n        1) Example\n        2) Description\n        3) Syntax\n        4) Semantics\n        \n        :param vnclass: A VerbNet class identifier; or an ElementTree\n            containing the xml contents of a VerbNet class.\n        :return: frames - a list of frame dictionaries\n        \"\"\"\n        if isinstance(vnclass, string_types):\n            vnclass = self.vnclass(vnclass)\n        frames = []\n        vnframes = vnclass.findall('FRAMES/FRAME')\n        for vnframe in vnframes:\n            frames.append({\n                'example': self._get_example_within_frame(vnframe),\n                'description': self._get_description_within_frame(vnframe),\n                'syntax': self._get_syntactic_list_within_frame(vnframe),\n                'semantics': self._get_semantics_within_frame(vnframe)\n            })\n        return frames\n\n    def subclasses(self, vnclass):\n        \"\"\"Returns subclass ids, if any exist \n        \n        Given a VerbNet class, this method returns subclass ids (if they exist)\n        in a list of strings.\n        \n        :param vnclass: A VerbNet class identifier; or an ElementTree\n            containing the xml contents of a VerbNet class.\n        :return: list of subclasses\n        \"\"\"\n        if isinstance(vnclass, string_types):\n            vnclass = self.vnclass(vnclass)\n\n        subclasses = [subclass.get('ID') for subclass in\n                      vnclass.findall('SUBCLASSES/VNSUBCLASS')]\n        return subclasses\n\n    def themroles(self, vnclass):\n        \"\"\"Returns thematic roles participating in a VerbNet class\n        \n        Members returned as part of roles are-\n        1) Type\n        2) Modifiers\n        \n        :param vnclass: A VerbNet class identifier; or an ElementTree\n            containing the xml contents of a VerbNet class.\n        :return: themroles: A list of thematic roles in the VerbNet class\n        \"\"\"\n        if isinstance(vnclass, string_types):\n            vnclass = self.vnclass(vnclass)\n\n        themroles = []\n        for trole in vnclass.findall('THEMROLES/THEMROLE'):\n            themroles.append({\n                'type': trole.get('type'),\n                'modifiers': [{'value': restr.get('Value'), 'type': restr.get('type')}\n                              for restr in trole.findall('SELRESTRS/SELRESTR')]\n            })\n        return themroles\n\n\n    def _index(self):\n        \"\"\"\n        Initialize the indexes ``_lemma_to_class``,\n        ``_wordnet_to_class``, and ``_class_to_fileid`` by scanning\n        through the corpus fileids.  This is fast with cElementTree\n        (<0.1 secs), but quite slow (>10 secs) with the python\n        implementation of ElementTree.\n        \"\"\"\n        for fileid in self._fileids:\n            self._index_helper(self.xml(fileid), fileid)\n\n    def _index_helper(self, xmltree, fileid):\n        vnclass = xmltree.get('ID')\n        self._class_to_fileid[vnclass] = fileid\n        self._shortid_to_longid[self.shortid(vnclass)] = vnclass\n        for member in xmltree.findall('MEMBERS/MEMBER'):\n            self._lemma_to_class[member.get('name')].append(vnclass)\n            for wn in member.get('wn', '').split():\n                self._wordnet_to_class[wn].append(vnclass)\n        for subclass in xmltree.findall('SUBCLASSES/VNSUBCLASS'):\n            self._index_helper(subclass, fileid)\n\n    def _quick_index(self):\n        \"\"\"\n        Initialize the indexes ``_lemma_to_class``,\n        ``_wordnet_to_class``, and ``_class_to_fileid`` by scanning\n        through the corpus fileids.  This doesn't do proper xml parsing,\n        but is good enough to find everything in the standard VerbNet\n        corpus -- and it runs about 30 times faster than xml parsing\n        (with the python ElementTree; only 2-3 times faster with\n        cElementTree).\n        \"\"\"\n        for fileid in self._fileids:\n            vnclass = fileid[:-4]  # strip the '.xml'\n            self._class_to_fileid[vnclass] = fileid\n            self._shortid_to_longid[self.shortid(vnclass)] = vnclass\n            for m in self._INDEX_RE.finditer(self.open(fileid).read()):\n                groups = m.groups()\n                if groups[0] is not None:\n                    self._lemma_to_class[groups[0]].append(vnclass)\n                    for wn in groups[1].split():\n                        self._wordnet_to_class[wn].append(vnclass)\n                elif groups[2] is not None:\n                    self._class_to_fileid[groups[2]] = fileid\n                    vnclass = groups[2]  # for <MEMBER> elts.\n                    self._shortid_to_longid[self.shortid(vnclass)] = vnclass\n                else:\n                    assert False, 'unexpected match condition'\n\n\n    def longid(self, shortid):\n        \"\"\"Returns longid of a VerbNet class\n        \n        Given a short VerbNet class identifier (eg '37.10'), map it\n        to a long id (eg 'confess-37.10').  If ``shortid`` is already a\n        long id, then return it as-is\"\"\"\n        if self._LONGID_RE.match(shortid):\n            return shortid  # it's already a longid.\n        elif not self._SHORTID_RE.match(shortid):\n            raise ValueError('vnclass identifier %r not found' % shortid)\n        try:\n            return self._shortid_to_longid[shortid]\n        except KeyError:\n            raise ValueError('vnclass identifier %r not found' % shortid)\n\n    def shortid(self, longid):\n        \"\"\"Returns shortid of a VerbNet class\n        \n        Given a long VerbNet class identifier (eg 'confess-37.10'),\n        map it to a short id (eg '37.10').  If ``longid`` is already a\n        short id, then return it as-is.\"\"\"\n        if self._SHORTID_RE.match(longid):\n            return longid  # it's already a shortid.\n        m = self._LONGID_RE.match(longid)\n        if m:\n            return m.group(2)\n        else:\n            raise ValueError('vnclass identifier %r not found' % longid)\n\n\n    def _get_semantics_within_frame(self, vnframe):\n        \"\"\"Returns semantics within a single frame\n        \n        A utility function to retrieve semantics within a frame in VerbNet\n        Members of the semantics dictionary:\n        1) Predicate value \n        2) Arguments\n        \n        :param vnframe: An ElementTree containing the xml contents of\n            a VerbNet frame.\n        :return: semantics: semantics dictionary\n        \"\"\"\n        semantics_within_single_frame = []\n        for pred in vnframe.findall('SEMANTICS/PRED'):\n            arguments = [{'type': arg.get('type'), 'value': arg.get('value')}\n                         for arg in pred.findall('ARGS/ARG')]\n            semantics_within_single_frame.append({\n                'predicate_value': pred.get('value'),\n                'arguments': arguments\n            })\n        return semantics_within_single_frame\n\n    def _get_example_within_frame(self, vnframe):\n        \"\"\"Returns example within a frame\n        \n        A utility function to retrieve an example within a frame in VerbNet.\n        \n        :param vnframe: An ElementTree containing the xml contents of\n            a VerbNet frame.\n        :return: example_text: The example sentence for this particular frame\n        \"\"\"\n        example_element = vnframe.find('EXAMPLES/EXAMPLE')\n        if example_element is not None:\n            example_text = example_element.text\n        else:\n            example_text = \"\"\n        return example_text\n\n    def _get_description_within_frame(self, vnframe):\n        \"\"\"Returns member description within frame\n         \n        A utility function to retrieve a description of participating members\n        within a frame in VerbNet.\n        \n        :param vnframe: An ElementTree containing the xml contents of\n            a VerbNet frame.\n        :return: description: a description dictionary with members - primary and secondary \n        \"\"\"\n        description_element = vnframe.find('DESCRIPTION')\n        return {\n            'primary': description_element.attrib['primary'],\n            'secondary': description_element.get('secondary', '')\n        }\n\n    def _get_syntactic_list_within_frame(self, vnframe):\n        \"\"\"Returns semantics within a frame\n        \n        A utility function to retrieve semantics within a frame in VerbNet.\n        Members of the syntactic dictionary:\n        1) POS Tag\n        2) Modifiers\n        \n        :param vnframe: An ElementTree containing the xml contents of\n            a VerbNet frame.\n        :return: syntax_within_single_frame\n        \"\"\"\n        syntax_within_single_frame = []\n        for elt in vnframe.find('SYNTAX'):\n            pos_tag = elt.tag\n            modifiers = dict()\n            modifiers['value'] = elt.get('value') if 'value' in elt.attrib else \"\"\n            modifiers['selrestrs'] = [{'value': restr.get('Value'), 'type': restr.get('type')}\n                                      for restr in elt.findall('SELRESTRS/SELRESTR')]\n            modifiers['synrestrs'] = [{'value': restr.get('Value'), 'type': restr.get('type')}\n                                      for restr in elt.findall('SYNRESTRS/SYNRESTR')]\n            syntax_within_single_frame.append({\n                'pos_tag': pos_tag,\n                'modifiers': modifiers\n            })\n        return syntax_within_single_frame\n\n\n    def pprint(self, vnclass):\n        \"\"\"Returns pretty printed version of a VerbNet class\n        \n        Return a string containing a pretty-printed representation of\n        the given VerbNet class.\n\n        :param vnclass: A VerbNet class identifier; or an ElementTree\n        containing the xml contents of a VerbNet class.\n        \"\"\"\n        if isinstance(vnclass, string_types):\n            vnclass = self.vnclass(vnclass)\n\n        s = vnclass.get('ID') + '\\n'\n        s += self.pprint_subclasses(vnclass, indent='  ') + '\\n'\n        s += self.pprint_members(vnclass, indent='  ') + '\\n'\n        s += '  Thematic roles:\\n'\n        s += self.pprint_themroles(vnclass, indent='    ') + '\\n'\n        s += '  Frames:\\n'\n        s += self.pprint_frames(vnclass, indent='    ')\n        return s\n\n    def pprint_subclasses(self, vnclass, indent=''):\n        \"\"\"Returns pretty printed version of subclasses of VerbNet class\n        \n        Return a string containing a pretty-printed representation of\n        the given VerbNet class's subclasses.\n\n        :param vnclass: A VerbNet class identifier; or an ElementTree\n            containing the xml contents of a VerbNet class.\n        \"\"\"\n        if isinstance(vnclass, string_types):\n            vnclass = self.vnclass(vnclass)\n\n        subclasses = self.subclasses(vnclass)\n        if not subclasses: subclasses = ['(none)']\n        s = 'Subclasses: ' + ' '.join(subclasses)\n        return textwrap.fill(s, 70, initial_indent=indent,\n                             subsequent_indent=indent + '  ')\n\n    def pprint_members(self, vnclass, indent=''):\n        \"\"\"Returns pretty printed version of members in a VerbNet class\n        \n        Return a string containing a pretty-printed representation of\n        the given VerbNet class's member verbs.\n\n        :param vnclass: A VerbNet class identifier; or an ElementTree\n            containing the xml contents of a VerbNet class.\n        \"\"\"\n        if isinstance(vnclass, string_types):\n            vnclass = self.vnclass(vnclass)\n\n        members = self.lemmas(vnclass)\n        if not members:\n            members = ['(none)']\n        s = 'Members: ' + ' '.join(members)\n        return textwrap.fill(s, 70, initial_indent=indent,\n                             subsequent_indent=indent + '  ')\n\n    def pprint_themroles(self, vnclass, indent=''):\n        \"\"\"Returns pretty printed version of thematic roles in a VerbNet class\n        \n        Return a string containing a pretty-printed representation of\n        the given VerbNet class's thematic roles.\n\n        :param vnclass: A VerbNet class identifier; or an ElementTree\n            containing the xml contents of a VerbNet class.\n        \"\"\"\n        if isinstance(vnclass, string_types):\n            vnclass = self.vnclass(vnclass)\n\n        pieces = []\n        for themrole in self.themroles(vnclass):\n            piece = indent + '* ' + themrole.get('type')\n            modifiers = [modifier['value'] + modifier['type']\n                         for modifier in themrole['modifiers']]\n            if modifiers:\n                piece += '[{}]'.format(' '.join(modifiers))\n            pieces.append(piece)\n        return '\\n'.join(pieces)\n\n    def pprint_frames(self, vnclass, indent=''):\n        \"\"\"Returns pretty version of all frames in a VerbNet class\n        \n        Return a string containing a pretty-printed representation of\n        the list of frames within the VerbNet class.\n\n        :param vnclass: A VerbNet class identifier; or an ElementTree\n            containing the xml contents of a VerbNet class.\n        \"\"\"\n        if isinstance(vnclass, string_types):\n            vnclass = self.vnclass(vnclass)\n        pieces = []\n        for vnframe in self.frames(vnclass):\n            pieces.append(self._pprint_single_frame(vnframe, indent))\n        return '\\n'.join(pieces)\n\n    def _pprint_single_frame(self, vnframe, indent=''):\n        \"\"\"Returns pretty printed version of a single frame in a VerbNet class\n        \n        Returns a string containing a pretty-printed representation of\n        the given frame.\n        \n        :param vnframe: An ElementTree containing the xml contents of\n            a VerbNet frame.\n        \"\"\"\n        frame_string = self._pprint_description_within_frame(vnframe, indent) + '\\n'\n        frame_string += self._pprint_example_within_frame(vnframe, indent + ' ') + '\\n'\n        frame_string += self._pprint_syntax_within_frame(vnframe, indent + '  Syntax: ') + '\\n'\n        frame_string += indent + '  Semantics:\\n'\n        frame_string += self._pprint_semantics_within_frame(vnframe, indent + '    ')\n        return frame_string\n\n    def _pprint_example_within_frame(self, vnframe, indent=''):\n        \"\"\"Returns pretty printed version of example within frame in a VerbNet class\n        \n        Return a string containing a pretty-printed representation of\n        the given VerbNet frame example.\n\n        :param vnframe: An ElementTree containing the xml contents of\n            a Verbnet frame.\n        \"\"\"\n        if vnframe['example']:\n            return indent + ' Example: ' + vnframe['example']\n\n    def _pprint_description_within_frame(self, vnframe, indent=''):\n        \"\"\"Returns pretty printed version of a VerbNet frame description\n        \n        Return a string containing a pretty-printed representation of\n        the given VerbNet frame description.\n\n        :param vnframe: An ElementTree containing the xml contents of\n            a VerbNet frame.\n        \"\"\"\n        description = indent + vnframe['description']['primary']\n        if vnframe['description']['secondary']:\n            description += ' ({})'.format(vnframe['description']['secondary'])\n        return description\n\n    def _pprint_syntax_within_frame(self, vnframe, indent=''):\n        \"\"\"Returns pretty printed version of syntax within a frame in a VerbNet class \n        \n        Return a string containing a pretty-printed representation of\n        the given VerbNet frame syntax.\n\n        :param vnframe: An ElementTree containing the xml contents of\n            a VerbNet frame.\n        \"\"\"\n        pieces = []\n        for element in vnframe['syntax']:\n            piece = element['pos_tag']\n            modifier_list = []\n            if 'value' in element['modifiers'] and element['modifiers']['value']:\n                modifier_list.append(element['modifiers']['value'])\n            modifier_list += ['{}{}'.format(restr['value'], restr['type'])\n                              for restr in (element['modifiers']['selrestrs'] +\n                                            element['modifiers']['synrestrs'])]\n            if modifier_list:\n                piece += '[{}]'.format(' '.join(modifier_list))\n            pieces.append(piece)\n\n        return indent + ' '.join(pieces)\n\n    def _pprint_semantics_within_frame(self, vnframe, indent=''):\n        \"\"\"Returns a pretty printed version of semantics within frame in a VerbNet class\n        \n        Return a string containing a pretty-printed representation of\n        the given VerbNet frame semantics.\n\n        :param vnframe: An ElementTree containing the xml contents of\n            a VerbNet frame.\n        \"\"\"\n        pieces = []\n        for predicate in vnframe['semantics']:\n            arguments = [argument['value'] for argument in predicate['arguments']]\n            pieces.append('{}({})'.format(predicate['predicate_value'], ', '.join(arguments)))\n        return '\\n'.join('{}* {}'.format(indent, piece) for piece in pieces)\n"], "nltk\\corpus\\reader\\wordlist": [".py", "from six import string_types\n\nfrom nltk.tokenize import line_tokenize\n\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\n\nclass WordListCorpusReader(CorpusReader):\n    def words(self, fileids=None, ignore_lines_startswith='\\n'):\n        return [line for line in line_tokenize(self.raw(fileids))\n                if not line.startswith(ignore_lines_startswith)]\n\n    def raw(self, fileids=None):\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n\nclass SwadeshCorpusReader(WordListCorpusReader):\n    def entries(self, fileids=None):\n        if not fileids:\n            fileids = self.fileids()\n\n        wordlists = [self.words(f) for f in fileids]\n        return list(zip(*wordlists))\n\n\nclass NonbreakingPrefixesCorpusReader(WordListCorpusReader):\n    available_langs = {'catalan': 'ca', 'czech': 'cs', 'german': 'de',\n                        'greek': 'el', 'english': 'en', 'spanish': 'es',\n                        'finnish': 'fi',  'french': 'fr', 'hungarian': 'hu',\n                        'icelandic': 'is', 'italian': 'it', 'latvian': 'lv',\n                        'dutch': 'nl', 'polish': 'pl', 'portuguese': 'pt',\n                        'romanian': 'ro', 'russian': 'ru', 'slovak': 'sk',\n                        'slovenian': 'sl', 'swedish': 'sv',  'tamil': 'ta'}\n    available_langs.update({v:v for v in available_langs.values()})\n\n    def words(self, lang=None, fileids=None, ignore_lines_startswith='#'):\n        if lang in self.available_langs:\n            lang = self.available_langs[lang]\n            fileids = ['nonbreaking_prefix.'+lang]\n        return [line for line in line_tokenize(self.raw(fileids))\n                if not line.startswith(ignore_lines_startswith)]\n\nclass UnicharsCorpusReader(WordListCorpusReader):\n    available_categories = ['Close_Punctuation', 'Currency_Symbol',\n                            'IsAlnum', 'IsAlpha', 'IsLower', 'IsN', 'IsSc',\n                            'IsSo', 'IsUpper', 'Line_Separator', 'Number',\n                            'Open_Punctuation', 'Punctuation', 'Separator',\n                            'Symbol']\n\n    def chars(self, category=None, fileids=None):\n        if category in self.available_categories:\n            fileids = [category+'.txt']\n        return list(self.raw(fileids).strip())\n\n\nclass MWAPPDBCorpusReader(WordListCorpusReader):\n    mwa_ppdb_xxxl_file = 'ppdb-1.0-xxxl-lexical.extended.synonyms.uniquepairs'\n    def entries(self, fileids=mwa_ppdb_xxxl_file):\n        return [tuple(line.split('\\t')) for line in line_tokenize(self.raw(fileids))]\n"], "nltk\\corpus\\reader\\wordnet": [".py", "\n\nfrom __future__ import print_function, unicode_literals\n\nimport math\nimport re\nfrom itertools import islice, chain\nfrom functools import total_ordering\nfrom operator import itemgetter\nfrom collections import defaultdict, deque\n\nfrom six import iteritems\nfrom six.moves import range\n\nfrom nltk.corpus.reader import CorpusReader\nfrom nltk.util import binary_search_file as _binary_search_file\nfrom nltk.probability import FreqDist\nfrom nltk.compat import python_2_unicode_compatible\nfrom nltk.internals import deprecated\n\n\n\n_INF = 1e300\n\nADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n\nPOS_LIST = [NOUN, VERB, ADJ, ADV]\n\nVERB_FRAME_STRINGS = (\n    None,\n    \"Something %s\",\n    \"Somebody %s\",\n    \"It is %sing\",\n    \"Something is %sing PP\",\n    \"Something %s something Adjective/Noun\",\n    \"Something %s Adjective/Noun\",\n    \"Somebody %s Adjective\",\n    \"Somebody %s something\",\n    \"Somebody %s somebody\",\n    \"Something %s somebody\",\n    \"Something %s something\",\n    \"Something %s to somebody\",\n    \"Somebody %s on something\",\n    \"Somebody %s somebody something\",\n    \"Somebody %s something to somebody\",\n    \"Somebody %s something from somebody\",\n    \"Somebody %s somebody with something\",\n    \"Somebody %s somebody of something\",\n    \"Somebody %s something on somebody\",\n    \"Somebody %s somebody PP\",\n    \"Somebody %s something PP\",\n    \"Somebody %s PP\",\n    \"Somebody's (body part) %s\",\n    \"Somebody %s somebody to INFINITIVE\",\n    \"Somebody %s somebody INFINITIVE\",\n    \"Somebody %s that CLAUSE\",\n    \"Somebody %s to somebody\",\n    \"Somebody %s to INFINITIVE\",\n    \"Somebody %s whether INFINITIVE\",\n    \"Somebody %s somebody into V-ing something\",\n    \"Somebody %s something with something\",\n    \"Somebody %s INFINITIVE\",\n    \"Somebody %s VERB-ing\",\n    \"It %s that CLAUSE\",\n    \"Something %s INFINITIVE\")\n\nSENSENUM_RE = re.compile(r'\\.[\\d]+\\.')\n\n\n\n\nclass WordNetError(Exception):\n\n\n@total_ordering\nclass _WordNetObject(object):\n\n    def hypernyms(self):\n        return self._related('@')\n\n    def _hypernyms(self):\n        return self._related('@')\n\n    def instance_hypernyms(self):\n        return self._related('@i')\n\n    def _instance_hypernyms(self):\n        return self._related('@i')\n\n    def hyponyms(self):\n        return self._related('~')\n\n    def instance_hyponyms(self):\n        return self._related('~i')\n\n    def member_holonyms(self):\n        return self._related('#m')\n\n    def substance_holonyms(self):\n        return self._related('#s')\n\n    def part_holonyms(self):\n        return self._related('#p')\n\n    def member_meronyms(self):\n        return self._related('%m')\n\n    def substance_meronyms(self):\n        return self._related('%s')\n\n    def part_meronyms(self):\n        return self._related('%p')\n\n    def topic_domains(self):\n        return self._related(';c')\n\n    def region_domains(self):\n        return self._related(';r')\n\n    def usage_domains(self):\n        return self._related(';u')\n\n    def attributes(self):\n        return self._related('=')\n\n    def entailments(self):\n        return self._related('*')\n\n    def causes(self):\n        return self._related('>')\n\n    def also_sees(self):\n        return self._related('^')\n\n    def verb_groups(self):\n        return self._related('$')\n\n    def similar_tos(self):\n        return self._related('&')\n\n    def __hash__(self):\n        return hash(self._name)\n\n    def __eq__(self, other):\n        return self._name == other._name\n\n    def __ne__(self, other):\n        return self._name != other._name\n\n    def __lt__(self, other):\n        return self._name < other._name\n\n\n@python_2_unicode_compatible\nclass Lemma(_WordNetObject):\n\n    __slots__ = ['_wordnet_corpus_reader', '_name', '_syntactic_marker',\n                 '_synset', '_frame_strings', '_frame_ids',\n                 '_lexname_index', '_lex_id', '_lang', '_key']\n\n    def __init__(self, wordnet_corpus_reader, synset, name,\n                 lexname_index, lex_id, syntactic_marker):\n        self._wordnet_corpus_reader = wordnet_corpus_reader\n        self._name = name\n        self._syntactic_marker = syntactic_marker\n        self._synset = synset\n        self._frame_strings = []\n        self._frame_ids = []\n        self._lexname_index = lexname_index\n        self._lex_id = lex_id\n        self._lang = 'eng'\n\n        self._key = None  # gets set later.\n\n    def name(self):\n        return self._name\n\n    def syntactic_marker(self):\n        return self._syntactic_marker\n\n    def synset(self):\n        return self._synset\n\n    def frame_strings(self):\n        return self._frame_strings\n\n    def frame_ids(self):\n        return self._frame_ids\n\n    def lang(self):\n        return self._lang\n\n    def key(self):\n        return self._key\n\n    def __repr__(self):\n        tup = type(self).__name__, self._synset._name, self._name\n        return \"%s('%s.%s')\" % tup\n\n    def _related(self, relation_symbol):\n        get_synset = self._wordnet_corpus_reader.synset_from_pos_and_offset\n        return [\n            get_synset(pos, offset)._lemmas[lemma_index]\n            for pos, offset, lemma_index\n            in self._synset._lemma_pointers[self._name, relation_symbol]\n        ]\n\n    def count(self):\n        return self._wordnet_corpus_reader.lemma_count(self)\n\n    def antonyms(self):\n        return self._related('!')\n\n    def derivationally_related_forms(self):\n        return self._related('+')\n\n    def pertainyms(self):\n        return self._related('\\\\')\n\n\n@python_2_unicode_compatible\nclass Synset(_WordNetObject):\n\n    __slots__ = ['_pos', '_offset', '_name', '_frame_ids',\n                 '_lemmas', '_lemma_names',\n                 '_definition', '_examples', '_lexname',\n                 '_pointers', '_lemma_pointers', '_max_depth',\n                 '_min_depth']\n\n    def __init__(self, wordnet_corpus_reader):\n        self._wordnet_corpus_reader = wordnet_corpus_reader\n\n        self._pos = None\n        self._offset = None\n        self._name = None\n        self._frame_ids = []\n        self._lemmas = []\n        self._lemma_names = []\n        self._definition = None\n        self._examples = []\n        self._lexname = None  # lexicographer name\n        self._all_hypernyms = None\n\n        self._pointers = defaultdict(set)\n        self._lemma_pointers = defaultdict(list)\n\n    def pos(self):\n        return self._pos\n\n    def offset(self):\n        return self._offset\n\n    def name(self):\n        return self._name\n\n    def frame_ids(self):\n        return self._frame_ids\n\n    def definition(self):\n        return self._definition\n\n    def examples(self):\n        return self._examples\n\n    def lexname(self):\n        return self._lexname\n\n    def _needs_root(self):\n        if self._pos == NOUN:\n            if self._wordnet_corpus_reader.get_version() == '1.6':\n                return True\n            else:\n                return False\n        elif self._pos == VERB:\n            return True\n\n    def lemma_names(self, lang='eng'):\n        '''Return all the lemma_names associated with the synset'''\n        if lang == 'eng':\n            return self._lemma_names\n        else:\n            self._wordnet_corpus_reader._load_lang_data(lang)\n\n            i = self._wordnet_corpus_reader.ss2of(self)\n            if i in self._wordnet_corpus_reader._lang_data[lang][0]:\n                return self._wordnet_corpus_reader._lang_data[lang][0][i]\n            else:\n                return []\n\n    def lemmas(self, lang='eng'):\n        '''Return all the lemma objects associated with the synset'''\n        if lang == 'eng':\n            return self._lemmas\n        else:\n            self._wordnet_corpus_reader._load_lang_data(lang)\n            lemmark = []\n            lemmy = self.lemma_names(lang)\n            for lem in lemmy:\n                temp = Lemma(\n                    self._wordnet_corpus_reader,\n                    self,\n                    lem,\n                    self._wordnet_corpus_reader._lexnames.index(\n                        self.lexname()\n                    ),\n                    0,\n                    None\n                )\n                temp._lang = lang\n                lemmark.append(temp)\n            return lemmark\n\n    def root_hypernyms(self):\n\n        result = []\n        seen = set()\n        todo = [self]\n        while todo:\n            next_synset = todo.pop()\n            if next_synset not in seen:\n                seen.add(next_synset)\n                next_hypernyms = next_synset.hypernyms() + \\\n                    next_synset.instance_hypernyms()\n                if not next_hypernyms:\n                    result.append(next_synset)\n                else:\n                    todo.extend(next_hypernyms)\n        return result\n\n    def max_depth(self):\n\n        if \"_max_depth\" not in self.__dict__:\n            hypernyms = self.hypernyms() + self.instance_hypernyms()\n            if not hypernyms:\n                self._max_depth = 0\n            else:\n                self._max_depth = 1 + max(h.max_depth() for h in hypernyms)\n        return self._max_depth\n\n    def min_depth(self):\n\n        if \"_min_depth\" not in self.__dict__:\n            hypernyms = self.hypernyms() + self.instance_hypernyms()\n            if not hypernyms:\n                self._min_depth = 0\n            else:\n                self._min_depth = 1 + min(h.min_depth() for h in hypernyms)\n        return self._min_depth\n\n    def closure(self, rel, depth=-1):\n        from nltk.util import breadth_first\n        synset_offsets = []\n        for synset in breadth_first(self, rel, depth):\n            if synset._offset != self._offset:\n                if synset._offset not in synset_offsets:\n                    synset_offsets.append(synset._offset)\n                    yield synset\n\n    def hypernym_paths(self):\n        paths = []\n\n        hypernyms = self.hypernyms() + self.instance_hypernyms()\n        if len(hypernyms) == 0:\n            paths = [[self]]\n\n        for hypernym in hypernyms:\n            for ancestor_list in hypernym.hypernym_paths():\n                ancestor_list.append(self)\n                paths.append(ancestor_list)\n        return paths\n\n    def common_hypernyms(self, other):\n        if not self._all_hypernyms:\n            self._all_hypernyms = set(\n                self_synset\n                for self_synsets in self._iter_hypernym_lists()\n                for self_synset in self_synsets\n            )\n        if not other._all_hypernyms:\n            other._all_hypernyms = set(\n                other_synset\n                for other_synsets in other._iter_hypernym_lists()\n                for other_synset in other_synsets\n            )\n        return list(self._all_hypernyms.intersection(other._all_hypernyms))\n\n    def lowest_common_hypernyms(\n        self, other, simulate_root=False, use_min_depth=False\n    ):\n        synsets = self.common_hypernyms(other)\n        if simulate_root:\n            fake_synset = Synset(None)\n            fake_synset._name = '*ROOT*'\n            fake_synset.hypernyms = lambda: []\n            fake_synset.instance_hypernyms = lambda: []\n            synsets.append(fake_synset)\n\n        try:\n            if use_min_depth:\n                max_depth = max(s.min_depth() for s in synsets)\n                unsorted_lch = [\n                    s for s in synsets if s.min_depth() == max_depth\n                ]\n            else:\n                max_depth = max(s.max_depth() for s in synsets)\n                unsorted_lch = [\n                    s for s in synsets if s.max_depth() == max_depth\n                ]\n            return sorted(unsorted_lch)\n        except ValueError:\n            return []\n\n    def hypernym_distances(self, distance=0, simulate_root=False):\n        distances = set([(self, distance)])\n        for hypernym in self._hypernyms() + self._instance_hypernyms():\n            distances |= hypernym.hypernym_distances(\n                distance+1,\n                simulate_root=False\n            )\n        if simulate_root:\n            fake_synset = Synset(None)\n            fake_synset._name = '*ROOT*'\n            fake_synset_distance = max(distances, key=itemgetter(1))[1]\n            distances.add((fake_synset, fake_synset_distance+1))\n        return distances\n\n    def _shortest_hypernym_paths(self, simulate_root):\n        if self._name == '*ROOT*':\n            return {self: 0}\n\n        queue = deque([(self, 0)])\n        path = {}\n\n        while queue:\n            s, depth = queue.popleft()\n            if s in path:\n                continue\n            path[s] = depth\n\n            depth += 1\n            queue.extend((hyp, depth) for hyp in s._hypernyms())\n            queue.extend((hyp, depth) for hyp in s._instance_hypernyms())\n\n        if simulate_root:\n            fake_synset = Synset(None)\n            fake_synset._name = '*ROOT*'\n            path[fake_synset] = max(path.values()) + 1\n\n        return path\n\n    def shortest_path_distance(self, other, simulate_root=False):\n\n        if self == other:\n            return 0\n\n        dist_dict1 = self._shortest_hypernym_paths(simulate_root)\n        dist_dict2 = other._shortest_hypernym_paths(simulate_root)\n\n\n        inf = float('inf')\n        path_distance = inf\n        for synset, d1 in iteritems(dist_dict1):\n            d2 = dist_dict2.get(synset, inf)\n            path_distance = min(path_distance, d1 + d2)\n\n        return None if math.isinf(path_distance) else path_distance\n\n    def tree(self, rel, depth=-1, cut_mark=None):\n\n        tree = [self]\n        if depth != 0:\n            tree += [x.tree(rel, depth-1, cut_mark) for x in rel(self)]\n        elif cut_mark:\n            tree += [cut_mark]\n        return tree\n\n    def path_similarity(self, other, verbose=False, simulate_root=True):\n\n        distance = self.shortest_path_distance(\n            other,\n            simulate_root=simulate_root and self._needs_root()\n        )\n        if distance is None or distance < 0:\n            return None\n        return 1.0 / (distance + 1)\n\n    def lch_similarity(self, other, verbose=False, simulate_root=True):\n\n        if self._pos != other._pos:\n            raise WordNetError(\n                'Computing the lch similarity requires '\n                '%s and %s to have the same part of speech.' %\n                (self, other)\n            )\n\n        need_root = self._needs_root()\n\n        if self._pos not in self._wordnet_corpus_reader._max_depth:\n            self._wordnet_corpus_reader._compute_max_depth(\n                self._pos, need_root\n            )\n\n        depth = self._wordnet_corpus_reader._max_depth[self._pos]\n\n        distance = self.shortest_path_distance(\n            other,\n            simulate_root=simulate_root and need_root\n        )\n\n        if distance is None or distance < 0 or depth == 0:\n            return None\n        return -math.log((distance + 1) / (2.0 * depth))\n\n    def wup_similarity(self, other, verbose=False, simulate_root=True):\n\n        need_root = self._needs_root()\n        subsumers = self.lowest_common_hypernyms(\n            other,\n            simulate_root=simulate_root and need_root, use_min_depth=True\n        )\n\n        if len(subsumers) == 0:\n            return None\n\n        subsumer = self if self in subsumers else subsumers[0]\n\n        depth = subsumer.max_depth() + 1\n\n\n        len1 = self.shortest_path_distance(\n            subsumer,\n            simulate_root=simulate_root and need_root\n        )\n        len2 = other.shortest_path_distance(\n            subsumer,\n            simulate_root=simulate_root and need_root\n        )\n        if len1 is None or len2 is None:\n            return None\n        len1 += depth\n        len2 += depth\n        return (2.0 * depth) / (len1 + len2)\n\n    def res_similarity(self, other, ic, verbose=False):\n\n        ic1, ic2, lcs_ic = _lcs_ic(self, other, ic)\n        return lcs_ic\n\n    def jcn_similarity(self, other, ic, verbose=False):\n\n        if self == other:\n            return _INF\n\n        ic1, ic2, lcs_ic = _lcs_ic(self, other, ic)\n\n        if ic1 == 0 or ic2 == 0:\n            return 0\n\n        ic_difference = ic1 + ic2 - 2 * lcs_ic\n\n        if ic_difference == 0:\n            return _INF\n\n        return 1 / ic_difference\n\n    def lin_similarity(self, other, ic, verbose=False):\n\n        ic1, ic2, lcs_ic = _lcs_ic(self, other, ic)\n        return (2.0 * lcs_ic) / (ic1 + ic2)\n\n    def _iter_hypernym_lists(self):\n        todo = [self]\n        seen = set()\n        while todo:\n            for synset in todo:\n                seen.add(synset)\n            yield todo\n            todo = [hypernym\n                    for synset in todo\n                    for hypernym in (\n                        synset.hypernyms() + synset.instance_hypernyms()\n                    )\n                    if hypernym not in seen]\n\n    def __repr__(self):\n        return \"%s('%s')\" % (type(self).__name__, self._name)\n\n    def _related(self, relation_symbol, sort=True):\n        get_synset = self._wordnet_corpus_reader.synset_from_pos_and_offset\n        pointer_tuples = self._pointers[relation_symbol]\n        r = [get_synset(pos, offset) for pos, offset in pointer_tuples]\n        if sort:\n            r.sort()\n        return r\n\n\n\nclass WordNetCorpusReader(CorpusReader):\n\n    _ENCODING = 'utf8'\n\n    ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n\n    _FILEMAP = {ADJ: 'adj', ADV: 'adv', NOUN: 'noun', VERB: 'verb'}\n\n    _pos_numbers = {NOUN: 1, VERB: 2, ADJ: 3, ADV: 4, ADJ_SAT: 5}\n    _pos_names = dict(tup[::-1] for tup in _pos_numbers.items())\n\n    _FILES = ('cntlist.rev', 'lexnames', 'index.sense',\n              'index.adj', 'index.adv', 'index.noun', 'index.verb',\n              'data.adj', 'data.adv', 'data.noun', 'data.verb',\n              'adj.exc', 'adv.exc', 'noun.exc', 'verb.exc', )\n\n    def __init__(self, root, omw_reader):\n        super(WordNetCorpusReader, self).__init__(root, self._FILES,\n                                                  encoding=self._ENCODING)\n\n        self._lemma_pos_offset_map = defaultdict(dict)\n\n        self._synset_offset_cache = defaultdict(dict)\n\n        self._max_depth = defaultdict(dict)\n\n        self._omw_reader = omw_reader\n\n        self._lang_data = defaultdict(list)\n\n        self._data_file_map = {}\n        self._exception_map = {}\n        self._lexnames = []\n        self._key_count_file = None\n        self._key_synset_file = None\n\n        for i, line in enumerate(self.open('lexnames')):\n            index, lexname, _ = line.split()\n            assert int(index) == i\n            self._lexnames.append(lexname)\n\n        self._load_lemma_pos_offset_map()\n\n        self._load_exception_map()\n\n\n    def of2ss(self, of):\n        ''' take an id and return the synsets '''\n        return self.synset_from_pos_and_offset(of[-1], int(of[:8]))\n\n    def ss2of(self, ss):\n        ''' return the ID of the synset '''\n        return (\"{:08d}-{}\".format(ss.offset(), ss.pos()))\n\n    def _load_lang_data(self, lang):\n        ''' load the wordnet data of the requested language from the file to\n        the cache, _lang_data '''\n\n        if lang in self._lang_data.keys():\n            return\n\n        if lang not in self.langs():\n            raise WordNetError(\"Language is not supported.\")\n\n        f = self._omw_reader.open('{0:}/wn-data-{0:}.tab'.format(lang))\n        self.custom_lemmas(f, lang)\n        f.close()\n\n    def langs(self):\n        ''' return a list of languages supported by Multilingual Wordnet '''\n        import os\n        langs = ['eng']\n        fileids = self._omw_reader.fileids()\n        for fileid in fileids:\n            file_name, file_extension = os.path.splitext(fileid)\n            if file_extension == '.tab':\n                langs.append(file_name.split('-')[-1])\n\n        return langs\n\n    def _load_lemma_pos_offset_map(self):\n        for suffix in self._FILEMAP.values():\n\n            for i, line in enumerate(self.open('index.%s' % suffix)):\n                if line.startswith(' '):\n                    continue\n\n                _iter = iter(line.split())\n\n                def _next_token(): return next(_iter)\n\n                try:\n\n                    lemma = _next_token()\n                    pos = _next_token()\n\n                    n_synsets = int(_next_token())\n                    assert n_synsets > 0\n\n                    n_pointers = int(_next_token())\n                    [_next_token() for _ in range(n_pointers)]\n\n                    n_senses = int(_next_token())\n                    assert n_synsets == n_senses\n\n                    _next_token()\n\n                    synset_offsets = [\n                        int(_next_token()) for _ in range(n_synsets)\n                    ]\n\n                except (AssertionError, ValueError) as e:\n                    tup = ('index.%s' % suffix), (i + 1), e\n                    raise WordNetError('file %s, line %i: %s' % tup)\n\n                self._lemma_pos_offset_map[lemma][pos] = synset_offsets\n                if pos == ADJ:\n                    self._lemma_pos_offset_map[lemma][ADJ_SAT] = synset_offsets\n\n    def _load_exception_map(self):\n        for pos, suffix in self._FILEMAP.items():\n            self._exception_map[pos] = {}\n            for line in self.open('%s.exc' % suffix):\n                terms = line.split()\n                self._exception_map[pos][terms[0]] = terms[1:]\n        self._exception_map[ADJ_SAT] = self._exception_map[ADJ]\n\n    def _compute_max_depth(self, pos, simulate_root):\n        depth = 0\n        for ii in self.all_synsets(pos):\n            try:\n                depth = max(depth, ii.max_depth())\n            except RuntimeError:\n                print(ii)\n        if simulate_root:\n            depth += 1\n        self._max_depth[pos] = depth\n\n    def get_version(self):\n        fh = self._data_file(ADJ)\n        for line in fh:\n            match = re.search(r'WordNet (\\d+\\.\\d+) Copyright', line)\n            if match is not None:\n                version = match.group(1)\n                fh.seek(0)\n                return version\n\n\n    def lemma(self, name, lang='eng'):\n        '''Return lemma object that matches the name'''\n        separator = SENSENUM_RE.search(name).start()\n\n        leadingZero = int(name[separator+1]) == 0\n        if (leadingZero):\n            synset_name, lemma_name = name[:separator+3], name[separator+4:]\n        else:\n            synset_name, lemma_name = name[:separator+2], name[separator+3:]\n        \n        synset = self.synset(synset_name)\n        for lemma in synset.lemmas(lang):\n            if lemma._name == lemma_name:\n                return lemma\n        raise WordNetError('no lemma %r in %r' % (lemma_name, synset_name))\n\n    def lemma_from_key(self, key):\n        key = key.lower()\n\n        lemma_name, lex_sense = key.split('%')\n        pos_number, lexname_index, lex_id, _, _ = lex_sense.split(':')\n        pos = self._pos_names[int(pos_number)]\n\n        if self._key_synset_file is None:\n            self._key_synset_file = self.open('index.sense')\n\n        synset_line = _binary_search_file(self._key_synset_file, key)\n        if not synset_line:\n            raise WordNetError(\"No synset found for key %r\" % key)\n        offset = int(synset_line.split()[1])\n        synset = self.synset_from_pos_and_offset(pos, offset)\n\n        for lemma in synset._lemmas:\n            if lemma._key == key:\n                return lemma\n        raise WordNetError(\"No lemma found for for key %r\" % key)\n\n    def synset(self, name):\n        lemma, pos, synset_index_str = name.lower().rsplit('.', 2)\n        synset_index = int(synset_index_str) - 1\n\n        try:\n            offset = self._lemma_pos_offset_map[lemma][pos][synset_index]\n        except KeyError:\n            message = 'no lemma %r with part of speech %r'\n            raise WordNetError(message % (lemma, pos))\n        except IndexError:\n            n_senses = len(self._lemma_pos_offset_map[lemma][pos])\n            message = \"lemma %r with part of speech %r has only %i %s\"\n            if n_senses == 1:\n                tup = lemma, pos, n_senses, \"sense\"\n            else:\n                tup = lemma, pos, n_senses, \"senses\"\n            raise WordNetError(message % tup)\n\n        synset = self.synset_from_pos_and_offset(pos, offset)\n\n        if pos == 's' and synset._pos == 'a':\n            message = ('adjective satellite requested but only plain '\n                       'adjective found for lemma %r')\n            raise WordNetError(message % lemma)\n        assert synset._pos == pos or (pos == 'a' and synset._pos == 's')\n\n        return synset\n\n    def _data_file(self, pos):\n        if pos == ADJ_SAT:\n            pos = ADJ\n        if self._data_file_map.get(pos) is None:\n            fileid = 'data.%s' % self._FILEMAP[pos]\n            self._data_file_map[pos] = self.open(fileid)\n        return self._data_file_map[pos]\n\n    def synset_from_pos_and_offset(self, pos, offset):\n        if offset in self._synset_offset_cache[pos]:\n            return self._synset_offset_cache[pos][offset]\n\n        data_file = self._data_file(pos)\n        data_file.seek(offset)\n        data_file_line = data_file.readline()\n        synset = self._synset_from_pos_and_line(pos, data_file_line)\n        assert synset._offset == offset\n        self._synset_offset_cache[pos][offset] = synset\n        return synset\n\n    @deprecated('Use public method synset_from_pos_and_offset() instead')\n    def _synset_from_pos_and_offset(self, *args, **kwargs):\n        return self.synset_from_pos_and_offset(*args, **kwargs)\n\n    def _synset_from_pos_and_line(self, pos, data_file_line):\n        synset = Synset(self)\n\n        try:\n\n            columns_str, gloss = data_file_line.split('|')\n            gloss = gloss.strip()\n            definitions = []\n            for gloss_part in gloss.split(';'):\n                gloss_part = gloss_part.strip()\n                if gloss_part.startswith('\"'):\n                    synset._examples.append(gloss_part.strip('\"'))\n                else:\n                    definitions.append(gloss_part)\n            synset._definition = '; '.join(definitions)\n\n            _iter = iter(columns_str.split())\n\n            def _next_token(): return next(_iter)\n\n            synset._offset = int(_next_token())\n\n            lexname_index = int(_next_token())\n            synset._lexname = self._lexnames[lexname_index]\n\n            synset._pos = _next_token()\n\n            n_lemmas = int(_next_token(), 16)\n            for _ in range(n_lemmas):\n                lemma_name = _next_token()\n                lex_id = int(_next_token(), 16)\n                m = re.match(r'(.*?)(\\(.*\\))?$', lemma_name)\n                lemma_name, syn_mark = m.groups()\n                lemma = Lemma(self, synset, lemma_name, lexname_index,\n                              lex_id, syn_mark)\n                synset._lemmas.append(lemma)\n                synset._lemma_names.append(lemma._name)\n\n            n_pointers = int(_next_token())\n            for _ in range(n_pointers):\n                symbol = _next_token()\n                offset = int(_next_token())\n                pos = _next_token()\n                lemma_ids_str = _next_token()\n                if lemma_ids_str == '0000':\n                    synset._pointers[symbol].add((pos, offset))\n                else:\n                    source_index = int(lemma_ids_str[:2], 16) - 1\n                    target_index = int(lemma_ids_str[2:], 16) - 1\n                    source_lemma_name = synset._lemmas[source_index]._name\n                    lemma_pointers = synset._lemma_pointers\n                    tups = lemma_pointers[source_lemma_name, symbol]\n                    tups.append((pos, offset, target_index))\n\n            try:\n                frame_count = int(_next_token())\n            except StopIteration:\n                pass\n            else:\n                for _ in range(frame_count):\n                    plus = _next_token()\n                    assert plus == '+'\n                    frame_number = int(_next_token())\n                    frame_string_fmt = VERB_FRAME_STRINGS[frame_number]\n                    lemma_number = int(_next_token(), 16)\n                    if lemma_number == 0:\n                        synset._frame_ids.append(frame_number)\n                        for lemma in synset._lemmas:\n                            lemma._frame_ids.append(frame_number)\n                            lemma._frame_strings.append(\n                                frame_string_fmt % lemma._name\n                            )\n                    else:\n                        lemma = synset._lemmas[lemma_number - 1]\n                        lemma._frame_ids.append(frame_number)\n                        lemma._frame_strings.append(\n                            frame_string_fmt % lemma._name\n                        )\n\n        except ValueError as e:\n            raise WordNetError('line %r: %s' % (data_file_line, e))\n\n        for lemma in synset._lemmas:\n            if synset._pos == ADJ_SAT:\n                head_lemma = synset.similar_tos()[0]._lemmas[0]\n                head_name = head_lemma._name\n                head_id = '%02d' % head_lemma._lex_id\n            else:\n                head_name = head_id = ''\n            tup = (lemma._name, WordNetCorpusReader._pos_numbers[synset._pos],\n                   lemma._lexname_index, lemma._lex_id, head_name, head_id)\n            lemma._key = ('%s%%%d:%02d:%02d:%s:%s' % tup).lower()\n\n        lemma_name = synset._lemmas[0]._name.lower()\n        offsets = self._lemma_pos_offset_map[lemma_name][synset._pos]\n        sense_index = offsets.index(synset._offset)\n        tup = lemma_name, synset._pos, sense_index + 1\n        synset._name = '%s.%s.%02i' % tup\n\n        return synset\n\n    def synset_from_sense_key(self, sense_key):\n        sense_key_regex = re.compile(r\"(.*)\\%(.*):(.*):(.*):(.*):(.*)\")\n        synset_types = {1:NOUN, 2:VERB, 3:ADJ, 4:ADV, 5:ADJ_SAT}\n        lemma, ss_type, _, lex_id, _, _ = sense_key_regex.match(sense_key).groups()\n\n        error = None\n        if not lemma:\n            error = \"lemma\"\n        elif int(ss_type) not in synset_types:\n            error = \"ss_type\"\n        elif int(lex_id) < 0 or int(lex_id) > 99:\n            error = \"lex_id\"\n        if error:\n            raise WordNetError(\"valid {} could not be extracted from the sense key\".format(error))\n\n        synset_id = '.'.join([lemma, synset_types[int(ss_type)], lex_id])\n        return self.synset(synset_id)\n\n\n    def synsets(self, lemma, pos=None, lang='eng', check_exceptions=True):\n        lemma = lemma.lower()\n\n        if lang == 'eng':\n            get_synset = self.synset_from_pos_and_offset\n            index = self._lemma_pos_offset_map\n            if pos is None:\n                pos = POS_LIST\n            return [get_synset(p, offset)\n                    for p in pos\n                    for form in self._morphy(lemma, p, check_exceptions)\n                    for offset in index[form].get(p, [])]\n\n        else:\n            self._load_lang_data(lang)\n            synset_list = []\n            for l in self._lang_data[lang][1][lemma]:\n                if pos is not None and l[-1] != pos:\n                    continue\n                synset_list.append(self.of2ss(l))\n            return synset_list\n\n    def lemmas(self, lemma, pos=None, lang='eng'):\n        if pos is None:\n            pos_tags = self._FILEMAP.keys()\n        else:\n            pos_tags = [pos]\n\n        cache = self._synset_offset_cache\n        from_pos_and_line = self._synset_from_pos_and_line\n\n        for pos_tag in pos_tags:\n            if pos_tag == ADJ_SAT:\n                pos_tag = ADJ\n            fileid = 'data.%s' % self._FILEMAP[pos_tag]\n            data_file = self.open(fileid)\n\n            try:\n                offset = data_file.tell()\n                line = data_file.readline()\n                while line:\n                    if not line[0].isspace():\n                        if offset in cache[pos_tag]:\n                            synset = cache[pos_tag][offset]\n                        else:\n                            synset = from_pos_and_line(pos_tag, line)\n                            cache[pos_tag][offset] = synset\n\n                        if synset._pos == ADJ_SAT:\n                            yield synset\n\n                        else:\n                            yield synset\n                    offset = data_file.tell()\n                    line = data_file.readline()\n\n            except:\n                data_file.close()\n                raise\n            else:\n                data_file.close()\n\n    def words(self, lang='eng'):\n        return self.all_lemma_names(lang=lang)\n\n    def license(self, lang='eng'):\n        \"\"\"Return the contents of LICENSE (for omw)\n           use lang=lang to get the license for an individual language\"\"\"\n        if lang == 'eng':\n            return self.open(\"LICENSE\").read()\n        elif lang in self.langs():\n            return self._omw_reader.open(\"{}/LICENSE\".format(lang)).read()\n        elif lang == 'omw':\n            return self._omw_reader.open(\"LICENSE\").read()\n        elif lang in self._lang_data:\n            raise WordNetError(\n                \"Cannot determine license for user-provided tab file\"\n            )\n        else:\n            raise WordNetError(\"Language is not supported.\")\n\n    def readme(self, lang='omw'):\n        \"\"\"Return the contents of README (for omw)\n           use lang=lang to get the readme for an individual language\"\"\"\n        if lang == 'eng':\n            return self.open(\"README\").read()\n        elif lang in self.langs():\n            return self._omw_reader.open(\"{}/README\".format(lang)).read()\n        elif lang == 'omw':\n            return self._omw_reader.open(\"README\").read()\n        elif lang in self._lang_data:\n            raise WordNetError(\"No README for user-provided tab file\")\n        else:\n            raise WordNetError(\"Language is not supported.\")\n\n    def citation(self, lang='omw'):\n        \"\"\"Return the contents of citation.bib file (for omw)\n           use lang=lang to get the citation for an individual language\"\"\"\n        if lang == 'eng':\n            return self.open(\"citation.bib\").read()\n        elif lang in self.langs():\n            return self._omw_reader.open(\"{}/citation.bib\".format(lang)).read()\n        elif lang == 'omw':\n            return self._omw_reader.open(\"citation.bib\").read()\n        elif lang in self._lang_data:\n            raise WordNetError(\"citation not known for user-provided tab file\")\n        else:\n            raise WordNetError(\"Language is not supported.\")\n\n    def lemma_count(self, lemma):\n        if lemma._lang != 'eng':\n            return 0\n        if self._key_count_file is None:\n            self._key_count_file = self.open('cntlist.rev')\n        line = _binary_search_file(self._key_count_file, lemma._key)\n        if line:\n            return int(line.rsplit(' ', 1)[-1])\n        else:\n            return 0\n\n    def path_similarity(\n        self, synset1, synset2, verbose=False, simulate_root=True\n    ):\n        return synset1.path_similarity(synset2, verbose, simulate_root)\n    path_similarity.__doc__ = Synset.path_similarity.__doc__\n\n    def lch_similarity(\n        self, synset1, synset2, verbose=False, simulate_root=True\n    ):\n        return synset1.lch_similarity(synset2, verbose, simulate_root)\n    lch_similarity.__doc__ = Synset.lch_similarity.__doc__\n\n    def wup_similarity(\n        self, synset1, synset2, verbose=False, simulate_root=True\n    ):\n        return synset1.wup_similarity(synset2, verbose, simulate_root)\n    wup_similarity.__doc__ = Synset.wup_similarity.__doc__\n\n    def res_similarity(self, synset1, synset2, ic, verbose=False):\n        return synset1.res_similarity(synset2, ic, verbose)\n    res_similarity.__doc__ = Synset.res_similarity.__doc__\n\n    def jcn_similarity(self, synset1, synset2, ic, verbose=False):\n        return synset1.jcn_similarity(synset2, ic, verbose)\n    jcn_similarity.__doc__ = Synset.jcn_similarity.__doc__\n\n    def lin_similarity(self, synset1, synset2, ic, verbose=False):\n        return synset1.lin_similarity(synset2, ic, verbose)\n    lin_similarity.__doc__ = Synset.lin_similarity.__doc__\n\n    def morphy(self, form, pos=None, check_exceptions=True):\n        \"\"\"\n        Find a possible base form for the given form, with the given\n        part of speech, by checking WordNet's list of exceptional\n        forms, and by recursively stripping affixes for this part of\n        speech until a form in WordNet is found.\n\n        >>> from nltk.corpus import wordnet as wn\n        >>> print(wn.morphy('dogs'))\n        dog\n        >>> print(wn.morphy('churches'))\n        church\n        >>> print(wn.morphy('aardwolves'))\n        aardwolf\n        >>> print(wn.morphy('abaci'))\n        abacus\n        >>> wn.morphy('hardrock', wn.ADV)\n        >>> print(wn.morphy('book', wn.NOUN))\n        book\n        >>> wn.morphy('book', wn.ADJ)\n        \"\"\"\n\n        if pos is None:\n            morphy = self._morphy\n            analyses = chain(a for p in POS_LIST for a in morphy(form, p))\n        else:\n            analyses = self._morphy(form, pos, check_exceptions)\n\n        first = list(islice(analyses, 1))\n        if len(first) == 1:\n            return first[0]\n        else:\n            return None\n\n    MORPHOLOGICAL_SUBSTITUTIONS = {\n        NOUN: [('s', ''), ('ses', 's'), ('ves', 'f'), ('xes', 'x'),\n               ('zes', 'z'), ('ches', 'ch'), ('shes', 'sh'),\n               ('men', 'man'), ('ies', 'y')],\n        VERB: [('s', ''), ('ies', 'y'), ('es', 'e'), ('es', ''),\n               ('ed', 'e'), ('ed', ''), ('ing', 'e'), ('ing', '')],\n        ADJ: [('er', ''), ('est', ''), ('er', 'e'), ('est', 'e')],\n        ADV: []}\n\n    MORPHOLOGICAL_SUBSTITUTIONS[ADJ_SAT] = MORPHOLOGICAL_SUBSTITUTIONS[ADJ]\n\n    def _morphy(self, form, pos, check_exceptions=True):\n\n        exceptions = self._exception_map[pos]\n        substitutions = self.MORPHOLOGICAL_SUBSTITUTIONS[pos]\n\n        def apply_rules(forms):\n            return [form[:-len(old)] + new\n                    for form in forms\n                    for old, new in substitutions\n                    if form.endswith(old)]\n\n        def filter_forms(forms):\n            result = []\n            seen = set()\n            for form in forms:\n                if form in self._lemma_pos_offset_map:\n                    if pos in self._lemma_pos_offset_map[form]:\n                        if form not in seen:\n                            result.append(form)\n                            seen.add(form)\n            return result\n\n        if check_exceptions:\n            if form in exceptions:\n                return filter_forms([form] + exceptions[form])\n\n        forms = apply_rules([form])\n\n        results = filter_forms([form] + forms)\n        if results:\n            return results\n\n        while forms:\n            forms = apply_rules(forms)\n            results = filter_forms(forms)\n            if results:\n                return results\n\n        return []\n\n    def ic(self, corpus, weight_senses_equally=False, smoothing=1.0):\n        \"\"\"\n        Creates an information content lookup dictionary from a corpus.\n\n        :type corpus: CorpusReader\n        :param corpus: The corpus from which we create an information\n        content dictionary.\n        :type weight_senses_equally: bool\n        :param weight_senses_equally: If this is True, gives all\n        possible senses equal weight rather than dividing by the\n        number of possible senses.  (If a word has 3 synses, each\n        sense gets 0.3333 per appearance when this is False, 1.0 when\n        it is true.)\n        :param smoothing: How much do we smooth synset counts (default is 1.0)\n        :type smoothing: float\n        :return: An information content dictionary\n        \"\"\"\n        counts = FreqDist()\n        for ww in corpus.words():\n            counts[ww] += 1\n\n        ic = {}\n        for pp in POS_LIST:\n            ic[pp] = defaultdict(float)\n\n        if smoothing > 0.0:\n            for ss in self.all_synsets():\n                pos = ss._pos\n                if pos == ADJ_SAT:\n                    pos = ADJ\n                ic[pos][ss._offset] = smoothing\n\n        for ww in counts:\n            possible_synsets = self.synsets(ww)\n            if len(possible_synsets) == 0:\n                continue\n\n            weight = float(counts[ww])\n            if not weight_senses_equally:\n                weight /= float(len(possible_synsets))\n\n            for ss in possible_synsets:\n                pos = ss._pos\n                if pos == ADJ_SAT:\n                    pos = ADJ\n                for level in ss._iter_hypernym_lists():\n                    for hh in level:\n                        ic[pos][hh._offset] += weight\n                ic[pos][0] += weight\n        return ic\n\n    def custom_lemmas(self, tab_file, lang):\n        \"\"\"\n        Reads a custom tab file containing mappings of lemmas in the given\n        language to Princeton WordNet 3.0 synset offsets, allowing NLTK's\n        WordNet functions to then be used with that language.\n\n        See the \"Tab files\" section at http://compling.hss.ntu.edu.sg/omw/ for\n        documentation on the Multilingual WordNet tab file format.\n\n        :param tab_file: Tab file as a file or file-like object\n        :type  lang str\n        :param lang ISO 639-3 code of the language of the tab file\n        \"\"\"\n        if len(lang) != 3:\n            raise ValueError('lang should be a (3 character) ISO 639-3 code')\n        self._lang_data[lang] = [defaultdict(list), defaultdict(list)]\n        for l in tab_file.readlines():\n            if isinstance(l, bytes):\n                l = l.decode('utf-8')\n            l = l.replace('\\n', '')\n            l = l.replace(' ', '_')\n            if l[0] != '#':\n                word = l.split('\\t')\n                self._lang_data[lang][0][word[0]].append(word[2])\n                self._lang_data[lang][1][word[2].lower()].append(word[0])\n\n\n\nclass WordNetICCorpusReader(CorpusReader):\n    \"\"\"\n    A corpus reader for the WordNet information content corpus.\n    \"\"\"\n\n    def __init__(self, root, fileids):\n        CorpusReader.__init__(self, root, fileids, encoding='utf8')\n\n    def ic(self, icfile):\n        \"\"\"\n        Load an information content file from the wordnet_ic corpus\n        and return a dictionary.  This dictionary has just two keys,\n        NOUN and VERB, whose values are dictionaries that map from\n        synsets to information content values.\n\n        :type icfile: str\n        :param icfile: The name of the wordnet_ic file (e.g. \"ic-brown.dat\")\n        :return: An information content dictionary\n        \"\"\"\n        ic = {}\n        ic[NOUN] = defaultdict(float)\n        ic[VERB] = defaultdict(float)\n        for num, line in enumerate(self.open(icfile)):\n            if num == 0:  # skip the header\n                continue\n            fields = line.split()\n            offset = int(fields[0][:-1])\n            value = float(fields[1])\n            pos = _get_pos(fields[0])\n            if len(fields) == 3 and fields[2] == \"ROOT\":\n                ic[pos][0] += value\n            if value != 0:\n                ic[pos][offset] = value\n        return ic\n\n\n\n\n\ndef path_similarity(synset1, synset2, verbose=False, simulate_root=True):\n    return synset1.path_similarity(synset2, verbose, simulate_root)\n\n\ndef lch_similarity(synset1, synset2, verbose=False, simulate_root=True):\n    return synset1.lch_similarity(synset2, verbose, simulate_root)\n\n\ndef wup_similarity(synset1, synset2, verbose=False, simulate_root=True):\n    return synset1.wup_similarity(synset2, verbose, simulate_root)\n\n\ndef res_similarity(synset1, synset2, ic, verbose=False):\n    return synset1.res_similarity(synset2, verbose)\n\n\ndef jcn_similarity(synset1, synset2, ic, verbose=False):\n    return synset1.jcn_similarity(synset2, verbose)\n\n\ndef lin_similarity(synset1, synset2, ic, verbose=False):\n    return synset1.lin_similarity(synset2, verbose)\n\n\npath_similarity.__doc__ = Synset.path_similarity.__doc__\nlch_similarity.__doc__ = Synset.lch_similarity.__doc__\nwup_similarity.__doc__ = Synset.wup_similarity.__doc__\nres_similarity.__doc__ = Synset.res_similarity.__doc__\njcn_similarity.__doc__ = Synset.jcn_similarity.__doc__\nlin_similarity.__doc__ = Synset.lin_similarity.__doc__\n\n\ndef _lcs_ic(synset1, synset2, ic, verbose=False):\n    \"\"\"\n    Get the information content of the least common subsumer that has\n    the highest information content value.  If two nodes have no\n    explicit common subsumer, assume that they share an artificial\n    root node that is the hypernym of all explicit roots.\n\n    :type synset1: Synset\n    :param synset1: First input synset.\n    :type synset2: Synset\n    :param synset2: Second input synset.  Must be the same part of\n    speech as the first synset.\n    :type  ic: dict\n    :param ic: an information content object (as returned by ``load_ic()``).\n    :return: The information content of the two synsets and their most\n    informative subsumer\n    \"\"\"\n    if synset1._pos != synset2._pos:\n        raise WordNetError(\n            'Computing the least common subsumer requires '\n            '%s and %s to have the same part of speech.' %\n            (synset1, synset2)\n        )\n\n    ic1 = information_content(synset1, ic)\n    ic2 = information_content(synset2, ic)\n    subsumers = synset1.common_hypernyms(synset2)\n    if len(subsumers) == 0:\n        subsumer_ic = 0\n    else:\n        subsumer_ic = max(information_content(s, ic) for s in subsumers)\n\n    if verbose:\n        print(\"> LCS Subsumer by content:\", subsumer_ic)\n\n    return ic1, ic2, subsumer_ic\n\n\n\ndef information_content(synset, ic):\n    try:\n        icpos = ic[synset._pos]\n    except KeyError:\n        msg = 'Information content file has no entries for part-of-speech: %s'\n        raise WordNetError(msg % synset._pos)\n\n    counts = icpos[synset._offset]\n    if counts == 0:\n        return _INF\n    else:\n        return -math.log(counts / icpos[0])\n\n\n\ndef _get_pos(field):\n    if field[-1] == 'n':\n        return NOUN\n    elif field[-1] == 'v':\n        return VERB\n    else:\n        msg = (\n            \"Unidentified part of speech in WordNet Information Content file \"\n            \"for field %s\" % field\n        )\n        raise ValueError(msg)\n\n\ndef teardown_module(module=None):\n    from nltk.corpus import wordnet\n    wordnet._unload()\n\n"], "nltk\\corpus\\reader\\xmldocs": [".py", "\nfrom __future__ import print_function, unicode_literals\n\nimport codecs\n\ntry: from xml.etree import cElementTree as ElementTree\nexcept ImportError: from xml.etree import ElementTree\n\nfrom six import string_types\n\nfrom nltk.data import SeekableUnicodeStreamReader\nfrom nltk.tokenize import WordPunctTokenizer\nfrom nltk.internals import ElementWrapper\n\nfrom nltk.corpus.reader.api import CorpusReader\nfrom nltk.corpus.reader.util import *\n\nclass XMLCorpusReader(CorpusReader):\n    def __init__(self, root, fileids, wrap_etree=False):\n        self._wrap_etree = wrap_etree\n        CorpusReader.__init__(self, root, fileids)\n\n    def xml(self, fileid=None):\n        if fileid is None and len(self._fileids) == 1:\n            fileid = self._fileids[0]\n        if not isinstance(fileid, string_types):\n            raise TypeError('Expected a single file identifier string')\n        elt = ElementTree.parse(self.abspath(fileid).open()).getroot()\n        if self._wrap_etree:\n            elt = ElementWrapper(elt)\n        return elt\n\n    def words(self, fileid=None):\n\n        elt = self.xml(fileid)\n        encoding = self.encoding(fileid)\n        word_tokenizer=WordPunctTokenizer()\n        iterator = elt.getiterator()\n        out = []\n\n        for node in iterator:\n            text = node.text\n            if text is not None:\n                if isinstance(text, bytes):\n                    text = text.decode(encoding)\n                toks = word_tokenizer.tokenize(text)\n                out.extend(toks)\n        return out\n\n    def raw(self, fileids=None):\n        if fileids is None: fileids = self._fileids\n        elif isinstance(fileids, string_types): fileids = [fileids]\n        return concat([self.open(f).read() for f in fileids])\n\n\nclass XMLCorpusView(StreamBackedCorpusView):\n\n    _DEBUG = False\n\n    _BLOCK_SIZE = 1024\n\n    def __init__(self, fileid, tagspec, elt_handler=None):\n        if elt_handler: self.handle_elt = elt_handler\n\n        self._tagspec = re.compile(tagspec+r'\\Z')\n\n        self._tag_context = {0: ()}\n        Convert an element into an appropriate value for inclusion in\n        the view.  Unless overridden by a subclass or by the\n        ``elt_handler`` constructor argument, this method simply\n        returns ``elt``.\n\n        :return: The view value corresponding to ``elt``.\n\n        :type elt: ElementTree\n        :param elt: The element that should be converted.\n\n        :type context: str\n        :param context: A string composed of element tags separated by\n            forward slashes, indicating the XML context of the given\n            element.  For example, the string ``'foo/bar/baz'``\n            indicates that the element is a ``baz`` element whose\n            parent is a ``bar`` element and whose grandparent is a\n            top-level ``foo`` element.\n        \"\"\"\n        return elt\n\n    _VALID_XML_RE = re.compile(r\"\"\"\n        [^<]*\n        (\n          ((<!--.*?-->)                         |  # comment\n           (<![CDATA[.*?]])                     |  # raw character data\n           (<!DOCTYPE\\s+[^\\[]*(\\[[^\\]]*])?\\s*>) |  # doctype decl\n           (<[^!>][^>]*>))                         # tag or PI\n          [^<]*)*\n        \\Z\"\"\",\n        re.DOTALL|re.VERBOSE)\n\n    _XML_TAG_NAME = re.compile('<\\s*/?\\s*([^\\s>]+)')\n\n    _XML_PIECE = re.compile(r\"\"\"\n        (?P<COMMENT>        <!--.*?-->                          )|\n        (?P<CDATA>          <![CDATA[.*?]]>                     )|\n        (?P<PI>             <\\?.*?\\?>                           )|\n        (?P<DOCTYPE>        <!DOCTYPE\\s+[^\\[^>]*(\\[[^\\]]*])?\\s*>)|\n        (?P<EMPTY_ELT_TAG>  <\\s*[^>/\\?!\\s][^>]*/\\s*>            )|\n        (?P<START_TAG>      <\\s*[^>/\\?!\\s][^>]*>                )|\n        (?P<END_TAG>        <\\s*/[^>/\\?!\\s][^>]*>               )\"\"\",\n        re.DOTALL|re.VERBOSE)\n\n    def _read_xml_fragment(self, stream):\n        \"\"\"\n        Read a string from the given stream that does not contain any\n        un-closed tags.  In particular, this function first reads a\n        block from the stream of size ``self._BLOCK_SIZE``.  It then\n        checks if that block contains an un-closed tag.  If it does,\n        then this function either backtracks to the last '<', or reads\n        another block.\n        \"\"\"\n        fragment = ''\n\n        if isinstance(stream, SeekableUnicodeStreamReader):\n            startpos = stream.tell()\n        while True:\n            xml_block = stream.read(self._BLOCK_SIZE)\n            fragment += xml_block\n\n            if self._VALID_XML_RE.match(fragment):\n                return fragment\n\n            if re.search('[<>]', fragment).group(0) == '>':\n                pos = stream.tell() - (\n                    len(fragment)-re.search('[<>]', fragment).end())\n                raise ValueError('Unexpected \">\" near char %s' % pos)\n\n            if not xml_block:\n                raise ValueError('Unexpected end of file: tag not closed')\n\n            last_open_bracket = fragment.rfind('<')\n            if last_open_bracket > 0:\n                if self._VALID_XML_RE.match(fragment[:last_open_bracket]):\n                    if isinstance(stream, SeekableUnicodeStreamReader):\n                        stream.seek(startpos)\n                        stream.char_seek_forward(last_open_bracket)\n                    else:\n                        stream.seek(-(len(fragment)-last_open_bracket), 1)\n                    return fragment[:last_open_bracket]\n\n\n    def read_block(self, stream, tagspec=None, elt_handler=None):\n        \"\"\"\n        Read from ``stream`` until we find at least one element that\n        matches ``tagspec``, and return the result of applying\n        ``elt_handler`` to each element found.\n        \"\"\"\n        if tagspec is None: tagspec = self._tagspec\n        if elt_handler is None: elt_handler = self.handle_elt\n\n        context = list(self._tag_context.get(stream.tell()))\n        assert context is not None # check this -- could it ever happen?\n\n        elts = []\n\n        elt_start = None # where does the elt start\n        elt_depth = None # what context depth\n        elt_text = ''\n\n        while elts==[] or elt_start is not None:\n            if isinstance(stream, SeekableUnicodeStreamReader):\n                startpos = stream.tell()\n            xml_fragment = self._read_xml_fragment(stream)\n\n            if not xml_fragment:\n                if elt_start is None: break\n                else: raise ValueError('Unexpected end of file')\n\n            for piece in self._XML_PIECE.finditer(xml_fragment):\n                if self._DEBUG:\n                    print('%25s %s' % ('/'.join(context)[-20:], piece.group()))\n\n                if piece.group('START_TAG'):\n                    name = self._XML_TAG_NAME.match(piece.group()).group(1)\n                    context.append(name)\n                    if elt_start is None:\n                        if re.match(tagspec, '/'.join(context)):\n                            elt_start = piece.start()\n                            elt_depth = len(context)\n\n                elif piece.group('END_TAG'):\n                    name = self._XML_TAG_NAME.match(piece.group()).group(1)\n                    if not context:\n                        raise ValueError('Unmatched tag </%s>' % name)\n                    if name != context[-1]:\n                        raise ValueError('Unmatched tag <%s>...</%s>' %\n                                         (context[-1], name))\n                    if elt_start is not None and elt_depth == len(context):\n                        elt_text += xml_fragment[elt_start:piece.end()]\n                        elts.append( (elt_text, '/'.join(context)) )\n                        elt_start = elt_depth = None\n                        elt_text = ''\n                    context.pop()\n\n                elif piece.group('EMPTY_ELT_TAG'):\n                    name = self._XML_TAG_NAME.match(piece.group()).group(1)\n                    if elt_start is None:\n                        if re.match(tagspec, '/'.join(context)+'/'+name):\n                            elts.append((piece.group(),\n                                         '/'.join(context)+'/'+name))\n\n            if elt_start is not None:\n                if elts == []:\n                    elt_text += xml_fragment[elt_start:]\n                    elt_start = 0\n\n                else:\n                    if self._DEBUG:\n                        print(' '*36+'(backtrack)')\n                    if isinstance(stream, SeekableUnicodeStreamReader):\n                        stream.seek(startpos)\n                        stream.char_seek_forward(elt_start)\n                    else:\n                        stream.seek(-(len(xml_fragment)-elt_start), 1)\n                    context = context[:elt_depth-1]\n                    elt_start = elt_depth = None\n                    elt_text = ''\n\n        pos = stream.tell()\n        if pos in self._tag_context:\n            assert tuple(context) == self._tag_context[pos]\n        else:\n            self._tag_context[pos] = tuple(context)\n\n        return [elt_handler(ElementTree.fromstring(\n                                  elt.encode('ascii', 'xmlcharrefreplace')),\n                            context)\n                for (elt, context) in elts]\n"], "nltk\\corpus\\reader\\ycoe": [".py", "\n\n\nimport os\nimport re\n\nfrom six import string_types\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus.reader.bracket_parse import BracketParseCorpusReader\nfrom nltk.corpus.reader.tagged import TaggedCorpusReader\n\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\n\nclass YCOECorpusReader(CorpusReader):\n    def __init__(self, root, encoding='utf8'):\n        CorpusReader.__init__(self, root, [], encoding)\n\n        self._psd_reader = YCOEParseCorpusReader(\n            self.root.join('psd'), '.*', '.psd', encoding=encoding)\n        self._pos_reader = YCOETaggedCorpusReader(\n            self.root.join('pos'), '.*', '.pos')\n\n        documents = set(f[:-4] for f in self._psd_reader.fileids())\n        if set(f[:-4] for f in self._pos_reader.fileids()) != documents:\n            raise ValueError('Items in \"psd\" and \"pos\" '\n                             'subdirectories do not match.')\n\n        fileids = sorted(['%s.psd' % doc for doc in documents] +\n                       ['%s.pos' % doc for doc in documents])\n        CorpusReader.__init__(self, root, fileids, encoding)\n        self._documents = sorted(documents)\n\n    def documents(self, fileids=None):\n        if fileids is None:\n            return self._documents\n        if isinstance(fileids, string_types):\n            fileids = [fileids]\n        for f in fileids:\n            if f not in self._fileids:\n                raise KeyError('File id %s not found' % fileids)\n        return sorted(set(f[:-4] for f in fileids))\n\n    def fileids(self, documents=None):\n        if documents is None:\n            return self._fileids\n        elif isinstance(documents, string_types):\n            documents = [documents]\n        return sorted(set(['%s.pos' % doc for doc in documents] +\n                          ['%s.psd' % doc for doc in documents]))\n\n    def _getfileids(self, documents, subcorpus):\n        if documents is None:\n            documents = self._documents\n        else:\n            if isinstance(documents, string_types):\n                documents = [documents]\n            for document in documents:\n                if document not in self._documents:\n                    if document[-4:] in ('.pos', '.psd'):\n                        raise ValueError(\n                            'Expected a document identifier, not a file '\n                            'identifier.  (Use corpus.documents() to get '\n                            'a list of document identifiers.')\n                    else:\n                        raise ValueError('Document identifier %s not found'\n                                         % document)\n        return ['%s.%s' % (d, subcorpus) for d in documents]\n\n    def words(self, documents=None):\n        return self._pos_reader.words(self._getfileids(documents, 'pos'))\n    def sents(self, documents=None):\n        return self._pos_reader.sents(self._getfileids(documents, 'pos'))\n    def paras(self, documents=None):\n        return self._pos_reader.paras(self._getfileids(documents, 'pos'))\n    def tagged_words(self, documents=None):\n        return self._pos_reader.tagged_words(self._getfileids(documents, 'pos'))\n    def tagged_sents(self, documents=None):\n        return self._pos_reader.tagged_sents(self._getfileids(documents, 'pos'))\n    def tagged_paras(self, documents=None):\n        return self._pos_reader.tagged_paras(self._getfileids(documents, 'pos'))\n    def parsed_sents(self, documents=None):\n        return self._psd_reader.parsed_sents(self._getfileids(documents, 'psd'))\n\n\nclass YCOEParseCorpusReader(BracketParseCorpusReader):"], "nltk\\corpus\\reader\\__init__": [".py", "\n\nfrom nltk.corpus.reader.plaintext import *\nfrom nltk.corpus.reader.util import *\nfrom nltk.corpus.reader.api import *\nfrom nltk.corpus.reader.tagged import *\nfrom nltk.corpus.reader.cmudict import *\nfrom nltk.corpus.reader.conll import *\nfrom nltk.corpus.reader.chunked import *\nfrom nltk.corpus.reader.wordlist import *\nfrom nltk.corpus.reader.xmldocs import *\nfrom nltk.corpus.reader.ppattach import *\nfrom nltk.corpus.reader.senseval import *\nfrom nltk.corpus.reader.ieer import *\nfrom nltk.corpus.reader.sinica_treebank import *\nfrom nltk.corpus.reader.bracket_parse import *\nfrom nltk.corpus.reader.indian import *\nfrom nltk.corpus.reader.toolbox import *\nfrom nltk.corpus.reader.timit import *\nfrom nltk.corpus.reader.ycoe import *\nfrom nltk.corpus.reader.rte import *\nfrom nltk.corpus.reader.string_category import *\nfrom nltk.corpus.reader.propbank import *\nfrom nltk.corpus.reader.verbnet import *\nfrom nltk.corpus.reader.bnc import *\nfrom nltk.corpus.reader.nps_chat import *\nfrom nltk.corpus.reader.wordnet import *\nfrom nltk.corpus.reader.switchboard import *\nfrom nltk.corpus.reader.dependency import *\nfrom nltk.corpus.reader.nombank import *\nfrom nltk.corpus.reader.ipipan import *\nfrom nltk.corpus.reader.pl196x import *\nfrom nltk.corpus.reader.knbc import *\nfrom nltk.corpus.reader.chasen import *\nfrom nltk.corpus.reader.childes import *\nfrom nltk.corpus.reader.aligned import *\nfrom nltk.corpus.reader.lin import *\nfrom nltk.corpus.reader.semcor import *\nfrom nltk.corpus.reader.framenet import *\nfrom nltk.corpus.reader.udhr import *\nfrom nltk.corpus.reader.bnc import *\nfrom nltk.corpus.reader.sentiwordnet import *\nfrom nltk.corpus.reader.twitter import *\nfrom nltk.corpus.reader.nkjp import *\nfrom nltk.corpus.reader.crubadan import *\nfrom nltk.corpus.reader.mte import *\nfrom nltk.corpus.reader.reviews import *\nfrom nltk.corpus.reader.opinion_lexicon import *\nfrom nltk.corpus.reader.pros_cons import *\nfrom nltk.corpus.reader.categorized_sents import *\nfrom nltk.corpus.reader.comparative_sents import *\nfrom nltk.corpus.reader.panlex_lite import *\n\nfrom nltk.corpus.reader import bracket_parse\n\n__all__ = [\n    'CorpusReader', 'CategorizedCorpusReader',\n    'PlaintextCorpusReader', 'find_corpus_fileids',\n    'TaggedCorpusReader', 'CMUDictCorpusReader',\n    'ConllChunkCorpusReader', 'WordListCorpusReader',\n    'PPAttachmentCorpusReader', 'SensevalCorpusReader',\n    'IEERCorpusReader', 'ChunkedCorpusReader',\n    'SinicaTreebankCorpusReader', 'BracketParseCorpusReader',\n    'IndianCorpusReader', 'ToolboxCorpusReader',\n    'TimitCorpusReader', 'YCOECorpusReader',\n    'MacMorphoCorpusReader', 'SyntaxCorpusReader',\n    'AlpinoCorpusReader', 'RTECorpusReader',\n    'StringCategoryCorpusReader','EuroparlCorpusReader',\n    'CategorizedBracketParseCorpusReader',\n    'CategorizedTaggedCorpusReader',\n    'CategorizedPlaintextCorpusReader',\n    'PortugueseCategorizedPlaintextCorpusReader',\n    'tagged_treebank_para_block_reader',\n    'PropbankCorpusReader', 'VerbnetCorpusReader',\n    'BNCCorpusReader', 'ConllCorpusReader',\n    'XMLCorpusReader', 'NPSChatCorpusReader',\n    'SwadeshCorpusReader', 'WordNetCorpusReader',\n    'WordNetICCorpusReader', 'SwitchboardCorpusReader',\n    'DependencyCorpusReader', 'NombankCorpusReader',\n    'IPIPANCorpusReader', 'Pl196xCorpusReader',\n    'TEICorpusView', 'KNBCorpusReader', 'ChasenCorpusReader',\n    'CHILDESCorpusReader', 'AlignedCorpusReader',\n    'TimitTaggedCorpusReader', 'LinThesaurusCorpusReader',\n    'SemcorCorpusReader', 'FramenetCorpusReader', 'UdhrCorpusReader',\n    'BNCCorpusReader', 'SentiWordNetCorpusReader', 'SentiSynset',\n    'TwitterCorpusReader', 'NKJPCorpusReader', 'CrubadanCorpusReader',\n    'MTECorpusReader', 'ReviewsCorpusReader', 'OpinionLexiconCorpusReader',\n    'ProsConsCorpusReader', 'CategorizedSentencesCorpusReader',\n    'ComparativeSentencesCorpusReader', 'PanLexLiteCorpusReader',\n    'NonbreakingPrefixesCorpusReader', 'UnicharsCorpusReader',\n    'MWAPPDBCorpusReader',\n]\n", 1], "nltk\\corpus\\util": [".py", "\n\nfrom __future__ import unicode_literals\nimport re\nimport gc\nimport nltk\nfrom nltk.compat import python_2_unicode_compatible\n\nTRY_ZIPFILE_FIRST = False\n\n@python_2_unicode_compatible\nclass LazyCorpusLoader(object):\n    def __init__(self, name, reader_cls, *args, **kwargs):\n        from nltk.corpus.reader.api import CorpusReader\n        assert issubclass(reader_cls, CorpusReader)\n        self.__name = self.__name__ = name\n        self.__reader_cls = reader_cls\n        if 'nltk_data_subdir' in kwargs:\n            self.subdir = kwargs['nltk_data_subdir']\n            kwargs.pop('nltk_data_subdir', None)\n        else: # Otherwise use 'nltk_data/corpora'\n            self.subdir = 'corpora'\n        self.__args = args\n        self.__kwargs = kwargs\n\n    def __load(self):\n        zip_name = re.sub(r'(([^/]*)(/.*)?)', r'\\2.zip/\\1/', self.__name)\n        if TRY_ZIPFILE_FIRST:\n            try:\n                root = nltk.data.find('{}/{}'.format(self.subdir, zip_name))\n            except LookupError as e:\n                try: root = nltk.data.find('{}/{}'.format(self.subdir, self.__name))\n                except LookupError: raise e\n        else:\n            try:\n                root = nltk.data.find('{}/{}'.format(self.subdir, self.__name))\n            except LookupError as e:\n                try: root = nltk.data.find('{}/{}'.format(self.subdir, zip_name))\n                except LookupError: raise e\n\n        corpus = self.__reader_cls(root, *self.__args, **self.__kwargs)\n\n\n        args, kwargs  = self.__args, self.__kwargs\n        name, reader_cls = self.__name, self.__reader_cls\n\n        self.__dict__ = corpus.__dict__\n        self.__class__ = corpus.__class__\n\n        def _unload(self):\n            lazy_reader = LazyCorpusLoader(name, reader_cls, *args, **kwargs)\n            self.__dict__ = lazy_reader.__dict__\n            self.__class__ = lazy_reader.__class__\n            gc.collect()\n\n        self._unload = _make_bound_method(_unload, self)\n\n    def __getattr__(self, attr):\n\n        if attr == '__bases__':\n            raise AttributeError(\"LazyCorpusLoader object has no attribute '__bases__'\")\n\n        self.__load()\n        return getattr(self, attr)\n\n    def __repr__(self):\n        return '<%s in %r (not loaded yet)>' % (\n            self.__reader_cls.__name__, '.../corpora/'+self.__name)\n\n    def _unload(self):\n        pass\n\n\ndef _make_bound_method(func, self):\n    class Foo(object):\n        def meth(self): pass\n    f = Foo()\n    bound_method = type(f.meth)\n\n    try:\n        return bound_method(func, self, self.__class__)\n    except TypeError: # python3\n        return bound_method(func, self)\n"], "nltk\\corpus\\__init__": [".py", "\n\nimport re\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus.util import LazyCorpusLoader\nfrom nltk.corpus.reader import *\n\nabc = LazyCorpusLoader(\n    'abc', PlaintextCorpusReader, r'(?!\\.).*\\.txt', encoding=[\n            ('science', 'latin_1'),\n            ('rural', 'utf8')])\nalpino = LazyCorpusLoader(\n    'alpino', AlpinoCorpusReader, tagset='alpino')\nbrown = LazyCorpusLoader(\n    'brown', CategorizedTaggedCorpusReader, r'c[a-z]\\d\\d',\n    cat_file='cats.txt', tagset='brown', encoding=\"ascii\")\ncess_cat = LazyCorpusLoader(\n    'cess_cat', BracketParseCorpusReader, r'(?!\\.).*\\.tbf',\n    tagset='unknown', encoding='ISO-8859-15')\ncess_esp = LazyCorpusLoader(\n    'cess_esp', BracketParseCorpusReader, r'(?!\\.).*\\.tbf',\n    tagset='unknown', encoding='ISO-8859-15')\ncmudict = LazyCorpusLoader(\n    'cmudict', CMUDictCorpusReader, ['cmudict'])\ncomtrans = LazyCorpusLoader(\n    'comtrans', AlignedCorpusReader, r'(?!\\.).*\\.txt')\ncomparative_sentences = LazyCorpusLoader(\n    'comparative_sentences', ComparativeSentencesCorpusReader, r'labeledSentences\\.txt',\n    encoding='latin-1')\nconll2000 = LazyCorpusLoader(\n    'conll2000', ConllChunkCorpusReader,\n    ['train.txt', 'test.txt'], ('NP','VP','PP'),\n    tagset='wsj', encoding='ascii')\nconll2002 = LazyCorpusLoader(\n    'conll2002', ConllChunkCorpusReader, '.*\\.(test|train).*',\n    ('LOC', 'PER', 'ORG', 'MISC'), encoding='utf-8')\nconll2007 = LazyCorpusLoader(\n    'conll2007', DependencyCorpusReader, '.*\\.(test|train).*', encoding=[\n        ('eus', 'ISO-8859-2'),\n        ('esp', 'utf8')])\ncrubadan = LazyCorpusLoader(\n    'crubadan', CrubadanCorpusReader, '.*\\.txt')\ndependency_treebank = LazyCorpusLoader(\n    'dependency_treebank', DependencyCorpusReader, '.*\\.dp',\n    encoding='ascii')\nfloresta = LazyCorpusLoader(\n    'floresta', BracketParseCorpusReader, r'(?!\\.).*\\.ptb', '#',\n    tagset='unknown', encoding='ISO-8859-15')\nframenet15 = LazyCorpusLoader(\n    'framenet_v15', FramenetCorpusReader, ['frRelation.xml','frameIndex.xml','fulltextIndex.xml','luIndex.xml','semTypes.xml'])\nframenet = LazyCorpusLoader(\n    'framenet_v17', FramenetCorpusReader, ['frRelation.xml','frameIndex.xml','fulltextIndex.xml','luIndex.xml','semTypes.xml'])\ngazetteers = LazyCorpusLoader(\n    'gazetteers', WordListCorpusReader, r'(?!LICENSE|\\.).*\\.txt',\n    encoding='ISO-8859-2')\ngenesis = LazyCorpusLoader(\n    'genesis', PlaintextCorpusReader, r'(?!\\.).*\\.txt', encoding=[\n        ('finnish|french|german', 'latin_1'),\n        ('swedish', 'cp865'),\n        ('.*', 'utf_8')])\ngutenberg = LazyCorpusLoader(\n    'gutenberg', PlaintextCorpusReader, r'(?!\\.).*\\.txt', encoding='latin1')\nieer = LazyCorpusLoader(\n    'ieer', IEERCorpusReader, r'(?!README|\\.).*')\ninaugural = LazyCorpusLoader(\n    'inaugural', PlaintextCorpusReader, r'(?!\\.).*\\.txt', encoding='latin1')\nindian = LazyCorpusLoader(\n    'indian', IndianCorpusReader, r'(?!\\.).*\\.pos',\n    tagset='unknown', encoding='utf8')\n\njeita = LazyCorpusLoader(\n    'jeita', ChasenCorpusReader, r'.*\\.chasen', encoding='utf-8')\nknbc = LazyCorpusLoader(\n    'knbc/corpus1', KNBCorpusReader, r'.*/KN.*', encoding='euc-jp')\nlin_thesaurus = LazyCorpusLoader(\n    'lin_thesaurus', LinThesaurusCorpusReader, r'.*\\.lsp')\nmac_morpho = LazyCorpusLoader(\n    'mac_morpho', MacMorphoCorpusReader, r'(?!\\.).*\\.txt',\n    tagset='unknown', encoding='latin-1')\nmachado = LazyCorpusLoader(\n    'machado', PortugueseCategorizedPlaintextCorpusReader,\n    r'(?!\\.).*\\.txt', cat_pattern=r'([a-z]*)/.*', encoding='latin-1')\nmasc_tagged = LazyCorpusLoader(\n    'masc_tagged', CategorizedTaggedCorpusReader, r'(spoken|written)/.*\\.txt',\n    cat_file='categories.txt', tagset='wsj', encoding=\"utf-8\", sep=\"_\")\nmovie_reviews = LazyCorpusLoader(\n    'movie_reviews', CategorizedPlaintextCorpusReader,\n    r'(?!\\.).*\\.txt', cat_pattern=r'(neg|pos)/.*',\n    encoding='ascii')\nmultext_east = LazyCorpusLoader(\n    'mte_teip5', MTECorpusReader, r'(oana).*\\.xml', encoding=\"utf-8\")\nnames = LazyCorpusLoader(\n    'names', WordListCorpusReader, r'(?!\\.).*\\.txt', encoding='ascii')\nnps_chat = LazyCorpusLoader(\n    'nps_chat', NPSChatCorpusReader, r'(?!README|\\.).*\\.xml', tagset='wsj')\nopinion_lexicon = LazyCorpusLoader(\n    'opinion_lexicon', OpinionLexiconCorpusReader, r'(\\w+)\\-words\\.txt',\n    encoding='ISO-8859-2')\nppattach = LazyCorpusLoader(\n    'ppattach', PPAttachmentCorpusReader, ['training', 'test', 'devset'])\nproduct_reviews_1 = LazyCorpusLoader(\n    'product_reviews_1', ReviewsCorpusReader, r'^(?!Readme).*\\.txt', encoding='utf8')\nproduct_reviews_2 = LazyCorpusLoader(\n    'product_reviews_2', ReviewsCorpusReader, r'^(?!Readme).*\\.txt', encoding='utf8')\npros_cons = LazyCorpusLoader(\n    'pros_cons', ProsConsCorpusReader, r'Integrated(Cons|Pros)\\.txt',\n    cat_pattern=r'Integrated(Cons|Pros)\\.txt', encoding='ISO-8859-2')\nptb = LazyCorpusLoader( # Penn Treebank v3: WSJ and Brown portions\n    'ptb', CategorizedBracketParseCorpusReader, r'(WSJ/\\d\\d/WSJ_\\d\\d|BROWN/C[A-Z]/C[A-Z])\\d\\d.MRG',\n    cat_file='allcats.txt', tagset='wsj')\nqc = LazyCorpusLoader(\n    'qc', StringCategoryCorpusReader, ['train.txt', 'test.txt'], encoding='ISO-8859-2')\nreuters = LazyCorpusLoader(\n    'reuters', CategorizedPlaintextCorpusReader, '(training|test).*',\n    cat_file='cats.txt', encoding='ISO-8859-2')\nrte = LazyCorpusLoader(\n    'rte', RTECorpusReader, r'(?!\\.).*\\.xml')\nsenseval = LazyCorpusLoader(\n    'senseval', SensevalCorpusReader, r'(?!\\.).*\\.pos')\nsentence_polarity = LazyCorpusLoader(\n    'sentence_polarity', CategorizedSentencesCorpusReader, r'rt-polarity\\.(neg|pos)',\n    cat_pattern=r'rt-polarity\\.(neg|pos)', encoding='utf-8')\nsentiwordnet = LazyCorpusLoader(\n    'sentiwordnet', SentiWordNetCorpusReader, 'SentiWordNet_3.0.0.txt', encoding='utf-8')\nshakespeare = LazyCorpusLoader(\n    'shakespeare', XMLCorpusReader, r'(?!\\.).*\\.xml')\nsinica_treebank = LazyCorpusLoader(\n    'sinica_treebank', SinicaTreebankCorpusReader, ['parsed'],\n    tagset='unknown', encoding='utf-8')\nstate_union = LazyCorpusLoader(\n    'state_union', PlaintextCorpusReader, r'(?!\\.).*\\.txt',\n    encoding='ISO-8859-2')\nstopwords = LazyCorpusLoader(\n    'stopwords', WordListCorpusReader, r'(?!README|\\.).*', encoding='utf8')\nsubjectivity = LazyCorpusLoader(\n    'subjectivity', CategorizedSentencesCorpusReader, r'(quote.tok.gt9|plot.tok.gt9)\\.5000',\n    cat_map={'quote.tok.gt9.5000':['subj'], 'plot.tok.gt9.5000':['obj']}, encoding='latin-1')\nswadesh = LazyCorpusLoader(\n    'swadesh', SwadeshCorpusReader, r'(?!README|\\.).*', encoding='utf8')\nswadesh110 = LazyCorpusLoader(\n    'panlex_swadesh', SwadeshCorpusReader, r'swadesh110/.*\\.txt', encoding='utf8')\nswadesh207 = LazyCorpusLoader(\n    'panlex_swadesh', SwadeshCorpusReader, r'swadesh207/.*\\.txt', encoding='utf8')\nswitchboard = LazyCorpusLoader(\n    'switchboard', SwitchboardCorpusReader, tagset='wsj')\ntimit = LazyCorpusLoader(\n    'timit', TimitCorpusReader)\ntimit_tagged = LazyCorpusLoader(\n    'timit', TimitTaggedCorpusReader, '.+\\.tags',\n    tagset='wsj', encoding='ascii')\ntoolbox = LazyCorpusLoader(\n    'toolbox', ToolboxCorpusReader, r'(?!.*(README|\\.)).*\\.(dic|txt)')\ntreebank = LazyCorpusLoader(\n    'treebank/combined', BracketParseCorpusReader, r'wsj_.*\\.mrg',\n    tagset='wsj', encoding='ascii')\ntreebank_chunk = LazyCorpusLoader(\n    'treebank/tagged', ChunkedCorpusReader, r'wsj_.*\\.pos',\n    sent_tokenizer=RegexpTokenizer(r'(?<=/\\.)\\s*(?![^\\[]*\\])', gaps=True),\n    para_block_reader=tagged_treebank_para_block_reader, tagset='wsj', encoding='ascii')\ntreebank_raw = LazyCorpusLoader(\n    'treebank/raw', PlaintextCorpusReader, r'wsj_.*', encoding='ISO-8859-2')\ntwitter_samples = LazyCorpusLoader(\n    'twitter_samples', TwitterCorpusReader, '.*\\.json')\nudhr = LazyCorpusLoader(\n    'udhr', UdhrCorpusReader)\nudhr2 = LazyCorpusLoader(\n    'udhr2', PlaintextCorpusReader, r'.*\\.txt', encoding='utf8')\nuniversal_treebanks = LazyCorpusLoader(\n    'universal_treebanks_v20', ConllCorpusReader, r'.*\\.conll',\n    columntypes = ('ignore', 'words', 'ignore', 'ignore', 'pos',\n                   'ignore', 'ignore', 'ignore', 'ignore', 'ignore'))\nverbnet = LazyCorpusLoader(\n    'verbnet', VerbnetCorpusReader, r'(?!\\.).*\\.xml')\nwebtext = LazyCorpusLoader(\n    'webtext', PlaintextCorpusReader, r'(?!README|\\.).*\\.txt', encoding='ISO-8859-2')\nwordnet = LazyCorpusLoader(\n    'wordnet', WordNetCorpusReader,\n    LazyCorpusLoader('omw', CorpusReader, r'.*/wn-data-.*\\.tab', encoding='utf8'))\nwordnet_ic = LazyCorpusLoader(\n    'wordnet_ic', WordNetICCorpusReader, '.*\\.dat')\nwords = LazyCorpusLoader(\n    'words', WordListCorpusReader, r'(?!README|\\.).*', encoding='ascii')\n\npropbank = LazyCorpusLoader(\n    'propbank', PropbankCorpusReader,\n    'prop.txt', 'frames/.*\\.xml', 'verbs.txt',\n    lambda filename: re.sub(r'^wsj/\\d\\d/', '', filename),\n    treebank) # Must be defined *after* treebank corpus.\nnombank = LazyCorpusLoader(\n    'nombank.1.0', NombankCorpusReader,\n    'nombank.1.0', 'frames/.*\\.xml', 'nombank.1.0.words',\n    lambda filename: re.sub(r'^wsj/\\d\\d/', '', filename),\n    treebank) # Must be defined *after* treebank corpus.\npropbank_ptb = LazyCorpusLoader(\n    'propbank', PropbankCorpusReader,\n    'prop.txt', 'frames/.*\\.xml', 'verbs.txt',\n    lambda filename: filename.upper(),\n    ptb) # Must be defined *after* ptb corpus.\nnombank_ptb = LazyCorpusLoader(\n    'nombank.1.0', NombankCorpusReader,\n    'nombank.1.0', 'frames/.*\\.xml', 'nombank.1.0.words',\n    lambda filename: filename.upper(),\n    ptb) # Must be defined *after* ptb corpus.\nsemcor = LazyCorpusLoader(\n    'semcor', SemcorCorpusReader, r'brown./tagfiles/br-.*\\.xml',\n    wordnet) # Must be defined *after* wordnet corpus.\n\nnonbreaking_prefixes = LazyCorpusLoader(\n    'nonbreaking_prefixes', NonbreakingPrefixesCorpusReader, r'(?!README|\\.).*', encoding='utf8')\nperluniprops = LazyCorpusLoader(\n    'perluniprops', UnicharsCorpusReader, r'(?!README|\\.).*', nltk_data_subdir='misc', encoding='utf8')\n\n\n\n\ndef demo():\n    abc.demo()\n    brown.demo()\n    cmudict.demo()\n    conll2000.demo()\n    conll2002.demo()\n    genesis.demo()\n    gutenberg.demo()\n    ieer.demo()\n    inaugural.demo()\n    indian.demo()\n    names.demo()\n    ppattach.demo()\n    senseval.demo()\n    shakespeare.demo()\n    sinica_treebank.demo()\n    state_union.demo()\n    stopwords.demo()\n    timit.demo()\n    toolbox.demo()\n    treebank.demo()\n    udhr.demo()\n    webtext.demo()\n    words.demo()\n\nif __name__ == '__main__':\n    pass\n\ndef teardown_module(module=None):\n    import nltk.corpus\n    for name in dir(nltk.corpus):\n        obj = getattr(nltk.corpus, name, None)\n        if isinstance(obj, CorpusReader) and hasattr(obj, '_unload'):\n            obj._unload()\n", 1], "nltk\\data": [".py", "\nfrom __future__ import print_function, unicode_literals\nfrom __future__ import division\nfrom abc import ABCMeta, abstractmethod\nfrom six import add_metaclass\n\nimport functools\nimport textwrap\nimport io\nimport os\nimport re\nimport sys\nimport zipfile\nimport codecs\n\nfrom gzip import GzipFile, READ as GZ_READ, WRITE as GZ_WRITE\n\ntry: # Python 3.\n    textwrap_indent = functools.partial(textwrap.indent, prefix='  ')\nexcept AttributeError: # Python 2; indent() not available for Python2.\n    textwrap_fill = functools.partial(textwrap.fill,\n                                        initial_indent='  ',\n                                        subsequent_indent='  ',\n                                        replace_whitespace=False)\n    def textwrap_indent(text):\n        return '\\n'.join(textwrap_fill(line) for line in text.splitlines())\n\ntry:\n    from zlib import Z_SYNC_FLUSH as FLUSH\nexcept ImportError:\n    from zlib import Z_FINISH as FLUSH\n\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\n\nfrom six import string_types, text_type\nfrom six.moves.urllib.request import urlopen, url2pathname\n\nimport nltk\nfrom nltk.compat import py3_data, add_py3_data, BytesIO\n\n\npath = []\n    Splits a resource url into \"<protocol>:<path>\".\n\n    >>> windows = sys.platform.startswith('win')\n    >>> split_resource_url('nltk:home/nltk')\n    ('nltk', 'home/nltk')\n    >>> split_resource_url('nltk:/home/nltk')\n    ('nltk', '/home/nltk')\n    >>> split_resource_url('file:/home/nltk')\n    ('file', '/home/nltk')\n    >>> split_resource_url('file:///home/nltk')\n    ('file', '/home/nltk')\n    >>> split_resource_url('file:///C:/home/nltk')\n    ('file', '/C:/home/nltk')\n    \"\"\"\n    protocol, path_ = resource_url.split(':', 1)\n    if protocol == 'nltk':\n        pass\n    elif protocol == 'file':\n        if path_.startswith('/'):\n            path_ = '/' + path_.lstrip('/')\n    else:\n        path_ = re.sub(r'^/{0,2}', '', path_)\n    return protocol, path_\n\n\ndef normalize_resource_url(resource_url):\n    r\"\"\"\n    Normalizes a resource url\n\n    >>> windows = sys.platform.startswith('win')\n    >>> os.path.normpath(split_resource_url(normalize_resource_url('file:grammar.fcfg'))[1]) == \\\n    ... ('\\\\' if windows else '') + os.path.abspath(os.path.join(os.curdir, 'grammar.fcfg'))\n    True\n    >>> not windows or normalize_resource_url('file:C:/dir/file') == 'file:///C:/dir/file'\n    True\n    >>> not windows or normalize_resource_url('file:C:\\\\dir\\\\file') == 'file:///C:/dir/file'\n    True\n    >>> not windows or normalize_resource_url('file:C:\\\\dir/file') == 'file:///C:/dir/file'\n    True\n    >>> not windows or normalize_resource_url('file://C:/dir/file') == 'file:///C:/dir/file'\n    True\n    >>> not windows or normalize_resource_url('file:////C:/dir/file') == 'file:///C:/dir/file'\n    True\n    >>> not windows or normalize_resource_url('nltk:C:/dir/file') == 'file:///C:/dir/file'\n    True\n    >>> not windows or normalize_resource_url('nltk:C:\\\\dir\\\\file') == 'file:///C:/dir/file'\n    True\n    >>> windows or normalize_resource_url('file:/dir/file/toy.cfg') == 'file:///dir/file/toy.cfg'\n    True\n    >>> normalize_resource_url('nltk:home/nltk')\n    'nltk:home/nltk'\n    >>> windows or normalize_resource_url('nltk:/home/nltk') == 'file:///home/nltk'\n    True\n    >>> normalize_resource_url('http://example.com/dir/file')\n    'http://example.com/dir/file'\n    >>> normalize_resource_url('dir/file')\n    'nltk:dir/file'\n    \"\"\"\n    try:\n        protocol, name = split_resource_url(resource_url)\n    except ValueError:\n        protocol = 'nltk'\n        name = resource_url\n    if protocol == 'nltk' and os.path.isabs(name):\n        protocol = 'file://'\n        name = normalize_resource_name(name, False, None)\n    elif protocol == 'file':\n        protocol = 'file://'\n        name = normalize_resource_name(name, False, None)\n    elif protocol == 'nltk':\n        protocol = 'nltk:'\n        name = normalize_resource_name(name, True)\n    else:\n        protocol += '://'\n    return ''.join([protocol, name])\n\n\ndef normalize_resource_name(resource_name, allow_relative=True, relative_path=None):\n    \"\"\"\n    :type resource_name: str or unicode\n    :param resource_name: The name of the resource to search for.\n        Resource names are posix-style relative path names, such as\n        ``corpora/brown``.  Directory names will automatically\n        be converted to a platform-appropriate path separator.\n        Directory trailing slashes are preserved\n\n    >>> windows = sys.platform.startswith('win')\n    >>> normalize_resource_name('.', True)\n    './'\n    >>> normalize_resource_name('./', True)\n    './'\n    >>> windows or normalize_resource_name('dir/file', False, '/') == '/dir/file'\n    True\n    >>> not windows or normalize_resource_name('C:/file', False, '/') == '/C:/file'\n    True\n    >>> windows or normalize_resource_name('/dir/file', False, '/') == '/dir/file'\n    True\n    >>> windows or normalize_resource_name('../dir/file', False, '/') == '/dir/file'\n    True\n    >>> not windows or normalize_resource_name('/dir/file', True, '/') == 'dir/file'\n    True\n    >>> windows or normalize_resource_name('/dir/file', True, '/') == '/dir/file'\n    True\n    \"\"\"\n    is_dir = bool(re.search(r'[\\\\/.]$', resource_name)) or resource_name.endswith(os.path.sep)\n    if sys.platform.startswith('win'):\n        resource_name = resource_name.lstrip('/')\n    else:\n        resource_name = re.sub(r'^/+', '/', resource_name)\n    if allow_relative:\n        resource_name = os.path.normpath(resource_name)\n    else:\n        if relative_path is None:\n            relative_path = os.curdir\n        resource_name = os.path.abspath(\n            os.path.join(relative_path, resource_name))\n    resource_name = resource_name.replace('\\\\', '/').replace(os.path.sep, '/')\n    if sys.platform.startswith('win') and os.path.isabs(resource_name):\n        resource_name = '/' + resource_name\n    if is_dir and not resource_name.endswith('/'):\n        resource_name += '/'\n    return resource_name\n\n\n\n@add_metaclass(ABCMeta)\nclass PathPointer(object):\n    \"\"\"\n    An abstract base class for 'path pointers,' used by NLTK's data\n    package to identify specific paths.  Two subclasses exist:\n    ``FileSystemPathPointer`` identifies a file that can be accessed\n    directly via a given absolute path.  ``ZipFilePathPointer``\n    identifies a file contained within a zipfile, that can be accessed\n    by reading that zipfile.\n    \"\"\"\n\n    @abstractmethod\n    def open(self, encoding=None):\n        \"\"\"\n        Return a seekable read-only stream that can be used to read\n        the contents of the file identified by this path pointer.\n\n        :raise IOError: If the path specified by this pointer does\n            not contain a readable file.\n        \"\"\"\n\n    @abstractmethod\n    def file_size(self):\n        \"\"\"\n        Return the size of the file pointed to by this path pointer,\n        in bytes.\n\n        :raise IOError: If the path specified by this pointer does\n            not contain a readable file.\n        \"\"\"\n\n    @abstractmethod\n    def join(self, fileid):\n        \"\"\"\n        Return a new path pointer formed by starting at the path\n        identified by this pointer, and then following the relative\n        path given by ``fileid``.  The path components of ``fileid``\n        should be separated by forward slashes, regardless of\n        the underlying file system's path seperator character.\n        \"\"\"\n\n\nclass FileSystemPathPointer(PathPointer, text_type):\n    \"\"\"\n    A path pointer that identifies a file which can be accessed\n    directly via a given absolute path.\n    \"\"\"\n    @py3_data\n    def __init__(self, _path):\n        \"\"\"\n        Create a new path pointer for the given absolute path.\n\n        :raise IOError: If the given path does not exist.\n        \"\"\"\n\n        _path = os.path.abspath(_path)\n        if not os.path.exists(_path):\n            raise IOError('No such file or directory: %r' % _path)\n        self._path = _path\n\n\n    @property\n    def path(self):\n        return self._path\n\n    def open(self, encoding=None):\n        stream = open(self._path, 'rb')\n        if encoding is not None:\n            stream = SeekableUnicodeStreamReader(stream, encoding)\n        return stream\n\n    def file_size(self):\n        return os.stat(self._path).st_size\n\n    def join(self, fileid):\n        _path = os.path.join(self._path, fileid)\n        return FileSystemPathPointer(_path)\n\n    def __repr__(self):\n        return str('FileSystemPathPointer(%r)' % self._path)\n\n    def __str__(self):\n        return self._path\n\n\nclass BufferedGzipFile(GzipFile):\n    \"\"\"\n    A ``GzipFile`` subclass that buffers calls to ``read()`` and ``write()``.\n    This allows faster reads and writes of data to and from gzip-compressed\n    files at the cost of using more memory.\n\n    The default buffer size is 2MB.\n\n    ``BufferedGzipFile`` is useful for loading large gzipped pickle objects\n    as well as writing large encoded feature files for classifier training.\n    \"\"\"\n    MB = 2 ** 20\n    SIZE = 2 * MB\n\n    @py3_data\n    def __init__(self, filename=None, mode=None, compresslevel=9,\n                 fileobj=None, **kwargs):\n        \"\"\"\n        Return a buffered gzip file object.\n\n        :param filename: a filesystem path\n        :type filename: str\n        :param mode: a file mode which can be any of 'r', 'rb', 'a', 'ab',\n            'w', or 'wb'\n        :type mode: str\n        :param compresslevel: The compresslevel argument is an integer from 1\n            to 9 controlling the level of compression; 1 is fastest and\n            produces the least compression, and 9 is slowest and produces the\n            most compression. The default is 9.\n        :type compresslevel: int\n        :param fileobj: a BytesIO stream to read from instead of a file.\n        :type fileobj: BytesIO\n        :param size: number of bytes to buffer during calls to read() and write()\n        :type size: int\n        :rtype: BufferedGzipFile\n        \"\"\"\n        GzipFile.__init__(self, filename, mode, compresslevel, fileobj)\n        self._size = kwargs.get('size', self.SIZE)\n        self._nltk_buffer = BytesIO()\n        self._len = 0\n\n    def _reset_buffer(self):\n        self._nltk_buffer = BytesIO()\n        self._len = 0\n\n    def _write_buffer(self, data):\n        if data is not None:\n            self._nltk_buffer.write(data)\n            self._len += len(data)\n\n    def _write_gzip(self, data):\n        GzipFile.write(self, self._nltk_buffer.getvalue())\n        self._reset_buffer()\n        self._write_buffer(data)\n\n    def close(self):\n        if self.mode == GZ_WRITE:\n            self._write_gzip(None)\n            self._reset_buffer()\n        return GzipFile.close(self)\n\n    def flush(self, lib_mode=FLUSH):\n        self._nltk_buffer.flush()\n        GzipFile.flush(self, lib_mode)\n\n    def read(self, size=None):\n        if not size:\n            size = self._size\n            contents = BytesIO()\n            while True:\n                blocks = GzipFile.read(self, size)\n                if not blocks:\n                    contents.flush()\n                    break\n                contents.write(blocks)\n            return contents.getvalue()\n        else:\n            return GzipFile.read(self, size)\n\n    def write(self, data, size=-1):\n        \"\"\"\n        :param data: bytes to write to file or buffer\n        :type data: bytes\n        :param size: buffer at least size bytes before writing to file\n        :type size: int\n        \"\"\"\n        if not size:\n            size = self._size\n        if self._len + len(data) <= size:\n            self._write_buffer(data)\n        else:\n            self._write_gzip(data)\n\n\nclass GzipFileSystemPathPointer(FileSystemPathPointer):\n    \"\"\"\n    A subclass of ``FileSystemPathPointer`` that identifies a gzip-compressed\n    file located at a given absolute path.  ``GzipFileSystemPathPointer`` is\n    appropriate for loading large gzip-compressed pickle objects efficiently.\n    \"\"\"\n\n    def open(self, encoding=None):\n        if sys.version.startswith('2.7') or sys.version.startswith('3.4'):\n            stream = BufferedGzipFile(self._path, 'rb')\n        else:\n            stream = GzipFile(self._path, 'rb')\n        if encoding:\n            stream = SeekableUnicodeStreamReader(stream, encoding)\n        return stream\n\n\nclass ZipFilePathPointer(PathPointer):\n    \"\"\"\n    A path pointer that identifies a file contained within a zipfile,\n    which can be accessed by reading that zipfile.\n    \"\"\"\n    @py3_data\n    def __init__(self, zipfile, entry=''):\n        \"\"\"\n        Create a new path pointer pointing at the specified entry\n        in the given zipfile.\n\n        :raise IOError: If the given zipfile does not exist, or if it\n        does not contain the specified entry.\n        \"\"\"\n        if isinstance(zipfile, string_types):\n            zipfile = OpenOnDemandZipFile(os.path.abspath(zipfile))\n\n        entry = normalize_resource_name(entry, True, '/').lstrip('/')\n\n        if entry != '.':\n            try:\n                zipfile.getinfo(entry)\n            except Exception:\n                if (entry.endswith('/') and\n                        [n for n in zipfile.namelist() if n.startswith(entry)]):\n                    pass  # zipfile contains a file in that directory.\n                else:\n                    raise IOError('Zipfile %r does not contain %r' %\n                                  (zipfile.filename, entry))\n        self._zipfile = zipfile\n        self._entry = entry\n\n    @property\n    def zipfile(self):\n        \"\"\"\n        The zipfile.ZipFile object used to access the zip file\n        containing the entry identified by this path pointer.\n        \"\"\"\n        return self._zipfile\n\n    @property\n    def entry(self):\n        \"\"\"\n        The name of the file within zipfile that this path\n        pointer points to.\n        \"\"\"\n        return self._entry\n\n    def open(self, encoding=None):\n        data = self._zipfile.read(self._entry)\n        stream = BytesIO(data)\n        if self._entry.endswith('.gz'):\n            if sys.version.startswith('2.7') or sys.version.startswith('3.4'):\n                stream = BufferedGzipFile(self._entry, fileobj=stream)\n            else:\n                stream = GzipFile(self._entry, fileobj=stream)\n        elif encoding is not None:\n            stream = SeekableUnicodeStreamReader(stream, encoding)\n        return stream\n\n    def file_size(self):\n        return self._zipfile.getinfo(self._entry).file_size\n\n    def join(self, fileid):\n        entry = '%s/%s' % (self._entry, fileid)\n        return ZipFilePathPointer(self._zipfile, entry)\n\n    def __repr__(self):\n        return str('ZipFilePathPointer(%r, %r)') % (\n            self._zipfile.filename, self._entry)\n\n    def __str__(self):\n        return os.path.normpath(os.path.join(self._zipfile.filename,\n                                             self._entry))\n\n\n\n_resource_cache = {}\n\"\"\"A dictionary used to cache resources so that they won't\n   need to be loaded more than once.\"\"\"\n\n\ndef find(resource_name, paths=None):\n    \"\"\"\n    Find the given resource by searching through the directories and\n    zip files in paths, where a None or empty string specifies an absolute path.\n    Returns a corresponding path name.  If the given resource is not\n    found, raise a ``LookupError``, whose message gives a pointer to\n    the installation instructions for the NLTK downloader.\n\n    Zip File Handling:\n\n      - If ``resource_name`` contains a component with a ``.zip``\n        extension, then it is assumed to be a zipfile; and the\n        remaining path components are used to look inside the zipfile.\n\n      - If any element of ``nltk.data.path`` has a ``.zip`` extension,\n        then it is assumed to be a zipfile.\n\n      - If a given resource name that does not contain any zipfile\n        component is not found initially, then ``find()`` will make a\n        second attempt to find that resource, by replacing each\n        component *p* in the path with *p.zip/p*.  For example, this\n        allows ``find()`` to map the resource name\n        ``corpora/chat80/cities.pl`` to a zip file path pointer to\n        ``corpora/chat80.zip/chat80/cities.pl``.\n\n      - When using ``find()`` to locate a directory contained in a\n        zipfile, the resource name must end with the forward slash\n        character.  Otherwise, ``find()`` will not locate the\n        directory.\n\n    :type resource_name: str or unicode\n    :param resource_name: The name of the resource to search for.\n        Resource names are posix-style relative path names, such as\n        ``corpora/brown``.  Directory names will be\n        automatically converted to a platform-appropriate path separator.\n    :rtype: str\n    \"\"\"\n    resource_name = normalize_resource_name(resource_name, True)\n\n    if paths is None:\n        paths = path\n\n    m = re.match(r'(.*\\.zip)/?(.*)$|', resource_name)\n    zipfile, zipentry = m.groups()\n\n    for path_ in paths:\n        if path_ and (os.path.isfile(path_) and path_.endswith('.zip')):\n            try:\n                return ZipFilePathPointer(path_, resource_name)\n            except IOError:\n                continue\n\n        elif not path_ or os.path.isdir(path_):\n            if zipfile is None:\n                p = os.path.join(path_, url2pathname(resource_name))\n                if os.path.exists(p):\n                    if p.endswith('.gz'):\n                        return GzipFileSystemPathPointer(p)\n                    else:\n                        return FileSystemPathPointer(p)\n            else:\n                p = os.path.join(path_, url2pathname(zipfile))\n                if os.path.exists(p):\n                    try:\n                        return ZipFilePathPointer(p, zipentry)\n                    except IOError:\n                        continue\n\n    if zipfile is None:\n        pieces = resource_name.split('/')\n        for i in range(len(pieces)):\n            modified_name = '/'.join(pieces[:i] +\n                                     [pieces[i] + '.zip'] + pieces[i:])\n            try:\n                return find(modified_name, paths)\n            except LookupError:\n                pass\n\n    resource_zipname = resource_name.split('/')[1]\n    if resource_zipname.endswith('.zip'):\n        resource_zipname = resource_zipname.rpartition('.')[0]\n    msg = str(\"Resource \\33[93m{resource}\\033[0m not found.\\n\"\n              \"Please use the NLTK Downloader to obtain the resource:\\n\\n\"\n              \"\\33[31m\" # To display red text in terminal.\n              \">>> import nltk\\n\"\n              \">>> nltk.download(\\'{resource}\\')\\n\"\n              \"\\033[0m\").format(resource=resource_zipname)\n    msg = textwrap_indent(msg)\n\n    msg += '\\n  Searched in:' + ''.join('\\n    - %r' % d for d in paths)\n    sep = '*' * 70\n    resource_not_found = '\\n%s\\n%s\\n%s\\n' % (sep, msg, sep)\n    raise LookupError(resource_not_found)\n\n\ndef retrieve(resource_url, filename=None, verbose=True):\n    \"\"\"\n    Copy the given resource to a local file.  If no filename is\n    specified, then use the URL's filename.  If there is already a\n    file named ``filename``, then raise a ``ValueError``.\n\n    :type resource_url: str\n    :param resource_url: A URL specifying where the resource should be\n        loaded from.  The default protocol is \"nltk:\", which searches\n        for the file in the the NLTK data package.\n    \"\"\"\n    resource_url = normalize_resource_url(resource_url)\n    if filename is None:\n        if resource_url.startswith('file:'):\n            filename = os.path.split(resource_url)[-1]\n        else:\n            filename = re.sub(r'(^\\w+:)?.*/', '', resource_url)\n    if os.path.exists(filename):\n        filename = os.path.abspath(filename)\n        raise ValueError(\"File %r already exists!\" % filename)\n\n    if verbose:\n        print('Retrieving %r, saving to %r' % (resource_url, filename))\n\n    infile = _open(resource_url)\n\n    with open(filename, \"wb\") as outfile:\n        while True:\n            s = infile.read(1024 * 64)  # 64k blocks.\n            outfile.write(s)\n            if not s:\n                break\n\n    infile.close()\n\n\nFORMATS = {\n    'pickle': \"A serialized python object, stored using the pickle module.\",\n    'json': \"A serialized python object, stored using the json module.\",\n    'yaml': \"A serialized python object, stored using the yaml module.\",\n    'cfg': \"A context free grammar.\",\n    'pcfg': \"A probabilistic CFG.\",\n    'fcfg': \"A feature CFG.\",\n    'fol': \"A list of first order logic expressions, parsed with \"\n            \"nltk.sem.logic.Expression.fromstring.\",\n    'logic': \"A list of first order logic expressions, parsed with \"\n            \"nltk.sem.logic.LogicParser.  Requires an additional logic_parser \"\n            \"parameter\",\n    'val': \"A semantic valuation, parsed by nltk.sem.Valuation.fromstring.\",\n    'raw': \"The raw (byte string) contents of a file.\",\n    'text': \"The raw (unicode string) contents of a file. \"\n}\n\nAUTO_FORMATS = {\n    'pickle': 'pickle',\n    'json': 'json',\n    'yaml': 'yaml',\n    'cfg': 'cfg',\n    'pcfg': 'pcfg',\n    'fcfg': 'fcfg',\n    'fol': 'fol',\n    'logic': 'logic',\n    'val': 'val',\n    'txt': 'text',\n    'text': 'text',\n}\n\n\ndef load(resource_url, format='auto', cache=True, verbose=False,\n         logic_parser=None, fstruct_reader=None, encoding=None):\n    \"\"\"\n    Load a given resource from the NLTK data package.  The following\n    resource formats are currently supported:\n\n      - ``pickle``\n      - ``json``\n      - ``yaml``\n      - ``cfg`` (context free grammars)\n      - ``pcfg`` (probabilistic CFGs)\n      - ``fcfg`` (feature-based CFGs)\n      - ``fol`` (formulas of First Order Logic)\n      - ``logic`` (Logical formulas to be parsed by the given logic_parser)\n      - ``val`` (valuation of First Order Logic model)\n      - ``text`` (the file contents as a unicode string)\n      - ``raw`` (the raw file contents as a byte string)\n\n    If no format is specified, ``load()`` will attempt to determine a\n    format based on the resource name's file extension.  If that\n    fails, ``load()`` will raise a ``ValueError`` exception.\n\n    For all text formats (everything except ``pickle``, ``json``, ``yaml`` and ``raw``),\n    it tries to decode the raw contents using UTF-8, and if that doesn't\n    work, it tries with ISO-8859-1 (Latin-1), unless the ``encoding``\n    is specified.\n\n    :type resource_url: str\n    :param resource_url: A URL specifying where the resource should be\n        loaded from.  The default protocol is \"nltk:\", which searches\n        for the file in the the NLTK data package.\n    :type cache: bool\n    :param cache: If true, add this resource to a cache.  If load()\n        finds a resource in its cache, then it will return it from the\n        cache rather than loading it.  The cache uses weak references,\n        so a resource wil automatically be expunged from the cache\n        when no more objects are using it.\n    :type verbose: bool\n    :param verbose: If true, print a message when loading a resource.\n        Messages are not displayed when a resource is retrieved from\n        the cache.\n    :type logic_parser: LogicParser\n    :param logic_parser: The parser that will be used to parse logical\n        expressions.\n    :type fstruct_reader: FeatStructReader\n    :param fstruct_reader: The parser that will be used to parse the\n        feature structure of an fcfg.\n    :type encoding: str\n    :param encoding: the encoding of the input; only used for text formats.\n    \"\"\"\n    resource_url = normalize_resource_url(resource_url)\n    resource_url = add_py3_data(resource_url)\n\n    if format == 'auto':\n        resource_url_parts = resource_url.split('.')\n        ext = resource_url_parts[-1]\n        if ext == 'gz':\n            ext = resource_url_parts[-2]\n        format = AUTO_FORMATS.get(ext)\n        if format is None:\n            raise ValueError('Could not determine format for %s based '\n                             'on its file\\nextension; use the \"format\" '\n                             'argument to specify the format explicitly.'\n                             % resource_url)\n\n    if format not in FORMATS:\n        raise ValueError('Unknown format type: %s!' % (format,))\n\n    if cache:\n        resource_val = _resource_cache.get((resource_url, format))\n        if resource_val is not None:\n            if verbose:\n                print('<<Using cached copy of %s>>' % (resource_url,))\n            return resource_val\n\n    if verbose:\n        print('<<Loading %s>>' % (resource_url,))\n\n    opened_resource = _open(resource_url)\n\n    if format == 'raw':\n        resource_val = opened_resource.read()\n    elif format == 'pickle':\n        resource_val = pickle.load(opened_resource)\n    elif format == 'json':\n        import json\n        from nltk.jsontags import json_tags\n        resource_val = json.load(opened_resource)\n        tag = None\n        if len(resource_val) != 1:\n            tag = next(resource_val.keys())\n        if tag not in json_tags:\n            raise ValueError('Unknown json tag.')\n    elif format == 'yaml':\n        import yaml\n        resource_val = yaml.load(opened_resource)\n    else:\n        binary_data = opened_resource.read()\n        if encoding is not None:\n            string_data = binary_data.decode(encoding)\n        else:\n            try:\n                string_data = binary_data.decode('utf-8')\n            except UnicodeDecodeError:\n                string_data = binary_data.decode('latin-1')\n        if format == 'text':\n            resource_val = string_data\n        elif format == 'cfg':\n            resource_val = nltk.grammar.CFG.fromstring(\n                string_data, encoding=encoding)\n        elif format == 'pcfg':\n            resource_val = nltk.grammar.PCFG.fromstring(\n                string_data, encoding=encoding)\n        elif format == 'fcfg':\n            resource_val = nltk.grammar.FeatureGrammar.fromstring(\n                string_data, logic_parser=logic_parser,\n                fstruct_reader=fstruct_reader, encoding=encoding)\n        elif format == 'fol':\n            resource_val = nltk.sem.read_logic(\n                string_data, logic_parser=nltk.sem.logic.LogicParser(),\n                encoding=encoding)\n        elif format == 'logic':\n            resource_val = nltk.sem.read_logic(\n                string_data, logic_parser=logic_parser, encoding=encoding)\n        elif format == 'val':\n            resource_val = nltk.sem.read_valuation(\n                string_data, encoding=encoding)\n        else:\n            raise AssertionError(\"Internal NLTK error: Format %s isn't \"\n                                 \"handled by nltk.data.load()\" % (format,))\n\n    opened_resource.close()\n\n    if cache:\n        try:\n            _resource_cache[(resource_url, format)] = resource_val\n        except TypeError:\n            pass\n\n    return resource_val\n\n\ndef show_cfg(resource_url, escape='##'):\n    \"\"\"\n    Write out a grammar file, ignoring escaped and empty lines.\n\n    :type resource_url: str\n    :param resource_url: A URL specifying where the resource should be\n        loaded from.  The default protocol is \"nltk:\", which searches\n        for the file in the the NLTK data package.\n    :type escape: str\n    :param escape: Prepended string that signals lines to be ignored\n    \"\"\"\n    resource_url = normalize_resource_url(resource_url)\n    resource_val = load(resource_url, format='text', cache=False)\n    lines = resource_val.splitlines()\n    for l in lines:\n        if l.startswith(escape):\n            continue\n        if re.match('^$', l):\n            continue\n        print(l)\n\n\ndef clear_cache():\n    \"\"\"\n    Remove all objects from the resource cache.\n    :see: load()\n    \"\"\"\n    _resource_cache.clear()\n\n\ndef _open(resource_url):\n    \"\"\"\n    Helper function that returns an open file object for a resource,\n    given its resource URL.  If the given resource URL uses the \"nltk:\"\n    protocol, or uses no protocol, then use ``nltk.data.find`` to find\n    its path, and open it with the given mode; if the resource URL\n    uses the 'file' protocol, then open the file with the given mode;\n    otherwise, delegate to ``urllib2.urlopen``.\n\n    :type resource_url: str\n    :param resource_url: A URL specifying where the resource should be\n        loaded from.  The default protocol is \"nltk:\", which searches\n        for the file in the the NLTK data package.\n    \"\"\"\n    resource_url = normalize_resource_url(resource_url)\n    protocol, path_ = split_resource_url(resource_url)\n\n    if protocol is None or protocol.lower() == 'nltk':\n        return find(path_, path + ['']).open()\n    elif protocol.lower() == 'file':\n        return find(path_, ['']).open()\n    else:\n        return urlopen(resource_url)\n\n\n\n\nclass LazyLoader(object):\n\n    @py3_data\n    def __init__(self, _path):\n        self._path = _path\n\n    def __load(self):\n        resource = load(self._path)\n        self.__dict__ = resource.__dict__\n        self.__class__ = resource.__class__\n\n    def __getattr__(self, attr):\n        self.__load()\n        return getattr(self, attr)\n\n    def __repr__(self):\n        self.__load()\n        return repr(self)\n\n\n\nclass OpenOnDemandZipFile(zipfile.ZipFile):\n    \"\"\"\n    A subclass of ``zipfile.ZipFile`` that closes its file pointer\n    whenever it is not using it; and re-opens it when it needs to read\n    data from the zipfile.  This is useful for reducing the number of\n    open file handles when many zip files are being accessed at once.\n    ``OpenOnDemandZipFile`` must be constructed from a filename, not a\n    file-like object (to allow re-opening).  ``OpenOnDemandZipFile`` is\n    read-only (i.e. ``write()`` and ``writestr()`` are disabled.\n    \"\"\"\n    @py3_data\n    def __init__(self, filename):\n        if not isinstance(filename, string_types):\n            raise TypeError('ReopenableZipFile filename must be a string')\n        zipfile.ZipFile.__init__(self, filename)\n        assert self.filename == filename\n        self.close()\n        self._fileRefCnt = 0\n\n    def read(self, name):\n        assert self.fp is None\n        self.fp = open(self.filename, 'rb')\n        value = zipfile.ZipFile.read(self, name)\n        self._fileRefCnt += 1\n        self.close()\n        return value\n\n    def write(self, *args, **kwargs):\n        raise NotImplementedError('OpenOnDemandZipfile is read-only')\n\n    def writestr(self, *args, **kwargs):\n        raise NotImplementedError('OpenOnDemandZipfile is read-only')\n\n    def __repr__(self):\n        return repr(str('OpenOnDemandZipFile(%r)') % self.filename)\n\n\n\nclass SeekableUnicodeStreamReader(object):\n    \"\"\"\n    A stream reader that automatically encodes the source byte stream\n    into unicode (like ``codecs.StreamReader``); but still supports the\n    ``seek()`` and ``tell()`` operations correctly.  This is in contrast\n    to ``codecs.StreamReader``, which provide *broken* ``seek()`` and\n    ``tell()`` methods.\n\n    This class was motivated by ``StreamBackedCorpusView``, which\n    makes extensive use of ``seek()`` and ``tell()``, and needs to be\n    able to handle unicode-encoded files.\n\n    Note: this class requires stateless decoders.  To my knowledge,\n    this shouldn't cause a problem with any of python's builtin\n    unicode encodings.\n    \"\"\"\n    DEBUG = True  # : If true, then perform extra sanity checks.\n\n    @py3_data\n    def __init__(self, stream, encoding, errors='strict'):\n        stream.seek(0)\n\n        self.stream = stream\n\n        self.encoding = encoding\n        \"\"\"The name of the encoding that should be used to encode the\n           underlying stream.\"\"\"\n\n        self.errors = errors\n        \"\"\"The error mode that should be used when decoding data from\n           the underlying stream.  Can be 'strict', 'ignore', or\n           'replace'.\"\"\"\n\n        self.decode = codecs.getdecoder(encoding)\n        \"\"\"The function that is used to decode byte strings into\n           unicode strings.\"\"\"\n\n        self.bytebuffer = b''\n        \"\"\"A buffer to use bytes that have been read but have not yet\n           been decoded.  This is only used when the final bytes from\n           a read do not form a complete encoding for a character.\"\"\"\n\n        self.linebuffer = None\n        \"\"\"A buffer used by ``readline()`` to hold characters that have\n           been read, but have not yet been returned by ``read()`` or\n           ``readline()``.  This buffer consists of a list of unicode\n           strings, where each string corresponds to a single line.\n           The final element of the list may or may not be a complete\n           line.  Note that the existence of a linebuffer makes the\n           ``tell()`` operation more complex, because it must backtrack\n           to the beginning of the buffer to determine the correct\n           file position in the underlying byte stream.\"\"\"\n\n        self._rewind_checkpoint = 0\n        \"\"\"The file position at which the most recent read on the\n           underlying stream began.  This is used, together with\n           ``_rewind_numchars``, to backtrack to the beginning of\n           ``linebuffer`` (which is required by ``tell()``).\"\"\"\n\n        self._rewind_numchars = None\n        \"\"\"The number of characters that have been returned since the\n           read that started at ``_rewind_checkpoint``.  This is used,\n           together with ``_rewind_checkpoint``, to backtrack to the\n           beginning of ``linebuffer`` (which is required by ``tell()``).\"\"\"\n\n        self._bom = self._check_bom()\n        \"\"\"The length of the byte order marker at the beginning of\n           the stream (or None for no byte order marker).\"\"\"\n\n\n    def read(self, size=None):\n        \"\"\"\n        Read up to ``size`` bytes, decode them using this reader's\n        encoding, and return the resulting unicode string.\n\n        :param size: The maximum number of bytes to read.  If not\n            specified, then read as many bytes as possible.\n        :type size: int\n        :rtype: unicode\n        \"\"\"\n        chars = self._read(size)\n\n        if self.linebuffer:\n            chars = ''.join(self.linebuffer) + chars\n            self.linebuffer = None\n            self._rewind_numchars = None\n\n        return chars\n\n    def readline(self, size=None):\n        \"\"\"\n        Read a line of text, decode it using this reader's encoding,\n        and return the resulting unicode string.\n\n        :param size: The maximum number of bytes to read.  If no\n            newline is encountered before ``size`` bytes have been read,\n            then the returned value may not be a complete line of text.\n        :type size: int\n        \"\"\"\n        if self.linebuffer and len(self.linebuffer) > 1:\n            line = self.linebuffer.pop(0)\n            self._rewind_numchars += len(line)\n            return line\n\n        readsize = size or 72\n        chars = ''\n\n        if self.linebuffer:\n            chars += self.linebuffer.pop()\n            self.linebuffer = None\n\n        while True:\n            startpos = self.stream.tell() - len(self.bytebuffer)\n            new_chars = self._read(readsize)\n\n            if new_chars and new_chars.endswith('\\r'):\n                new_chars += self._read(1)\n\n            chars += new_chars\n            lines = chars.splitlines(True)\n            if len(lines) > 1:\n                line = lines[0]\n                self.linebuffer = lines[1:]\n                self._rewind_numchars = (len(new_chars) -\n                                         (len(chars) - len(line)))\n                self._rewind_checkpoint = startpos\n                break\n            elif len(lines) == 1:\n                line0withend = lines[0]\n                line0withoutend = lines[0].splitlines(False)[0]\n                if line0withend != line0withoutend:  # complete line\n                    line = line0withend\n                    break\n\n            if not new_chars or size is not None:\n                line = chars\n                break\n\n            if readsize < 8000:\n                readsize *= 2\n\n        return line\n\n    def readlines(self, sizehint=None, keepends=True):\n        \"\"\"\n        Read this file's contents, decode them using this reader's\n        encoding, and return it as a list of unicode lines.\n\n        :rtype: list(unicode)\n        :param sizehint: Ignored.\n        :param keepends: If false, then strip newlines.\n        \"\"\"\n        return self.read().splitlines(keepends)\n\n    def next(self):\n        line = self.readline()\n        if line:\n            return line\n        else:\n            raise StopIteration\n\n    def __next__(self):\n        return self.next()\n\n    def __iter__(self):\n        return self\n\n    def xreadlines(self):\n        return self\n\n\n    @property\n    def closed(self):\n        return self.stream.closed\n\n    @property\n    def name(self):\n        return self.stream.name\n\n    @property\n    def mode(self):\n        return self.stream.mode\n\n    def close(self):\n        \"\"\"\n        Close the underlying stream.\n        \"\"\"\n        self.stream.close()\n\n\n    def seek(self, offset, whence=0):\n        \"\"\"\n        Move the stream to a new file position.  If the reader is\n        maintaining any buffers, then they will be cleared.\n\n        :param offset: A byte count offset.\n        :param whence: If 0, then the offset is from the start of the file\n            (offset should be positive), if 1, then the offset is from the\n            current position (offset may be positive or negative); and if 2,\n            then the offset is from the end of the file (offset should\n            typically be negative).\n        \"\"\"\n        if whence == 1:\n            raise ValueError('Relative seek is not supported for '\n                             'SeekableUnicodeStreamReader -- consider '\n                             'using char_seek_forward() instead.')\n        self.stream.seek(offset, whence)\n        self.linebuffer = None\n        self.bytebuffer = b''\n        self._rewind_numchars = None\n        self._rewind_checkpoint = self.stream.tell()\n\n    def char_seek_forward(self, offset):\n        \"\"\"\n        Move the read pointer forward by ``offset`` characters.\n        \"\"\"\n        if offset < 0:\n            raise ValueError('Negative offsets are not supported')\n        self.seek(self.tell())\n        self._char_seek_forward(offset)\n\n    def _char_seek_forward(self, offset, est_bytes=None):\n        \"\"\"\n        Move the file position forward by ``offset`` characters,\n        ignoring all buffers.\n\n        :param est_bytes: A hint, giving an estimate of the number of\n            bytes that will be needed to move forward by ``offset`` chars.\n            Defaults to ``offset``.\n        \"\"\"\n        if est_bytes is None:\n            est_bytes = offset\n        bytes = b''\n\n        while True:\n            newbytes = self.stream.read(est_bytes - len(bytes))\n            bytes += newbytes\n\n            chars, bytes_decoded = self._incr_decode(bytes)\n\n            if len(chars) == offset:\n                self.stream.seek(-len(bytes) + bytes_decoded, 1)\n                return\n\n            if len(chars) > offset:\n                while len(chars) > offset:\n                    est_bytes += offset - len(chars)\n                    chars, bytes_decoded = self._incr_decode(bytes[:est_bytes])\n                self.stream.seek(-len(bytes) + bytes_decoded, 1)\n                return\n\n            est_bytes += offset - len(chars)\n\n    def tell(self):\n        \"\"\"\n        Return the current file position on the underlying byte\n        stream.  If this reader is maintaining any buffers, then the\n        returned file position will be the position of the beginning\n        of those buffers.\n        \"\"\"\n        if self.linebuffer is None:\n            return self.stream.tell() - len(self.bytebuffer)\n\n\n        orig_filepos = self.stream.tell()\n\n        bytes_read = ((orig_filepos - len(self.bytebuffer)) -\n                      self._rewind_checkpoint)\n        buf_size = sum(len(line) for line in self.linebuffer)\n        est_bytes = int((bytes_read * self._rewind_numchars /\n                         (self._rewind_numchars + buf_size)))\n\n        self.stream.seek(self._rewind_checkpoint)\n        self._char_seek_forward(self._rewind_numchars, est_bytes)\n        filepos = self.stream.tell()\n\n        if self.DEBUG:\n            self.stream.seek(filepos)\n            check1 = self._incr_decode(self.stream.read(50))[0]\n            check2 = ''.join(self.linebuffer)\n            assert check1.startswith(check2) or check2.startswith(check1)\n\n        self.stream.seek(orig_filepos)\n\n        return filepos\n\n\n    def _read(self, size=None):\n        \"\"\"\n        Read up to ``size`` bytes from the underlying stream, decode\n        them using this reader's encoding, and return the resulting\n        unicode string.  ``linebuffer`` is not included in the result.\n        \"\"\"\n        if size == 0:\n            return ''\n\n        if self._bom and self.stream.tell() == 0:\n            self.stream.read(self._bom)\n\n        if size is None:\n            new_bytes = self.stream.read()\n        else:\n            new_bytes = self.stream.read(size)\n        bytes = self.bytebuffer + new_bytes\n\n        chars, bytes_decoded = self._incr_decode(bytes)\n\n        if (size is not None) and (not chars) and (len(new_bytes) > 0):\n            while not chars:\n                new_bytes = self.stream.read(1)\n                if not new_bytes:\n                    break  # end of file.\n                bytes += new_bytes\n                chars, bytes_decoded = self._incr_decode(bytes)\n\n        self.bytebuffer = bytes[bytes_decoded:]\n\n        return chars\n\n    def _incr_decode(self, bytes):\n        \"\"\"\n        Decode the given byte string into a unicode string, using this\n        reader's encoding.  If an exception is encountered that\n        appears to be caused by a truncation error, then just decode\n        the byte string without the bytes that cause the trunctaion\n        error.\n\n        Return a tuple ``(chars, num_consumed)``, where ``chars`` is\n        the decoded unicode string, and ``num_consumed`` is the\n        number of bytes that were consumed.\n        \"\"\"\n        while True:\n            try:\n                return self.decode(bytes, 'strict')\n            except UnicodeDecodeError as exc:\n                if exc.end == len(bytes):\n                    return self.decode(bytes[:exc.start], self.errors)\n\n                elif self.errors == 'strict':\n                    raise\n\n                else:\n                    return self.decode(bytes, self.errors)\n\n    _BOM_TABLE = {\n        'utf8': [(codecs.BOM_UTF8, None)],\n        'utf16': [(codecs.BOM_UTF16_LE, 'utf16-le'),\n                  (codecs.BOM_UTF16_BE, 'utf16-be')],\n        'utf16le': [(codecs.BOM_UTF16_LE, None)],\n        'utf16be': [(codecs.BOM_UTF16_BE, None)],\n        'utf32': [(codecs.BOM_UTF32_LE, 'utf32-le'),\n                  (codecs.BOM_UTF32_BE, 'utf32-be')],\n        'utf32le': [(codecs.BOM_UTF32_LE, None)],\n        'utf32be': [(codecs.BOM_UTF32_BE, None)],\n    }\n\n    def _check_bom(self):\n        enc = re.sub('[ -]', '', self.encoding.lower())\n\n        bom_info = self._BOM_TABLE.get(enc)\n\n        if bom_info:\n            bytes = self.stream.read(16)\n            self.stream.seek(0)\n\n            for (bom, new_encoding) in bom_info:\n                if bytes.startswith(bom):\n                    if new_encoding:\n                        self.encoding = new_encoding\n                    return len(bom)\n\n        return None\n\n\n__all__ = ['path', 'PathPointer', 'FileSystemPathPointer', 'BufferedGzipFile',\n           'GzipFileSystemPathPointer', 'GzipFileSystemPathPointer',\n           'find', 'retrieve', 'FORMATS', 'AUTO_FORMATS', 'load',\n           'show_cfg', 'clear_cache', 'LazyLoader', 'OpenOnDemandZipFile',\n           'GzipFileSystemPathPointer', 'SeekableUnicodeStreamReader']\n"], "nltk\\decorators": [".py", "from __future__ import print_function\n__docformat__ = 'restructuredtext en'\n\n\n__all__ = [\"decorator\", \"new_wrapper\", \"getinfo\"]\n\nimport sys\n\nold_sys_path = sys.path[:]\nsys.path = [p for p in sys.path if \"nltk\" not in p]\nimport inspect\nsys.path = old_sys_path\n\ntry:\n    set\nexcept NameError:\n    from sets import Set as set\n\ndef getinfo(func):\n    assert inspect.ismethod(func) or inspect.isfunction(func)\n    if sys.version_info[0] >= 3:\n        argspec = inspect.getfullargspec(func)\n    else:\n        argspec = inspect.getargspec(func)\n    regargs, varargs, varkwargs, defaults = argspec[:4]\n    argnames = list(regargs)\n    if varargs:\n        argnames.append(varargs)\n    if varkwargs:\n        argnames.append(varkwargs)\n    signature = inspect.formatargspec(regargs, varargs, varkwargs, defaults,\n                                      formatvalue=lambda value: \"\")[1:-1]\n\n    if hasattr(func, '__closure__'):\n        _closure = func.__closure__\n        _globals = func.__globals__\n    else:\n        _closure = func.func_closure\n        _globals = func.func_globals\n\n    return dict(name=func.__name__, argnames=argnames, signature=signature,\n                defaults = func.__defaults__, doc=func.__doc__,\n                module=func.__module__, dict=func.__dict__,\n                globals=_globals, closure=_closure)\n\ndef update_wrapper(wrapper, model, infodict=None):\n    infodict = infodict or getinfo(model)\n    wrapper.__name__ = infodict['name']\n    wrapper.__doc__ = infodict['doc']\n    wrapper.__module__ = infodict['module']\n    wrapper.__dict__.update(infodict['dict'])\n    wrapper.__defaults__ = infodict['defaults']\n    wrapper.undecorated = model\n    return wrapper\n\ndef new_wrapper(wrapper, model):\n    if isinstance(model, dict):\n        infodict = model\n    else: # assume model is a function\n        infodict = getinfo(model)\n    assert not '_wrapper_' in infodict[\"argnames\"], (\n        '\"_wrapper_\" is a reserved argument name!')\n    src = \"lambda %(signature)s: _wrapper_(%(signature)s)\" % infodict\n    funcopy = eval(src, dict(_wrapper_=wrapper))\n    return update_wrapper(funcopy, model, infodict)\n\ndef __call__(self, func):\n    return new_wrapper(lambda *a, **k : self.call(func, *a, **k), func)\n\ndef decorator_factory(cls):\n    attrs = set(dir(cls))\n    if '__call__' in attrs:\n        raise TypeError('You cannot decorate a class with a nontrivial '\n                        '__call__ method')\n    if 'call' not in attrs:\n        raise TypeError('You cannot decorate a class without a '\n                        '.call method')\n    cls.__call__ = __call__\n    return cls\n\ndef decorator(caller):\n    if inspect.isclass(caller):\n        return decorator_factory(caller)\n    def _decorator(func): # the real meat is here\n        infodict = getinfo(func)\n        argnames = infodict['argnames']\n        assert not ('_call_' in argnames or '_func_' in argnames), (\n            'You cannot use _call_ or _func_ as argument names!')\n        src = \"lambda %(signature)s: _call_(_func_, %(signature)s)\" % infodict\n        dec_func = eval(src, dict(_func_=func, _call_=caller))\n        return update_wrapper(dec_func, func, infodict)\n    return update_wrapper(_decorator, caller)\n\ndef getattr_(obj, name, default_thunk):\n    \"Similar to .setdefault in dictionaries.\"\n    try:\n        return getattr(obj, name)\n    except AttributeError:\n        default = default_thunk()\n        setattr(obj, name, default)\n        return default\n\n@decorator\ndef memoize(func, *args):\n    dic = getattr_(func, \"memoize_dic\", dict)\n    if args in dic:\n        return dic[args]\n    else:\n        result = func(*args)\n        dic[args] = result\n        return result\n\n\n\n\n"], "nltk\\downloader": [".py", "\nfrom __future__ import print_function, division, unicode_literals\n\nimport time, os, zipfile, sys, textwrap, threading, itertools, shutil\nfrom hashlib import md5\n\ntry:\n    TKINTER = True\n    from six.moves.tkinter import (Tk, Frame, Label, Entry, Button, Canvas,\n                                   Menu, IntVar, TclError)\n    from six.moves.tkinter_messagebox import showerror\n    from nltk.draw.table import Table\n    from nltk.draw.util import ShowText\nexcept:\n    TKINTER = False\n    TclError = ValueError\n\nfrom xml.etree import ElementTree\n\nfrom six import string_types, text_type\nfrom six.moves import input\nfrom six.moves.urllib.request import urlopen\nfrom six.moves.urllib.error import HTTPError, URLError\n\nimport nltk\nfrom nltk.compat import python_2_unicode_compatible\n\n\n\n@python_2_unicode_compatible\nclass Package(object):\n    def __init__(self, id, url, name=None, subdir='',\n                 size=None, unzipped_size=None,\n                 checksum=None, svn_revision=None,\n                 copyright='Unknown', contact='Unknown',\n                 license='Unknown', author='Unknown',\n                 unzip=True,\n                 **kw):\n        self.id = id\n\n        self.name = name or id\n\n        self.subdir = subdir\n    A directory entry for a collection of downloadable packages.\n    These entries are extracted from the XML index file that is\n    downloaded by ``Downloader``.\n    \"\"\"\n    def __init__(self, id, children, name=None, **kw):\n        self.id = id\n\n        self.name = name or id\n\n        self.children = children\n        \"\"\"A list of the ``Collections`` or ``Packages`` directly\n           contained by this collection.\"\"\"\n\n        self.packages = None\n        \"\"\"A list of ``Packages`` contained by this collection or any\n           collections it recursively contains.\"\"\"\n\n        self.__dict__.update(kw)\n\n    @staticmethod\n    def fromxml(xml):\n        if isinstance(xml, string_types):\n            xml = ElementTree.parse(xml)\n        for key in xml.attrib:\n            xml.attrib[key] = text_type(xml.attrib[key])\n        children = [child.get('ref') for child in xml.findall('item')]\n        return Collection(children=children, **xml.attrib)\n\n    def __lt__(self, other):\n        return self.id < other.id\n\n    def __repr__(self):\n        return '<Collection %s>' % self.id\n\n\nclass DownloaderMessage(object):\n    \"\"\"A status message object, used by ``incr_download`` to\n       communicate its progress.\"\"\"\nclass StartCollectionMessage(DownloaderMessage):\n    def __init__(self, collection): self.collection = collection\nclass FinishCollectionMessage(DownloaderMessage):\n    def __init__(self, collection): self.collection = collection\nclass StartPackageMessage(DownloaderMessage):\n    def __init__(self, package): self.package = package\nclass FinishPackageMessage(DownloaderMessage):\n    def __init__(self, package): self.package = package\nclass StartDownloadMessage(DownloaderMessage):\n    def __init__(self, package): self.package = package\nclass FinishDownloadMessage(DownloaderMessage):\n    def __init__(self, package): self.package = package\nclass StartUnzipMessage(DownloaderMessage):\n    def __init__(self, package): self.package = package\nclass FinishUnzipMessage(DownloaderMessage):\n    def __init__(self, package): self.package = package\nclass UpToDateMessage(DownloaderMessage):\n    def __init__(self, package): self.package = package\nclass StaleMessage(DownloaderMessage):\n    def __init__(self, package): self.package = package\nclass ErrorMessage(DownloaderMessage):\n    def __init__(self, package, message):\n        self.package = package\n        if isinstance(message, Exception):\n            self.message = str(message)\n        else:\n            self.message = message\n\nclass ProgressMessage(DownloaderMessage):\n    def __init__(self, progress): self.progress = progress\nclass SelectDownloadDirMessage(DownloaderMessage):\n    def __init__(self, download_dir): self.download_dir = download_dir\n\n\nclass Downloader(object):\n    \"\"\"\n    A class used to access the NLTK data server, which can be used to\n    download corpora and other data packages.\n    \"\"\"\n\n\n    INDEX_TIMEOUT = 60*60 # 1 hour\n    \"\"\"The amount of time after which the cached copy of the data\n       server index will be considered 'stale,' and will be\n       re-downloaded.\"\"\"\n\n    DEFAULT_URL = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml'\n    \"\"\"The default URL for the NLTK data server's index.  An\n       alternative URL can be specified when creating a new\n       ``Downloader`` object.\"\"\"\n\n\n    INSTALLED = 'installed'\n    \"\"\"A status string indicating that a package or collection is\n       installed and up-to-date.\"\"\"\n    NOT_INSTALLED = 'not installed'\n    \"\"\"A status string indicating that a package or collection is\n       not installed.\"\"\"\n    STALE = 'out of date'\n    \"\"\"A status string indicating that a package or collection is\n       corrupt or out-of-date.\"\"\"\n    PARTIAL = 'partial'\n    \"\"\"A status string indicating that a collection is partially\n       installed (i.e., only some of its packages are installed.)\"\"\"\n\n\n    def __init__(self, server_index_url=None, download_dir=None):\n        self._url = server_index_url or self.DEFAULT_URL\n\n        self._collections = {}\n\n        self._packages = {}\n\n        self._download_dir = download_dir\n\n        self._index = None\n\n        self._index_timestamp = None\n        \"\"\"Time at which ``self._index`` was downloaded.  If it is more\n           than ``INDEX_TIMEOUT`` seconds old, it will be re-downloaded.\"\"\"\n\n        self._status_cache = {}\n        \"\"\"Dictionary from package/collection identifier to status\n           string (``INSTALLED``, ``NOT_INSTALLED``, ``STALE``, or\n           ``PARTIAL``).  Cache is used for packages only, not\n           collections.\"\"\"\n\n        self._errors = None\n\n        if self._download_dir is None:\n            self._download_dir = self.default_download_dir()\n\n\n    def list(self, download_dir=None, show_packages=True,\n             show_collections=True, header=True, more_prompt=False,\n             skip_installed=False):\n        lines = 0 # for more_prompt\n        if download_dir is None:\n            download_dir = self._download_dir\n            print('Using default data directory (%s)' % download_dir)\n        if header:\n            print('='*(26+len(self._url)))\n            print(' Data server index for <%s>' % self._url)\n            print('='*(26+len(self._url)))\n            lines += 3 # for more_prompt\n        stale = partial = False\n\n        categories = []\n        if show_packages: categories.append('packages')\n        if show_collections: categories.append('collections')\n        for category in categories:\n            print('%s:' % category.capitalize())\n            lines += 1 # for more_prompt\n            for info in sorted(getattr(self, category)(), key=str):\n                status = self.status(info, download_dir)\n                if status == self.INSTALLED and skip_installed: continue\n                if status == self.STALE: stale = True\n                if status == self.PARTIAL: partial = True\n                prefix = {self.INSTALLED:'*', self.STALE:'-',\n                          self.PARTIAL:'P', self.NOT_INSTALLED: ' '}[status]\n                name = textwrap.fill('-'*27 + (info.name or info.id),\n                                     75, subsequent_indent=27*' ')[27:]\n                print('  [%s] %s %s' % (prefix, info.id.ljust(20, '.'), name))\n                lines += len(name.split('\\n')) # for more_prompt\n                if more_prompt and lines > 20:\n                    user_input = input(\"Hit Enter to continue: \")\n                    if (user_input.lower() in ('x', 'q')): return\n                    lines = 0\n            print()\n        msg = '([*] marks installed packages'\n        if stale: msg += '; [-] marks out-of-date or corrupt packages'\n        if partial: msg += '; [P] marks partially installed collections'\n        print(textwrap.fill(msg+')', subsequent_indent=' ', width=76))\n\n    def packages(self):\n        self._update_index()\n        return self._packages.values()\n\n    def corpora(self):\n        self._update_index()\n        return [pkg for (id,pkg) in self._packages.items()\n                if pkg.subdir == 'corpora']\n\n    def models(self):\n        self._update_index()\n        return [pkg for (id,pkg) in self._packages.items()\n                if pkg.subdir != 'corpora']\n\n    def collections(self):\n        self._update_index()\n        return self._collections.values()\n\n\n    def _info_or_id(self, info_or_id):\n        if isinstance(info_or_id, string_types):\n            return self.info(info_or_id)\n        else:\n            return info_or_id\n\n\n    def incr_download(self, info_or_id, download_dir=None, force=False):\n        if download_dir is None:\n            download_dir = self._download_dir\n            yield SelectDownloadDirMessage(download_dir)\n\n        if isinstance(info_or_id, (list,tuple)):\n            for msg in self._download_list(info_or_id, download_dir, force):\n                yield msg\n            return\n\n        try: info = self._info_or_id(info_or_id)\n        except (IOError, ValueError) as e:\n            yield ErrorMessage(None, 'Error loading %s: %s' %\n                               (info_or_id, e))\n            return\n\n        if isinstance(info, Collection):\n            yield StartCollectionMessage(info)\n            for msg in self.incr_download(info.children, download_dir, force):\n                yield msg\n            yield FinishCollectionMessage(info)\n\n        else:\n            for msg in self._download_package(info, download_dir, force):\n                yield msg\n\n    def _num_packages(self, item):\n        if isinstance(item, Package): return 1\n        else: return len(item.packages)\n\n    def _download_list(self, items, download_dir, force):\n        for i in range(len(items)):\n            try: items[i] = self._info_or_id(items[i])\n            except (IOError, ValueError) as e:\n                yield ErrorMessage(items[i], e)\n                return\n\n        num_packages = sum(self._num_packages(item) for item in items)\n        progress = 0\n        for i, item in enumerate(items):\n            if isinstance(item, Package):\n                delta = 1./num_packages\n            else:\n                delta = len(item.packages)/num_packages\n            for msg in self.incr_download(item, download_dir, force):\n                if isinstance(msg, ProgressMessage):\n                    yield ProgressMessage(progress + msg.progress*delta)\n                else:\n                    yield msg\n\n            progress += 100*delta\n\n    def _download_package(self, info, download_dir, force):\n        yield StartPackageMessage(info)\n        yield ProgressMessage(0)\n\n        status = self.status(info, download_dir)\n        if not force and status == self.INSTALLED:\n            yield UpToDateMessage(info)\n            yield ProgressMessage(100)\n            yield FinishPackageMessage(info)\n            return\n\n        self._status_cache.pop(info.id, None)\n\n        filepath = os.path.join(download_dir, info.filename)\n        if os.path.exists(filepath):\n            if status == self.STALE:\n                yield StaleMessage(info)\n            os.remove(filepath)\n\n        if not os.path.exists(download_dir):\n            os.mkdir(download_dir)\n        if not os.path.exists(os.path.join(download_dir, info.subdir)):\n            os.mkdir(os.path.join(download_dir, info.subdir))\n\n        yield StartDownloadMessage(info)\n        yield ProgressMessage(5)\n        try:\n            infile = urlopen(info.url)\n            with open(filepath, 'wb') as outfile:\n                num_blocks = max(1, info.size/(1024*16))\n                for block in itertools.count():\n                    s = infile.read(1024*16) # 16k blocks.\n                    outfile.write(s)\n                    if not s: break\n                    if block % 2 == 0: # how often?\n                        yield ProgressMessage(min(80, 5+75*(block/num_blocks)))\n            infile.close()\n        except IOError as e:\n            yield ErrorMessage(info, 'Error downloading %r from <%s>:'\n                               '\\n  %s' % (info.id, info.url, e))\n            return\n        yield FinishDownloadMessage(info)\n        yield ProgressMessage(80)\n\n        if info.filename.endswith('.zip'):\n            zipdir = os.path.join(download_dir, info.subdir)\n            if info.unzip or os.path.exists(os.path.join(zipdir, info.id)):\n                yield StartUnzipMessage(info)\n                for msg in _unzip_iter(filepath, zipdir, verbose=False):\n                    msg.package = info\n                    yield msg\n                yield FinishUnzipMessage(info)\n\n        yield FinishPackageMessage(info)\n\n    def download(self, info_or_id=None, download_dir=None, quiet=False,\n                 force=False, prefix='[nltk_data] ', halt_on_error=True,\n                 raise_on_error=False):\n        if info_or_id is None:\n            if download_dir is not None: self._download_dir = download_dir\n            self._interactive_download()\n            return True\n\n        else:\n            def show(s, prefix2=''):\n                print(textwrap.fill(s, initial_indent=prefix+prefix2,\n                                    subsequent_indent=prefix+prefix2+' '*4))\n\n            for msg in self.incr_download(info_or_id, download_dir, force):\n                if isinstance(msg, ErrorMessage):\n                    show(msg.message)\n                    if raise_on_error:\n                        raise ValueError(msg.message)\n                    if halt_on_error:\n                        return False\n                    self._errors = True\n                    if not quiet:\n                        print(\"Error installing package. Retry? [n/y/e]\")\n                        choice = input().strip()\n                        if choice in ['y', 'Y']:\n                            if not self.download(msg.package.id, download_dir,\n                                                 quiet, force, prefix,\n                                                 halt_on_error, raise_on_error):\n                                return False\n                        elif choice in ['e', 'E']:\n                            return False\n\n                if not quiet:\n                    if isinstance(msg, StartCollectionMessage):\n                        show('Downloading collection %r' % msg.collection.id)\n                        prefix += '   | '\n                        print(prefix)\n                    elif isinstance(msg, FinishCollectionMessage):\n                        print(prefix)\n                        prefix = prefix[:-4]\n                        if self._errors:\n                            show('Downloaded collection %r with errors' %\n                                 msg.collection.id)\n                        else:\n                            show('Done downloading collection %s' %\n                                 msg.collection.id)\n\n                    elif isinstance(msg, StartPackageMessage):\n                        show('Downloading package %s to %s...' %\n                             (msg.package.id, download_dir))\n                    elif isinstance(msg, UpToDateMessage):\n                        show('Package %s is already up-to-date!' %\n                             msg.package.id, '  ')\n                    elif isinstance(msg, StartUnzipMessage):\n                        show('Unzipping %s.' % msg.package.filename, '  ')\n\n                    elif isinstance(msg, SelectDownloadDirMessage):\n                        download_dir = msg.download_dir\n        return True\n\n    def is_stale(self, info_or_id, download_dir=None):\n        return self.status(info_or_id, download_dir) == self.STALE\n\n    def is_installed(self, info_or_id, download_dir=None):\n        return self.status(info_or_id, download_dir) == self.INSTALLED\n\n    def clear_status_cache(self, id=None):\n        if id is None:\n            self._status_cache.clear()\n        else:\n            self._status_cache.pop(id, None)\n\n    def status(self, info_or_id, download_dir=None):\n        \"\"\"\n        Return a constant describing the status of the given package\n        or collection.  Status can be one of ``INSTALLED``,\n        ``NOT_INSTALLED``, ``STALE``, or ``PARTIAL``.\n        \"\"\"\n        if download_dir is None: download_dir = self._download_dir\n        info = self._info_or_id(info_or_id)\n\n        if isinstance(info, Collection):\n            pkg_status = [self.status(pkg.id) for pkg in info.packages]\n            if self.STALE in pkg_status:\n                return self.STALE\n            elif self.PARTIAL in pkg_status:\n                return self.PARTIAL\n            elif (self.INSTALLED in pkg_status and\n                  self.NOT_INSTALLED in pkg_status):\n                return self.PARTIAL\n            elif self.NOT_INSTALLED in pkg_status:\n                return self.NOT_INSTALLED\n            else:\n                return self.INSTALLED\n\n        else:\n            filepath = os.path.join(download_dir, info.filename)\n            if download_dir != self._download_dir:\n                return self._pkg_status(info, filepath)\n            else:\n                if info.id not in self._status_cache:\n                    self._status_cache[info.id] = self._pkg_status(info,\n                                                                   filepath)\n                return self._status_cache[info.id]\n\n    def _pkg_status(self, info, filepath):\n        if not os.path.exists(filepath):\n            return self.NOT_INSTALLED\n\n        try: filestat = os.stat(filepath)\n        except OSError: return self.NOT_INSTALLED\n        if filestat.st_size != int(info.size):\n            return self.STALE\n\n        if md5_hexdigest(filepath) != info.checksum:\n            return self.STALE\n\n        if filepath.endswith('.zip'):\n            unzipdir = filepath[:-4]\n            if not os.path.exists(unzipdir):\n                return self.INSTALLED # but not unzipped -- ok!\n            if not os.path.isdir(unzipdir):\n                return self.STALE\n\n            unzipped_size = sum(os.stat(os.path.join(d, f)).st_size\n                                for d, _, files in os.walk(unzipdir)\n                                for f in files)\n            if unzipped_size != info.unzipped_size:\n                return self.STALE\n\n        return self.INSTALLED\n\n    def update(self, quiet=False, prefix='[nltk_data] '):\n        \"\"\"\n        Re-download any packages whose status is STALE.\n        \"\"\"\n        self.clear_status_cache()\n        for pkg in self.packages():\n            if self.status(pkg) == self.STALE:\n                self.download(pkg, quiet=quiet, prefix=prefix)\n\n\n    def _update_index(self, url=None):\n        \"\"\"A helper function that ensures that self._index is\n        up-to-date.  If the index is older than self.INDEX_TIMEOUT,\n        then download it again.\"\"\"\n        if not (self._index is None or url is not None or\n                time.time()-self._index_timestamp > self.INDEX_TIMEOUT):\n            return\n\n        self._url = url or self._url\n\n        self._index = nltk.internals.ElementWrapper(\n            ElementTree.parse(urlopen(self._url)).getroot())\n        self._index_timestamp = time.time()\n\n        packages = [Package.fromxml(p) for p in\n                    self._index.findall('packages/package')]\n        self._packages = dict((p.id, p) for p in packages)\n\n        collections = [Collection.fromxml(c) for c in\n                       self._index.findall('collections/collection')]\n        self._collections = dict((c.id, c) for c in collections)\n\n        for collection in self._collections.values():\n            for i, child_id in enumerate(collection.children):\n                if child_id in self._packages:\n                    collection.children[i] = self._packages[child_id]\n                elif child_id in self._collections:\n                    collection.children[i] = self._collections[child_id]\n                else:\n                    print('removing collection member with no package: {}'.format(child_id))\n                    del collection.children[i]\n\n        for collection in self._collections.values():\n            packages = {}\n            queue = [collection]\n            for child in queue:\n                if isinstance(child, Collection):\n                    queue.extend(child.children)\n                elif isinstance(child, Package):\n                    packages[child.id] = child\n                else:\n                    pass\n            collection.packages = packages.values()\n\n        self._status_cache.clear()\n\n    def index(self):\n        \"\"\"\n        Return the XML index describing the packages available from\n        the data server.  If necessary, this index will be downloaded\n        from the data server.\n        \"\"\"\n        self._update_index()\n        return self._index\n\n    def info(self, id):\n        \"\"\"Return the ``Package`` or ``Collection`` record for the\n           given item.\"\"\"\n        self._update_index()\n        if id in self._packages: return self._packages[id]\n        if id in self._collections: return self._collections[id]\n        raise ValueError('Package %r not found in index' % id)\n\n    def xmlinfo(self, id):\n        self._update_index()\n        for package in self._index.findall('packages/package'):\n            if package.get('id') == id:\n                return package\n        for collection in self._index.findall('collections/collection'):\n            if collection.get('id') == id:\n                return collection\n        raise ValueError('Package %r not found in index' % id)\n\n\n    def _get_url(self):\n        return self._url\n    def _set_url(self, url):\n        \"\"\"\n        Set a new URL for the data server. If we're unable to contact\n        the given url, then the original url is kept.\n        \"\"\"\n        original_url = self._url\n        try:\n            self._update_index(url)\n        except:\n            self._url = original_url\n            raise\n    url = property(_get_url, _set_url)\n\n    def default_download_dir(self):\n        \"\"\"\n        Return the directory to which packages will be downloaded by\n        default.  This value can be overridden using the constructor,\n        or on a case-by-case basis using the ``download_dir`` argument when\n        calling ``download()``.\n\n        On Windows, the default download directory is\n        ``PYTHONHOME/lib/nltk``, where *PYTHONHOME* is the\n        directory containing Python, e.g. ``C:\\\\Python25``.\n\n        On all other platforms, the default directory is the first of\n        the following which exists or which can be created with write\n        permission: ``/usr/share/nltk_data``, ``/usr/local/share/nltk_data``,\n        ``/usr/lib/nltk_data``, ``/usr/local/lib/nltk_data``, ``~/nltk_data``.\n        \"\"\"\n        if 'APPENGINE_RUNTIME' in os.environ:\n            return\n\n        for nltkdir in nltk.data.path:\n            if (os.path.exists(nltkdir) and\n                nltk.internals.is_writable(nltkdir)):\n                return nltkdir\n\n        if sys.platform == 'win32' and 'APPDATA' in os.environ:\n            homedir = os.environ['APPDATA']\n\n        else:\n            homedir = os.path.expanduser('~/')\n            if homedir == '~/':\n                raise ValueError(\"Could not find a default download directory\")\n\n        return os.path.join(homedir, 'nltk_data')\n\n    def _get_download_dir(self):\n        \"\"\"\n        The default directory to which packages will be downloaded.\n        This defaults to the value returned by ``default_download_dir()``.\n        To override this default on a case-by-case basis, use the\n        ``download_dir`` argument when calling ``download()``.\n        \"\"\"\n        return self._download_dir\n    def _set_download_dir(self, download_dir):\n        self._download_dir = download_dir\n        self._status_cache.clear()\n    download_dir = property(_get_download_dir, _set_download_dir)\n\n\n    def _interactive_download(self):\n        if TKINTER:\n            try:\n                DownloaderGUI(self).mainloop()\n            except TclError:\n                DownloaderShell(self).run()\n        else:\n            DownloaderShell(self).run()\n\nclass DownloaderShell(object):\n    def __init__(self, dataserver):\n        self._ds = dataserver\n\n    def _simple_interactive_menu(self, *options):\n        print('-'*75)\n        spc = (68 - sum(len(o) for o in options))//(len(options)-1)*' '\n        print('    ' + spc.join(options))\n        print('-'*75)\n\n    def run(self):\n        print('NLTK Downloader')\n        while True:\n            self._simple_interactive_menu(\n                'd) Download', 'l) List', ' u) Update', 'c) Config', 'h) Help', 'q) Quit')\n            user_input = input('Downloader> ').strip()\n            if not user_input: print(); continue\n            command = user_input.lower().split()[0]\n            args = user_input.split()[1:]\n            try:\n                if command == 'l':\n                    print()\n                    self._ds.list(self._ds.download_dir, header=False,\n                                  more_prompt=True)\n                elif command == 'h':\n                    self._simple_interactive_help()\n                elif command == 'c':\n                    self._simple_interactive_config()\n                elif command in ('q', 'x'):\n                    return\n                elif command == 'd':\n                    self._simple_interactive_download(args)\n                elif command == 'u':\n                    self._simple_interactive_update()\n                else:\n                    print('Command %r unrecognized' % user_input)\n            except HTTPError as e:\n                print('Error reading from server: %s'%e)\n            except URLError as e:\n                print('Error connecting to server: %s'%e.reason)\n            print()\n\n    def _simple_interactive_download(self, args):\n        if args:\n            for arg in args:\n                try: self._ds.download(arg, prefix='    ')\n                except (IOError, ValueError) as e: print(e)\n        else:\n            while True:\n                print()\n                print('Download which package (l=list; x=cancel)?')\n                user_input = input('  Identifier> ')\n                if user_input.lower()=='l':\n                    self._ds.list(self._ds.download_dir, header=False,\n                                  more_prompt=True, skip_installed=True)\n                    continue\n                elif user_input.lower() in ('x', 'q', ''):\n                    return\n                elif user_input:\n                    for id in user_input.split():\n                        try: self._ds.download(id, prefix='    ')\n                        except (IOError, ValueError) as e: print(e)\n                    break\n\n    def _simple_interactive_update(self):\n        while True:\n            stale_packages = []\n            stale = partial = False\n            for info in sorted(getattr(self._ds, 'packages')(), key=str):\n                if self._ds.status(info) == self._ds.STALE:\n                    stale_packages.append((info.id, info.name))\n\n            print()\n            if stale_packages:\n                print('Will update following packages (o=ok; x=cancel)')\n                for pid, pname in stale_packages:\n                    name = textwrap.fill('-'*27 + (pname),\n                                     75, subsequent_indent=27*' ')[27:]\n                    print('  [ ] %s %s' % (pid.ljust(20, '.'), name))\n                print()\n\n                user_input = input('  Identifier> ')\n                if user_input.lower()=='o':\n                    for pid, pname in stale_packages:\n                        try: self._ds.download(pid, prefix='    ')\n                        except (IOError, ValueError) as e: print(e)\n                    break\n                elif user_input.lower() in ('x', 'q', ''):\n                    return\n            else:\n                print('Nothing to update.')\n                return\n\n    def _simple_interactive_help(self):\n        print()\n        print('Commands:')\n        print('  d) Download a package or collection     u) Update out of date packages')\n        print('  l) List packages & collections          h) Help')\n        print('  c) View & Modify Configuration          q) Quit')\n\n    def _show_config(self):\n        print()\n        print('Data Server:')\n        print('  - URL: <%s>' % self._ds.url)\n        print(('  - %d Package Collections Available' %\n               len(self._ds.collections())))\n        print(('  - %d Individual Packages Available' %\n               len(self._ds.packages())))\n        print()\n        print('Local Machine:')\n        print('  - Data directory: %s' % self._ds.download_dir)\n\n    def _simple_interactive_config(self):\n        self._show_config()\n        while True:\n            print()\n            self._simple_interactive_menu(\n                's) Show Config', 'u) Set Server URL',\n                'd) Set Data Dir', 'm) Main Menu')\n            user_input = input('Config> ').strip().lower()\n            if user_input == 's':\n                self._show_config()\n            elif user_input == 'd':\n                new_dl_dir = input('  New Directory> ').strip()\n                if new_dl_dir in ('', 'x', 'q', 'X', 'Q'):\n                    print('  Cancelled!')\n                elif os.path.isdir(new_dl_dir):\n                    self._ds.download_dir = new_dl_dir\n                else:\n                    print(('Directory %r not found!  Create it first.' %\n                           new_dl_dir))\n            elif user_input == 'u':\n                new_url = input('  New URL> ').strip()\n                if new_url in ('', 'x', 'q', 'X', 'Q'):\n                    print('  Cancelled!')\n                else:\n                    if not new_url.startswith(('http://', 'https://')):\n                        new_url = 'http://'+new_url\n                    try: self._ds.url = new_url\n                    except Exception as e:\n                        print('Error reading <%r>:\\n  %s' % (new_url, e))\n            elif user_input == 'm':\n                break\n\nclass DownloaderGUI(object):\n    \"\"\"\n    Graphical interface for downloading packages from the NLTK data\n    server.\n    \"\"\"\n\n\n    COLUMNS = ['', 'Identifier', 'Name', 'Size', 'Status',\n               'Unzipped Size',\n               'Copyright', 'Contact', 'License', 'Author',\n               'Subdir', 'Checksum']\n    \"\"\"A list of the names of columns.  This controls the order in\n       which the columns will appear.  If this is edited, then\n       ``_package_to_columns()`` may need to be edited to match.\"\"\"\n\n    COLUMN_WEIGHTS = {'': 0, 'Name': 5, 'Size': 0, 'Status': 0}\n    \"\"\"A dictionary specifying how columns should be resized when the\n       table is resized.  Columns with weight 0 will not be resized at\n       all; and columns with high weight will be resized more.\n       Default weight (for columns not explicitly listed) is 1.\"\"\"\n\n    COLUMN_WIDTHS = {'':1, 'Identifier':20, 'Name':45,\n                     'Size': 10, 'Unzipped Size': 10,\n                     'Status': 12}\n    \"\"\"A dictionary specifying how wide each column should be, in\n       characters.  The default width (for columns not explicitly\n       listed) is specified by ``DEFAULT_COLUMN_WIDTH``.\"\"\"\n\n    DEFAULT_COLUMN_WIDTH = 30\n    \"\"\"The default width for columns that are not explicitly listed\n       in ``COLUMN_WIDTHS``.\"\"\"\n\n    INITIAL_COLUMNS = ['', 'Identifier', 'Name', 'Size', 'Status']\n\n    for c in COLUMN_WEIGHTS: assert c in COLUMNS\n    for c in COLUMN_WIDTHS: assert c in COLUMNS\n    for c in INITIAL_COLUMNS: assert c in COLUMNS\n\n\n    _BACKDROP_COLOR = ('#000', '#ccc')\n\n    _ROW_COLOR = {Downloader.INSTALLED: ('#afa', '#080'),\n                  Downloader.PARTIAL: ('#ffa', '#880'),\n                  Downloader.STALE: ('#faa', '#800'),\n                  Downloader.NOT_INSTALLED: ('#fff', '#888')}\n\n    _MARK_COLOR = ('#000', '#ccc')\n\n    _FRONT_TAB_COLOR = ('#fff', '#45c')\n    _BACK_TAB_COLOR = ('#aaa', '#67a')\n\n    _PROGRESS_COLOR = ('#f00', '#aaa')\n\n    _TAB_FONT = 'helvetica -16 bold'\n\n\n    def __init__(self, dataserver, use_threads=True):\n        self._ds = dataserver\n        self._use_threads = use_threads\n\n        self._download_lock = threading.Lock()\n        self._download_msg_queue = []\n        self._download_abort_queue = []\n        self._downloading = False\n\n        self._afterid = {}\n\n        self._log_messages = []\n        self._log_indent = 0\n        self._log('NLTK Downloader Started!')\n\n        top = self.top = Tk()\n        top.geometry('+50+50')\n        top.title('NLTK Downloader')\n        top.configure(background=self._BACKDROP_COLOR[1])\n\n        top.bind('<Control-q>', self.destroy)\n        top.bind('<Control-x>', self.destroy)\n        self._destroyed = False\n\n        self._column_vars = {}\n\n        self._init_widgets()\n        self._init_menu()\n        try:\n            self._fill_table()\n        except HTTPError as e:\n            showerror('Error reading from server', e)\n        except URLError as e:\n            showerror('Error connecting to server', e.reason)\n\n        self._show_info()\n        self._select_columns()\n        self._table.select(0)\n\n        self._table.bind('<Destroy>', self._destroy)\n\n    def _log(self, msg):\n        self._log_messages.append('%s %s%s' % (time.ctime(),\n                                     ' | '*self._log_indent, msg))\n\n\n    def _init_widgets(self):\n        f1 = Frame(self.top, relief='raised', border=2, padx=8, pady=0)\n        f1.pack(sid='top', expand=True, fill='both')\n        f1.grid_rowconfigure(2, weight=1)\n        f1.grid_columnconfigure(0, weight=1)\n        Frame(f1, height=8).grid(column=0, row=0) # spacer\n        tabframe = Frame(f1)\n        tabframe.grid(column=0, row=1, sticky='news')\n        tableframe = Frame(f1)\n        tableframe.grid(column=0, row=2, sticky='news')\n        buttonframe = Frame(f1)\n        buttonframe.grid(column=0, row=3, sticky='news')\n        Frame(f1, height=8).grid(column=0, row=4) # spacer\n        infoframe = Frame(f1)\n        infoframe.grid(column=0, row=5, sticky='news')\n        Frame(f1, height=8).grid(column=0, row=6) # spacer\n        progressframe = Frame(self.top, padx=3, pady=3,\n                              background=self._BACKDROP_COLOR[1])\n        progressframe.pack(side='bottom', fill='x')\n        self.top['border'] = 0\n        self.top['highlightthickness'] = 0\n\n        self._tab_names = ['Collections', 'Corpora',\n                           'Models', 'All Packages',]\n        self._tabs = {}\n        for i, tab in enumerate(self._tab_names):\n            label = Label(tabframe, text=tab, font=self._TAB_FONT)\n            label.pack(side='left', padx=((i+1)%2)*10)\n            label.bind('<Button-1>', self._select_tab)\n            self._tabs[tab.lower()] = label\n\n        column_weights = [self.COLUMN_WEIGHTS.get(column, 1)\n                          for column in self.COLUMNS]\n        self._table = Table(tableframe, self.COLUMNS,\n                            column_weights=column_weights,\n                            highlightthickness=0, listbox_height=16,\n                            reprfunc=self._table_reprfunc)\n        self._table.columnconfig(0, foreground=self._MARK_COLOR[0]) # marked\n        for i, column in enumerate(self.COLUMNS):\n            width = self.COLUMN_WIDTHS.get(column, self.DEFAULT_COLUMN_WIDTH)\n            self._table.columnconfig(i, width=width)\n        self._table.pack(expand=True, fill='both')\n        self._table.focus()\n        self._table.bind_to_listboxes('<Double-Button-1>',\n                                      self._download)\n        self._table.bind('<space>', self._table_mark)\n        self._table.bind('<Return>', self._download)\n        self._table.bind('<Left>', self._prev_tab)\n        self._table.bind('<Right>', self._next_tab)\n        self._table.bind('<Control-a>', self._mark_all)\n\n        infoframe.grid_columnconfigure(1, weight=1)\n\n        info = [('url', 'Server Index:', self._set_url),\n                ('download_dir','Download Directory:',self._set_download_dir)]\n        self._info = {}\n        for (i, (key, label, callback)) in enumerate(info):\n            Label(infoframe, text=label).grid(column=0, row=i, sticky='e')\n            entry = Entry(infoframe, font='courier', relief='groove',\n                          disabledforeground='black')\n            self._info[key] = (entry, callback)\n            entry.bind('<Return>', self._info_save)\n            entry.bind('<Button-1>', lambda e,key=key: self._info_edit(key))\n            entry.grid(column=1, row=i, sticky='ew')\n\n        self.top.bind('<Button-1>', self._info_save)\n\n        self._download_button = Button(\n            buttonframe, text='Download', command=self._download, width=8)\n        self._download_button.pack(side='left')\n        self._refresh_button = Button(\n            buttonframe, text='Refresh', command=self._refresh, width=8)\n        self._refresh_button.pack(side='right')\n\n        self._progresslabel = Label(progressframe, text='',\n                                    foreground=self._BACKDROP_COLOR[0],\n                                    background=self._BACKDROP_COLOR[1])\n        self._progressbar = Canvas(progressframe, width=200, height=16,\n                                   background=self._PROGRESS_COLOR[1],\n                                   relief='sunken', border=1)\n        self._init_progressbar()\n        self._progressbar.pack(side='right')\n        self._progresslabel.pack(side='left')\n\n    def _init_menu(self):\n        menubar = Menu(self.top)\n\n        filemenu = Menu(menubar, tearoff=0)\n        filemenu.add_command(label='Download', underline=0,\n                             command=self._download, accelerator='Return')\n        filemenu.add_separator()\n        filemenu.add_command(label='Change Server Index', underline=7,\n                             command=lambda: self._info_edit('url'))\n        filemenu.add_command(label='Change Download Directory', underline=0,\n                             command=lambda: self._info_edit('download_dir'))\n        filemenu.add_separator()\n        filemenu.add_command(label='Show Log', underline=5,\n                             command=self._show_log)\n        filemenu.add_separator()\n        filemenu.add_command(label='Exit', underline=1,\n                             command=self.destroy, accelerator='Ctrl-x')\n        menubar.add_cascade(label='File', underline=0, menu=filemenu)\n\n        viewmenu = Menu(menubar, tearoff=0)\n        for column in self._table.column_names[2:]:\n            var = IntVar(self.top)\n            assert column not in self._column_vars\n            self._column_vars[column] = var\n            if column in self.INITIAL_COLUMNS: var.set(1)\n            viewmenu.add_checkbutton(label=column, underline=0, variable=var,\n                                     command=self._select_columns)\n        menubar.add_cascade(label='View', underline=0, menu=viewmenu)\n\n        sortmenu = Menu(menubar, tearoff=0)\n        for column in self._table.column_names[1:]:\n            sortmenu.add_command(label='Sort by %s' % column,\n                      command=(lambda c=column:\n                               self._table.sort_by(c, 'ascending')))\n        sortmenu.add_separator()\n        for column in self._table.column_names[1:]:\n            sortmenu.add_command(label='Reverse sort by %s' % column,\n                      command=(lambda c=column:\n                               self._table.sort_by(c, 'descending')))\n        menubar.add_cascade(label='Sort', underline=0, menu=sortmenu)\n\n        helpmenu = Menu(menubar, tearoff=0)\n        helpmenu.add_command(label='About', underline=0,\n                             command=self.about)\n        helpmenu.add_command(label='Instructions', underline=0,\n                             command=self.help, accelerator='F1')\n        menubar.add_cascade(label='Help', underline=0, menu=helpmenu)\n        self.top.bind('<F1>', self.help)\n\n        self.top.config(menu=menubar)\n\n    def _select_columns(self):\n        for (column, var) in self._column_vars.items():\n            if var.get():\n                self._table.show_column(column)\n            else:\n                self._table.hide_column(column)\n\n    def _refresh(self):\n        self._ds.clear_status_cache()\n        try:\n            self._fill_table()\n        except HTTPError as e:\n            showerror('Error reading from server', e)\n        except URLError as e:\n            showerror('Error connecting to server', e.reason)\n        self._table.select(0)\n\n    def _info_edit(self, info_key):\n        self._info_save() # just in case.\n        (entry, callback) = self._info[info_key]\n        entry['state'] = 'normal'\n        entry['relief'] = 'sunken'\n        entry.focus()\n\n    def _info_save(self, e=None):\n        focus = self._table\n        for entry, callback in self._info.values():\n            if entry['state'] == 'disabled': continue\n            if e is not None and e.widget is entry and e.keysym != 'Return':\n                focus = entry\n            else:\n                entry['state'] = 'disabled'\n                entry['relief'] = 'groove'\n                callback(entry.get())\n        focus.focus()\n\n    def _table_reprfunc(self, row, col, val):\n        if self._table.column_names[col].endswith('Size'):\n            if isinstance(val, string_types): return '  %s' % val\n            elif val < 1024**2: return '  %.1f KB' % (val/1024.**1)\n            elif val < 1024**3: return '  %.1f MB' % (val/1024.**2)\n            else: return '  %.1f GB' % (val/1024.**3)\n\n        if col in (0, ''): return str(val)\n        else: return '  %s' % val\n\n    def _set_url(self, url):\n        if url == self._ds.url: return\n        try:\n            self._ds.url = url\n            self._fill_table()\n        except IOError as e:\n            showerror('Error Setting Server Index', str(e))\n        self._show_info()\n\n\n    def _set_download_dir(self, download_dir):\n        if self._ds.download_dir == download_dir: return\n\n        self._ds.download_dir = download_dir\n        try:\n            self._fill_table()\n        except HTTPError as e:\n            showerror('Error reading from server', e)\n        except URLError as e:\n            showerror('Error connecting to server', e.reason)\n        self._show_info()\n\n    def _show_info(self):\n        print('showing info', self._ds.url)\n        for entry,cb in self._info.values():\n            entry['state'] = 'normal'\n            entry.delete(0, 'end')\n        self._info['url'][0].insert(0, self._ds.url)\n        self._info['download_dir'][0].insert(0, self._ds.download_dir)\n        for entry,cb in self._info.values():\n            entry['state'] = 'disabled'\n\n    def _prev_tab(self, *e):\n        for i, tab in enumerate(self._tab_names):\n            if tab.lower() == self._tab and i > 0:\n                self._tab = self._tab_names[i-1].lower()\n                try:\n                    return self._fill_table()\n                except HTTPError as e:\n                    showerror('Error reading from server', e)\n                except URLError as e:\n                    showerror('Error connecting to server', e.reason)\n\n    def _next_tab(self, *e):\n        for i, tab in enumerate(self._tab_names):\n            if tab.lower() == self._tab and i < (len(self._tabs)-1):\n                self._tab = self._tab_names[i+1].lower()\n                try:\n                    return self._fill_table()\n                except HTTPError as e:\n                    showerror('Error reading from server', e)\n                except URLError as e:\n                    showerror('Error connecting to server', e.reason)\n\n    def _select_tab(self, event):\n        self._tab = event.widget['text'].lower()\n        try:\n            self._fill_table()\n        except HTTPError as e:\n            showerror('Error reading from server', e)\n        except URLError as e:\n            showerror('Error connecting to server', e.reason)\n\n    _tab = 'collections'\n    _rows = None\n    def _fill_table(self):\n        selected_row = self._table.selected_row()\n        self._table.clear()\n        if self._tab == 'all packages':\n            items = self._ds.packages()\n        elif self._tab == 'corpora':\n            items = self._ds.corpora()\n        elif self._tab == 'models':\n            items = self._ds.models()\n        elif self._tab == 'collections':\n            items = self._ds.collections()\n        else:\n            assert 0, 'bad tab value %r' % self._tab\n        rows = [self._package_to_columns(item) for item in items]\n        self._table.extend(rows)\n\n        for tab, label in self._tabs.items():\n            if tab == self._tab:\n                label.configure(foreground=self._FRONT_TAB_COLOR[0],\n                                background=self._FRONT_TAB_COLOR[1])\n            else:\n                label.configure(foreground=self._BACK_TAB_COLOR[0],\n                                background=self._BACK_TAB_COLOR[1])\n\n        self._table.sort_by('Identifier', order='ascending')\n        self._color_table()\n        self._table.select(selected_row)\n\n        self.top.after(150, self._table._scrollbar.set,\n                       *self._table._mlb.yview())\n        self.top.after(300, self._table._scrollbar.set,\n                       *self._table._mlb.yview())\n\n    def _update_table_status(self):\n        for row_num in range(len(self._table)):\n            status = self._ds.status(self._table[row_num, 'Identifier'])\n            self._table[row_num, 'Status'] = status\n        self._color_table()\n\n    def _download(self, *e):\n        if self._use_threads:\n            return self._download_threaded(*e)\n\n        marked = [self._table[row, 'Identifier']\n                  for row in range(len(self._table))\n                  if self._table[row, 0] != '']\n        selection = self._table.selected_row()\n        if not marked and selection is not None:\n            marked = [self._table[selection, 'Identifier']]\n\n        download_iter = self._ds.incr_download(marked, self._ds.download_dir)\n        self._log_indent = 0\n        self._download_cb(download_iter, marked)\n\n    _DL_DELAY=10\n    def _download_cb(self, download_iter, ids):\n        try: msg = next(download_iter)\n        except StopIteration:\n            self._update_table_status()\n            afterid = self.top.after(10, self._show_progress, 0)\n            self._afterid['_download_cb'] = afterid\n            return\n\n        def show(s):\n            self._progresslabel['text'] = s\n            self._log(s)\n        if isinstance(msg, ProgressMessage):\n            self._show_progress(msg.progress)\n        elif isinstance(msg, ErrorMessage):\n            show(msg.message)\n            if msg.package is not None:\n                self._select(msg.package.id)\n            self._show_progress(None)\n            return # halt progress.\n        elif isinstance(msg, StartCollectionMessage):\n            show('Downloading collection %s' % msg.collection.id)\n            self._log_indent += 1\n        elif isinstance(msg, StartPackageMessage):\n            show('Downloading package %s' % msg.package.id)\n        elif isinstance(msg, UpToDateMessage):\n            show('Package %s is up-to-date!' % msg.package.id)\n        elif isinstance(msg, FinishDownloadMessage):\n            show('Finished downloading %r.' % msg.package.id)\n        elif isinstance(msg, StartUnzipMessage):\n            show('Unzipping %s' % msg.package.filename)\n        elif isinstance(msg, FinishCollectionMessage):\n            self._log_indent -= 1\n            show('Finished downloading collection %r.' % msg.collection.id)\n            self._clear_mark(msg.collection.id)\n        elif isinstance(msg, FinishPackageMessage):\n            self._clear_mark(msg.package.id)\n        afterid = self.top.after(self._DL_DELAY, self._download_cb,\n                                 download_iter, ids)\n        self._afterid['_download_cb'] = afterid\n\n    def _select(self, id):\n        for row in range(len(self._table)):\n            if self._table[row, 'Identifier'] == id:\n                self._table.select(row)\n                return\n\n    def _color_table(self):\n        for row in range(len(self._table)):\n            bg, sbg = self._ROW_COLOR[self._table[row, 'Status']]\n            fg, sfg = ('black', 'white')\n            self._table.rowconfig(row, foreground=fg, selectforeground=sfg,\n                                  background=bg, selectbackground=sbg)\n            self._table.itemconfigure(row, 0,\n                                      foreground=self._MARK_COLOR[0],\n                                      background=self._MARK_COLOR[1])\n\n\n    def _clear_mark(self, id):\n        for row in range(len(self._table)):\n            if self._table[row, 'Identifier'] == id:\n                self._table[row, 0] = ''\n\n    def _mark_all(self, *e):\n        for row in range(len(self._table)):\n            self._table[row,0] = 'X'\n\n    def _table_mark(self, *e):\n        selection = self._table.selected_row()\n        if selection >= 0:\n            if self._table[selection][0] != '':\n                self._table[selection,0] = ''\n            else:\n                self._table[selection,0] = 'X'\n        self._table.select(delta=1)\n\n    def _show_log(self):\n        text = '\\n'.join(self._log_messages)\n        ShowText(self.top, 'NLTK Downloader Log', text)\n\n    def _package_to_columns(self, pkg):\n        \"\"\"\n        Given a package, return a list of values describing that\n        package, one for each column in ``self.COLUMNS``.\n        \"\"\"\n        row = []\n        for column_index, column_name in enumerate(self.COLUMNS):\n            if column_index == 0: # Mark:\n                row.append('')\n            elif column_name == 'Identifier':\n                row.append(pkg.id)\n            elif column_name == 'Status':\n                row.append(self._ds.status(pkg))\n            else:\n                attr = column_name.lower().replace(' ', '_')\n                row.append(getattr(pkg, attr, 'n/a'))\n        return row\n\n\n    def destroy(self, *e):\n        if self._destroyed: return\n        self.top.destroy()\n        self._destroyed = True\n\n    def _destroy(self, *e):\n        if self.top is not None:\n            for afterid in self._afterid.values():\n                self.top.after_cancel(afterid)\n\n        if self._downloading and self._use_threads:\n            self._abort_download()\n\n        self._column_vars.clear()\n\n    def mainloop(self, *args, **kwargs):\n        self.top.mainloop(*args, **kwargs)\n\n\n    HELP = textwrap.dedent(\"\"\"\\\n    This tool can be used to download a variety of corpora and models\n    that can be used with NLTK.  Each corpus or model is distributed\n    in a single zip file, known as a \\\"package file.\\\"  You can\n    download packages individually, or you can download pre-defined\n    collections of packages.\n\n    When you download a package, it will be saved to the \\\"download\n    directory.\\\"  A default download directory is chosen when you run\n\n    the downloader; but you may also select a different download\n    directory.  On Windows, the default download directory is\n\n\n    \\\"package.\\\"\n\n    The NLTK downloader can be used to download a variety of corpora,\n    models, and other data packages.\n\n    Keyboard shortcuts::\n      [return]\\t Download\n      [up]\\t Select previous package\n      [down]\\t Select next package\n      [left]\\t Select previous tab\n      [right]\\t Select next tab\n    \"\"\")\n\n    def help(self, *e):\n        try:\n            ShowText(self.top, 'Help: NLTK Dowloader',\n                     self.HELP.strip(), width=75, font='fixed')\n        except:\n            ShowText(self.top, 'Help: NLTK Downloader',\n                     self.HELP.strip(), width=75)\n\n    def about(self, *e):\n        ABOUT = (\"NLTK Downloader\\n\"+\n                 \"Written by Edward Loper\")\n        TITLE = 'About: NLTK Downloader'\n        try:\n            from six.moves.tkinter_messagebox import Message\n            Message(message=ABOUT, title=TITLE).show()\n        except ImportError:\n            ShowText(self.top, TITLE, ABOUT)\n\n\n    _gradient_width = 5\n    def _init_progressbar(self):\n        c = self._progressbar\n        width, height = int(c['width']), int(c['height'])\n        for i in range(0, (int(c['width'])*2)//self._gradient_width):\n            c.create_line(i*self._gradient_width+20, -20,\n                          i*self._gradient_width-height-20, height+20,\n                          width=self._gradient_width,\n                          fill='#%02x0000' % (80 + abs(i%6-3)*12))\n        c.addtag_all('gradient')\n        c.itemconfig('gradient', state='hidden')\n\n        c.addtag_withtag('redbox', c.create_rectangle(\n            0, 0, 0, 0, fill=self._PROGRESS_COLOR[0]))\n\n    def _show_progress(self, percent):\n        c = self._progressbar\n        if percent is None:\n            c.coords('redbox', 0, 0, 0, 0)\n            c.itemconfig('gradient', state='hidden')\n        else:\n            width, height = int(c['width']), int(c['height'])\n            x = percent * int(width) // 100 + 1\n            c.coords('redbox', 0, 0, x, height+1)\n\n    def _progress_alive(self):\n        c = self._progressbar\n        if not self._downloading:\n            c.itemconfig('gradient', state='hidden')\n        else:\n            c.itemconfig('gradient', state='normal')\n            x1, y1, x2, y2 = c.bbox('gradient')\n            if x1 <= -100:\n                c.move('gradient', (self._gradient_width*6)-4, 0)\n            else:\n                c.move('gradient', -4, 0)\n            afterid = self.top.after(200, self._progress_alive)\n            self._afterid['_progress_alive'] = afterid\n\n\n    def _download_threaded(self, *e):\n        if self._downloading:\n            self._abort_download()\n            return\n\n        self._download_button['text'] = 'Cancel'\n\n        marked = [self._table[row, 'Identifier']\n                  for row in range(len(self._table))\n                  if self._table[row, 0] != '']\n        selection = self._table.selected_row()\n        if not marked and selection is not None:\n            marked = [self._table[selection, 'Identifier']]\n\n        ds = Downloader(self._ds.url, self._ds.download_dir)\n\n        assert self._download_msg_queue == []\n        assert self._download_abort_queue == []\n        self._DownloadThread(ds, marked, self._download_lock,\n                             self._download_msg_queue,\n                             self._download_abort_queue).start()\n\n        self._log_indent = 0\n        self._downloading = True\n        self._monitor_message_queue()\n\n        self._progress_alive()\n\n    def _abort_download(self):\n        if self._downloading:\n            self._download_lock.acquire()\n            self._download_abort_queue.append('abort')\n            self._download_lock.release()\n\n    class _DownloadThread(threading.Thread):\n        def __init__(self, data_server, items, lock, message_queue, abort):\n            self.data_server = data_server\n            self.items = items\n            self.lock = lock\n            self.message_queue = message_queue\n            self.abort = abort\n            threading.Thread.__init__(self)\n\n        def run (self):\n            for msg in self.data_server.incr_download(self.items):\n                self.lock.acquire()\n                self.message_queue.append(msg)\n                if self.abort:\n                    self.message_queue.append('aborted')\n                    self.lock.release()\n                    return\n                self.lock.release()\n            self.lock.acquire()\n            self.message_queue.append('finished')\n            self.lock.release()\n\n    _MONITOR_QUEUE_DELAY=100\n    def _monitor_message_queue(self):\n        def show(s):\n            self._progresslabel['text'] = s\n            self._log(s)\n\n        if not self._download_lock.acquire():\n            return\n        for msg in self._download_msg_queue:\n\n            if msg == 'finished' or msg == 'aborted':\n                self._update_table_status()\n                self._downloading = False\n                self._download_button['text'] = 'Download'\n                del self._download_msg_queue[:]\n                del self._download_abort_queue[:]\n                self._download_lock.release()\n                if msg == 'aborted':\n                    show('Download aborted!')\n                    self._show_progress(None)\n                else:\n                    afterid = self.top.after(100, self._show_progress, None)\n                    self._afterid['_monitor_message_queue'] = afterid\n                return\n\n            elif isinstance(msg, ProgressMessage):\n                self._show_progress(msg.progress)\n            elif isinstance(msg, ErrorMessage):\n                show(msg.message)\n                if msg.package is not None:\n                    self._select(msg.package.id)\n                self._show_progress(None)\n                self._downloading = False\n                return # halt progress.\n            elif isinstance(msg, StartCollectionMessage):\n                show('Downloading collection %r' % msg.collection.id)\n                self._log_indent += 1\n            elif isinstance(msg, StartPackageMessage):\n                self._ds.clear_status_cache(msg.package.id)\n                show('Downloading package %r' % msg.package.id)\n            elif isinstance(msg, UpToDateMessage):\n                show('Package %s is up-to-date!' % msg.package.id)\n            elif isinstance(msg, FinishDownloadMessage):\n                show('Finished downloading %r.' % msg.package.id)\n            elif isinstance(msg, StartUnzipMessage):\n                show('Unzipping %s' % msg.package.filename)\n            elif isinstance(msg, FinishUnzipMessage):\n                show('Finished installing %s' % msg.package.id)\n            elif isinstance(msg, FinishCollectionMessage):\n                self._log_indent -= 1\n                show('Finished downloading collection %r.' % msg.collection.id)\n                self._clear_mark(msg.collection.id)\n            elif isinstance(msg, FinishPackageMessage):\n                self._update_table_status()\n                self._clear_mark(msg.package.id)\n\n        if self._download_abort_queue:\n            self._progresslabel['text'] = 'Aborting download...'\n\n        del self._download_msg_queue[:]\n        self._download_lock.release()\n\n        afterid = self.top.after(self._MONITOR_QUEUE_DELAY,\n                                 self._monitor_message_queue)\n        self._afterid['_monitor_message_queue'] = afterid\n\n\ndef md5_hexdigest(file):\n    \"\"\"\n    Calculate and return the MD5 checksum for a given file.\n    ``file`` may either be a filename or an open stream.\n    \"\"\"\n    if isinstance(file, string_types):\n        with open(file, 'rb') as infile:\n            return _md5_hexdigest(infile)\n    return _md5_hexdigest(file)\n\ndef _md5_hexdigest(fp):\n    md5_digest = md5()\n    while True:\n        block = fp.read(1024*16)  # 16k blocks\n        if not block: break\n        md5_digest.update(block)\n    return md5_digest.hexdigest()\n\n\ndef unzip(filename, root, verbose=True):\n    \"\"\"\n    Extract the contents of the zip file ``filename`` into the\n    directory ``root``.\n    \"\"\"\n    for message in _unzip_iter(filename, root, verbose):\n        if isinstance(message, ErrorMessage):\n            raise Exception(message)\n\ndef _unzip_iter(filename, root, verbose=True):\n    if verbose:\n        sys.stdout.write('Unzipping %s' % os.path.split(filename)[1])\n        sys.stdout.flush()\n\n    try: zf = zipfile.ZipFile(filename)\n    except zipfile.error as e:\n        yield ErrorMessage(filename, 'Error with downloaded zip file')\n        return\n    except Exception as e:\n        yield ErrorMessage(filename, e)\n        return\n\n    namelist = zf.namelist()\n    dirlist = set()\n    for x in namelist:\n        if x.endswith('/'):\n            dirlist.add(x)\n        else:\n            dirlist.add(x.rsplit('/',1)[0] + '/')\n    filelist = [x for x in namelist if not x.endswith('/')]\n\n    if not os.path.exists(root):\n        os.mkdir(root)\n\n    for dirname in sorted(dirlist):\n        pieces = dirname[:-1].split('/')\n        for i in range(len(pieces)):\n            dirpath = os.path.join(root, *pieces[:i+1])\n            if not os.path.exists(dirpath):\n                os.mkdir(dirpath)\n\n    for i, filename in enumerate(filelist):\n        filepath = os.path.join(root, *filename.split('/'))\n\n        try:\n            with open(filepath, 'wb') as dstfile, zf.open(filename) as srcfile:\n                shutil.copyfileobj(srcfile, dstfile)\n        except Exception as e:\n            yield ErrorMessage(filename, e)\n            return\n\n        if verbose and (i*10/len(filelist) > (i-1)*10/len(filelist)):\n            sys.stdout.write('.')\n            sys.stdout.flush()\n    if verbose:\n        print()\n\nimport subprocess, zipfile\n\ndef build_index(root, base_url):\n    \"\"\"\n    Create a new data.xml index file, by combining the xml description\n    files for various packages and collections.  ``root`` should be the\n    path to a directory containing the package xml and zip files; and\n    the collection xml files.  The ``root`` directory is expected to\n    have the following subdirectories::\n\n      root/\n        packages/ .................. subdirectory for packages\n          corpora/ ................. zip & xml files for corpora\n          grammars/ ................ zip & xml files for grammars\n          taggers/ ................. zip & xml files for taggers\n          tokenizers/ .............. zip & xml files for tokenizers\n          etc.\n        collections/ ............... xml files for collections\n\n    For each package, there should be two files: ``package.zip``\n    (where *package* is the package name)\n    which contains the package itself as a compressed zip file; and\n    ``package.xml``, which is an xml description of the package.  The\n    zipfile ``package.zip`` should expand to a single subdirectory\n    named ``package/``.  The base filename ``package`` must match\n    the identifier given in the package's xml file.\n\n    For each collection, there should be a single file ``collection.zip``\n    describing the collection, where *collection* is the name of the collection.\n\n    All identifiers (for both packages and collections) must be unique.\n    \"\"\"\n    packages = []\n    for pkg_xml, zf, subdir in _find_packages(os.path.join(root, 'packages')):\n        zipstat = os.stat(zf.filename)\n        url = '%s/%s/%s' % (base_url, subdir, os.path.split(zf.filename)[1])\n        unzipped_size = sum(zf_info.file_size for zf_info in zf.infolist())\n\n        pkg_xml.set('unzipped_size', '%s' % unzipped_size)\n        pkg_xml.set('size', '%s' % zipstat.st_size)\n        pkg_xml.set('checksum', '%s' % md5_hexdigest(zf.filename))\n        pkg_xml.set('subdir', subdir)\n        if not pkg_xml.get('url'):\n            pkg_xml.set('url', url)\n\n        packages.append(pkg_xml)\n\n    collections = list(_find_collections(os.path.join(root, 'collections')))\n\n    uids = set()\n    for item in packages+collections:\n        if item.get('id') in uids:\n            raise ValueError('Duplicate UID: %s' % item.get('id'))\n        uids.add(item.get('id'))\n\n    top_elt = ElementTree.Element('nltk_data')\n    top_elt.append(ElementTree.Element('packages'))\n    for package in packages: top_elt[0].append(package)\n    top_elt.append(ElementTree.Element('collections'))\n    for collection in collections: top_elt[1].append(collection)\n\n    _indent_xml(top_elt)\n    return top_elt\n\ndef _indent_xml(xml, prefix=''):\n    \"\"\"\n    Helper for ``build_index()``: Given an XML ``ElementTree``, modify it\n    (and its descendents) ``text`` and ``tail`` attributes to generate\n    an indented tree, where each nested element is indented by 2\n    spaces with respect to its parent.\n    \"\"\"\n    if len(xml) > 0:\n        xml.text = (xml.text or '').strip() + '\\n' + prefix + '  '\n        for child in xml:\n            _indent_xml(child, prefix+'  ')\n        for child in xml[:-1]:\n            child.tail = (child.tail or '').strip() + '\\n' + prefix + '  '\n        xml[-1].tail = (xml[-1].tail or '').strip() + '\\n' + prefix\n\ndef _check_package(pkg_xml, zipfilename, zf):\n    \"\"\"\n    Helper for ``build_index()``: Perform some checks to make sure that\n    the given package is consistent.\n    \"\"\"\n    uid = os.path.splitext(os.path.split(zipfilename)[1])[0]\n    if pkg_xml.get('id') != uid:\n        raise ValueError('package identifier mismatch (%s vs %s)' %\n                         (pkg_xml.get('id'), uid))\n\n    if sum( (name!=uid and not name.startswith(uid+'/'))\n            for name in zf.namelist() ):\n        raise ValueError('Zipfile %s.zip does not expand to a single '\n                         'subdirectory %s/' % (uid, uid))\n\ndef _svn_revision(filename):\n    \"\"\"\n    Helper for ``build_index()``: Calculate the subversion revision\n    number for a given file (by using ``subprocess`` to run ``svn``).\n    \"\"\"\n    p = subprocess.Popen(['svn', 'status', '-v', filename],\n                         stdout=subprocess.PIPE,\n                         stderr=subprocess.PIPE)\n    (stdout, stderr) = p.communicate()\n    if p.returncode != 0 or stderr or not stdout:\n        raise ValueError('Error determining svn_revision for %s: %s' %\n                         (os.path.split(filename)[1], textwrap.fill(stderr)))\n    return stdout.split()[2]\n\ndef _find_collections(root):\n    \"\"\"\n    Helper for ``build_index()``: Yield a list of ElementTree.Element\n    objects, each holding the xml for a single package collection.\n    \"\"\"\n    packages = []\n    for dirname, subdirs, files in os.walk(root):\n        for filename in files:\n            if filename.endswith('.xml'):\n                xmlfile = os.path.join(dirname, filename)\n                yield ElementTree.parse(xmlfile).getroot()\n\ndef _find_packages(root):\n    \"\"\"\n    Helper for ``build_index()``: Yield a list of tuples\n    ``(pkg_xml, zf, subdir)``, where:\n      - ``pkg_xml`` is an ``ElementTree.Element`` holding the xml for a\n        package\n      - ``zf`` is a ``zipfile.ZipFile`` for the package's contents.\n      - ``subdir`` is the subdirectory (relative to ``root``) where\n        the package was found (e.g. 'corpora' or 'grammars').\n    \"\"\"\n    from nltk.corpus.reader.util import _path_from\n    packages = []\n    for dirname, subdirs, files in os.walk(root):\n        relpath = '/'.join(_path_from(root, dirname))\n        for filename in files:\n            if filename.endswith('.xml'):\n                xmlfilename = os.path.join(dirname, filename)\n                zipfilename = xmlfilename[:-4]+'.zip'\n                try: zf = zipfile.ZipFile(zipfilename)\n                except Exception as e:\n                    raise ValueError('Error reading file %r!\\n%s' %\n                                     (zipfilename, e))\n                try: pkg_xml = ElementTree.parse(xmlfilename).getroot()\n                except Exception as e:\n                    raise ValueError('Error reading file %r!\\n%s' %\n                                     (xmlfilename, e))\n\n                uid = os.path.split(xmlfilename[:-4])[1]\n                if pkg_xml.get('id') != uid:\n                    raise ValueError('package identifier mismatch (%s '\n                                     'vs %s)' % (pkg_xml.get('id'), uid))\n\n                if sum( (name!=uid and not name.startswith(uid+'/'))\n                        for name in zf.namelist() ):\n                    raise ValueError('Zipfile %s.zip does not expand to a '\n                                     'single subdirectory %s/' % (uid, uid))\n\n                yield pkg_xml, zf, relpath\n        try: subdirs.remove('.svn')\n        except ValueError: pass\n\n\n\n_downloader = Downloader()\ndownload = _downloader.download\n\ndef download_shell():\n    DownloaderShell(_downloader).run()\n\ndef download_gui():\n    DownloaderGUI(_downloader).mainloop()\n\ndef update():\n    _downloader.update()\n\nif __name__ == '__main__':\n    from optparse import OptionParser\n    parser = OptionParser()\n    parser.add_option(\"-d\", \"--dir\", dest=\"dir\",\n        help=\"download package to directory DIR\", metavar=\"DIR\")\n    parser.add_option(\"-q\", \"--quiet\", dest=\"quiet\", action=\"store_true\",\n        default=False, help=\"work quietly\")\n    parser.add_option(\"-f\", \"--force\", dest=\"force\", action=\"store_true\",\n        default=False, help=\"download even if already installed\")\n    parser.add_option(\"-e\", \"--exit-on-error\", dest=\"halt_on_error\", action=\"store_true\",\n        default=False, help=\"exit if an error occurs\")\n    parser.add_option(\"-u\", \"--url\", dest=\"server_index_url\",\n        default=os.environ.get('NLTK_DOWNLOAD_URL'),\n        help=\"download server index url\")\n\n    (options, args) = parser.parse_args()\n\n    downloader = Downloader(server_index_url = options.server_index_url)\n\n    if args:\n        for pkg_id in args:\n            rv = downloader.download(info_or_id=pkg_id, download_dir=options.dir,\n                quiet=options.quiet, force=options.force,\n                halt_on_error=options.halt_on_error)\n            if rv==False and options.halt_on_error:\n                break\n    else:\n        downloader.download(download_dir=options.dir,\n            quiet=options.quiet, force=options.force,\n            halt_on_error=options.halt_on_error)\n"], "nltk\\draw\\cfg": [".py", "\n\n\nimport re\n\nfrom six import string_types\nfrom six.moves.tkinter import (Button, Canvas, Entry, Frame, IntVar, Label,\n                               Scrollbar, Text, Tk, Toplevel)\n\nfrom nltk.grammar import (CFG, _read_cfg_production,\n                          Nonterminal, nonterminals)\nfrom nltk.tree import Tree\nfrom nltk.draw.tree import TreeSegmentWidget, tree_to_treesegment\nfrom nltk.draw.util import (CanvasFrame, ColorizedList, ShowText,\n                            SymbolWidget, TextWidget)\n\n\nclass ProductionList(ColorizedList):\n    ARROW = SymbolWidget.SYMBOLS['rightarrow']\n\n    def _init_colortags(self, textwidget, options):\n        textwidget.tag_config('terminal', foreground='#006000')\n        textwidget.tag_config('arrow', font='symbol', underline='0')\n        textwidget.tag_config('nonterminal', foreground='blue',\n                              font=('helvetica', -12, 'bold'))\n\n    def _item_repr(self, item):\n        contents = []\n        contents.append(('%s\\t' % item.lhs(), 'nonterminal'))\n        contents.append((self.ARROW, 'arrow'))\n        for elt in item.rhs():\n            if isinstance(elt, Nonterminal):\n                contents.append((' %s' % elt.symbol(), 'nonterminal'))\n            else:\n                contents.append((' %r' % elt, 'terminal'))\n        return contents\n\n\n_CFGEditor_HELP = \"\"\"\n\nThe CFG Editor can be used to create or modify context free grammars.\nA context free grammar consists of a start symbol and a list of\nproductions.  The start symbol is specified by the text entry field in\nthe upper right hand corner of the editor; and the list of productions\nare specified in the main text editing box.\n\nEvery non-blank line specifies a single production.  Each production\nhas the form \"LHS -> RHS,\" where LHS is a single nonterminal, and RHS\nis a list of nonterminals and terminals.\n\nNonterminals must be a single word, such as S or NP or NP_subj.\nCurrently, nonterminals must consists of alphanumeric characters and\nunderscores (_).  Nonterminals are colored blue.  If you place the\nmouse over any nonterminal, then all occurrences of that nonterminal\nwill be highlighted.\n\nTerminals must be surrounded by single quotes (') or double\nquotes(\\\").  For example, \"dog\" and \"New York\" are terminals.\nCurrently, the string within the quotes must consist of alphanumeric\ncharacters, underscores, and spaces.\n\nTo enter a new production, go to a blank line, and type a nonterminal,\nfollowed by an arrow (->), followed by a sequence of terminals and\nnonterminals.  Note that \"->\" (dash + greater-than) is automatically\nconverted to an arrow symbol.  When you move your cursor to a\ndifferent line, your production will automatically be colorized.  If\nthere are any errors, they will be highlighted in red.\n\nNote that the order of the productions is significant for some\nalgorithms.  To re-order the productions, use cut and paste to move\nthem.\n\nUse the buttons at the bottom of the window when you are done editing\nthe CFG:\n  - Ok: apply the new CFG, and exit the editor.\n  - Apply: apply the new CFG, and do not exit the editor.\n  - Reset: revert to the original CFG, and do not exit the editor.\n  - Cancel: revert to the original CFG, and exit the editor.\n\n\"\"\"\n\nclass CFGEditor(object):\n    \"\"\"\n    A dialog window for creating and editing context free grammars.\n    ``CFGEditor`` imposes the following restrictions:\n\n    - All nonterminals must be strings consisting of word\n      characters.\n    - All terminals must be strings consisting of word characters\n      and space characters.\n    \"\"\"\n    ARROW = SymbolWidget.SYMBOLS['rightarrow']\n    _LHS_RE = re.compile(r\"(^\\s*\\w+\\s*)(->|(\"+ARROW+\"))\")\n    _ARROW_RE = re.compile(\"\\s*(->|(\"+ARROW+\"))\\s*\")\n    _PRODUCTION_RE = re.compile(r\"(^\\s*\\w+\\s*)\" +              # LHS\n                                \"(->|(\"+ARROW+\"))\\s*\" +        # arrow\n                                r\"((\\w+|'[\\w ]*'|\\\"[\\w ]*\\\"|\\|)\\s*)*$\") # RHS\n    _TOKEN_RE = re.compile(\"\\\\w+|->|'[\\\\w ]+'|\\\"[\\\\w ]+\\\"|(\"+ARROW+\")\")\n    _BOLD = ('helvetica', -12, 'bold')\n\n    def __init__(self, parent, cfg=None, set_cfg_callback=None):\n        self._parent = parent\n        if cfg is not None: self._cfg = cfg\n        else: self._cfg = CFG(Nonterminal('S'), [])\n        self._set_cfg_callback = set_cfg_callback\n\n        self._highlight_matching_nonterminals = 1\n\n        self._top = Toplevel(parent)\n        self._init_bindings()\n\n        self._init_startframe()\n        self._startframe.pack(side='top', fill='x', expand=0)\n        self._init_prodframe()\n        self._prodframe.pack(side='top', fill='both', expand=1)\n        self._init_buttons()\n        self._buttonframe.pack(side='bottom', fill='x', expand=0)\n\n        self._textwidget.focus()\n\n    def _init_startframe(self):\n        frame = self._startframe = Frame(self._top)\n        self._start = Entry(frame)\n        self._start.pack(side='right')\n        Label(frame, text='Start Symbol:').pack(side='right')\n        Label(frame, text='Productions:').pack(side='left')\n        self._start.insert(0, self._cfg.start().symbol())\n\n    def _init_buttons(self):\n        frame = self._buttonframe = Frame(self._top)\n        Button(frame, text='Ok', command=self._ok,\n               underline=0, takefocus=0).pack(side='left')\n        Button(frame, text='Apply', command=self._apply,\n               underline=0, takefocus=0).pack(side='left')\n        Button(frame, text='Reset', command=self._reset,\n               underline=0, takefocus=0,).pack(side='left')\n        Button(frame, text='Cancel', command=self._cancel,\n               underline=0, takefocus=0).pack(side='left')\n        Button(frame, text='Help', command=self._help,\n               underline=0, takefocus=0).pack(side='right')\n\n    def _init_bindings(self):\n        self._top.title('CFG Editor')\n        self._top.bind('<Control-q>', self._cancel)\n        self._top.bind('<Alt-q>', self._cancel)\n        self._top.bind('<Control-d>', self._cancel)\n        self._top.bind('<Alt-x>', self._cancel)\n        self._top.bind('<Escape>', self._cancel)\n        self._top.bind('<Alt-c>', self._cancel)\n\n        self._top.bind('<Control-o>', self._ok)\n        self._top.bind('<Alt-o>', self._ok)\n        self._top.bind('<Control-a>', self._apply)\n        self._top.bind('<Alt-a>', self._apply)\n        self._top.bind('<Control-r>', self._reset)\n        self._top.bind('<Alt-r>', self._reset)\n        self._top.bind('<Control-h>', self._help)\n        self._top.bind('<Alt-h>', self._help)\n        self._top.bind('<F1>', self._help)\n\n    def _init_prodframe(self):\n        self._prodframe = Frame(self._top)\n\n        self._textwidget = Text(self._prodframe, background='#e0e0e0',\n                                exportselection=1)\n        self._textscroll = Scrollbar(self._prodframe, takefocus=0,\n                                     orient='vertical')\n        self._textwidget.config(yscrollcommand = self._textscroll.set)\n        self._textscroll.config(command=self._textwidget.yview)\n        self._textscroll.pack(side='right', fill='y')\n        self._textwidget.pack(expand=1, fill='both', side='left')\n\n        self._textwidget.tag_config('terminal', foreground='#006000')\n        self._textwidget.tag_config('arrow', font='symbol')\n        self._textwidget.tag_config('error', background='red')\n\n        self._linenum = 0\n\n        self._top.bind('>', self._replace_arrows)\n\n        self._top.bind('<<Paste>>', self._analyze)\n        self._top.bind('<KeyPress>', self._check_analyze)\n        self._top.bind('<ButtonPress>', self._check_analyze)\n\n        def cycle(e, textwidget=self._textwidget):\n            textwidget.tk_focusNext().focus()\n        self._textwidget.bind('<Tab>', cycle)\n\n        prod_tuples = [(p.lhs(),[p.rhs()]) for p in self._cfg.productions()]\n        for i in range(len(prod_tuples)-1,0,-1):\n            if (prod_tuples[i][0] == prod_tuples[i-1][0]):\n                if () in prod_tuples[i][1]: continue\n                if () in prod_tuples[i-1][1]: continue\n                print(prod_tuples[i-1][1])\n                print(prod_tuples[i][1])\n                prod_tuples[i-1][1].extend(prod_tuples[i][1])\n                del prod_tuples[i]\n\n        for lhs, rhss in prod_tuples:\n            print(lhs, rhss)\n            s = '%s ->' % lhs\n            for rhs in rhss:\n                for elt in rhs:\n                    if isinstance(elt, Nonterminal): s += ' %s' % elt\n                    else: s += ' %r' % elt\n                s += ' |'\n            s = s[:-2] + '\\n'\n            self._textwidget.insert('end', s)\n\n        self._analyze()\n\n\n\n    def _clear_tags(self, linenum):\n        \"\"\"\n        Remove all tags (except ``arrow`` and ``sel``) from the given\n        line of the text widget used for editing the productions.\n        \"\"\"\n        start = '%d.0'%linenum\n        end = '%d.end'%linenum\n        for tag in self._textwidget.tag_names():\n            if tag not in ('arrow', 'sel'):\n                self._textwidget.tag_remove(tag, start, end)\n\n    def _check_analyze(self, *e):\n        \"\"\"\n        Check if we've moved to a new line.  If we have, then remove\n        all colorization from the line we moved to, and re-colorize\n        the line that we moved from.\n        \"\"\"\n        linenum = int(self._textwidget.index('insert').split('.')[0])\n        if linenum != self._linenum:\n            self._clear_tags(linenum)\n            self._analyze_line(self._linenum)\n            self._linenum = linenum\n\n    def _replace_arrows(self, *e):\n        \"\"\"\n        Replace any ``'->'`` text strings with arrows (char \\\\256, in\n        symbol font).  This searches the whole buffer, but is fast\n        enough to be done anytime they press '>'.\n        \"\"\"\n        arrow = '1.0'\n        while True:\n            arrow = self._textwidget.search('->', arrow, 'end+1char')\n            if arrow == '': break\n            self._textwidget.delete(arrow, arrow+'+2char')\n            self._textwidget.insert(arrow, self.ARROW, 'arrow')\n            self._textwidget.insert(arrow, '\\t')\n\n        arrow = '1.0'\n        while True:\n            arrow = self._textwidget.search(self.ARROW, arrow+'+1char',\n                                            'end+1char')\n            if arrow == '': break\n            self._textwidget.tag_add('arrow', arrow, arrow+'+1char')\n\n    def _analyze_token(self, match, linenum):\n        \"\"\"\n        Given a line number and a regexp match for a token on that\n        line, colorize the token.  Note that the regexp match gives us\n        the token's text, start index (on the line), and end index (on\n        the line).\n        \"\"\"\n        if match.group()[0] in \"'\\\"\": tag = 'terminal'\n        elif match.group() in ('->', self.ARROW): tag = 'arrow'\n        else:\n            tag = 'nonterminal_'+match.group()\n            if tag not in self._textwidget.tag_names():\n                self._init_nonterminal_tag(tag)\n\n        start = '%d.%d' % (linenum, match.start())\n        end = '%d.%d' % (linenum, match.end())\n        self._textwidget.tag_add(tag, start, end)\n\n    def _init_nonterminal_tag(self, tag, foreground='blue'):\n        self._textwidget.tag_config(tag, foreground=foreground,\n                                    font=CFGEditor._BOLD)\n        if not self._highlight_matching_nonterminals:\n            return\n        def enter(e, textwidget=self._textwidget, tag=tag):\n            textwidget.tag_config(tag, background='#80ff80')\n        def leave(e, textwidget=self._textwidget, tag=tag):\n            textwidget.tag_config(tag, background='')\n        self._textwidget.tag_bind(tag, '<Enter>', enter)\n        self._textwidget.tag_bind(tag, '<Leave>', leave)\n\n    def _analyze_line(self, linenum):\n        \"\"\"\n        Colorize a given line.\n        \"\"\"\n        self._clear_tags(linenum)\n\n        line = self._textwidget.get(repr(linenum)+'.0', repr(linenum)+'.end')\n\n        if CFGEditor._PRODUCTION_RE.match(line):\n            def analyze_token(match, self=self, linenum=linenum):\n                self._analyze_token(match, linenum)\n                return ''\n            CFGEditor._TOKEN_RE.sub(analyze_token, line)\n        elif line.strip() != '':\n            self._mark_error(linenum, line)\n\n    def _mark_error(self, linenum, line):\n        \"\"\"\n        Mark the location of an error in a line.\n        \"\"\"\n        arrowmatch = CFGEditor._ARROW_RE.search(line)\n        if not arrowmatch:\n            start = '%d.0' % linenum\n            end = '%d.end' % linenum\n        elif not CFGEditor._LHS_RE.match(line):\n            start = '%d.0' % linenum\n            end = '%d.%d' % (linenum, arrowmatch.start())\n        else:\n            start = '%d.%d' % (linenum, arrowmatch.end())\n            end = '%d.end' % linenum\n\n        if self._textwidget.compare(start, '==', end):\n            start = '%d.0' % linenum\n            end = '%d.end' % linenum\n        self._textwidget.tag_add('error', start, end)\n\n    def _analyze(self, *e):\n        \"\"\"\n        Replace ``->`` with arrows, and colorize the entire buffer.\n        \"\"\"\n        self._replace_arrows()\n        numlines = int(self._textwidget.index('end').split('.')[0])\n        for linenum in range(1, numlines+1):  # line numbers start at 1.\n            self._analyze_line(linenum)\n\n    def _parse_productions(self):\n        \"\"\"\n        Parse the current contents of the textwidget buffer, to create\n        a list of productions.\n        \"\"\"\n        productions = []\n\n        text = self._textwidget.get('1.0', 'end')\n        text = re.sub(self.ARROW, '->', text)\n        text = re.sub('\\t', ' ', text)\n        lines = text.split('\\n')\n\n        for line in lines:\n            line = line.strip()\n            if line=='': continue\n            productions += _read_cfg_production(line)\n\n        return productions\n\n    def _destroy(self, *e):\n        if self._top is None: return\n        self._top.destroy()\n        self._top = None\n\n    def _ok(self, *e):\n        self._apply()\n        self._destroy()\n\n    def _apply(self, *e):\n        productions = self._parse_productions()\n        start = Nonterminal(self._start.get())\n        cfg = CFG(start, productions)\n        if self._set_cfg_callback is not None:\n            self._set_cfg_callback(cfg)\n\n    def _reset(self, *e):\n        self._textwidget.delete('1.0', 'end')\n        for production in self._cfg.productions():\n            self._textwidget.insert('end', '%s\\n' % production)\n        self._analyze()\n        if self._set_cfg_callback is not None:\n            self._set_cfg_callback(self._cfg)\n\n    def _cancel(self, *e):\n        try: self._reset()\n        except: pass\n        self._destroy()\n\n    def _help(self, *e):\n        try:\n            ShowText(self._parent, 'Help: Chart Parser Demo',\n                     (_CFGEditor_HELP).strip(), width=75, font='fixed')\n        except:\n            ShowText(self._parent, 'Help: Chart Parser Demo',\n                     (_CFGEditor_HELP).strip(), width=75)\n\n\nclass CFGDemo(object):\n    def __init__(self, grammar, text):\n        self._grammar = grammar\n        self._text = text\n\n        self._top = Tk()\n        self._top.title('Context Free Grammar Demo')\n\n        self._size = IntVar(self._top)\n        self._size.set(12) # = medium\n\n        self._init_bindings(self._top)\n\n        frame1 = Frame(self._top)\n        frame1.pack(side='left', fill='y', expand=0)\n        self._init_menubar(self._top)\n        self._init_buttons(self._top)\n        self._init_grammar(frame1)\n        self._init_treelet(frame1)\n        self._init_workspace(self._top)\n\n\n    def _init_bindings(self, top):\n        top.bind('<Control-q>', self.destroy)\n\n    def _init_menubar(self, parent): pass\n\n    def _init_buttons(self, parent): pass\n\n    def _init_grammar(self, parent):\n        self._prodlist = ProductionList(parent, self._grammar, width=20)\n        self._prodlist.pack(side='top', fill='both', expand=1)\n        self._prodlist.focus()\n        self._prodlist.add_callback('select', self._selectprod_cb)\n        self._prodlist.add_callback('move', self._selectprod_cb)\n\n    def _init_treelet(self, parent):\n        self._treelet_canvas = Canvas(parent, background='white')\n        self._treelet_canvas.pack(side='bottom', fill='x')\n        self._treelet = None\n\n    def _init_workspace(self, parent):\n        self._workspace = CanvasFrame(parent, background='white')\n        self._workspace.pack(side='right', fill='both', expand=1)\n        self._tree = None\n        self.reset_workspace()\n\n\n    def reset_workspace(self):\n        c = self._workspace.canvas()\n        fontsize = int(self._size.get())\n        node_font = ('helvetica', -(fontsize+4), 'bold')\n        leaf_font = ('helvetica', -(fontsize+2))\n\n        if self._tree is not None:\n            self._workspace.remove_widget(self._tree)\n\n        start = self._grammar.start().symbol()\n        rootnode = TextWidget(c, start, font=node_font, draggable=1)\n\n        leaves = []\n        for word in self._text:\n            leaves.append(TextWidget(c, word, font=leaf_font, draggable=1))\n\n        self._tree = TreeSegmentWidget(c, rootnode, leaves,\n                                       color='white')\n\n        self._workspace.add_widget(self._tree)\n\n        for leaf in leaves: leaf.move(0,100)\n\n\n    def workspace_markprod(self, production):\n        pass\n\n    def _markproduction(self, prod, tree=None):\n        if tree is None: tree = self._tree\n        for i in range(len(tree.subtrees())-len(prod.rhs())):\n            if tree['color', i] == 'white':\n                self._markproduction\n\n            for j, node in enumerate(prod.rhs()):\n                widget = tree.subtrees()[i+j]\n                if (isinstance(node, Nonterminal) and\n                    isinstance(widget, TreeSegmentWidget) and\n                    node.symbol == widget.label().text()):\n                    pass # matching nonterminal\n                elif (isinstance(node, string_types) and\n                      isinstance(widget, TextWidget) and\n                      node == widget.text()):\n                    pass # matching nonterminal\n                else: break\n            else:\n                print('MATCH AT', i)\n\n\n    def _selectprod_cb(self, production):\n        canvas = self._treelet_canvas\n\n        self._prodlist.highlight(production)\n        if self._treelet is not None: self._treelet.destroy()\n\n        rhs = production.rhs()\n        for (i, elt) in enumerate(rhs):\n            if isinstance(elt, Nonterminal): elt = Tree(elt)\n        tree = Tree(production.lhs().symbol(), *rhs)\n\n        fontsize = int(self._size.get())\n        node_font = ('helvetica', -(fontsize+4), 'bold')\n        leaf_font = ('helvetica', -(fontsize+2))\n        self._treelet = tree_to_treesegment(canvas, tree,\n                                            node_font=node_font,\n                                            leaf_font=leaf_font)\n        self._treelet['draggable'] = 1\n\n        (x1, y1, x2, y2) = self._treelet.bbox()\n        w, h = int(canvas['width']), int(canvas['height'])\n        self._treelet.move((w-x1-x2)/2, (h-y1-y2)/2)\n\n        self._markproduction(production)\n\n    def destroy(self, *args):\n        self._top.destroy()\n\n    def mainloop(self, *args, **kwargs):\n        self._top.mainloop(*args, **kwargs)\n\ndef demo2():\n    from nltk import Nonterminal, Production, CFG\n    nonterminals = 'S VP NP PP P N Name V Det'\n    (S, VP, NP, PP, P, N, Name, V, Det) = [Nonterminal(s)\n                                           for s in nonterminals.split()]\n    productions = (\n        Production(S, [NP, VP]),\n        Production(NP, [Det, N]),\n        Production(NP, [NP, PP]),\n        Production(VP, [VP, PP]),\n        Production(VP, [V, NP, PP]),\n        Production(VP, [V, NP]),\n        Production(PP, [P, NP]),\n        Production(PP, []),\n\n        Production(PP, ['up', 'over', NP]),\n\n        Production(NP, ['I']),   Production(Det, ['the']),\n        Production(Det, ['a']),  Production(N, ['man']),\n        Production(V, ['saw']),  Production(P, ['in']),\n        Production(P, ['with']), Production(N, ['park']),\n        Production(N, ['dog']),  Production(N, ['statue']),\n        Production(Det, ['my']),\n        )\n    grammar = CFG(S, productions)\n\n    text = 'I saw a man in the park'.split()\n    d=CFGDemo(grammar, text)\n    d.mainloop()\n\n\ndef demo():\n    from nltk import Nonterminal, CFG\n    nonterminals = 'S VP NP PP P N Name V Det'\n    (S, VP, NP, PP, P, N, Name, V, Det) = [Nonterminal(s)\n                                           for s in nonterminals.split()]\n\n    grammar = CFG.fromstring(\"\"\"\n    S -> NP VP\n    PP -> P NP\n    NP -> Det N\n    NP -> NP PP\n    VP -> V NP\n    VP -> VP PP\n    Det -> 'a'\n    Det -> 'the'\n    Det -> 'my'\n    NP -> 'I'\n    N -> 'dog'\n    N -> 'man'\n    N -> 'park'\n    N -> 'statue'\n    V -> 'saw'\n    P -> 'in'\n    P -> 'up'\n    P -> 'over'\n    P -> 'with'\n    \"\"\")\n\n    def cb(grammar): print(grammar)\n    top = Tk()\n    editor = CFGEditor(top, grammar, cb)\n    Label(top, text='\\nTesting CFG Editor\\n').pack()\n    Button(top, text='Quit', command=top.destroy).pack()\n    top.mainloop()\n\ndef demo3():\n    from nltk import Production\n    (S, VP, NP, PP, P, N, Name, V, Det) = \\\n        nonterminals('S, VP, NP, PP, P, N, Name, V, Det')\n\n    productions = (\n        Production(S, [NP, VP]),\n        Production(NP, [Det, N]),\n        Production(NP, [NP, PP]),\n        Production(VP, [VP, PP]),\n        Production(VP, [V, NP, PP]),\n        Production(VP, [V, NP]),\n        Production(PP, [P, NP]),\n        Production(PP, []),\n\n        Production(PP, ['up', 'over', NP]),\n\n        Production(NP, ['I']),   Production(Det, ['the']),\n        Production(Det, ['a']),  Production(N, ['man']),\n        Production(V, ['saw']),  Production(P, ['in']),\n        Production(P, ['with']), Production(N, ['park']),\n        Production(N, ['dog']),  Production(N, ['statue']),\n        Production(Det, ['my']),\n        )\n\n    t = Tk()\n    def destroy(e, t=t): t.destroy()\n    t.bind('q', destroy)\n    p = ProductionList(t, productions)\n    p.pack(expand=1, fill='both')\n    p.add_callback('select', p.markonly)\n    p.add_callback('move', p.markonly)\n    p.focus()\n    p.mark(productions[2])\n    p.mark(productions[8])\n\nif __name__ == '__main__': demo()\n"], "nltk\\draw\\dispersion": [".py", "\n\ndef dispersion_plot(text, words, ignore_case=False, title=\"Lexical Dispersion Plot\"):\n\n    try:\n        from matplotlib import pylab\n    except ImportError:\n        raise ValueError('The plot function requires matplotlib to be installed.'\n                     'See http://matplotlib.org/')\n\n    text = list(text)\n    words.reverse()\n\n    if ignore_case:\n        words_to_comp = list(map(str.lower, words))\n        text_to_comp = list(map(str.lower, text))\n    else:\n        words_to_comp = words\n        text_to_comp = text\n\n    points = [(x,y) for x in range(len(text_to_comp))\n                    for y in range(len(words_to_comp))\n                    if text_to_comp[x] == words_to_comp[y]]\n    if points:\n        x, y = list(zip(*points))\n    else:\n        x = y = ()\n    pylab.plot(x, y, \"b|\", scalex=.1)\n    pylab.yticks(list(range(len(words))), words, color=\"b\")\n    pylab.ylim(-1, len(words))\n    pylab.title(title)\n    pylab.xlabel(\"Word Offset\")\n    pylab.show()\n\nif __name__ == '__main__':\n    import nltk.compat\n    from nltk.corpus import gutenberg\n    words = ['Elinor', 'Marianne', 'Edward', 'Willoughby']\n    dispersion_plot(gutenberg.words('austen-sense.txt'), words)\n"], "nltk\\draw\\table": [".py", "\n\nfrom __future__ import division\n\n\nimport operator\n\nfrom six.moves.tkinter import (Frame, Label, Listbox, Scrollbar, Tk)\n\n\n\nclass MultiListbox(Frame):\n\n    FRAME_CONFIG = dict(background='#888',\n                        takefocus=True,\n                        highlightthickness=1)\n\n    LABEL_CONFIG = dict(borderwidth=1, relief='raised',\n                        font='helvetica -16 bold',\n                      background='#444', foreground='white')\n\n    LISTBOX_CONFIG = dict(borderwidth=1,\n                          selectborderwidth=0,\n                          highlightthickness=0,\n                          exportselection=False,\n                          selectbackground='#888',\n                          activestyle='none',\n                          takefocus=False)\n\n\n    def __init__(self, master, columns, column_weights=None, cnf={}, **kw):\n        if isinstance(columns, int):\n            columns = list(range(columns))\n            include_labels = False\n        else:\n            include_labels = True\n\n        if len(columns) == 0:\n            raise ValueError(\"Expected at least one column\")\n\n        self._column_names = tuple(columns)\n        self._listboxes = []\n        self._labels = []\n\n        if column_weights is None:\n            column_weights = [1] * len(columns)\n        elif len(column_weights) != len(columns):\n            raise ValueError('Expected one column_weight for each column')\n        self._column_weights = column_weights\n\n        Frame.__init__(self, master, **self.FRAME_CONFIG)\n        self.grid_rowconfigure(1, weight=1)\n        for i, label in enumerate(self._column_names):\n            self.grid_columnconfigure(i, weight=column_weights[i])\n\n            if include_labels:\n                l = Label(self, text=label, **self.LABEL_CONFIG)\n                self._labels.append(l)\n                l.grid(column=i, row=0, sticky='news', padx=0, pady=0)\n                l.column_index = i\n\n            lb = Listbox(self, **self.LISTBOX_CONFIG)\n            self._listboxes.append(lb)\n            lb.grid(column=i, row=1, sticky='news', padx=0, pady=0)\n            lb.column_index = i\n\n            lb.bind('<Button-1>', self._select)\n            lb.bind('<B1-Motion>', self._select)\n            lb.bind('<Button-4>', lambda e: self._scroll(-1))\n            lb.bind('<Button-5>', lambda e: self._scroll(+1))\n            lb.bind('<MouseWheel>', lambda e: self._scroll(e.delta))\n            lb.bind('<Button-2>', lambda e: self.scan_mark(e.x, e.y))\n            lb.bind('<B2-Motion>', lambda e: self.scan_dragto(e.x, e.y))\n            lb.bind('<B1-Leave>', lambda e: 'break')\n            l.bind('<Button-1>', self._resize_column)\n\n        self.bind('<Button-1>', self._resize_column)\n\n        self.bind('<Up>', lambda e: self.select(delta=-1))\n        self.bind('<Down>', lambda e: self.select(delta=1))\n        self.bind('<Prior>', lambda e: self.select(delta=-self._pagesize()))\n        self.bind('<Next>', lambda e: self.select(delta=self._pagesize()))\n\n        self.configure(cnf, **kw)\n\n\n    def _resize_column(self, event):\n        if event.widget.bind('<ButtonRelease>'):\n            return False\n\n        self._resize_column_index = None\n        if event.widget is self:\n            for i, lb in enumerate(self._listboxes):\n                if abs(event.x-(lb.winfo_x()+lb.winfo_width())) < 10:\n                    self._resize_column_index = i\n        elif event.x > (event.widget.winfo_width()-5):\n            self._resize_column_index = event.widget.column_index\n        elif event.x < 5 and event.widget.column_index != 0:\n            self._resize_column_index = event.widget.column_index-1\n\n        if self._resize_column_index is not None:\n            event.widget.bind('<Motion>', self._resize_column_motion_cb)\n            event.widget.bind('<ButtonRelease-%d>' % event.num,\n                              self._resize_column_buttonrelease_cb)\n            return True\n        else:\n            return False\n\n    def _resize_column_motion_cb(self, event):\n        lb = self._listboxes[self._resize_column_index]\n        charwidth = lb.winfo_width() / lb['width']\n\n        x1 = event.x + event.widget.winfo_x()\n        x2 = lb.winfo_x() + lb.winfo_width()\n\n        lb['width'] = max(3, lb['width'] + (x1-x2) // charwidth)\n\n    def _resize_column_buttonrelease_cb(self, event):\n        event.widget.unbind('<ButtonRelease-%d>' % event.num)\n        event.widget.unbind('<Motion>')\n\n\n    @property\n    def column_names(self):\n        return self._column_names\n\n    @property\n    def column_labels(self):\n        return tuple(self._labels)\n\n    @property\n    def listboxes(self):\n        return tuple(self._listboxes)\n\n\n    def _select(self, e):\n        i = e.widget.nearest(e.y)\n        self.selection_clear(0, 'end')\n        self.selection_set(i)\n        self.activate(i)\n        self.focus()\n\n    def _scroll(self, delta):\n        for lb in self._listboxes:\n            lb.yview_scroll(delta, 'unit')\n        return 'break'\n\n    def _pagesize(self):\n        return int(self.index('@0,1000000')) - int(self.index('@0,0'))\n\n\n    def select(self, index=None, delta=None, see=True):\n        if (index is not None) and (delta is not None):\n            raise ValueError('specify index or delta, but not both')\n\n        if delta is not None:\n            if len(self.curselection()) == 0:\n                index = -1 + delta\n            else:\n                index = int(self.curselection()[0]) + delta\n\n        self.selection_clear(0, 'end')\n\n        if index is not None:\n            index = min(max(index, 0), self.size()-1)\n            self.selection_set(index)\n            if see: self.see(index)\n\n\n    def configure(self, cnf={}, **kw):\n        cnf = dict(list(cnf.items()) + list(kw.items()))\n        for (key, val) in list(cnf.items()):\n            if key.startswith('label_') or key.startswith('label-'):\n                for label in self._labels:\n                    label.configure({key[6:]: val})\n            elif key.startswith('listbox_') or key.startswith('listbox-'):\n                for listbox in self._listboxes:\n                    listbox.configure({key[8:]: val})\n            else:\n                Frame.configure(self, {key:val})\n\n    def __setitem__(self, key, val):\n        self.configure({key:val})\n\n    def rowconfigure(self, row_index, cnf={}, **kw):\n        for lb in self._listboxes: lb.itemconfigure(row_index, cnf, **kw)\n\n    def columnconfigure(self, col_index, cnf={}, **kw):\n        lb = self._listboxes[col_index]\n\n        cnf = dict(list(cnf.items()) + list(kw.items()))\n        for (key, val) in list(cnf.items()):\n            if key in ('background', 'bg', 'foreground', 'fg',\n                       'selectbackground', 'selectforeground'):\n                for i in range(lb.size()): lb.itemconfigure(i, {key:val})\n            else:\n                lb.configure({key:val})\n\n    def itemconfigure(self, row_index, col_index, cnf=None, **kw):\n        lb = self._listboxes[col_index]\n        return lb.itemconfigure(row_index, cnf, **kw)\n\n\n    def insert(self, index, *rows):\n        for elt in rows:\n            if len(elt) != len(self._column_names):\n                raise ValueError('rows should be tuples whose length '\n                                 'is equal to the number of columns')\n        for (lb,elts) in zip(self._listboxes, list(zip(*rows))):\n            lb.insert(index, *elts)\n\n    def get(self, first, last=None):\n        values = [lb.get(first, last) for lb in self._listboxes]\n        if last:\n            return [tuple(row) for row in zip(*values)]\n        else:\n            return tuple(values)\n\n    def bbox(self, row, col):\n        dx, dy, _, _ = self.grid_bbox(row=0, column=col)\n        x, y, w, h = self._listboxes[col].bbox(row)\n        return int(x)+int(dx), int(y)+int(dy), int(w), int(h)\n\n\n    def hide_column(self, col_index):\n        if self._labels:\n            self._labels[col_index].grid_forget()\n        self.listboxes[col_index].grid_forget()\n        self.grid_columnconfigure(col_index, weight=0)\n\n    def show_column(self, col_index):\n        weight = self._column_weights[col_index]\n        if self._labels:\n            self._labels[col_index].grid(column=col_index, row=0,\n                                         sticky='news', padx=0, pady=0)\n        self._listboxes[col_index].grid(column=col_index, row=1,\n                                        sticky='news', padx=0, pady=0)\n        self.grid_columnconfigure(col_index, weight=weight)\n\n\n    def bind_to_labels(self, sequence=None, func=None, add=None):\n        return [label.bind(sequence, func, add)\n                for label in self.column_labels]\n\n    def bind_to_listboxes(self, sequence=None, func=None, add=None):\n        for listbox in self.listboxes:\n            listbox.bind(sequence, func, add)\n\n    def bind_to_columns(self, sequence=None, func=None, add=None):\n        return (self.bind_to_labels(sequence, func, add) +\n                self.bind_to_listboxes(sequence, func, add))\n\n\n    def curselection(self, *args, **kwargs):\n        return self._listboxes[0].curselection(*args, **kwargs)\n    def selection_includes(self, *args, **kwargs):\n        return self._listboxes[0].selection_includes(*args, **kwargs)\n    def itemcget(self, *args, **kwargs):\n        return self._listboxes[0].itemcget(*args, **kwargs)\n    def size(self, *args, **kwargs):\n        return self._listboxes[0].size(*args, **kwargs)\n    def index(self, *args, **kwargs):\n        return self._listboxes[0].index(*args, **kwargs)\n    def nearest(self, *args, **kwargs):\n        return self._listboxes[0].nearest(*args, **kwargs)\n\n    def activate(self, *args, **kwargs):\n        for lb in self._listboxes: lb.activate(*args, **kwargs)\n    def delete(self, *args, **kwargs):\n        for lb in self._listboxes: lb.delete(*args, **kwargs)\n    def scan_mark(self, *args, **kwargs):\n        for lb in self._listboxes: lb.scan_mark(*args, **kwargs)\n    def scan_dragto(self, *args, **kwargs):\n        for lb in self._listboxes: lb.scan_dragto(*args, **kwargs)\n    def see(self, *args, **kwargs):\n        for lb in self._listboxes: lb.see(*args, **kwargs)\n    def selection_anchor(self, *args, **kwargs):\n        for lb in self._listboxes: lb.selection_anchor(*args, **kwargs)\n    def selection_clear(self, *args, **kwargs):\n        for lb in self._listboxes: lb.selection_clear(*args, **kwargs)\n    def selection_set(self, *args, **kwargs):\n        for lb in self._listboxes: lb.selection_set(*args, **kwargs)\n    def yview(self, *args, **kwargs):\n        for lb in self._listboxes: v = lb.yview(*args, **kwargs)\n        return v # if called with no arguments\n    def yview_moveto(self, *args, **kwargs):\n        for lb in self._listboxes: lb.yview_moveto(*args, **kwargs)\n    def yview_scroll(self, *args, **kwargs):\n        for lb in self._listboxes: lb.yview_scroll(*args, **kwargs)\n\n\n    itemconfig = itemconfigure\n    rowconfig = rowconfigure\n    columnconfig = columnconfigure\n    select_anchor = selection_anchor\n    select_clear = selection_clear\n    select_includes = selection_includes\n    select_set = selection_set\n\n\n\nclass Table(object):\n    def __init__(self, master, column_names, rows=None,\n                 column_weights=None,\n                 scrollbar=True, click_to_sort=True,\n                 reprfunc=None, cnf={}, **kw):\n        self._num_columns = len(column_names)\n        self._reprfunc = reprfunc\n        self._frame = Frame(master)\n\n        self._column_name_to_index = dict((c,i) for (i,c) in\n                                          enumerate(column_names))\n\n        if rows is None: self._rows = []\n        else: self._rows = [[v for v in row] for row in rows]\n        for row in self._rows: self._checkrow(row)\n\n        self._mlb = MultiListbox(self._frame, column_names,\n                                 column_weights, cnf, **kw)\n        self._mlb.pack(side='left', expand=True, fill='both')\n\n        if scrollbar:\n            sb = Scrollbar(self._frame, orient='vertical',\n                           command=self._mlb.yview)\n            self._mlb.listboxes[0]['yscrollcommand'] = sb.set\n            sb.pack(side='right', fill='y')\n            self._scrollbar = sb\n\n        self._sortkey = None\n        if click_to_sort:\n            for i, l in enumerate(self._mlb.column_labels):\n                l.bind('<Button-1>', self._sort)\n\n        self._fill_table()\n\n\n    def pack(self, *args, **kwargs):\n        Insert a new row into the table, so that its row index will be\n        ``row_index``.  If the table contains any rows whose row index\n        is greater than or equal to ``row_index``, then they will be\n        shifted down.\n\n        :param rowvalue: A tuple of cell values, one for each column\n            in the new row.\n        \"\"\"\n        self._checkrow(rowvalue)\n        self._rows.insert(row_index, rowvalue)\n        if self._reprfunc is not None:\n            rowvalue = [self._reprfunc(row_index,j,v)\n                        for (j,v) in enumerate(rowvalue)]\n        self._mlb.insert(row_index, rowvalue)\n        if self._DEBUG: self._check_table_vs_mlb()\n\n    def extend(self, rowvalues):\n        \"\"\"\n        Add new rows at the end of the table.\n\n        :param rowvalues: A list of row values used to initialze the\n            table.  Each row value should be a tuple of cell values,\n            one for each column in the row.\n        \"\"\"\n        for rowvalue in rowvalues: self.append(rowvalue)\n        if self._DEBUG: self._check_table_vs_mlb()\n\n    def append(self, rowvalue):\n        \"\"\"\n        Add a new row to the end of the table.\n\n        :param rowvalue: A tuple of cell values, one for each column\n            in the new row.\n        \"\"\"\n        self.insert(len(self._rows), rowvalue)\n        if self._DEBUG: self._check_table_vs_mlb()\n\n    def clear(self):\n        \"\"\"\n        Delete all rows in this table.\n        \"\"\"\n        self._rows = []\n        self._mlb.delete(0, 'end')\n        if self._DEBUG: self._check_table_vs_mlb()\n\n    def __getitem__(self, index):\n        \"\"\"\n        Return the value of a row or a cell in this table.  If\n        ``index`` is an integer, then the row value for the ``index``th\n        row.  This row value consists of a tuple of cell values, one\n        for each column in the row.  If ``index`` is a tuple of two\n        integers, ``(i,j)``, then return the value of the cell in the\n        ``i``th row and the ``j``th column.\n        \"\"\"\n        if isinstance(index, slice):\n            raise ValueError('Slicing not supported')\n        elif isinstance(index, tuple) and len(index)==2:\n            return self._rows[index[0]][self.column_index(index[1])]\n        else:\n            return tuple(self._rows[index])\n\n    def __setitem__(self, index, val):\n        \"\"\"\n        Replace the value of a row or a cell in this table with\n        ``val``.\n\n        If ``index`` is an integer, then ``val`` should be a row value\n        (i.e., a tuple of cell values, one for each column).  In this\n        case, the values of the ``index``th row of the table will be\n        replaced with the values in ``val``.\n\n        If ``index`` is a tuple of integers, ``(i,j)``, then replace the\n        value of the cell in the ``i``th row and ``j``th column with\n        ``val``.\n        \"\"\"\n        if isinstance(index, slice):\n            raise ValueError('Slicing not supported')\n\n\n        elif isinstance(index, tuple) and len(index)==2:\n            i, j = index[0], self.column_index(index[1])\n            config_cookie = self._save_config_info([i])\n            self._rows[i][j] = val\n            if self._reprfunc is not None:\n                val = self._reprfunc(i, j, val)\n            self._mlb.listboxes[j].insert(i, val)\n            self._mlb.listboxes[j].delete(i+1)\n            self._restore_config_info(config_cookie)\n\n        else:\n            config_cookie = self._save_config_info([index])\n            self._checkrow(val)\n            self._rows[index] = list(val)\n            if self._reprfunc is not None:\n                val = [self._reprfunc(index,j,v) for (j,v) in enumerate(val)]\n            self._mlb.insert(index, val)\n            self._mlb.delete(index+1)\n            self._restore_config_info(config_cookie)\n\n    def __delitem__(self, row_index):\n        \"\"\"\n        Delete the ``row_index``th row from this table.\n        \"\"\"\n        if isinstance(row_index, slice):\n            raise ValueError('Slicing not supported')\n        if isinstance(row_index, tuple) and len(row_index)==2:\n            raise ValueError('Cannot delete a single cell!')\n        del self._rows[row_index]\n        self._mlb.delete(row_index)\n        if self._DEBUG: self._check_table_vs_mlb()\n\n    def __len__(self):\n        \"\"\"\n        :return: the number of rows in this table.\n        \"\"\"\n        return len(self._rows)\n\n    def _checkrow(self, rowvalue):\n        \"\"\"\n        Helper function: check that a given row value has the correct\n        number of elements; and if not, raise an exception.\n        \"\"\"\n        if len(rowvalue) != self._num_columns:\n            raise ValueError('Row %r has %d columns; expected %d' %\n                             (rowvalue, len(rowvalue), self._num_columns))\n\n\n    @property\n    def column_names(self):\n        return self._mlb.column_names\n\n    def column_index(self, i):\n        \"\"\"\n        If ``i`` is a valid column index integer, then return it as is.\n        Otherwise, check if ``i`` is used as the name for any column;\n        if so, return that column's index.  Otherwise, raise a\n        ``KeyError`` exception.\n        \"\"\"\n        if isinstance(i, int) and 0 <= i < self._num_columns:\n            return i\n        else:\n            return self._column_name_to_index[i]\n\n    def hide_column(self, column_index):\n        self._mlb.hide_column(self.column_index(column_index))\n\n    def show_column(self, column_index):\n        self._mlb.show_column(self.column_index(column_index))\n\n\n    def selected_row(self):\n        \"\"\"\n        Return the index of the currently selected row, or None if\n        no row is selected.  To get the row value itself, use\n        ``table[table.selected_row()]``.\n        \"\"\"\n        sel = self._mlb.curselection()\n        if sel: return int(sel[0])\n        else: return None\n\n    def select(self, index=None, delta=None, see=True):\n        self._mlb.select(index, delta, see)\n\n\n    def sort_by(self, column_index, order='toggle'):\n        \"\"\"\n        Sort the rows in this table, using the specified column's\n        values as a sort key.\n\n        :param column_index: Specifies which column to sort, using\n            either a column index (int) or a column's label name\n            (str).\n\n        :param order: Specifies whether to sort the values in\n            ascending or descending order:\n\n              - ``'ascending'``: Sort from least to greatest.\n              - ``'descending'``: Sort from greatest to least.\n              - ``'toggle'``: If the most recent call to ``sort_by()``\n                sorted the table by the same column (``column_index``),\n                then reverse the rows; otherwise sort in ascending\n                order.\n        \"\"\"\n        if order not in ('ascending', 'descending', 'toggle'):\n            raise ValueError('sort_by(): order should be \"ascending\", '\n                             '\"descending\", or \"toggle\".')\n        column_index = self.column_index(column_index)\n        config_cookie = self._save_config_info(index_by_id=True)\n\n        if order == 'toggle' and column_index == self._sortkey:\n            self._rows.reverse()\n        else:\n            self._rows.sort(key=operator.itemgetter(column_index),\n                            reverse=(order=='descending'))\n            self._sortkey = column_index\n\n        self._fill_table()\n        self._restore_config_info(config_cookie, index_by_id=True, see=True)\n        if self._DEBUG: self._check_table_vs_mlb()\n\n    def _sort(self, event):\n        \"\"\"Event handler for clicking on a column label -- sort by\n        that column.\"\"\"\n        column_index = event.widget.column_index\n\n        if self._mlb._resize_column(event):\n            return 'continue'\n\n        else:\n            self.sort_by(column_index)\n            return 'continue'\n\n\n    def _fill_table(self, save_config=True):\n        \"\"\"\n        Re-draw the table from scratch, by clearing out the table's\n        multi-column listbox; and then filling it in with values from\n        ``self._rows``.  Note that any cell-, row-, or column-specific\n        color configuration that has been done will be lost.  The\n        selection will also be lost -- i.e., no row will be selected\n        after this call completes.\n        \"\"\"\n        self._mlb.delete(0, 'end')\n        for i, row in enumerate(self._rows):\n            if self._reprfunc is not None:\n                row = [self._reprfunc(i,j,v) for (j,v) in enumerate(row)]\n            self._mlb.insert('end', row)\n\n    def _get_itemconfig(self, r, c):\n        return dict( (k, self._mlb.itemconfig(r, c, k)[-1])\n                     for k in ('foreground', 'selectforeground',\n                               'background', 'selectbackground') )\n\n    def _save_config_info(self, row_indices=None, index_by_id=False):\n        \"\"\"\n        Return a 'cookie' containing information about which row is\n        selected, and what color configurations have been applied.\n        this information can the be re-applied to the table (after\n        making modifications) using ``_restore_config_info()``.  Color\n        configuration information will be saved for any rows in\n        ``row_indices``, or in the entire table, if\n        ``row_indices=None``.  If ``index_by_id=True``, the the cookie\n        will associate rows with their configuration information based\n        on the rows' python id.  This is useful when performing\n        operations that re-arrange the rows (e.g. ``sort``).  If\n        ``index_by_id=False``, then it is assumed that all rows will be\n        in the same order when ``_restore_config_info()`` is called.\n        \"\"\"\n        if row_indices is None:\n            row_indices = list(range(len(self._rows)))\n\n        selection = self.selected_row()\n        if index_by_id and selection is not None:\n            selection = id(self._rows[selection])\n\n        if index_by_id:\n            config = dict((id(self._rows[r]), [self._get_itemconfig(r, c)\n                                        for c in range(self._num_columns)])\n                          for r in row_indices)\n        else:\n            config = dict((r, [self._get_itemconfig(r, c)\n                               for c in range(self._num_columns)])\n                          for r in row_indices)\n\n\n        return selection, config\n\n    def _restore_config_info(self, cookie, index_by_id=False, see=False):\n        \"\"\"\n        Restore selection & color configuration information that was\n        saved using ``_save_config_info``.\n        \"\"\"\n        selection, config = cookie\n\n        if selection is None:\n            self._mlb.selection_clear(0, 'end')\n\n        if index_by_id:\n            for r, row in enumerate(self._rows):\n                if id(row) in config:\n                    for c in range(self._num_columns):\n                        self._mlb.itemconfigure(r, c, config[id(row)][c])\n                if id(row) == selection:\n                    self._mlb.select(r, see=see)\n        else:\n            if selection is not None:\n                self._mlb.select(selection, see=see)\n            for r in config:\n                for c in range(self._num_columns):\n                    self._mlb.itemconfigure(r, c, config[r][c])\n\n\n    _DEBUG = False\n    \"\"\"If true, then run ``_check_table_vs_mlb()`` after any operation\n       that modifies the table.\"\"\"\n\n    def _check_table_vs_mlb(self):\n        \"\"\"\n        Verify that the contents of the table's ``_rows`` variable match\n        the contents of its multi-listbox (``_mlb``).  This is just\n        included for debugging purposes, to make sure that the\n        list-modifying operations are working correctly.\n        \"\"\"\n        for col in self._mlb.listboxes:\n            assert len(self) == col.size()\n        for row in self:\n            assert len(row) == self._num_columns\n        assert self._num_columns == len(self._mlb.column_names)\n        for i, row in enumerate(self):\n            for j, cell in enumerate(row):\n                if self._reprfunc is not None:\n                    cell = self._reprfunc(i, j, cell)\n                assert self._mlb.get(i)[j] == cell\n\n\ndef demo():\n    root = Tk()\n    root.bind('<Control-q>', lambda e: root.destroy())\n\n    table = Table(root, 'Word Synset Hypernym Hyponym'.split(),\n                  column_weights=[0, 1, 1, 1],\n                  reprfunc=(lambda i,j,s: '  %s' % s))\n    table.pack(expand=True, fill='both')\n\n    from nltk.corpus import wordnet\n    from nltk.corpus import brown\n    for word, pos in sorted(set(brown.tagged_words()[:500])):\n        if pos[0] != 'N': continue\n        word = word.lower()\n        for synset in wordnet.synsets(word):\n            try:\n                hyper_def = synset.hypernyms()[0].definition()\n            except:\n                hyper_def = '*none*'\n            try:\n                hypo_def = synset.hypernyms()[0].definition()\n            except:\n                hypo_def = '*none*'\n            table.append([word,\n                          synset.definition(),\n                          hyper_def,\n                          hypo_def])\n\n    table.columnconfig('Word', background='#afa')\n    table.columnconfig('Synset', background='#efe')\n    table.columnconfig('Hypernym', background='#fee')\n    table.columnconfig('Hyponym', background='#ffe')\n    for row in range(len(table)):\n        for column in ('Hypernym', 'Hyponym'):\n            if table[row, column] == '*none*':\n                table.itemconfig(row, column, foreground='#666',\n                                 selectforeground='#666')\n    root.mainloop()\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\draw\\tree": [".py", "\n\nfrom six.moves.tkinter import IntVar, Menu, Tk\n\nfrom nltk.util import in_idle\nfrom nltk.tree import Tree\nfrom nltk.draw.util import (CanvasFrame, CanvasWidget, BoxWidget,\n                            TextWidget, ParenWidget, OvalWidget)\n\n\nclass TreeSegmentWidget(CanvasWidget):\n    def __init__(self, canvas, label, subtrees, **attribs):\n        self._label = label\n        self._subtrees = subtrees\n\n        self._horizontal = 0\n        self._roof = 0\n        self._xspace = 10\n        self._yspace = 15\n        self._ordered = False\n\n        self._lines = [canvas.create_line(0,0,0,0, fill='#006060')\n                       for c in subtrees]\n        self._polygon = canvas.create_polygon(0,0, fill='', state='hidden',\n                                              outline='#006060')\n\n        self._add_child_widget(label)\n        for subtree in subtrees:\n            self._add_child_widget(subtree)\n\n        self._managing = False\n\n        CanvasWidget.__init__(self, canvas, **attribs)\n\n    def __setitem__(self, attr, value):\n        canvas = self.canvas()\n        if attr == 'roof':\n            self._roof = value\n            if self._roof:\n                for l in self._lines: canvas.itemconfig(l, state='hidden')\n                canvas.itemconfig(self._polygon, state='normal')\n            else:\n                for l in self._lines: canvas.itemconfig(l, state='normal')\n                canvas.itemconfig(self._polygon, state='hidden')\n        elif attr == 'orientation':\n            if value == 'horizontal': self._horizontal = 1\n            elif value == 'vertical': self._horizontal = 0\n            else:\n                raise ValueError('orientation must be horizontal or vertical')\n        elif attr == 'color':\n            for l in self._lines: canvas.itemconfig(l, fill=value)\n            canvas.itemconfig(self._polygon, outline=value)\n        elif isinstance(attr, tuple) and attr[0] == 'color':\n            l = self._lines[int(attr[1])]\n            canvas.itemconfig(l, fill=value)\n        elif attr == 'fill':\n            canvas.itemconfig(self._polygon, fill=value)\n        elif attr == 'width':\n            canvas.itemconfig(self._polygon, {attr:value})\n            for l in self._lines: canvas.itemconfig(l, {attr:value})\n        elif attr in ('xspace', 'yspace'):\n            if attr == 'xspace': self._xspace = value\n            elif attr == 'yspace': self._yspace = value\n            self.update(self._label)\n        elif attr == 'ordered':\n            self._ordered = value\n        else:\n            CanvasWidget.__setitem__(self, attr, value)\n\n    def __getitem__(self, attr):\n        if attr == 'roof': return self._roof\n        elif attr == 'width':\n            return self.canvas().itemcget(self._polygon, attr)\n        elif attr == 'color':\n            return self.canvas().itemcget(self._polygon, 'outline')\n        elif isinstance(attr, tuple) and attr[0] == 'color':\n            l = self._lines[int(attr[1])]\n            return self.canvas().itemcget(l, 'fill')\n        elif attr == 'xspace': return self._xspace\n        elif attr == 'yspace': return self._yspace\n        elif attr == 'orientation':\n            if self._horizontal: return 'horizontal'\n            else: return 'vertical'\n        elif attr == 'ordered':\n            return self._ordered\n        else:\n            return CanvasWidget.__getitem__(self, attr)\n\n    def label(self):\n        return self._label\n\n    def subtrees(self):\n        return self._subtrees[:]\n\n    def set_label(self, label):\n        self._remove_child_widget(self._label)\n        self._add_child_widget(label)\n        self._label = label\n        self.update(self._label)\n\n    def replace_child(self, oldchild, newchild):\n        index = self._subtrees.index(oldchild)\n        self._subtrees[index] = newchild\n        self._remove_child_widget(oldchild)\n        self._add_child_widget(newchild)\n        self.update(newchild)\n\n    def remove_child(self, child):\n        index = self._subtrees.index(child)\n        del self._subtrees[index]\n        self._remove_child_widget(child)\n        self.canvas().delete(self._lines.pop())\n        self.update(self._label)\n\n    def insert_child(self, index, child):\n        canvas = self.canvas()\n        self._subtrees.insert(index, child)\n        self._add_child_widget(child)\n        self._lines.append(canvas.create_line(0,0,0,0, fill='#006060'))\n        self.update(self._label)\n\n\n    def _tags(self):\n        if self._roof:\n            return [self._polygon]\n        else:\n            return self._lines\n\n    def _subtree_top(self, child):\n        if isinstance(child, TreeSegmentWidget):\n            bbox = child.label().bbox()\n        else:\n            bbox = child.bbox()\n        if self._horizontal:\n            return (bbox[0], (bbox[1]+bbox[3])/2.0)\n        else:\n            return ((bbox[0]+bbox[2])/2.0, bbox[1])\n\n    def _node_bottom(self):\n        bbox = self._label.bbox()\n        if self._horizontal:\n            return (bbox[2], (bbox[1]+bbox[3])/2.0)\n        else:\n            return ((bbox[0]+bbox[2])/2.0, bbox[3])\n\n    def _update(self, child):\n        if len(self._subtrees) == 0: return\n        if self._label.bbox() is None: return # [XX] ???\n\n        if child is self._label: need_update = self._subtrees\n        else: need_update = [child]\n\n        if self._ordered and not self._managing:\n            need_update = self._maintain_order(child)\n\n        (nodex, nodey) = self._node_bottom()\n        (xmin, ymin, xmax, ymax) = self._subtrees[0].bbox()\n        for subtree in self._subtrees[1:]:\n            bbox = subtree.bbox()\n            xmin = min(xmin, bbox[0])\n            ymin = min(ymin, bbox[1])\n            xmax = max(xmax, bbox[2])\n            ymax = max(ymax, bbox[3])\n\n        if self._horizontal:\n            self.canvas().coords(self._polygon, nodex, nodey, xmin,\n                                 ymin, xmin, ymax, nodex, nodey)\n        else:\n            self.canvas().coords(self._polygon, nodex, nodey, xmin,\n                                 ymin, xmax, ymin, nodex, nodey)\n\n        for subtree in need_update:\n            (nodex, nodey) = self._node_bottom()\n            line = self._lines[self._subtrees.index(subtree)]\n            (subtreex, subtreey) = self._subtree_top(subtree)\n            self.canvas().coords(line, nodex, nodey, subtreex, subtreey)\n\n    def _maintain_order(self, child):\n        if self._horizontal:\n            return self._maintain_order_horizontal(child)\n        else:\n            return self._maintain_order_vertical(child)\n\n    def _maintain_order_vertical(self, child):\n        (left, top, right, bot) = child.bbox()\n\n        if child is self._label:\n            for subtree in self._subtrees:\n                (x1, y1, x2, y2) = subtree.bbox()\n                if bot+self._yspace > y1:\n                    subtree.move(0,bot+self._yspace-y1)\n\n            return self._subtrees\n        else:\n            moved = [child]\n            index = self._subtrees.index(child)\n\n            x = right + self._xspace\n            for i in range(index+1, len(self._subtrees)):\n                (x1, y1, x2, y2) = self._subtrees[i].bbox()\n                if x > x1:\n                    self._subtrees[i].move(x-x1, 0)\n                    x += x2-x1 + self._xspace\n                    moved.append(self._subtrees[i])\n\n            x = left - self._xspace\n            for i in range(index-1, -1, -1):\n                (x1, y1, x2, y2) = self._subtrees[i].bbox()\n                if x < x2:\n                    self._subtrees[i].move(x-x2, 0)\n                    x -= x2-x1 + self._xspace\n                    moved.append(self._subtrees[i])\n\n            (x1, y1, x2, y2) = self._label.bbox()\n            if y2 > top-self._yspace:\n                self._label.move(0, top-self._yspace-y2)\n                moved = self._subtrees\n\n        return moved\n\n    def _maintain_order_horizontal(self, child):\n        (left, top, right, bot) = child.bbox()\n\n        if child is self._label:\n            for subtree in self._subtrees:\n                (x1, y1, x2, y2) = subtree.bbox()\n                if right+self._xspace > x1:\n                    subtree.move(right+self._xspace-x1)\n\n            return self._subtrees\n        else:\n            moved = [child]\n            index = self._subtrees.index(child)\n\n            y = bot + self._yspace\n            for i in range(index+1, len(self._subtrees)):\n                (x1, y1, x2, y2) = self._subtrees[i].bbox()\n                if y > y1:\n                    self._subtrees[i].move(0, y-y1)\n                    y += y2-y1 + self._yspace\n                    moved.append(self._subtrees[i])\n\n            y = top - self._yspace\n            for i in range(index-1, -1, -1):\n                (x1, y1, x2, y2) = self._subtrees[i].bbox()\n                if y < y2:\n                    self._subtrees[i].move(0, y-y2)\n                    y -= y2-y1 + self._yspace\n                    moved.append(self._subtrees[i])\n\n            (x1, y1, x2, y2) = self._label.bbox()\n            if x2 > left-self._xspace:\n                self._label.move(left-self._xspace-x2, 0)\n                moved = self._subtrees\n\n        return moved\n\n    def _manage_horizontal(self):\n        (nodex, nodey) = self._node_bottom()\n\n        y = 20\n        for subtree in self._subtrees:\n            subtree_bbox = subtree.bbox()\n            dx = nodex - subtree_bbox[0] + self._xspace\n            dy = y - subtree_bbox[1]\n            subtree.move(dx, dy)\n            y += subtree_bbox[3] - subtree_bbox[1] + self._yspace\n\n        center = 0.0\n        for subtree in self._subtrees:\n            center += self._subtree_top(subtree)[1]\n        center /= len(self._subtrees)\n\n        for subtree in self._subtrees:\n            subtree.move(0, nodey-center)\n\n    def _manage_vertical(self):\n        (nodex, nodey) = self._node_bottom()\n\n        x = 0\n        for subtree in self._subtrees:\n            subtree_bbox = subtree.bbox()\n            dy = nodey - subtree_bbox[1] + self._yspace\n            dx = x - subtree_bbox[0]\n            subtree.move(dx, dy)\n            x += subtree_bbox[2] - subtree_bbox[0] + self._xspace\n\n        center = 0.0\n        for subtree in self._subtrees:\n            center += self._subtree_top(subtree)[0]/len(self._subtrees)\n\n        for subtree in self._subtrees:\n            subtree.move(nodex-center, 0)\n\n    def _manage(self):\n        self._managing = True\n        (nodex, nodey) = self._node_bottom()\n        if len(self._subtrees) == 0: return\n\n        if self._horizontal: self._manage_horizontal()\n        else: self._manage_vertical()\n\n        for subtree in self._subtrees:\n            self._update(subtree)\n\n        self._managing = False\n\n    def __repr__(self):\n        return '[TreeSeg %s: %s]' % (self._label, self._subtrees)\n\ndef _tree_to_treeseg(canvas, t, make_node, make_leaf,\n                     tree_attribs, node_attribs,\n                     leaf_attribs, loc_attribs):\n    if isinstance(t, Tree):\n        label = make_node(canvas, t.label(), **node_attribs)\n        subtrees = [_tree_to_treeseg(canvas, child, make_node, make_leaf,\n                                     tree_attribs, node_attribs,\n                                     leaf_attribs, loc_attribs)\n                    for child in t]\n        return TreeSegmentWidget(canvas, label, subtrees, **tree_attribs)\n    else:\n        return make_leaf(canvas, t, **leaf_attribs)\n\ndef tree_to_treesegment(canvas, t, make_node=TextWidget,\n                        make_leaf=TextWidget, **attribs):\n    tree_attribs = {}\n    node_attribs = {}\n    leaf_attribs = {}\n    loc_attribs = {}\n\n    for (key, value) in list(attribs.items()):\n        if key[:5] == 'tree_': tree_attribs[key[5:]] = value\n        elif key[:5] == 'node_': node_attribs[key[5:]] = value\n        elif key[:5] == 'leaf_': leaf_attribs[key[5:]] = value\n        elif key[:4] == 'loc_': loc_attribs[key[4:]] = value\n        else: raise ValueError('Bad attribute: %s' % key)\n    return _tree_to_treeseg(canvas, t, make_node, make_leaf,\n                                tree_attribs, node_attribs,\n                                leaf_attribs, loc_attribs)\n\n\nclass TreeWidget(CanvasWidget):\n    def __init__(self, canvas, t, make_node=TextWidget,\n                 make_leaf=TextWidget, **attribs):\n        self._make_node = make_node\n        self._make_leaf = make_leaf\n        self._tree = t\n\n        self._nodeattribs = {}\n        self._leafattribs = {}\n        self._locattribs = {'color': '#008000'}\n        self._line_color = '#008080'\n        self._line_width = 1\n        self._roof_color = '#008080'\n        self._roof_fill = '#c0c0c0'\n        self._shapeable = False\n        self._xspace = 10\n        self._yspace = 10\n        self._orientation = 'vertical'\n        self._ordered = False\n\n        self._keys = {} # treeseg -> key\n        self._expanded_trees = {}\n        self._collapsed_trees = {}\n        self._nodes = []\n        self._leaves = []\n        self._make_collapsed_trees(canvas, t, ())\n        self._treeseg = self._make_expanded_tree(canvas, t, ())\n        self._add_child_widget(self._treeseg)\n\n        CanvasWidget.__init__(self, canvas, **attribs)\n\n    def expanded_tree(self, *path_to_tree):\n        return self._expanded_trees[path_to_tree]\n\n    def collapsed_tree(self, *path_to_tree):\n        return self._collapsed_trees[path_to_tree]\n\n    def bind_click_trees(self, callback, button=1):\n        for tseg in list(self._expanded_trees.values()):\n            tseg.bind_click(callback, button)\n        for tseg in list(self._collapsed_trees.values()):\n            tseg.bind_click(callback, button)\n\n    def bind_drag_trees(self, callback, button=1):\n        for tseg in list(self._expanded_trees.values()):\n            tseg.bind_drag(callback, button)\n        for tseg in list(self._collapsed_trees.values()):\n            tseg.bind_drag(callback, button)\n\n    def bind_click_leaves(self, callback, button=1):\n        for leaf in self._leaves: leaf.bind_click(callback, button)\n        for leaf in self._leaves: leaf.bind_click(callback, button)\n\n    def bind_drag_leaves(self, callback, button=1):\n        for leaf in self._leaves: leaf.bind_drag(callback, button)\n        for leaf in self._leaves: leaf.bind_drag(callback, button)\n\n    def bind_click_nodes(self, callback, button=1):\n        for node in self._nodes: node.bind_click(callback, button)\n        for node in self._nodes: node.bind_click(callback, button)\n\n    def bind_drag_nodes(self, callback, button=1):\n        for node in self._nodes: node.bind_drag(callback, button)\n        for node in self._nodes: node.bind_drag(callback, button)\n\n    def _make_collapsed_trees(self, canvas, t, key):\n        if not isinstance(t, Tree): return\n        make_node = self._make_node\n        make_leaf = self._make_leaf\n\n        node = make_node(canvas, t.label(), **self._nodeattribs)\n        self._nodes.append(node)\n        leaves = [make_leaf(canvas, l, **self._leafattribs)\n                  for l in t.leaves()]\n        self._leaves += leaves\n        treeseg = TreeSegmentWidget(canvas, node, leaves, roof=1,\n                                    color=self._roof_color,\n                                    fill=self._roof_fill,\n                                    width=self._line_width)\n\n        self._collapsed_trees[key] = treeseg\n        self._keys[treeseg] = key\n        treeseg.hide()\n\n        for i in range(len(t)):\n            child = t[i]\n            self._make_collapsed_trees(canvas, child, key + (i,))\n\n    def _make_expanded_tree(self, canvas, t, key):\n        make_node = self._make_node\n        make_leaf = self._make_leaf\n\n        if isinstance(t, Tree):\n            node = make_node(canvas, t.label(), **self._nodeattribs)\n            self._nodes.append(node)\n            children = t\n            subtrees = [self._make_expanded_tree(canvas, children[i], key+(i,))\n                        for i in range(len(children))]\n            treeseg = TreeSegmentWidget(canvas, node, subtrees,\n                                        color=self._line_color,\n                                        width=self._line_width)\n            self._expanded_trees[key] = treeseg\n            self._keys[treeseg] = key\n            return treeseg\n        else:\n            leaf = make_leaf(canvas, t, **self._leafattribs)\n            self._leaves.append(leaf)\n            return leaf\n\n    def __setitem__(self, attr, value):\n        if attr[:5] == 'node_':\n            for node in self._nodes: node[attr[5:]] = value\n        elif attr[:5] == 'leaf_':\n            for leaf in self._leaves: leaf[attr[5:]] = value\n        elif attr == 'line_color':\n            self._line_color = value\n            for tseg in list(self._expanded_trees.values()): tseg['color'] = value\n        elif attr == 'line_width':\n            self._line_width = value\n            for tseg in list(self._expanded_trees.values()): tseg['width'] = value\n            for tseg in list(self._collapsed_trees.values()): tseg['width'] = value\n        elif attr == 'roof_color':\n            self._roof_color = value\n            for tseg in list(self._collapsed_trees.values()): tseg['color'] = value\n        elif attr == 'roof_fill':\n            self._roof_fill = value\n            for tseg in list(self._collapsed_trees.values()): tseg['fill'] = value\n        elif attr == 'shapeable':\n            self._shapeable = value\n            for tseg in list(self._expanded_trees.values()):\n                tseg['draggable'] = value\n            for tseg in list(self._collapsed_trees.values()):\n                tseg['draggable'] = value\n            for leaf in self._leaves: leaf['draggable'] = value\n        elif attr == 'xspace':\n            self._xspace = value\n            for tseg in list(self._expanded_trees.values()):\n                tseg['xspace'] = value\n            for tseg in list(self._collapsed_trees.values()):\n                tseg['xspace'] = value\n            self.manage()\n        elif attr == 'yspace':\n            self._yspace = value\n            for tseg in list(self._expanded_trees.values()):\n                tseg['yspace'] = value\n            for tseg in list(self._collapsed_trees.values()):\n                tseg['yspace'] = value\n            self.manage()\n        elif attr == 'orientation':\n            self._orientation = value\n            for tseg in list(self._expanded_trees.values()):\n                tseg['orientation'] = value\n            for tseg in list(self._collapsed_trees.values()):\n                tseg['orientation'] = value\n            self.manage()\n        elif attr == 'ordered':\n            self._ordered = value\n            for tseg in list(self._expanded_trees.values()):\n                tseg['ordered'] = value\n            for tseg in list(self._collapsed_trees.values()):\n                tseg['ordered'] = value\n        else: CanvasWidget.__setitem__(self, attr, value)\n\n    def __getitem__(self, attr):\n        if attr[:5] == 'node_':\n            return self._nodeattribs.get(attr[5:], None)\n        elif attr[:5] == 'leaf_':\n            return self._leafattribs.get(attr[5:], None)\n        elif attr[:4] == 'loc_':\n            return self._locattribs.get(attr[4:], None)\n        elif attr == 'line_color': return self._line_color\n        elif attr == 'line_width': return self._line_width\n        elif attr == 'roof_color': return self._roof_color\n        elif attr == 'roof_fill': return self._roof_fill\n        elif attr == 'shapeable': return self._shapeable\n        elif attr == 'xspace': return self._xspace\n        elif attr == 'yspace': return self._yspace\n        elif attr == 'orientation': return self._orientation\n        else: return CanvasWidget.__getitem__(self, attr)\n\n    def _tags(self): return []\n\n    def _manage(self):\n        segs = list(self._expanded_trees.values()) + list(self._collapsed_trees.values())\n        for tseg in segs:\n            if tseg.hidden():\n                tseg.show()\n                tseg.manage()\n                tseg.hide()\n\n    def toggle_collapsed(self, treeseg):\n        old_treeseg = treeseg\n        if old_treeseg['roof']:\n            new_treeseg = self._expanded_trees[self._keys[old_treeseg]]\n        else:\n            new_treeseg = self._collapsed_trees[self._keys[old_treeseg]]\n\n        if old_treeseg.parent() is self:\n            self._remove_child_widget(old_treeseg)\n            self._add_child_widget(new_treeseg)\n            self._treeseg = new_treeseg\n        else:\n            old_treeseg.parent().replace_child(old_treeseg, new_treeseg)\n\n        new_treeseg.show()\n        (newx, newy) = new_treeseg.label().bbox()[:2]\n        (oldx, oldy) = old_treeseg.label().bbox()[:2]\n        new_treeseg.move(oldx-newx, oldy-newy)\n\n        old_treeseg.hide()\n\n        new_treeseg.parent().update(new_treeseg)\n\n\nclass TreeView(object):\n    def __init__(self, *trees):\n        from math import sqrt, ceil\n\n        self._trees = trees\n\n        self._top = Tk()\n        self._top.title('NLTK')\n        self._top.bind('<Control-x>', self.destroy)\n        self._top.bind('<Control-q>', self.destroy)\n\n        cf = self._cframe = CanvasFrame(self._top)\n        self._top.bind('<Control-p>', self._cframe.print_to_file)\n\n        self._size = IntVar(self._top)\n        self._size.set(12)\n        bold = ('helvetica', -self._size.get(), 'bold')\n        helv = ('helvetica', -self._size.get())\n\n        self._width = int(ceil(sqrt(len(trees))))\n        self._widgets = []\n        for i in range(len(trees)):\n            widget = TreeWidget(cf.canvas(), trees[i], node_font=bold,\n                                leaf_color='#008040', node_color='#004080',\n                                roof_color='#004040', roof_fill='white',\n                                line_color='#004040', draggable=1,\n                                leaf_font=helv)\n            widget.bind_click_trees(widget.toggle_collapsed)\n            self._widgets.append(widget)\n            cf.add_widget(widget, 0, 0)\n\n        self._layout()\n        self._cframe.pack(expand=1, fill='both')\n        self._init_menubar()\n\n    def _layout(self):\n        i = x = y = ymax = 0\n        width = self._width\n        for i in range(len(self._widgets)):\n            widget = self._widgets[i]\n            (oldx, oldy) = widget.bbox()[:2]\n            if i % width == 0:\n                y = ymax\n                x = 0\n            widget.move(x-oldx, y-oldy)\n            x = widget.bbox()[2] + 10\n            ymax = max(ymax, widget.bbox()[3] + 10)\n\n    def _init_menubar(self):\n        menubar = Menu(self._top)\n\n        filemenu = Menu(menubar, tearoff=0)\n        filemenu.add_command(label='Print to Postscript', underline=0,\n                             command=self._cframe.print_to_file,\n                             accelerator='Ctrl-p')\n        filemenu.add_command(label='Exit', underline=1,\n                             command=self.destroy, accelerator='Ctrl-x')\n        menubar.add_cascade(label='File', underline=0, menu=filemenu)\n\n        zoommenu = Menu(menubar, tearoff=0)\n        zoommenu.add_radiobutton(label='Tiny', variable=self._size,\n                                 underline=0, value=10, command=self.resize)\n        zoommenu.add_radiobutton(label='Small', variable=self._size,\n                                 underline=0, value=12, command=self.resize)\n        zoommenu.add_radiobutton(label='Medium', variable=self._size,\n                                 underline=0, value=14, command=self.resize)\n        zoommenu.add_radiobutton(label='Large', variable=self._size,\n                                 underline=0, value=28, command=self.resize)\n        zoommenu.add_radiobutton(label='Huge', variable=self._size,\n                                 underline=0, value=50, command=self.resize)\n        menubar.add_cascade(label='Zoom', underline=0, menu=zoommenu)\n\n        self._top.config(menu=menubar)\n\n    def resize(self, *e):\n        bold = ('helvetica', -self._size.get(), 'bold')\n        helv = ('helvetica', -self._size.get())\n        xspace = self._size.get()\n        yspace = self._size.get()\n        for widget in self._widgets:\n            widget['node_font'] = bold\n            widget['leaf_font'] = helv\n            widget['xspace'] = xspace\n            widget['yspace'] = yspace\n            if self._size.get() < 20: widget['line_width'] = 1\n            elif self._size.get() < 30: widget['line_width'] = 2\n            else: widget['line_width'] = 3\n        self._layout()\n\n    def destroy(self, *e):\n        if self._top is None: return\n        self._top.destroy()\n        self._top = None\n\n    def mainloop(self, *args, **kwargs):\n        if in_idle(): return\n        self._top.mainloop(*args, **kwargs)\n\ndef draw_trees(*trees):\n    TreeView(*trees).mainloop()\n    return\n\n\ndef demo():\n    import random\n    def fill(cw):\n        cw['fill'] = '#%06d' % random.randint(0,999999)\n\n    cf = CanvasFrame(width=550, height=450, closeenough=2)\n\n    t = Tree.fromstring('''\n    (S (NP the very big cat)\n       (VP (Adv sorta) (V saw) (NP (Det the) (N dog))))''')\n\n    tc = TreeWidget(cf.canvas(), t, draggable=1,\n                    node_font=('helvetica', -14, 'bold'),\n                    leaf_font=('helvetica', -12, 'italic'),\n                    roof_fill='white', roof_color='black',\n                    leaf_color='green4', node_color='blue2')\n    cf.add_widget(tc,10,10)\n\n    def boxit(canvas, text):\n        big = ('helvetica', -16, 'bold')\n        return BoxWidget(canvas, TextWidget(canvas, text,\n                                            font=big), fill='green')\n    def ovalit(canvas, text):\n        return OvalWidget(canvas, TextWidget(canvas, text),\n                          fill='cyan')\n\n    treetok = Tree.fromstring('(S (NP this tree) (VP (V is) (AdjP shapeable)))')\n    tc2 = TreeWidget(cf.canvas(), treetok, boxit, ovalit, shapeable=1)\n\n    def color(node):\n        node['color'] = '#%04d00' % random.randint(0,9999)\n    def color2(treeseg):\n        treeseg.label()['fill'] = '#%06d' % random.randint(0,9999)\n        treeseg.label().child()['color'] = 'white'\n\n    tc.bind_click_trees(tc.toggle_collapsed)\n    tc2.bind_click_trees(tc2.toggle_collapsed)\n    tc.bind_click_nodes(color, 3)\n    tc2.expanded_tree(1).bind_click(color2, 3)\n    tc2.expanded_tree().bind_click(color2, 3)\n\n    paren = ParenWidget(cf.canvas(), tc2)\n    cf.add_widget(paren, tc.bbox()[2]+10, 10)\n\n    tree3 = Tree.fromstring('''\n    (S (NP this tree) (AUX was)\n       (VP (V built) (PP (P with) (NP (N tree_to_treesegment)))))''')\n    tc3 = tree_to_treesegment(cf.canvas(), tree3, tree_color='green4',\n                              tree_xspace=2, tree_width=2)\n    tc3['draggable'] = 1\n    cf.add_widget(tc3, 10, tc.bbox()[3]+10)\n\n    def orientswitch(treewidget):\n        if treewidget['orientation'] == 'horizontal':\n            treewidget.expanded_tree(1,1).subtrees()[0].set_text('vertical')\n            treewidget.collapsed_tree(1,1).subtrees()[0].set_text('vertical')\n            treewidget.collapsed_tree(1).subtrees()[1].set_text('vertical')\n            treewidget.collapsed_tree().subtrees()[3].set_text('vertical')\n            treewidget['orientation'] = 'vertical'\n        else:\n            treewidget.expanded_tree(1,1).subtrees()[0].set_text('horizontal')\n            treewidget.collapsed_tree(1,1).subtrees()[0].set_text('horizontal')\n            treewidget.collapsed_tree(1).subtrees()[1].set_text('horizontal')\n            treewidget.collapsed_tree().subtrees()[3].set_text('horizontal')\n            treewidget['orientation'] = 'horizontal'\n\n    text = \"\"\"\nTry clicking, right clicking, and dragging\ndifferent elements of each of the trees.\nThe top-left tree is a TreeWidget built from\na Tree.  The top-right is a TreeWidget built\nfrom a Tree, using non-default widget\nconstructors for the nodes & leaves (BoxWidget\nand OvalWidget).  The bottom-left tree is\nbuilt from tree_to_treesegment.\"\"\"\n    twidget = TextWidget(cf.canvas(), text.strip())\n    textbox = BoxWidget(cf.canvas(), twidget, fill='white', draggable=1)\n    cf.add_widget(textbox, tc3.bbox()[2]+10, tc2.bbox()[3]+10)\n\n    tree4 = Tree.fromstring('(S (NP this tree) (VP (V is) (Adj horizontal)))')\n    tc4 = TreeWidget(cf.canvas(), tree4, draggable=1,\n                     line_color='brown2', roof_color='brown2',\n                     node_font=('helvetica', -12, 'bold'),\n                     node_color='brown4', orientation='horizontal')\n    tc4.manage()\n    cf.add_widget(tc4, tc3.bbox()[2]+10, textbox.bbox()[3]+10)\n    tc4.bind_click(orientswitch)\n    tc4.bind_click_trees(tc4.toggle_collapsed, 3)\n\n    cf.mainloop()\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\draw\\util": [".py", "\nfrom abc import ABCMeta, abstractmethod\nfrom six import add_metaclass\nfrom six.moves.tkinter import (Button, Canvas, Entry, Frame, Label, Menu,\n                               Menubutton, Scrollbar, StringVar, Text, Tk,\n                               Toplevel, Widget, RAISED)\nfrom six.moves.tkinter_tkfiledialog import asksaveasfilename\n\nfrom nltk.util import in_idle\n\n\n\n@add_metaclass(ABCMeta)\nclass CanvasWidget(object):\n    def __init__(self, canvas, parent=None, **attribs):\n        if self.__class__ == CanvasWidget:\n            raise TypeError('CanvasWidget is an abstract base class')\n\n        if not isinstance(canvas, Canvas):\n            raise TypeError('Expected a canvas!')\n\n        self.__canvas = canvas\n        self.__parent = parent\n\n        if not hasattr(self, '_CanvasWidget__children'): self.__children = []\n\n        self.__hidden = 0\n\n        self.__updating = 0\n\n        self.__press = None\n        self.__drag_x = self.__drag_y = 0\n        self.__callbacks = {}\n        self.__draggable = 0\n\n        for (attr, value) in list(attribs.items()): self[attr] = value\n\n        self._manage()\n\n        for tag in self._tags():\n            self.__canvas.tag_bind(tag, '<ButtonPress-1>',\n                                   self.__press_cb)\n            self.__canvas.tag_bind(tag, '<ButtonPress-2>',\n                                   self.__press_cb)\n            self.__canvas.tag_bind(tag, '<ButtonPress-3>',\n                                   self.__press_cb)\n\n\n    def bbox(self):\n        if self.__hidden: return (0,0,0,0)\n        if len(self.tags()) == 0: raise ValueError('No tags')\n        return self.__canvas.bbox(*self.tags())\n\n    def width(self):\n        if len(self.tags()) == 0: raise ValueError('No tags')\n        bbox = self.__canvas.bbox(*self.tags())\n        return bbox[2]-bbox[0]\n\n    def height(self):\n        if len(self.tags()) == 0: raise ValueError('No tags')\n        bbox = self.__canvas.bbox(*self.tags())\n        return bbox[3]-bbox[1]\n\n    def parent(self):\n        return self.__parent\n\n    def child_widgets(self):\n        return self.__children\n\n    def canvas(self):\n        return self.__canvas\n\n    def move(self, dx, dy):\n        if dx == dy == 0: return\n        for tag in self.tags():\n            self.__canvas.move(tag, dx, dy)\n        if self.__parent: self.__parent.update(self)\n\n    def moveto(self, x, y, anchor='NW'):\n        x1,y1,x2,y2 = self.bbox()\n        if anchor == 'NW': self.move(x-x1,        y-y1)\n        if anchor == 'N':  self.move(x-x1/2-x2/2, y-y1)\n        if anchor == 'NE': self.move(x-x2,        y-y1)\n        if anchor == 'E':  self.move(x-x2,        y-y1/2-y2/2)\n        if anchor == 'SE': self.move(x-x2,        y-y2)\n        if anchor == 'S':  self.move(x-x1/2-x2/2, y-y2)\n        if anchor == 'SW': self.move(x-x1,        y-y2)\n        if anchor == 'W':  self.move(x-x1,        y-y1/2-y2/2)\n\n    def destroy(self):\n        if self.__parent is not None:\n            self.__parent.destroy()\n            return\n\n        for tag in self.tags():\n            self.__canvas.tag_unbind(tag, '<ButtonPress-1>')\n            self.__canvas.tag_unbind(tag, '<ButtonPress-2>')\n            self.__canvas.tag_unbind(tag, '<ButtonPress-3>')\n        self.__canvas.delete(*self.tags())\n        self.__canvas = None\n\n    def update(self, child):\n        if self.__hidden or child.__hidden: return\n        if self.__updating: return\n        self.__updating = 1\n\n        self._update(child)\n\n        if self.__parent: self.__parent.update(self)\n\n        self.__updating = 0\n\n    def manage(self):\n        if self.__hidden: return\n        for child in self.__children: child.manage()\n        self._manage()\n\n    def tags(self):\n        if self.__canvas is None:\n            raise ValueError('Attempt to access a destroyed canvas widget')\n        tags = []\n        tags += self._tags()\n        for child in self.__children:\n            tags += child.tags()\n        return tags\n\n    def __setitem__(self, attr, value):\n        if attr == 'draggable':\n            self.__draggable = value\n        else:\n            raise ValueError('Unknown attribute %r' % attr)\n\n    def __getitem__(self, attr):\n        if attr == 'draggable':\n            return self.__draggable\n        else:\n            raise ValueError('Unknown attribute %r' % attr)\n\n    def __repr__(self):\n        return '<%s>' % self.__class__.__name__\n\n    def hide(self):\n        self.__hidden = 1\n        for tag in self.tags():\n            self.__canvas.itemconfig(tag, state='hidden')\n\n    def show(self):\n        self.__hidden = 0\n        for tag in self.tags():\n            self.__canvas.itemconfig(tag, state='normal')\n\n    def hidden(self):\n        return self.__hidden\n\n\n    def bind_click(self, callback, button=1):\n        self.__callbacks[button] = callback\n\n    def bind_drag(self, callback):\n        self.__draggable = 1\n        self.__callbacks['drag'] = callback\n\n    def unbind_click(self, button=1):\n        try: del self.__callbacks[button]\n        except: pass\n\n    def unbind_drag(self):\n        try: del self.__callbacks['drag']\n        except: pass\n\n\n    def __press_cb(self, event):\n        if (self.__canvas.bind('<ButtonRelease-1>') or\n            self.__canvas.bind('<ButtonRelease-2>') or\n            self.__canvas.bind('<ButtonRelease-3>')):\n            return\n\n        self.__canvas.unbind('<Motion>')\n\n        self.__press = event\n\n        if event.num == 1:\n            widget = self\n            while widget is not None:\n                if widget['draggable']:\n                    widget.__start_drag(event)\n                    break\n                widget = widget.parent()\n\n        self.__canvas.bind('<ButtonRelease-%d>' % event.num,\n                          self.__release_cb)\n\n    def __start_drag(self, event):\n        self.__canvas.bind('<Motion>', self.__motion_cb)\n        self.__drag_x = event.x\n        self.__drag_y = event.y\n\n    def __motion_cb(self, event):\n        self.move(event.x-self.__drag_x, event.y-self.__drag_y)\n        self.__drag_x = event.x\n        self.__drag_y = event.y\n\n    def __release_cb(self, event):\n        self.__canvas.unbind('<ButtonRelease-%d>' % event.num)\n        self.__canvas.unbind('<Motion>')\n\n        if (event.time - self.__press.time < 100 and\n            abs(event.x-self.__press.x) + abs(event.y-self.__press.y) < 5):\n            if self.__draggable and event.num == 1:\n                self.move(self.__press.x - self.__drag_x,\n                          self.__press.y - self.__drag_y)\n            self.__click(event.num)\n        elif event.num == 1:\n            self.__drag()\n\n        self.__press = None\n\n    def __drag(self):\n        if self.__draggable:\n            if 'drag' in self.__callbacks:\n                cb = self.__callbacks['drag']\n                try:\n                    cb(self)\n                except:\n                    print('Error in drag callback for %r' % self)\n        elif self.__parent is not None:\n            self.__parent.__drag()\n\n    def __click(self, button):\n        if button in self.__callbacks:\n            cb = self.__callbacks[button]\n            cb(self)\n        elif self.__parent is not None:\n            self.__parent.__click(button)\n\n\n    def _add_child_widget(self, child):\n        if not hasattr(self, '_CanvasWidget__children'): self.__children = []\n        if child.__parent is not None:\n            raise ValueError('%s already has a parent', child)\n        child.__parent = self\n        self.__children.append(child)\n\n    def _remove_child_widget(self, child):\n        self.__children.remove(child)\n        child.__parent = None\n\n\n    @abstractmethod\n    def _tags(self):\n\n    def _manage(self):\n\n    def _update(self, child):\n\n\nclass TextWidget(CanvasWidget):\n    def __init__(self, canvas, text, **attribs):\n        self._text = text\n        self._tag = canvas.create_text(1, 1, text=text)\n        CanvasWidget.__init__(self, canvas, **attribs)\n\n    def __setitem__(self, attr, value):\n        if attr in ('color', 'font', 'justify', 'width'):\n            if attr == 'color': attr = 'fill'\n            self.canvas().itemconfig(self._tag, {attr:value})\n        else:\n            CanvasWidget.__setitem__(self, attr, value)\n\n    def __getitem__(self, attr):\n        if attr == 'width':\n            return int(self.canvas().itemcget(self._tag, attr))\n        elif attr in ('color', 'font', 'justify'):\n            if attr == 'color': attr = 'fill'\n            return self.canvas().itemcget(self._tag, attr)\n        else:\n            return CanvasWidget.__getitem__(self, attr)\n\n    def _tags(self): return [self._tag]\n\n    def text(self):\n        return self.canvas().itemcget(self._tag, 'TEXT')\n\n    def set_text(self, text):\n        self.canvas().itemconfig(self._tag, text=text)\n        if self.parent() is not None:\n            self.parent().update(self)\n\n    def __repr__(self):\n        return '[Text: %r]' % self._text\n\nclass SymbolWidget(TextWidget):\n    SYMBOLS = {'neg':'\\330', 'disj':'\\332', 'conj': '\\331',\n               'lambda': '\\154', 'merge': '\\304',\n               'forall': '\\042', 'exists': '\\044',\n               'subseteq': '\\315', 'subset': '\\314',\n               'notsubset': '\\313', 'emptyset': '\\306',\n               'imp': '\\336', 'rightarrow': chr(222), #'\\256',\n               'equal': '\\75', 'notequal': '\\271',\n               'intersection': '\\307', 'union': '\\310',\n               'epsilon': 'e',\n               }\n\n    def __init__(self, canvas, symbol, **attribs):\n        attribs['font'] = 'symbol'\n        TextWidget.__init__(self, canvas, '', **attribs)\n        self.set_symbol(symbol)\n\n    def symbol(self):\n        return self._symbol\n\n    def set_symbol(self, symbol):\n        if symbol not in SymbolWidget.SYMBOLS:\n            raise ValueError('Unknown symbol: %s' % symbol)\n        self._symbol = symbol\n        self.set_text(SymbolWidget.SYMBOLS[symbol])\n\n    def __repr__(self):\n        return '[Symbol: %r]' % self._symbol\n\n    @staticmethod\n    def symbolsheet(size=20):\n        top = Tk()\n        def destroy(e, top=top): top.destroy()\n        top.bind('q', destroy)\n        Button(top, text='Quit', command=top.destroy).pack(side='bottom')\n        text = Text(top, font=('helvetica', -size), width=20, height=30)\n        text.pack(side='left')\n        sb=Scrollbar(top, command=text.yview)\n        text['yscrollcommand']=sb.set\n        sb.pack(side='right', fill='y')\n        text.tag_config('symbol', font=('symbol', -size))\n        for i in range(256):\n            if i in (0,10): continue # null and newline\n            for k,v in list(SymbolWidget.SYMBOLS.items()):\n                if v == chr(i):\n                    text.insert('end', '%-10s\\t' % k)\n                    break\n            else:\n                text.insert('end', '%-10d  \\t' % i)\n            text.insert('end', '[%s]\\n' % chr(i), 'symbol')\n        top.mainloop()\n\n\nclass AbstractContainerWidget(CanvasWidget):\n    def __init__(self, canvas, child, **attribs):\n        self._child = child\n        self._add_child_widget(child)\n        CanvasWidget.__init__(self, canvas, **attribs)\n\n    def _manage(self):\n        self._update(self._child)\n\n    def child(self):\n        return self._child\n\n    def set_child(self, child):\n        self._remove_child_widget(self._child)\n        self._add_child_widget(child)\n        self._child = child\n        self.update(child)\n\n    def __repr__(self):\n        name = self.__class__.__name__\n        if name[-6:] == 'Widget': name = name[:-6]\n        return '[%s: %r]' % (name, self._child)\n\nclass BoxWidget(AbstractContainerWidget):\n    def __init__(self, canvas, child, **attribs):\n        self._child = child\n        self._margin = 1\n        self._box = canvas.create_rectangle(1,1,1,1)\n        canvas.tag_lower(self._box)\n        AbstractContainerWidget.__init__(self, canvas, child, **attribs)\n\n    def __setitem__(self, attr, value):\n        if attr == 'margin': self._margin = value\n        elif attr in ('outline', 'fill', 'width'):\n            self.canvas().itemconfig(self._box, {attr:value})\n        else:\n            CanvasWidget.__setitem__(self, attr, value)\n\n    def __getitem__(self, attr):\n        if attr == 'margin': return self._margin\n        elif attr == 'width':\n            return float(self.canvas().itemcget(self._box, attr))\n        elif attr in ('outline', 'fill', 'width'):\n            return self.canvas().itemcget(self._box, attr)\n        else:\n            return CanvasWidget.__getitem__(self, attr)\n\n    def _update(self, child):\n        (x1, y1, x2, y2) = child.bbox()\n        margin = self._margin + self['width']/2\n        self.canvas().coords(self._box, x1-margin, y1-margin,\n                             x2+margin, y2+margin)\n\n    def _tags(self): return [self._box]\n\nclass OvalWidget(AbstractContainerWidget):\n    def __init__(self, canvas, child, **attribs):\n        self._child = child\n        self._margin = 1\n        self._oval = canvas.create_oval(1,1,1,1)\n        self._circle = attribs.pop('circle', False)\n        self._double = attribs.pop('double', False)\n        if self._double:\n            self._oval2 = canvas.create_oval(1,1,1,1)\n        else:\n            self._oval2 = None\n        canvas.tag_lower(self._oval)\n        AbstractContainerWidget.__init__(self, canvas, child, **attribs)\n\n    def __setitem__(self, attr, value):\n        c = self.canvas()\n        if attr == 'margin': self._margin = value\n        elif attr == 'double':\n            if value==True and self._oval2 is None:\n                x1, y1, x2, y2 = c.bbox(self._oval)\n                w = self['width']*2\n                self._oval2 = c.create_oval(x1-w, y1-w, x2+w, y2+w,\n                                outline=c.itemcget(self._oval, 'outline'),\n                                width=c.itemcget(self._oval, 'width'))\n                c.tag_lower(self._oval2)\n            if value==False and self._oval2 is not None:\n                c.delete(self._oval2)\n                self._oval2 = None\n        elif attr in ('outline', 'fill', 'width'):\n            c.itemconfig(self._oval, {attr:value})\n            if self._oval2 is not None and attr!='fill':\n                c.itemconfig(self._oval2, {attr:value})\n            if self._oval2 is not None and attr!='fill':\n                self.canvas().itemconfig(self._oval2, {attr:value})\n        else:\n            CanvasWidget.__setitem__(self, attr, value)\n\n    def __getitem__(self, attr):\n        if attr == 'margin': return self._margin\n        elif attr == 'double': return self._double is not None\n        elif attr == 'width':\n            return float(self.canvas().itemcget(self._oval, attr))\n        elif attr in ('outline', 'fill', 'width'):\n            return self.canvas().itemcget(self._oval, attr)\n        else:\n            return CanvasWidget.__getitem__(self, attr)\n\n    RATIO = 1.4142135623730949\n\n    def _update(self, child):\n        R = OvalWidget.RATIO\n        (x1, y1, x2, y2) = child.bbox()\n        margin = self._margin\n\n        if self._circle:\n            dx, dy = abs(x1-x2), abs(y1-y2)\n            if dx > dy:\n                y = (y1+y2)/2\n                y1, y2 = y-dx/2, y+dx/2\n            elif dy > dx:\n                x = (x1+x2)/2\n                x1, x2 = x-dy/2, x+dy/2\n\n        left = int(( x1*(1+R) + x2*(1-R) ) / 2)\n        right = left + int((x2-x1)*R)\n        top = int(( y1*(1+R) + y2*(1-R) ) / 2)\n        bot = top + int((y2-y1)*R)\n        self.canvas().coords(self._oval, left-margin, top-margin,\n                             right+margin, bot+margin)\n        if self._oval2 is not None:\n            self.canvas().coords(self._oval2, left-margin+2, top-margin+2,\n                                 right+margin-2, bot+margin-2)\n\n    def _tags(self):\n        if self._oval2 is None:\n            return [self._oval]\n        else:\n            return [self._oval, self._oval2]\n\nclass ParenWidget(AbstractContainerWidget):\n    def __init__(self, canvas, child, **attribs):\n        self._child = child\n        self._oparen = canvas.create_arc(1,1,1,1, style='arc',\n                                         start=90, extent=180)\n        self._cparen = canvas.create_arc(1,1,1,1, style='arc',\n                                         start=-90, extent=180)\n        AbstractContainerWidget.__init__(self, canvas, child, **attribs)\n\n    def __setitem__(self, attr, value):\n        if attr == 'color':\n            self.canvas().itemconfig(self._oparen, outline=value)\n            self.canvas().itemconfig(self._cparen, outline=value)\n        elif attr == 'width':\n            self.canvas().itemconfig(self._oparen, width=value)\n            self.canvas().itemconfig(self._cparen, width=value)\n        else:\n            CanvasWidget.__setitem__(self, attr, value)\n\n    def __getitem__(self, attr):\n        if attr == 'color':\n            return self.canvas().itemcget(self._oparen, 'outline')\n        elif attr == 'width':\n            return self.canvas().itemcget(self._oparen, 'width')\n        else:\n            return CanvasWidget.__getitem__(self, attr)\n\n    def _update(self, child):\n        (x1, y1, x2, y2) = child.bbox()\n        width = max((y2-y1)/6, 4)\n        self.canvas().coords(self._oparen, x1-width, y1, x1+width, y2)\n        self.canvas().coords(self._cparen, x2-width, y1, x2+width, y2)\n\n    def _tags(self): return [self._oparen, self._cparen]\n\nclass BracketWidget(AbstractContainerWidget):\n    def __init__(self, canvas, child, **attribs):\n        self._child = child\n        self._obrack = canvas.create_line(1,1,1,1,1,1,1,1)\n        self._cbrack = canvas.create_line(1,1,1,1,1,1,1,1)\n        AbstractContainerWidget.__init__(self, canvas, child, **attribs)\n\n    def __setitem__(self, attr, value):\n        if attr == 'color':\n            self.canvas().itemconfig(self._obrack, fill=value)\n            self.canvas().itemconfig(self._cbrack, fill=value)\n        elif attr == 'width':\n            self.canvas().itemconfig(self._obrack, width=value)\n            self.canvas().itemconfig(self._cbrack, width=value)\n        else:\n            CanvasWidget.__setitem__(self, attr, value)\n\n    def __getitem__(self, attr):\n        if attr == 'color':\n            return self.canvas().itemcget(self._obrack, 'outline')\n        elif attr == 'width':\n            return self.canvas().itemcget(self._obrack, 'width')\n        else:\n            return CanvasWidget.__getitem__(self, attr)\n\n    def _update(self, child):\n        (x1, y1, x2, y2) = child.bbox()\n        width = max((y2-y1)/8, 2)\n        self.canvas().coords(self._obrack, x1, y1, x1-width, y1,\n                             x1-width, y2, x1, y2)\n        self.canvas().coords(self._cbrack, x2, y1, x2+width, y1,\n                             x2+width, y2, x2, y2)\n\n    def _tags(self): return [self._obrack, self._cbrack]\n\nclass SequenceWidget(CanvasWidget):\n    def __init__(self, canvas, *children, **attribs):\n        self._align = 'center'\n        self._space = 1\n        self._ordered = False\n        self._children = list(children)\n        for child in children: self._add_child_widget(child)\n        CanvasWidget.__init__(self, canvas, **attribs)\n\n    def __setitem__(self, attr, value):\n        if attr == 'align':\n            if value not in ('top', 'bottom', 'center'):\n                raise ValueError('Bad alignment: %r' % value)\n            self._align = value\n        elif attr == 'space': self._space = value\n        elif attr == 'ordered': self._ordered = value\n        else: CanvasWidget.__setitem__(self, attr, value)\n\n    def __getitem__(self, attr):\n        if attr == 'align': return self._align\n        elif attr == 'space': return self._space\n        elif attr == 'ordered': return self._ordered\n        else: return CanvasWidget.__getitem__(self, attr)\n\n    def _tags(self): return []\n\n    def _yalign(self, top, bot):\n        if self._align == 'top': return top\n        if self._align == 'bottom': return bot\n        if self._align == 'center': return (top+bot)/2\n\n    def _update(self, child):\n        (left, top, right, bot) = child.bbox()\n        y = self._yalign(top, bot)\n        for c in self._children:\n            (x1, y1, x2, y2) = c.bbox()\n            c.move(0, y-self._yalign(y1,y2))\n\n        if self._ordered and len(self._children) > 1:\n            index = self._children.index(child)\n\n            x = right + self._space\n            for i in range(index+1, len(self._children)):\n                (x1, y1, x2, y2) = self._children[i].bbox()\n                if x > x1:\n                    self._children[i].move(x-x1, 0)\n                    x += x2-x1 + self._space\n\n            x = left - self._space\n            for i in range(index-1, -1, -1):\n                (x1, y1, x2, y2) = self._children[i].bbox()\n                if x < x2:\n                    self._children[i].move(x-x2, 0)\n                    x -= x2-x1 + self._space\n\n    def _manage(self):\n        if len(self._children) == 0: return\n        child = self._children[0]\n\n        (left, top, right, bot) = child.bbox()\n        y = self._yalign(top, bot)\n\n        index = self._children.index(child)\n\n        x = right + self._space\n        for i in range(index+1, len(self._children)):\n            (x1, y1, x2, y2) = self._children[i].bbox()\n            self._children[i].move(x-x1, y-self._yalign(y1,y2))\n            x += x2-x1 + self._space\n\n        x = left - self._space\n        for i in range(index-1, -1, -1):\n            (x1, y1, x2, y2) = self._children[i].bbox()\n            self._children[i].move(x-x2, y-self._yalign(y1,y2))\n            x -= x2-x1 + self._space\n\n    def __repr__(self):\n        return '[Sequence: ' + repr(self._children)[1:-1]+']'\n\n    children = CanvasWidget.child_widgets\n\n    def replace_child(self, oldchild, newchild):\n        index = self._children.index(oldchild)\n        self._children[index] = newchild\n        self._remove_child_widget(oldchild)\n        self._add_child_widget(newchild)\n        self.update(newchild)\n\n    def remove_child(self, child):\n        index = self._children.index(child)\n        del self._children[index]\n        self._remove_child_widget(child)\n        if len(self._children) > 0:\n            self.update(self._children[0])\n\n    def insert_child(self, index, child):\n        self._children.insert(index, child)\n        self._add_child_widget(child)\n\nclass StackWidget(CanvasWidget):\n    def __init__(self, canvas, *children, **attribs):\n        self._align = 'center'\n        self._space = 1\n        self._ordered = False\n        self._children = list(children)\n        for child in children: self._add_child_widget(child)\n        CanvasWidget.__init__(self, canvas, **attribs)\n\n    def __setitem__(self, attr, value):\n        if attr == 'align':\n            if value not in ('left', 'right', 'center'):\n                raise ValueError('Bad alignment: %r' % value)\n            self._align = value\n        elif attr == 'space': self._space = value\n        elif attr == 'ordered': self._ordered = value\n        else: CanvasWidget.__setitem__(self, attr, value)\n\n    def __getitem__(self, attr):\n        if attr == 'align': return self._align\n        elif attr == 'space': return self._space\n        elif attr == 'ordered': return self._ordered\n        else: return CanvasWidget.__getitem__(self, attr)\n\n    def _tags(self): return []\n\n    def _xalign(self, left, right):\n        if self._align == 'left': return left\n        if self._align == 'right': return right\n        if self._align == 'center': return (left+right)/2\n\n    def _update(self, child):\n        (left, top, right, bot) = child.bbox()\n        x = self._xalign(left, right)\n        for c in self._children:\n            (x1, y1, x2, y2) = c.bbox()\n            c.move(x-self._xalign(x1,x2), 0)\n\n        if self._ordered and len(self._children) > 1:\n            index = self._children.index(child)\n\n            y = bot + self._space\n            for i in range(index+1, len(self._children)):\n                (x1, y1, x2, y2) = self._children[i].bbox()\n                if y > y1:\n                    self._children[i].move(0, y-y1)\n                    y += y2-y1 + self._space\n\n            y = top - self._space\n            for i in range(index-1, -1, -1):\n                (x1, y1, x2, y2) = self._children[i].bbox()\n                if y < y2:\n                    self._children[i].move(0, y-y2)\n                    y -= y2-y1 + self._space\n\n    def _manage(self):\n        if len(self._children) == 0: return\n        child = self._children[0]\n\n        (left, top, right, bot) = child.bbox()\n        x = self._xalign(left, right)\n\n        index = self._children.index(child)\n\n        y = bot + self._space\n        for i in range(index+1, len(self._children)):\n            (x1, y1, x2, y2) = self._children[i].bbox()\n            self._children[i].move(x-self._xalign(x1,x2), y-y1)\n            y += y2-y1 + self._space\n\n        y = top - self._space\n        for i in range(index-1, -1, -1):\n            (x1, y1, x2, y2) = self._children[i].bbox()\n            self._children[i].move(x-self._xalign(x1,x2), y-y2)\n            y -= y2-y1 + self._space\n\n    def __repr__(self):\n        return '[Stack: ' + repr(self._children)[1:-1]+']'\n\n    children = CanvasWidget.child_widgets\n\n    def replace_child(self, oldchild, newchild):\n        index = self._children.index(oldchild)\n        self._children[index] = newchild\n        self._remove_child_widget(oldchild)\n        self._add_child_widget(newchild)\n        self.update(newchild)\n\n    def remove_child(self, child):\n        index = self._children.index(child)\n        del self._children[index]\n        self._remove_child_widget(child)\n        if len(self._children) > 0:\n            self.update(self._children[0])\n\n    def insert_child(self, index, child):\n        self._children.insert(index, child)\n        self._add_child_widget(child)\n\nclass SpaceWidget(CanvasWidget):\n    def __init__(self, canvas, width, height, **attribs):\n        if width > 4: width -= 4\n        if height > 4: height -= 4\n        self._tag = canvas.create_line(1, 1, width, height, fill='')\n        CanvasWidget.__init__(self, canvas, **attribs)\n\n    def set_width(self, width):\n        [x1, y1, x2, y2] = self.bbox()\n        self.canvas().coords(self._tag, x1, y1, x1+width, y2)\n\n    def set_height(self, height):\n        [x1, y1, x2, y2] = self.bbox()\n        self.canvas().coords(self._tag, x1, y1, x2, y1+height)\n\n    def _tags(self): return [self._tag]\n\n    def __repr__(self): return '[Space]'\n\nclass ScrollWatcherWidget(CanvasWidget):\n    def __init__(self, canvas, *children, **attribs):\n        for child in children: self._add_child_widget(child)\n        CanvasWidget.__init__(self, canvas, **attribs)\n\n    def add_child(self, canvaswidget):\n        self._add_child_widget(canvaswidget)\n        self.update(canvaswidget)\n\n    def remove_child(self, canvaswidget):\n        self._remove_child_widget(canvaswidget)\n\n    def _tags(self): return []\n\n    def _update(self, child):\n        self._adjust_scrollregion()\n\n    def _adjust_scrollregion(self):\n        bbox = self.bbox()\n        canvas = self.canvas()\n        scrollregion = [int(n) for n in canvas['scrollregion'].split()]\n        if len(scrollregion) != 4: return\n        if (bbox[0] < scrollregion[0] or bbox[1] < scrollregion[1] or\n            bbox[2] > scrollregion[2] or bbox[3] > scrollregion[3]):\n            scrollregion = ('%d %d %d %d' %\n                            (min(bbox[0], scrollregion[0]),\n                             min(bbox[1], scrollregion[1]),\n                             max(bbox[2], scrollregion[2]),\n                         max(bbox[3], scrollregion[3])))\n            canvas['scrollregion'] = scrollregion\n\n\nclass CanvasFrame(object):\n    def __init__(self, parent=None, **kw):\n        if parent is None:\n            self._parent = Tk()\n            self._parent.title('NLTK')\n            self._parent.bind('<Control-p>', lambda e: self.print_to_file())\n            self._parent.bind('<Control-x>', self.destroy)\n            self._parent.bind('<Control-q>', self.destroy)\n        else:\n            self._parent = parent\n\n        self._frame = frame = Frame(self._parent)\n        self._canvas = canvas = Canvas(frame, **kw)\n        xscrollbar = Scrollbar(self._frame, orient='horizontal')\n        yscrollbar = Scrollbar(self._frame, orient='vertical')\n        xscrollbar['command'] = canvas.xview\n        yscrollbar['command'] = canvas.yview\n        canvas['xscrollcommand'] = xscrollbar.set\n        canvas['yscrollcommand'] = yscrollbar.set\n        yscrollbar.pack(fill='y', side='right')\n        xscrollbar.pack(fill='x', side='bottom')\n        canvas.pack(expand=1, fill='both', side='left')\n\n        scrollregion = '0 0 %s %s' % (canvas['width'], canvas['height'])\n        canvas['scrollregion'] = scrollregion\n\n        self._scrollwatcher = ScrollWatcherWidget(canvas)\n\n        if parent is None:\n            self.pack(expand=1, fill='both')\n            self._init_menubar()\n\n    def _init_menubar(self):\n        menubar = Menu(self._parent)\n\n        filemenu = Menu(menubar, tearoff=0)\n        filemenu.add_command(label='Print to Postscript', underline=0,\n                             command=self.print_to_file, accelerator='Ctrl-p')\n        filemenu.add_command(label='Exit', underline=1,\n                             command=self.destroy, accelerator='Ctrl-x')\n        menubar.add_cascade(label='File', underline=0, menu=filemenu)\n\n        self._parent.config(menu=menubar)\n\n    def print_to_file(self, filename=None):\n        if filename is None:\n            ftypes = [('Postscript files', '.ps'),\n                      ('All files', '*')]\n            filename = asksaveasfilename(filetypes=ftypes,\n                                         defaultextension='.ps')\n            if not filename: return\n        (x0, y0, w, h) = self.scrollregion()\n        postscript = self._canvas.postscript(x=x0, y=y0,\n                                width=w+2, height=h+2,\n                                pagewidth=w+2, # points = 1/72 inch\n                                pageheight=h+2, # points = 1/72 inch\n                                pagex=0, pagey=0)\n        postscript = postscript.replace(' 0 scalefont ', ' 9 scalefont ')\n        with open(filename, 'wb') as f:\n            f.write(postscript.encode('utf8'))\n\n    def scrollregion(self):\n        (x1, y1, x2, y2) = self._canvas['scrollregion'].split()\n        return (int(x1), int(y1), int(x2), int(y2))\n\n    def canvas(self):\n        return self._canvas\n\n    def add_widget(self, canvaswidget, x=None, y=None):\n        if x is None or y is None:\n            (x, y) = self._find_room(canvaswidget, x, y)\n\n        (x1,y1,x2,y2) = canvaswidget.bbox()\n        canvaswidget.move(x-x1,y-y1)\n\n        self._scrollwatcher.add_child(canvaswidget)\n\n    def _find_room(self, widget, desired_x, desired_y):\n        (left, top, right, bot) = self.scrollregion()\n        w = widget.width()\n        h = widget.height()\n\n        if w >= (right-left): return (0,0)\n        if h >= (bot-top): return (0,0)\n\n        (x1,y1,x2,y2) = widget.bbox()\n        widget.move(left-x2-50, top-y2-50)\n\n        if desired_x is not None:\n            x = desired_x\n            for y in range(top, bot-h, int((bot-top-h)/10)):\n                if not self._canvas.find_overlapping(x-5, y-5, x+w+5, y+h+5):\n                    return (x,y)\n\n        if desired_y is not None:\n            y = desired_y\n            for x in range(left, right-w, int((right-left-w)/10)):\n                if not self._canvas.find_overlapping(x-5, y-5, x+w+5, y+h+5):\n                    return (x,y)\n\n        for y in range(top, bot-h, int((bot-top-h)/10)):\n            for x in range(left, right-w, int((right-left-w)/10)):\n                if not self._canvas.find_overlapping(x-5, y-5, x+w+5, y+h+5):\n                    return (x,y)\n        return (0,0)\n\n    def destroy_widget(self, canvaswidget):\n        self.remove_widget(canvaswidget)\n        canvaswidget.destroy()\n\n    def remove_widget(self, canvaswidget):\n        self._scrollwatcher.remove_child(canvaswidget)\n\n    def pack(self, cnf={}, **kw):\n        self._frame.pack(cnf, **kw)\n\n    def destroy(self, *e):\n        if self._parent is None: return\n        self._parent.destroy()\n        self._parent = None\n\n    def mainloop(self, *args, **kwargs):\n        if in_idle(): return\n        self._parent.mainloop(*args, **kwargs)\n\n\nclass ShowText(object):\n    def __init__(self, root, title, text, width=None, height=None,\n                 **textbox_options):\n        if width is None or height is None:\n            (width, height) = self.find_dimentions(text, width, height)\n\n        if root is None:\n            self._top = top = Tk()\n        else:\n            self._top = top = Toplevel(root)\n        top.title(title)\n\n        b = Button(top, text='Ok', command=self.destroy)\n        b.pack(side='bottom')\n\n        tbf = Frame(top)\n        tbf.pack(expand=1, fill='both')\n        scrollbar = Scrollbar(tbf, orient='vertical')\n        scrollbar.pack(side='right', fill='y')\n        textbox = Text(tbf, wrap='word', width=width,\n                       height=height, **textbox_options)\n        textbox.insert('end', text)\n        textbox['state'] = 'disabled'\n        textbox.pack(side='left', expand=1, fill='both')\n        scrollbar['command'] = textbox.yview\n        textbox['yscrollcommand'] = scrollbar.set\n\n        top.bind('q', self.destroy)\n        top.bind('x', self.destroy)\n        top.bind('c', self.destroy)\n        top.bind('<Return>', self.destroy)\n        top.bind('<Escape>', self.destroy)\n\n        scrollbar.focus()\n\n    def find_dimentions(self, text, width, height):\n        lines = text.split('\\n')\n        if width is None:\n            maxwidth = max(len(line) for line in lines)\n            width = min(maxwidth, 80)\n\n        height = 0\n        for line in lines:\n            while len(line) > width:\n                brk = line[:width].rfind(' ')\n                line = line[brk:]\n                height += 1\n            height += 1\n        height = min(height, 25)\n\n        return (width, height)\n\n    def destroy(self, *e):\n        if self._top is None: return\n        self._top.destroy()\n        self._top = None\n\n    def mainloop(self, *args, **kwargs):\n        if in_idle(): return\n        self._top.mainloop(*args, **kwargs)\n\n\nclass EntryDialog(object):\n    def __init__(self, parent, original_text='', instructions='',\n                 set_callback=None, title=None):\n        self._parent = parent\n        self._original_text = original_text\n        self._set_callback = set_callback\n\n        width = int(max(30, len(original_text)*3/2))\n        self._top = Toplevel(parent)\n\n        if title: self._top.title(title)\n\n        entryframe = Frame(self._top)\n        entryframe.pack(expand=1, fill='both', padx=5, pady=5,ipady=10)\n        if instructions:\n            l=Label(entryframe, text=instructions)\n            l.pack(side='top', anchor='w', padx=30)\n        self._entry = Entry(entryframe, width=width)\n        self._entry.pack(expand=1, fill='x', padx=30)\n        self._entry.insert(0, original_text)\n\n        divider = Frame(self._top, borderwidth=1, relief='sunken')\n        divider.pack(fill='x', ipady=1, padx=10)\n\n        buttons = Frame(self._top)\n        buttons.pack(expand=0, fill='x', padx=5, pady=5)\n        b = Button(buttons, text='Cancel', command=self._cancel, width=8)\n        b.pack(side='right', padx=5)\n        b = Button(buttons, text='Ok', command=self._ok,\n                   width=8, default='active')\n        b.pack(side='left', padx=5)\n        b = Button(buttons, text='Apply', command=self._apply, width=8)\n        b.pack(side='left')\n\n        self._top.bind('<Return>', self._ok)\n        self._top.bind('<Control-q>', self._cancel)\n        self._top.bind('<Escape>', self._cancel)\n\n        self._entry.focus()\n\n    def _reset(self, *e):\n        self._entry.delete(0,'end')\n        self._entry.insert(0, self._original_text)\n        if self._set_callback:\n            self._set_callback(self._original_text)\n\n    def _cancel(self, *e):\n        try: self._reset()\n        except: pass\n        self._destroy()\n\n    def _ok(self, *e):\n        self._apply()\n        self._destroy()\n\n    def _apply(self, *e):\n        if self._set_callback:\n            self._set_callback(self._entry.get())\n\n    def _destroy(self, *e):\n        if self._top is None: return\n        self._top.destroy()\n        self._top = None\n\n\nclass ColorizedList(object):\n    def __init__(self, parent, items=[], **options):\n        self._parent = parent\n        self._callbacks = {}\n\n        self._marks = {}\n\n        self._init_itemframe(options.copy())\n\n        self._textwidget.bind('<KeyPress>', self._keypress)\n        self._textwidget.bind('<ButtonPress>', self._buttonpress)\n\n        self._items = None\n        self.set(items)\n\n    @abstractmethod\n    def _init_colortags(self, textwidget, options):\n\n    @abstractmethod\n    def _item_repr(self, item):\n\n\n    def get(self, index=None):\n        if index is None:\n            return self._items[:]\n        else:\n            return self._items[index]\n\n    def set(self, items):\n        items = list(items)\n        if self._items == items: return\n        self._items = list(items)\n\n        self._textwidget['state'] = 'normal'\n        self._textwidget.delete('1.0', 'end')\n        for item in items:\n            for (text, colortag) in self._item_repr(item):\n                assert '\\n' not in text, 'item repr may not contain newline'\n                self._textwidget.insert('end', text, colortag)\n            self._textwidget.insert('end', '\\n')\n        self._textwidget.delete('end-1char', 'end')\n        self._textwidget.mark_set('insert', '1.0')\n        self._textwidget['state'] = 'disabled'\n        self._marks.clear()\n\n    def unmark(self, item=None):\n        if item is None:\n            self._marks.clear()\n            self._textwidget.tag_remove('highlight', '1.0', 'end+1char')\n        else:\n            index = self._items.index(item)\n            del self._marks[item]\n            (start, end) = ('%d.0' % (index+1), '%d.0' % (index+2))\n            self._textwidget.tag_remove('highlight', start, end)\n\n    def mark(self, item):\n        self._marks[item] = 1\n        index = self._items.index(item)\n        (start, end) = ('%d.0' % (index+1), '%d.0' % (index+2))\n        self._textwidget.tag_add('highlight', start, end)\n\n    def markonly(self, item):\n        self.unmark()\n        self.mark(item)\n\n    def view(self, item):\n        index = self._items.index(item)\n        self._textwidget.see('%d.0' % (index+1))\n\n\n    def add_callback(self, event, func):\n        if event == 'select': events = ['click1', 'space', 'return']\n        elif event == 'move': events = ['up', 'down', 'next', 'prior']\n        else: events = [event]\n\n        for e in events:\n            self._callbacks.setdefault(e,{})[func] = 1\n\n    def remove_callback(self, event, func=None):\n        if event is None: events = list(self._callbacks.keys())\n        elif event == 'select': events = ['click1', 'space', 'return']\n        elif event == 'move': events = ['up', 'down', 'next', 'prior']\n        else: events = [event]\n\n        for e in events:\n            if func is None: del self._callbacks[e]\n            else:\n                try: del self._callbacks[e][func]\n                except: pass\n\n\n    def pack(self, cnf={}, **kw):\n        self._itemframe.pack(cnf, **kw)\n\n    def grid(self, cnf={}, **kw):\n        self._itemframe.grid(cnf, *kw)\n\n    def focus(self):\n        self._textwidget.focus()\n\n\n    def _init_itemframe(self, options):\n        self._itemframe = Frame(self._parent)\n\n        options.setdefault('background', '#e0e0e0')\n        self._textwidget = Text(self._itemframe, **options)\n        self._textscroll = Scrollbar(self._itemframe, takefocus=0,\n                                     orient='vertical')\n        self._textwidget.config(yscrollcommand = self._textscroll.set)\n        self._textscroll.config(command=self._textwidget.yview)\n        self._textscroll.pack(side='right', fill='y')\n        self._textwidget.pack(expand=1, fill='both', side='left')\n\n        self._textwidget.tag_config('highlight', background='#e0ffff',\n                                    border='1', relief='raised')\n        self._init_colortags(self._textwidget, options)\n\n        self._textwidget.tag_config('sel', foreground='')\n        self._textwidget.tag_config('sel', foreground='', background='',\n                                    border='', underline=1)\n        self._textwidget.tag_lower('highlight', 'sel')\n\n    def _fire_callback(self, event, itemnum):\n        if event not in self._callbacks: return\n        if 0 <= itemnum < len(self._items):\n            item = self._items[itemnum]\n        else:\n            item = None\n        for cb_func in list(self._callbacks[event].keys()):\n            cb_func(item)\n\n    def _buttonpress(self, event):\n        clickloc = '@%d,%d' % (event.x,event.y)\n        insert_point = self._textwidget.index(clickloc)\n        itemnum = int(insert_point.split('.')[0])-1\n        self._fire_callback('click%d' % event.num, itemnum)\n\n    def _keypress(self, event):\n        if event.keysym == 'Return' or event.keysym == 'space':\n            insert_point = self._textwidget.index('insert')\n            itemnum = int(insert_point.split('.')[0])-1\n            self._fire_callback(event.keysym.lower(), itemnum)\n            return\n        elif event.keysym == 'Down': delta='+1line'\n        elif event.keysym == 'Up': delta='-1line'\n        elif event.keysym == 'Next': delta='+10lines'\n        elif event.keysym == 'Prior': delta='-10lines'\n        else: return 'continue'\n\n        self._textwidget.mark_set('insert', 'insert'+delta)\n        self._textwidget.see('insert')\n        self._textwidget.tag_remove('sel', '1.0', 'end+1char')\n        self._textwidget.tag_add('sel', 'insert linestart', 'insert lineend')\n\n        insert_point = self._textwidget.index('insert')\n        itemnum = int(insert_point.split('.')[0])-1\n        self._fire_callback(event.keysym.lower(), itemnum)\n\n        return 'break'\n\n\nclass MutableOptionMenu(Menubutton):\n    def __init__(self, master, values, **options):\n        self._callback = options.get('command')\n        if 'command' in options: del options['command']\n\n        self._variable = variable = StringVar()\n        if len(values) > 0:\n            variable.set(values[0])\n\n        kw = {\"borderwidth\": 2, \"textvariable\": variable,\n              \"indicatoron\": 1, \"relief\": RAISED, \"anchor\": \"c\",\n              \"highlightthickness\": 2}\n        kw.update(options)\n        Widget.__init__(self, master, \"menubutton\", kw)\n        self.widgetName = 'tk_optionMenu'\n        self._menu = Menu(self, name=\"menu\", tearoff=0,)\n        self.menuname = self._menu._w\n\n        self._values = []\n        for value in values: self.add(value)\n\n        self[\"menu\"] = self._menu\n\n    def add(self, value):\n        if value in self._values: return\n        def set(value=value): self.set(value)\n        self._menu.add_command(label=value, command=set)\n        self._values.append(value)\n\n    def set(self, value):\n        self._variable.set(value)\n        if self._callback:\n            self._callback(value)\n\n    def remove(self, value):\n        i = self._values.index(value)\n        del self._values[i]\n        self._menu.delete(i, i)\n\n    def __getitem__(self, name):\n        if name == 'menu':\n            return self.__menu\n        return Widget.__getitem__(self, name)\n\n    def destroy(self):\n        Menubutton.destroy(self)\n        self._menu = None\n\n\ndef demo():\n    def fill(cw):\n        from random import randint\n        cw['fill'] = '#00%04d' % randint(0,9999)\n    def color(cw):\n        from random import randint\n        cw['color'] = '#ff%04d' % randint(0,9999)\n\n    cf = CanvasFrame(closeenough=10, width=300, height=300)\n    c = cf.canvas()\n    ct3 = TextWidget(c, 'hiya there', draggable=1)\n    ct2 = TextWidget(c, 'o  o\\n||\\n___\\n  U', draggable=1, justify='center')\n    co = OvalWidget(c, ct2, outline='red')\n    ct = TextWidget(c, 'o  o\\n||\\n\\\\___/', draggable=1, justify='center')\n    cp = ParenWidget(c, ct, color='red')\n    cb = BoxWidget(c, cp, fill='cyan', draggable=1, width=3, margin=10)\n    equation = SequenceWidget(c,\n                              SymbolWidget(c, 'forall'), TextWidget(c, 'x'),\n                              SymbolWidget(c, 'exists'), TextWidget(c, 'y: '),\n                              TextWidget(c, 'x'), SymbolWidget(c, 'notequal'),\n                              TextWidget(c, 'y'))\n    space = SpaceWidget(c, 0, 30)\n    cstack = StackWidget(c, cb, ct3, space, co, equation, align='center')\n    foo = TextWidget(c, 'try clicking\\nand dragging',\n                     draggable=1, justify='center')\n    cs = SequenceWidget(c, cstack, foo)\n    zz = BracketWidget(c, cs, color='green4', width=3)\n    cf.add_widget(zz, 60, 30)\n\n    cb.bind_click(fill)\n    ct.bind_click(color)\n    co.bind_click(fill)\n    ct2.bind_click(color)\n    ct3.bind_click(color)\n\n    cf.mainloop()\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\draw\\__init__": [".py", "\ntry:\n    from six.moves import tkinter\nexcept ImportError:\n    import warnings\n    warnings.warn(\"nltk.draw package not loaded \"\n                  \"(please install Tkinter library).\")\nelse:\n    from nltk.draw.cfg import ProductionList, CFGEditor, CFGDemo\n    from nltk.draw.tree import (TreeSegmentWidget, tree_to_treesegment,\n                      TreeWidget, TreeView, draw_trees)\n    from nltk.draw.table import Table\n\nfrom nltk.draw.dispersion import dispersion_plot\n\ndef setup_module(module):\n    from nose import SkipTest\n    raise SkipTest(\"nltk.draw examples are not doctests\")\n", 1], "nltk\\featstruct": [".py", "\nfrom __future__ import print_function, unicode_literals, division\n\nimport re\nimport copy\nfrom functools import total_ordering\n\nfrom six import integer_types, string_types\n\nfrom nltk.internals import read_str, raise_unorderable_types\nfrom nltk.sem.logic import (Variable, Expression, SubstituteBindingsI,\n                            LogicParser, LogicalExpressionException)\nfrom nltk.compat import python_2_unicode_compatible, unicode_repr\n\n\n\n@total_ordering\nclass FeatStruct(SubstituteBindingsI):\n\n    _frozen = False\n        Construct and return a new feature structure.  If this\n        constructor is called directly, then the returned feature\n        structure will be an instance of either the ``FeatDict`` class\n        or the ``FeatList`` class.\n\n        :param features: The initial feature values for this feature\n            structure:\n              - FeatStruct(string) -> FeatStructReader().read(string)\n              - FeatStruct(mapping) -> FeatDict(mapping)\n              - FeatStruct(sequence) -> FeatList(sequence)\n              - FeatStruct() -> FeatDict()\n        :param morefeatures: If ``features`` is a mapping or None,\n            then ``morefeatures`` provides additional features for the\n            ``FeatDict`` constructor.\n        \"\"\"\n        if cls is FeatStruct:\n            if features is None:\n                return FeatDict.__new__(FeatDict, **morefeatures)\n            elif _is_mapping(features):\n                return FeatDict.__new__(FeatDict, features, **morefeatures)\n            elif morefeatures:\n                raise TypeError('Keyword arguments may only be specified '\n                                'if features is None or is a mapping.')\n            if isinstance(features, string_types):\n                if FeatStructReader._START_FDICT_RE.match(features):\n                    return FeatDict.__new__(FeatDict, features, **morefeatures)\n                else:\n                    return FeatList.__new__(FeatList, features, **morefeatures)\n            elif _is_sequence(features):\n                return FeatList.__new__(FeatList, features)\n            else:\n                raise TypeError('Expected string or mapping or sequence')\n\n        else:\n            return super(FeatStruct, cls).__new__(cls, features,\n                                                  **morefeatures)\n\n\n    def _keys(self):\n        \"\"\"Return an iterable of the feature identifiers used by this\n        FeatStruct.\"\"\"\n        raise NotImplementedError()  # Implemented by subclasses.\n\n    def _values(self):\n        \"\"\"Return an iterable of the feature values directly defined\n        by this FeatStruct.\"\"\"\n        raise NotImplementedError()  # Implemented by subclasses.\n\n    def _items(self):\n        \"\"\"Return an iterable of (fid,fval) pairs, where fid is a\n        feature identifier and fval is the corresponding feature\n        value, for all features defined by this FeatStruct.\"\"\"\n        raise NotImplementedError()  # Implemented by subclasses.\n\n\n    def equal_values(self, other, check_reentrance=False):\n        \"\"\"\n        Return True if ``self`` and ``other`` assign the same value to\n        to every feature.  In particular, return true if\n        ``self[p]==other[p]`` for every feature path *p* such\n        that ``self[p]`` or ``other[p]`` is a base value (i.e.,\n        not a nested feature structure).\n\n        :param check_reentrance: If True, then also return False if\n            there is any difference between the reentrances of ``self``\n            and ``other``.\n        :note: the ``==`` is equivalent to ``equal_values()`` with\n            ``check_reentrance=True``.\n        \"\"\"\n        return self._equal(other, check_reentrance, set(), set(), set())\n\n    def __eq__(self, other):\n        \"\"\"\n        Return true if ``self`` and ``other`` are both feature structures,\n        assign the same values to all features, and contain the same\n        reentrances.  I.e., return\n        ``self.equal_values(other, check_reentrance=True)``.\n\n        :see: ``equal_values()``\n        \"\"\"\n        return self._equal(other, True, set(), set(), set())\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __lt__(self, other):\n        if not isinstance(other, FeatStruct):\n            return self.__class__.__name__ < other.__class__.__name__\n        else:\n            return len(self) < len(other)\n\n    def __hash__(self):\n        \"\"\"\n        If this feature structure is frozen, return its hash value;\n        otherwise, raise ``TypeError``.\n        \"\"\"\n        if not self._frozen:\n            raise TypeError('FeatStructs must be frozen before they '\n                            'can be hashed.')\n        try:\n            return self._hash\n        except AttributeError:\n            self._hash = self._calculate_hashvalue(set())\n            return self._hash\n\n    def _equal(self, other, check_reentrance, visited_self,\n               visited_other, visited_pairs):\n        \"\"\"\n        Return True iff self and other have equal values.\n\n        :param visited_self: A set containing the ids of all ``self``\n            feature structures we've already visited.\n        :param visited_other: A set containing the ids of all ``other``\n            feature structures we've already visited.\n        :param visited_pairs: A set containing ``(selfid, otherid)`` pairs\n            for all pairs of feature structures we've already visited.\n        \"\"\"\n        if self is other: return True\n\n        if self.__class__ != other.__class__: return False\n\n        if len(self) != len(other): return False\n        if set(self._keys()) != set(other._keys()): return False\n\n        if check_reentrance:\n            if id(self) in visited_self or id(other) in visited_other:\n                return (id(self), id(other)) in visited_pairs\n\n        else:\n            if (id(self), id(other)) in visited_pairs:\n                return True\n\n        visited_self.add(id(self))\n        visited_other.add(id(other))\n        visited_pairs.add((id(self), id(other)))\n\n        for (fname, self_fval) in self._items():\n            other_fval = other[fname]\n            if isinstance(self_fval, FeatStruct):\n                if not self_fval._equal(other_fval, check_reentrance,\n                                        visited_self, visited_other,\n                                        visited_pairs):\n                    return False\n            else:\n                if self_fval != other_fval: return False\n\n        return True\n\n    def _calculate_hashvalue(self, visited):\n        \"\"\"\n        Return a hash value for this feature structure.\n\n        :require: ``self`` must be frozen.\n        :param visited: A set containing the ids of all feature\n            structures we've already visited while hashing.\n        \"\"\"\n        if id(self) in visited: return 1\n        visited.add(id(self))\n\n        hashval = 5831\n        for (fname, fval) in sorted(self._items()):\n            hashval *= 37\n            hashval += hash(fname)\n            hashval *= 37\n            if isinstance(fval, FeatStruct):\n                hashval += fval._calculate_hashvalue(visited)\n            else:\n                hashval += hash(fval)\n            hashval = int(hashval & 0x7fffffff)\n        return hashval\n\n\n    _FROZEN_ERROR = \"Frozen FeatStructs may not be modified.\"\n\n    def freeze(self):\n        \"\"\"\n        Make this feature structure, and any feature structures it\n        contains, immutable.  Note: this method does not attempt to\n        'freeze' any feature value that is not a ``FeatStruct``; it\n        is recommended that you use only immutable feature values.\n        \"\"\"\n        if self._frozen: return\n        self._freeze(set())\n\n    def frozen(self):\n        \"\"\"\n        Return True if this feature structure is immutable.  Feature\n        structures can be made immutable with the ``freeze()`` method.\n        Immutable feature structures may not be made mutable again,\n        but new mutable copies can be produced with the ``copy()`` method.\n        \"\"\"\n        return self._frozen\n\n    def _freeze(self, visited):\n        \"\"\"\n        Make this feature structure, and any feature structure it\n        contains, immutable.\n\n        :param visited: A set containing the ids of all feature\n            structures we've already visited while freezing.\n        \"\"\"\n        if id(self) in visited: return\n        visited.add(id(self))\n        self._frozen = True\n        for (fname, fval) in sorted(self._items()):\n            if isinstance(fval, FeatStruct):\n                fval._freeze(visited)\n\n\n    def copy(self, deep=True):\n        \"\"\"\n        Return a new copy of ``self``.  The new copy will not be frozen.\n\n        :param deep: If true, create a deep copy; if false, create\n            a shallow copy.\n        \"\"\"\n        if deep:\n            return copy.deepcopy(self)\n        else:\n            return self.__class__(self)\n\n    def __deepcopy__(self, memo):\n        raise NotImplementedError()  # Implemented by subclasses.\n\n\n    def cyclic(self):\n        \"\"\"\n        Return True if this feature structure contains itself.\n        \"\"\"\n        return self._find_reentrances({})[id(self)]\n\n    def walk(self):\n        \"\"\"\n        Return an iterator that generates this feature structure, and\n        each feature structure it contains.  Each feature structure will\n        be generated exactly once.\n        \"\"\"\n        return self._walk(set())\n\n    def _walk(self, visited):\n        \"\"\"\n        Return an iterator that generates this feature structure, and\n        each feature structure it contains.\n\n        :param visited: A set containing the ids of all feature\n            structures we've already visited while freezing.\n        \"\"\"\n        raise NotImplementedError()  # Implemented by subclasses.\n\n    def _walk(self, visited):\n        if id(self) in visited: return\n        visited.add(id(self))\n        yield self\n        for fval in self._values():\n            if isinstance(fval, FeatStruct):\n                for elt in fval._walk(visited):\n                    yield elt\n\n    def _find_reentrances(self, reentrances):\n        \"\"\"\n        Return a dictionary that maps from the ``id`` of each feature\n        structure contained in ``self`` (including ``self``) to a\n        boolean value, indicating whether it is reentrant or not.\n        \"\"\"\n        if id(self) in reentrances:\n            reentrances[id(self)] = True\n        else:\n            reentrances[id(self)] = False\n\n            for fval in self._values():\n                if isinstance(fval, FeatStruct):\n                    fval._find_reentrances(reentrances)\n\n        return reentrances\n\n\n    def substitute_bindings(self, bindings):\n        return substitute_bindings(self, bindings)\n\n    def retract_bindings(self, bindings):\n        return retract_bindings(self, bindings)\n\n    def variables(self):\n        return find_variables(self)\n\n    def rename_variables(self, vars=None, used_vars=(), new_vars=None):\n        return rename_variables(self, vars, used_vars, new_vars)\n\n    def remove_variables(self):\n        \"\"\"\n        Return the feature structure that is obtained by deleting\n        any feature whose value is a ``Variable``.\n\n        :rtype: FeatStruct\n        \"\"\"\n        return remove_variables(self)\n\n\n    def unify(self, other, bindings=None, trace=False,\n              fail=None, rename_vars=True):\n        return unify(self, other, bindings, trace, fail, rename_vars)\n\n    def subsumes(self, other):\n        \"\"\"\n        Return True if ``self`` subsumes ``other``.  I.e., return true\n        If unifying ``self`` with ``other`` would result in a feature\n        structure equal to ``other``.\n        \"\"\"\n        return subsumes(self, other)\n\n\n    def __repr__(self):\n        \"\"\"\n        Display a single-line representation of this feature structure,\n        suitable for embedding in other representations.\n        \"\"\"\n        return self._repr(self._find_reentrances({}), {})\n\n    def _repr(self, reentrances, reentrance_ids):\n        \"\"\"\n        Return a string representation of this feature structure.\n\n        :param reentrances: A dictionary that maps from the ``id`` of\n            each feature value in self, indicating whether that value\n            is reentrant or not.\n        :param reentrance_ids: A dictionary mapping from each ``id``\n            of a feature value to a unique identifier.  This is modified\n            by ``repr``: the first time a reentrant feature value is\n            displayed, an identifier is added to ``reentrance_ids`` for it.\n        \"\"\"\n        raise NotImplementedError()\n\n\n_FROZEN_ERROR = \"Frozen FeatStructs may not be modified.\"\n_FROZEN_NOTICE = \"\\n%sIf self is frozen, raise ValueError.\"\n\n\ndef _check_frozen(method, indent=''):\n    \"\"\"\n    Given a method function, return a new method function that first\n    checks if ``self._frozen`` is true; and if so, raises ``ValueError``\n    with an appropriate message.  Otherwise, call the method and return\n    its result.\n    \"\"\"\n\n    def wrapped(self, *args, **kwargs):\n        if self._frozen:\n            raise ValueError(_FROZEN_ERROR)\n        else:\n            return method(self, *args, **kwargs)\n\n    wrapped.__name__ = method.__name__\n    wrapped.__doc__ = (method.__doc__ or '') + (_FROZEN_NOTICE % indent)\n    return wrapped\n\n\n\n@python_2_unicode_compatible\nclass FeatDict(FeatStruct, dict):\n    \"\"\"\n    A feature structure that acts like a Python dictionary.  I.e., a\n    mapping from feature identifiers to feature values, where a feature\n    identifier can be a string or a ``Feature``; and where a feature value\n    can be either a basic value (such as a string or an integer), or a nested\n    feature structure.  A feature identifiers for a ``FeatDict`` is\n    sometimes called a \"feature name\".\n\n    Two feature dicts are considered equal if they assign the same\n    values to all features, and have the same reentrances.\n\n    :see: ``FeatStruct`` for information about feature paths, reentrance,\n        cyclic feature structures, mutability, freezing, and hashing.\n    \"\"\"\n\n    def __init__(self, features=None, **morefeatures):\n        \"\"\"\n        Create a new feature dictionary, with the specified features.\n\n        :param features: The initial value for this feature\n            dictionary.  If ``features`` is a ``FeatStruct``, then its\n            features are copied (shallow copy).  If ``features`` is a\n            dict, then a feature is created for each item, mapping its\n            key to its value.  If ``features`` is a string, then it is\n            processed using ``FeatStructReader``.  If ``features`` is a list of\n            tuples ``(name, val)``, then a feature is created for each tuple.\n        :param morefeatures: Additional features for the new feature\n            dictionary.  If a feature is listed under both ``features`` and\n            ``morefeatures``, then the value from ``morefeatures`` will be\n            used.\n        \"\"\"\n        if isinstance(features, string_types):\n            FeatStructReader().fromstring(features, self)\n            self.update(**morefeatures)\n        else:\n            self.update(features, **morefeatures)\n\n    _INDEX_ERROR = str(\"Expected feature name or path.  Got %r.\")\n\n    def __getitem__(self, name_or_path):\n        \"\"\"If the feature with the given name or path exists, return\n        its value; otherwise, raise ``KeyError``.\"\"\"\n        if isinstance(name_or_path, (string_types, Feature)):\n            return dict.__getitem__(self, name_or_path)\n        elif isinstance(name_or_path, tuple):\n            try:\n                val = self\n                for fid in name_or_path:\n                    if not isinstance(val, FeatStruct):\n                        raise KeyError  # path contains base value\n                    val = val[fid]\n                return val\n            except (KeyError, IndexError):\n                raise KeyError(name_or_path)\n        else:\n            raise TypeError(self._INDEX_ERROR % name_or_path)\n\n    def get(self, name_or_path, default=None):\n        \"\"\"If the feature with the given name or path exists, return its\n        value; otherwise, return ``default``.\"\"\"\n        try:\n            return self[name_or_path]\n        except KeyError:\n            return default\n\n    def __contains__(self, name_or_path):\n        try:\n            self[name_or_path]; return True\n        except KeyError:\n            return False\n\n    def has_key(self, name_or_path):\n        return name_or_path in self\n\n    def __delitem__(self, name_or_path):\n        \"\"\"If the feature with the given name or path exists, delete\n        its value; otherwise, raise ``KeyError``.\"\"\"\n        if self._frozen: raise ValueError(_FROZEN_ERROR)\n        if isinstance(name_or_path, (string_types, Feature)):\n            return dict.__delitem__(self, name_or_path)\n        elif isinstance(name_or_path, tuple):\n            if len(name_or_path) == 0:\n                raise ValueError(\"The path () can not be set\")\n            else:\n                parent = self[name_or_path[:-1]]\n                if not isinstance(parent, FeatStruct):\n                    raise KeyError(name_or_path)  # path contains base value\n                del parent[name_or_path[-1]]\n        else:\n            raise TypeError(self._INDEX_ERROR % name_or_path)\n\n    def __setitem__(self, name_or_path, value):\n        \"\"\"Set the value for the feature with the given name or path\n        to ``value``.  If ``name_or_path`` is an invalid path, raise\n        ``KeyError``.\"\"\"\n        if self._frozen: raise ValueError(_FROZEN_ERROR)\n        if isinstance(name_or_path, (string_types, Feature)):\n            return dict.__setitem__(self, name_or_path, value)\n        elif isinstance(name_or_path, tuple):\n            if len(name_or_path) == 0:\n                raise ValueError(\"The path () can not be set\")\n            else:\n                parent = self[name_or_path[:-1]]\n                if not isinstance(parent, FeatStruct):\n                    raise KeyError(name_or_path)  # path contains base value\n                parent[name_or_path[-1]] = value\n        else:\n            raise TypeError(self._INDEX_ERROR % name_or_path)\n\n    clear = _check_frozen(dict.clear)\n    pop = _check_frozen(dict.pop)\n    popitem = _check_frozen(dict.popitem)\n    setdefault = _check_frozen(dict.setdefault)\n\n    def update(self, features=None, **morefeatures):\n        if self._frozen: raise ValueError(_FROZEN_ERROR)\n        if features is None:\n            items = ()\n        elif hasattr(features, 'items') and callable(features.items):\n            items = features.items()\n        elif hasattr(features, '__iter__'):\n            items = features\n        else:\n            raise ValueError('Expected mapping or list of tuples')\n\n        for key, val in items:\n            if not isinstance(key, (string_types, Feature)):\n                raise TypeError('Feature names must be strings')\n            self[key] = val\n        for key, val in morefeatures.items():\n            if not isinstance(key, (string_types, Feature)):\n                raise TypeError('Feature names must be strings')\n            self[key] = val\n\n\n    def __deepcopy__(self, memo):\n        memo[id(self)] = selfcopy = self.__class__()\n        for (key, val) in self._items():\n            selfcopy[copy.deepcopy(key, memo)] = copy.deepcopy(val, memo)\n        return selfcopy\n\n\n    def _keys(self):\n        return self.keys()\n\n    def _values(self):\n        return self.values()\n\n    def _items(self):\n        return self.items()\n\n\n    def __str__(self):\n        \"\"\"\n        Display a multi-line representation of this feature dictionary\n        as an FVM (feature value matrix).\n        \"\"\"\n        return '\\n'.join(self._str(self._find_reentrances({}), {}))\n\n    def _repr(self, reentrances, reentrance_ids):\n        segments = []\n        prefix = ''\n        suffix = ''\n\n        if reentrances[id(self)]:\n            assert id(self) not in reentrance_ids\n            reentrance_ids[id(self)] = repr(len(reentrance_ids) + 1)\n\n        for (fname, fval) in sorted(self.items()):\n            display = getattr(fname, 'display', None)\n            if id(fval) in reentrance_ids:\n                segments.append('%s->(%s)' %\n                                (fname, reentrance_ids[id(fval)]))\n            elif (display == 'prefix' and not prefix and\n                  isinstance(fval, (Variable, string_types))):\n                prefix = '%s' % fval\n            elif display == 'slash' and not suffix:\n                if isinstance(fval, Variable):\n                    suffix = '/%s' % fval.name\n                else:\n                    suffix = '/%s' % unicode_repr(fval)\n            elif isinstance(fval, Variable):\n                segments.append('%s=%s' % (fname, fval.name))\n            elif fval is True:\n                segments.append('+%s' % fname)\n            elif fval is False:\n                segments.append('-%s' % fname)\n            elif isinstance(fval, Expression):\n                segments.append('%s=<%s>' % (fname, fval))\n            elif not isinstance(fval, FeatStruct):\n                segments.append('%s=%s' % (fname, unicode_repr(fval)))\n            else:\n                fval_repr = fval._repr(reentrances, reentrance_ids)\n                segments.append('%s=%s' % (fname, fval_repr))\n        if reentrances[id(self)]:\n            prefix = '(%s)%s' % (reentrance_ids[id(self)], prefix)\n        return '%s[%s]%s' % (prefix, ', '.join(segments), suffix)\n\n    def _str(self, reentrances, reentrance_ids):\n        \"\"\"\n        :return: A list of lines composing a string representation of\n            this feature dictionary.\n        :param reentrances: A dictionary that maps from the ``id`` of\n            each feature value in self, indicating whether that value\n            is reentrant or not.\n        :param reentrance_ids: A dictionary mapping from each ``id``\n            of a feature value to a unique identifier.  This is modified\n            by ``repr``: the first time a reentrant feature value is\n            displayed, an identifier is added to ``reentrance_ids`` for\n            it.\n        \"\"\"\n        if reentrances[id(self)]:\n            assert id(self) not in reentrance_ids\n            reentrance_ids[id(self)] = repr(len(reentrance_ids) + 1)\n\n        if len(self) == 0:\n            if reentrances[id(self)]:\n                return ['(%s) []' % reentrance_ids[id(self)]]\n            else:\n                return ['[]']\n\n        maxfnamelen = max(len(\"%s\" % k) for k in self.keys())\n\n        lines = []\n        for (fname, fval) in sorted(self.items()):\n            fname = (\"%s\" % fname).ljust(maxfnamelen)\n            if isinstance(fval, Variable):\n                lines.append('%s = %s' % (fname, fval.name))\n\n            elif isinstance(fval, Expression):\n                lines.append('%s = <%s>' % (fname, fval))\n\n            elif isinstance(fval, FeatList):\n                fval_repr = fval._repr(reentrances, reentrance_ids)\n                lines.append('%s = %s' % (fname, unicode_repr(fval_repr)))\n\n            elif not isinstance(fval, FeatDict):\n                lines.append('%s = %s' % (fname, unicode_repr(fval)))\n\n            elif id(fval) in reentrance_ids:\n                lines.append('%s -> (%s)' % (fname, reentrance_ids[id(fval)]))\n\n            else:\n                if lines and lines[-1] != '': lines.append('')\n\n                fval_lines = fval._str(reentrances, reentrance_ids)\n\n                fval_lines = [(' ' * (maxfnamelen + 3)) + l for l in fval_lines]\n\n                nameline = (len(fval_lines) - 1) // 2\n                fval_lines[nameline] = (\n                        fname + ' =' + fval_lines[nameline][maxfnamelen + 2:])\n\n                lines += fval_lines\n\n                lines.append('')\n\n        if lines[-1] == '': lines.pop()\n\n        maxlen = max(len(line) for line in lines)\n        lines = ['[ %s%s ]' % (line, ' ' * (maxlen - len(line))) for line in lines]\n\n        if reentrances[id(self)]:\n            idstr = '(%s) ' % reentrance_ids[id(self)]\n            lines = [(' ' * len(idstr)) + l for l in lines]\n            idline = (len(lines) - 1) // 2\n            lines[idline] = idstr + lines[idline][len(idstr):]\n\n        return lines\n\n\n\nclass FeatList(FeatStruct, list):\n    \"\"\"\n    A list of feature values, where each feature value is either a\n    basic value (such as a string or an integer), or a nested feature\n    structure.\n\n    Feature lists may contain reentrant feature values.  A \"reentrant\n    feature value\" is a single feature value that can be accessed via\n    multiple feature paths.  Feature lists may also be cyclic.\n\n    Two feature lists are considered equal if they assign the same\n    values to all features, and have the same reentrances.\n\n    :see: ``FeatStruct`` for information about feature paths, reentrance,\n        cyclic feature structures, mutability, freezing, and hashing.\n    \"\"\"\n\n    def __init__(self, features=()):\n        \"\"\"\n        Create a new feature list, with the specified features.\n\n        :param features: The initial list of features for this feature\n            list.  If ``features`` is a string, then it is paresd using\n            ``FeatStructReader``.  Otherwise, it should be a sequence\n            of basic values and nested feature structures.\n        \"\"\"\n        if isinstance(features, string_types):\n            FeatStructReader().fromstring(features, self)\n        else:\n            list.__init__(self, features)\n\n    _INDEX_ERROR = \"Expected int or feature path.  Got %r.\"\n\n    def __getitem__(self, name_or_path):\n        if isinstance(name_or_path, integer_types):\n            return list.__getitem__(self, name_or_path)\n        elif isinstance(name_or_path, tuple):\n            try:\n                val = self\n                for fid in name_or_path:\n                    if not isinstance(val, FeatStruct):\n                        raise KeyError  # path contains base value\n                    val = val[fid]\n                return val\n            except (KeyError, IndexError):\n                raise KeyError(name_or_path)\n        else:\n            raise TypeError(self._INDEX_ERROR % name_or_path)\n\n    def __delitem__(self, name_or_path):\n        \"\"\"If the feature with the given name or path exists, delete\n        its value; otherwise, raise ``KeyError``.\"\"\"\n        if self._frozen: raise ValueError(_FROZEN_ERROR)\n        if isinstance(name_or_path, (integer_types, slice)):\n            return list.__delitem__(self, name_or_path)\n        elif isinstance(name_or_path, tuple):\n            if len(name_or_path) == 0:\n                raise ValueError(\"The path () can not be set\")\n            else:\n                parent = self[name_or_path[:-1]]\n                if not isinstance(parent, FeatStruct):\n                    raise KeyError(name_or_path)  # path contains base value\n                del parent[name_or_path[-1]]\n        else:\n            raise TypeError(self._INDEX_ERROR % name_or_path)\n\n    def __setitem__(self, name_or_path, value):\n        \"\"\"Set the value for the feature with the given name or path\n        to ``value``.  If ``name_or_path`` is an invalid path, raise\n        ``KeyError``.\"\"\"\n        if self._frozen: raise ValueError(_FROZEN_ERROR)\n        if isinstance(name_or_path, (integer_types, slice)):\n            return list.__setitem__(self, name_or_path, value)\n        elif isinstance(name_or_path, tuple):\n            if len(name_or_path) == 0:\n                raise ValueError(\"The path () can not be set\")\n            else:\n                parent = self[name_or_path[:-1]]\n                if not isinstance(parent, FeatStruct):\n                    raise KeyError(name_or_path)  # path contains base value\n                parent[name_or_path[-1]] = value\n        else:\n            raise TypeError(self._INDEX_ERROR % name_or_path)\n\n    __iadd__ = _check_frozen(list.__iadd__)\n    __imul__ = _check_frozen(list.__imul__)\n    append = _check_frozen(list.append)\n    extend = _check_frozen(list.extend)\n    insert = _check_frozen(list.insert)\n    pop = _check_frozen(list.pop)\n    remove = _check_frozen(list.remove)\n    reverse = _check_frozen(list.reverse)\n    sort = _check_frozen(list.sort)\n\n\n    def __deepcopy__(self, memo):\n        memo[id(self)] = selfcopy = self.__class__()\n        selfcopy.extend(copy.deepcopy(fval, memo) for fval in self)\n        return selfcopy\n\n\n    def _keys(self):\n        return list(range(len(self)))\n\n    def _values(self):\n        return self\n\n    def _items(self):\n        return enumerate(self)\n\n\n    def _repr(self, reentrances, reentrance_ids):\n        if reentrances[id(self)]:\n            assert id(self) not in reentrance_ids\n            reentrance_ids[id(self)] = repr(len(reentrance_ids) + 1)\n            prefix = '(%s)' % reentrance_ids[id(self)]\n        else:\n            prefix = ''\n\n        segments = []\n        for fval in self:\n            if id(fval) in reentrance_ids:\n                segments.append('->(%s)' % reentrance_ids[id(fval)])\n            elif isinstance(fval, Variable):\n                segments.append(fval.name)\n            elif isinstance(fval, Expression):\n                segments.append('%s' % fval)\n            elif isinstance(fval, FeatStruct):\n                segments.append(fval._repr(reentrances, reentrance_ids))\n            else:\n                segments.append('%s' % unicode_repr(fval))\n\n        return '%s[%s]' % (prefix, ', '.join(segments))\n\n\n\ndef substitute_bindings(fstruct, bindings, fs_class='default'):\n    \"\"\"\n    Return the feature structure that is obtained by replacing each\n    variable bound by ``bindings`` with its binding.  If a variable is\n    aliased to a bound variable, then it will be replaced by that\n    variable's value.  If a variable is aliased to an unbound\n    variable, then it will be replaced by that variable.\n\n    :type bindings: dict(Variable -> any)\n    :param bindings: A dictionary mapping from variables to values.\n    \"\"\"\n    if fs_class == 'default': fs_class = _default_fs_class(fstruct)\n    fstruct = copy.deepcopy(fstruct)\n    _substitute_bindings(fstruct, bindings, fs_class, set())\n    return fstruct\n\n\ndef _substitute_bindings(fstruct, bindings, fs_class, visited):\n    if id(fstruct) in visited: return\n    visited.add(id(fstruct))\n\n    if _is_mapping(fstruct):\n        items = fstruct.items()\n    elif _is_sequence(fstruct):\n        items = enumerate(fstruct)\n    else:\n        raise ValueError('Expected mapping or sequence')\n    for (fname, fval) in items:\n        while (isinstance(fval, Variable) and fval in bindings):\n            fval = fstruct[fname] = bindings[fval]\n        if isinstance(fval, fs_class):\n            _substitute_bindings(fval, bindings, fs_class, visited)\n        elif isinstance(fval, SubstituteBindingsI):\n            fstruct[fname] = fval.substitute_bindings(bindings)\n\n\ndef retract_bindings(fstruct, bindings, fs_class='default'):\n    \"\"\"\n    Return the feature structure that is obtained by replacing each\n    feature structure value that is bound by ``bindings`` with the\n    variable that binds it.  A feature structure value must be\n    identical to a bound value (i.e., have equal id) to be replaced.\n\n    ``bindings`` is modified to point to this new feature structure,\n    rather than the original feature structure.  Feature structure\n    values in ``bindings`` may be modified if they are contained in\n    ``fstruct``.\n    \"\"\"\n    if fs_class == 'default': fs_class = _default_fs_class(fstruct)\n    (fstruct, new_bindings) = copy.deepcopy((fstruct, bindings))\n    bindings.update(new_bindings)\n    inv_bindings = dict((id(val), var) for (var, val) in bindings.items())\n    _retract_bindings(fstruct, inv_bindings, fs_class, set())\n    return fstruct\n\n\ndef _retract_bindings(fstruct, inv_bindings, fs_class, visited):\n    if id(fstruct) in visited: return\n    visited.add(id(fstruct))\n\n    if _is_mapping(fstruct):\n        items = fstruct.items()\n    elif _is_sequence(fstruct):\n        items = enumerate(fstruct)\n    else:\n        raise ValueError('Expected mapping or sequence')\n    for (fname, fval) in items:\n        if isinstance(fval, fs_class):\n            if id(fval) in inv_bindings:\n                fstruct[fname] = inv_bindings[id(fval)]\n            _retract_bindings(fval, inv_bindings, fs_class, visited)\n\n\ndef find_variables(fstruct, fs_class='default'):\n    \"\"\"\n    :return: The set of variables used by this feature structure.\n    :rtype: set(Variable)\n    \"\"\"\n    if fs_class == 'default': fs_class = _default_fs_class(fstruct)\n    return _variables(fstruct, set(), fs_class, set())\n\n\ndef _variables(fstruct, vars, fs_class, visited):\n    if id(fstruct) in visited: return\n    visited.add(id(fstruct))\n    if _is_mapping(fstruct):\n        items = fstruct.items()\n    elif _is_sequence(fstruct):\n        items = enumerate(fstruct)\n    else:\n        raise ValueError('Expected mapping or sequence')\n    for (fname, fval) in items:\n        if isinstance(fval, Variable):\n            vars.add(fval)\n        elif isinstance(fval, fs_class):\n            _variables(fval, vars, fs_class, visited)\n        elif isinstance(fval, SubstituteBindingsI):\n            vars.update(fval.variables())\n    return vars\n\n\ndef rename_variables(fstruct, vars=None, used_vars=(), new_vars=None,\n                     fs_class='default'):\n    \"\"\"\n    Return the feature structure that is obtained by replacing\n    any of this feature structure's variables that are in ``vars``\n    with new variables.  The names for these new variables will be\n    names that are not used by any variable in ``vars``, or in\n    ``used_vars``, or in this feature structure.\n\n    :type vars: set\n    :param vars: The set of variables that should be renamed.\n        If not specified, ``find_variables(fstruct)`` is used; i.e., all\n        variables will be given new names.\n    :type used_vars: set\n    :param used_vars: A set of variables whose names should not be\n        used by the new variables.\n    :type new_vars: dict(Variable -> Variable)\n    :param new_vars: A dictionary that is used to hold the mapping\n        from old variables to new variables.  For each variable *v*\n        in this feature structure:\n\n        - If ``new_vars`` maps *v* to *v'*, then *v* will be\n          replaced by *v'*.\n        - If ``new_vars`` does not contain *v*, but ``vars``\n          does contain *v*, then a new entry will be added to\n          ``new_vars``, mapping *v* to the new variable that is used\n          to replace it.\n\n    To consistently rename the variables in a set of feature\n    structures, simply apply rename_variables to each one, using\n    the same dictionary:\n\n        >>> from nltk.featstruct import FeatStruct\n        >>> fstruct1 = FeatStruct('[subj=[agr=[gender=?y]], obj=[agr=[gender=?y]]]')\n        >>> fstruct2 = FeatStruct('[subj=[agr=[number=?z,gender=?y]], obj=[agr=[number=?z,gender=?y]]]')\n        >>> new_vars = {}  # Maps old vars to alpha-renamed vars\n        >>> fstruct1.rename_variables(new_vars=new_vars)\n        [obj=[agr=[gender=?y2]], subj=[agr=[gender=?y2]]]\n        >>> fstruct2.rename_variables(new_vars=new_vars)\n        [obj=[agr=[gender=?y2, number=?z2]], subj=[agr=[gender=?y2, number=?z2]]]\n\n    If new_vars is not specified, then an empty dictionary is used.\n    \"\"\"\n    if fs_class == 'default': fs_class = _default_fs_class(fstruct)\n\n    if new_vars is None: new_vars = {}\n    if vars is None:\n        vars = find_variables(fstruct, fs_class)\n    else:\n        vars = set(vars)\n\n    used_vars = find_variables(fstruct, fs_class).union(used_vars)\n\n    return _rename_variables(copy.deepcopy(fstruct), vars, used_vars,\n                             new_vars, fs_class, set())\n\n\ndef _rename_variables(fstruct, vars, used_vars, new_vars, fs_class, visited):\n    if id(fstruct) in visited: return\n    visited.add(id(fstruct))\n    if _is_mapping(fstruct):\n        items = fstruct.items()\n    elif _is_sequence(fstruct):\n        items = enumerate(fstruct)\n    else:\n        raise ValueError('Expected mapping or sequence')\n    for (fname, fval) in items:\n        if isinstance(fval, Variable):\n            if fval in new_vars:\n                fstruct[fname] = new_vars[fval]\n            elif fval in vars:\n                new_vars[fval] = _rename_variable(fval, used_vars)\n                fstruct[fname] = new_vars[fval]\n                used_vars.add(new_vars[fval])\n        elif isinstance(fval, fs_class):\n            _rename_variables(fval, vars, used_vars, new_vars,\n                              fs_class, visited)\n        elif isinstance(fval, SubstituteBindingsI):\n            for var in fval.variables():\n                if var in vars and var not in new_vars:\n                    new_vars[var] = _rename_variable(var, used_vars)\n                    used_vars.add(new_vars[var])\n            fstruct[fname] = fval.substitute_bindings(new_vars)\n    return fstruct\n\n\ndef _rename_variable(var, used_vars):\n    name, n = re.sub('\\d+$', '', var.name), 2\n    if not name: name = '?'\n    while Variable('%s%s' % (name, n)) in used_vars: n += 1\n    return Variable('%s%s' % (name, n))\n\n\ndef remove_variables(fstruct, fs_class='default'):\n    \"\"\"\n    :rtype: FeatStruct\n    :return: The feature structure that is obtained by deleting\n        all features whose values are ``Variables``.\n    \"\"\"\n    if fs_class == 'default': fs_class = _default_fs_class(fstruct)\n    return _remove_variables(copy.deepcopy(fstruct), fs_class, set())\n\n\ndef _remove_variables(fstruct, fs_class, visited):\n    if id(fstruct) in visited:\n        return\n    visited.add(id(fstruct))\n\n    if _is_mapping(fstruct):\n        items = list(fstruct.items())\n    elif _is_sequence(fstruct):\n        items = list(enumerate(fstruct))\n    else:\n        raise ValueError('Expected mapping or sequence')\n\n    for (fname, fval) in items:\n        if isinstance(fval, Variable):\n            del fstruct[fname]\n        elif isinstance(fval, fs_class):\n            _remove_variables(fval, fs_class, visited)\n    return fstruct\n\n\n\n@python_2_unicode_compatible\nclass _UnificationFailure(object):\n    def __repr__(self):\n        return 'nltk.featstruct.UnificationFailure'\n\n\nUnificationFailure = _UnificationFailure()\n\"\"\"A unique value used to indicate unification failure.  It can be\n   returned by ``Feature.unify_base_values()`` or by custom ``fail()``\n   functions to indicate that unificaiton should fail.\"\"\"\n\n\ndef unify(fstruct1, fstruct2, bindings=None, trace=False,\n          fail=None, rename_vars=True, fs_class='default'):\n    \"\"\"\n    Unify ``fstruct1`` with ``fstruct2``, and return the resulting feature\n    structure.  This unified feature structure is the minimal\n    feature structure that contains all feature value assignments from both\n    ``fstruct1`` and ``fstruct2``, and that preserves all reentrancies.\n\n    If no such feature structure exists (because ``fstruct1`` and\n    ``fstruct2`` specify incompatible values for some feature), then\n    unification fails, and ``unify`` returns None.\n\n    Bound variables are replaced by their values.  Aliased\n    variables are replaced by their representative variable\n    (if unbound) or the value of their representative variable\n    (if bound).  I.e., if variable *v* is in ``bindings``,\n    then *v* is replaced by ``bindings[v]``.  This will\n    be repeated until the variable is replaced by an unbound\n    variable or a non-variable value.\n\n    Unbound variables are bound when they are unified with\n    values; and aliased when they are unified with variables.\n    I.e., if variable *v* is not in ``bindings``, and is\n    unified with a variable or value *x*, then\n    ``bindings[v]`` is set to *x*.\n\n    If ``bindings`` is unspecified, then all variables are\n    assumed to be unbound.  I.e., ``bindings`` defaults to an\n    empty dict.\n\n        >>> from nltk.featstruct import FeatStruct\n        >>> FeatStruct('[a=?x]').unify(FeatStruct('[b=?x]'))\n        [a=?x, b=?x2]\n\n    :type bindings: dict(Variable -> any)\n    :param bindings: A set of variable bindings to be used and\n        updated during unification.\n    :type trace: bool\n    :param trace: If true, generate trace output.\n    :type rename_vars: bool\n    :param rename_vars: If True, then rename any variables in\n        ``fstruct2`` that are also used in ``fstruct1``, in order to\n        avoid collisions on variable names.\n    \"\"\"\n    if fs_class == 'default':\n        fs_class = _default_fs_class(fstruct1)\n        if _default_fs_class(fstruct2) != fs_class:\n            raise ValueError(\"Mixing FeatStruct objects with Python \"\n                             \"dicts and lists is not supported.\")\n    assert isinstance(fstruct1, fs_class)\n    assert isinstance(fstruct2, fs_class)\n\n    user_bindings = (bindings is not None)\n    if bindings is None: bindings = {}\n\n    (fstruct1copy, fstruct2copy, bindings_copy) = (\n        copy.deepcopy((fstruct1, fstruct2, bindings)))\n\n    bindings.update(bindings_copy)\n\n    if rename_vars:\n        vars1 = find_variables(fstruct1copy, fs_class)\n        vars2 = find_variables(fstruct2copy, fs_class)\n        _rename_variables(fstruct2copy, vars1, vars2, {}, fs_class, set())\n\n    forward = {}\n    if trace: _trace_unify_start((), fstruct1copy, fstruct2copy)\n    try:\n        result = _destructively_unify(fstruct1copy, fstruct2copy, bindings,\n                                      forward, trace, fail, fs_class, ())\n    except _UnificationFailureError:\n        return None\n\n    if result is UnificationFailure:\n        if fail is None:\n            return None\n        else:\n            return fail(fstruct1copy, fstruct2copy, ())\n\n    result = _apply_forwards(result, forward, fs_class, set())\n    if user_bindings: _apply_forwards_to_bindings(forward, bindings)\n\n    _resolve_aliases(bindings)\n    _substitute_bindings(result, bindings, fs_class, set())\n\n    if trace: _trace_unify_succeed((), result)\n    if trace: _trace_bindings((), bindings)\n    return result\n\n\nclass _UnificationFailureError(Exception):\n    \"\"\"An exception that is used by ``_destructively_unify`` to abort\n    unification when a failure is encountered.\"\"\"\n\n\ndef _destructively_unify(fstruct1, fstruct2, bindings, forward,\n                         trace, fail, fs_class, path):\n    \"\"\"\n    Attempt to unify ``fstruct1`` and ``fstruct2`` by modifying them\n    in-place.  If the unification succeeds, then ``fstruct1`` will\n    contain the unified value, the value of ``fstruct2`` is undefined,\n    and forward[id(fstruct2)] is set to fstruct1.  If the unification\n    fails, then a _UnificationFailureError is raised, and the\n    values of ``fstruct1`` and ``fstruct2`` are undefined.\n\n    :param bindings: A dictionary mapping variables to values.\n    :param forward: A dictionary mapping feature structures ids\n        to replacement structures.  When two feature structures\n        are merged, a mapping from one to the other will be added\n        to the forward dictionary; and changes will be made only\n        to the target of the forward dictionary.\n        ``_destructively_unify`` will always 'follow' any links\n        in the forward dictionary for fstruct1 and fstruct2 before\n        actually unifying them.\n    :param trace: If true, generate trace output\n    :param path: The feature path that led us to this unification\n        step.  Used for trace output.\n    \"\"\"\n    if fstruct1 is fstruct2:\n        if trace: _trace_unify_identity(path, fstruct1)\n        return fstruct1\n\n    forward[id(fstruct2)] = fstruct1\n\n    if _is_mapping(fstruct1) and _is_mapping(fstruct2):\n        for fname in fstruct1:\n            if getattr(fname, 'default', None) is not None:\n                fstruct2.setdefault(fname, fname.default)\n        for fname in fstruct2:\n            if getattr(fname, 'default', None) is not None:\n                fstruct1.setdefault(fname, fname.default)\n\n        for fname, fval2 in sorted(fstruct2.items()):\n            if fname in fstruct1:\n                fstruct1[fname] = _unify_feature_values(\n                    fname, fstruct1[fname], fval2, bindings,\n                    forward, trace, fail, fs_class, path + (fname,))\n            else:\n                fstruct1[fname] = fval2\n\n        return fstruct1  # Contains the unified value.\n\n    elif _is_sequence(fstruct1) and _is_sequence(fstruct2):\n        if len(fstruct1) != len(fstruct2):\n            return UnificationFailure\n\n        for findex in range(len(fstruct1)):\n            fstruct1[findex] = _unify_feature_values(\n                findex, fstruct1[findex], fstruct2[findex], bindings,\n                forward, trace, fail, fs_class, path + (findex,))\n\n        return fstruct1  # Contains the unified value.\n\n    elif ((_is_sequence(fstruct1) or _is_mapping(fstruct1)) and\n          (_is_sequence(fstruct2) or _is_mapping(fstruct2))):\n        return UnificationFailure\n\n    raise TypeError('Expected mappings or sequences')\n\n\ndef _unify_feature_values(fname, fval1, fval2, bindings, forward,\n                          trace, fail, fs_class, fpath):\n    \"\"\"\n    Attempt to unify ``fval1`` and and ``fval2``, and return the\n    resulting unified value.  The method of unification will depend on\n    the types of ``fval1`` and ``fval2``:\n\n      1. If they're both feature structures, then destructively\n         unify them (see ``_destructively_unify()``.\n      2. If they're both unbound variables, then alias one variable\n         to the other (by setting bindings[v2]=v1).\n      3. If one is an unbound variable, and the other is a value,\n         then bind the unbound variable to the value.\n      4. If one is a feature structure, and the other is a base value,\n         then fail.\n      5. If they're both base values, then unify them.  By default,\n         this will succeed if they are equal, and fail otherwise.\n    \"\"\"\n    if trace: _trace_unify_start(fpath, fval1, fval2)\n\n    while id(fval1) in forward: fval1 = forward[id(fval1)]\n    while id(fval2) in forward: fval2 = forward[id(fval2)]\n\n    fvar1 = fvar2 = None\n    while isinstance(fval1, Variable) and fval1 in bindings:\n        fvar1 = fval1\n        fval1 = bindings[fval1]\n    while isinstance(fval2, Variable) and fval2 in bindings:\n        fvar2 = fval2\n        fval2 = bindings[fval2]\n\n    if isinstance(fval1, fs_class) and isinstance(fval2, fs_class):\n        result = _destructively_unify(fval1, fval2, bindings, forward,\n                                      trace, fail, fs_class, fpath)\n\n    elif (isinstance(fval1, Variable) and\n          isinstance(fval2, Variable)):\n        if fval1 != fval2: bindings[fval2] = fval1\n        result = fval1\n\n    elif isinstance(fval1, Variable):\n        bindings[fval1] = fval2\n        result = fval1\n    elif isinstance(fval2, Variable):\n        bindings[fval2] = fval1\n        result = fval2\n\n    elif isinstance(fval1, fs_class) or isinstance(fval2, fs_class):\n        result = UnificationFailure\n\n    else:\n        if isinstance(fname, Feature):\n            result = fname.unify_base_values(fval1, fval2, bindings)\n        elif isinstance(fval1, CustomFeatureValue):\n            result = fval1.unify(fval2)\n            if (isinstance(fval2, CustomFeatureValue) and\n                    result != fval2.unify(fval1)):\n                raise AssertionError(\n                    'CustomFeatureValue objects %r and %r disagree '\n                    'about unification value: %r vs. %r' %\n                    (fval1, fval2, result, fval2.unify(fval1)))\n        elif isinstance(fval2, CustomFeatureValue):\n            result = fval2.unify(fval1)\n        else:\n            if fval1 == fval2:\n                result = fval1\n            else:\n                result = UnificationFailure\n\n        if result is not UnificationFailure:\n            if fvar1 is not None:\n                bindings[fvar1] = result\n                result = fvar1\n            if fvar2 is not None and fvar2 != fvar1:\n                bindings[fvar2] = result\n                result = fvar2\n\n    if result is UnificationFailure:\n        if fail is not None: result = fail(fval1, fval2, fpath)\n        if trace: _trace_unify_fail(fpath[:-1], result)\n        if result is UnificationFailure:\n            raise _UnificationFailureError\n\n    if isinstance(result, fs_class):\n        result = _apply_forwards(result, forward, fs_class, set())\n\n    if trace: _trace_unify_succeed(fpath, result)\n    if trace and isinstance(result, fs_class):\n        _trace_bindings(fpath, bindings)\n\n    return result\n\n\ndef _apply_forwards_to_bindings(forward, bindings):\n    \"\"\"\n    Replace any feature structure that has a forward pointer with\n    the target of its forward pointer (to preserve reentrancy).\n    \"\"\"\n    for (var, value) in bindings.items():\n        while id(value) in forward:\n            value = forward[id(value)]\n        bindings[var] = value\n\n\ndef _apply_forwards(fstruct, forward, fs_class, visited):\n    \"\"\"\n    Replace any feature structure that has a forward pointer with\n    the target of its forward pointer (to preserve reentrancy).\n    \"\"\"\n    while id(fstruct) in forward: fstruct = forward[id(fstruct)]\n\n    if id(fstruct) in visited: return\n    visited.add(id(fstruct))\n\n    if _is_mapping(fstruct):\n        items = fstruct.items()\n    elif _is_sequence(fstruct):\n        items = enumerate(fstruct)\n    else:\n        raise ValueError('Expected mapping or sequence')\n    for fname, fval in items:\n        if isinstance(fval, fs_class):\n            while id(fval) in forward:\n                fval = forward[id(fval)]\n            fstruct[fname] = fval\n            _apply_forwards(fval, forward, fs_class, visited)\n\n    return fstruct\n\n\ndef _resolve_aliases(bindings):\n    \"\"\"\n    Replace any bound aliased vars with their binding; and replace\n    any unbound aliased vars with their representative var.\n    \"\"\"\n    for (var, value) in bindings.items():\n        while isinstance(value, Variable) and value in bindings:\n            value = bindings[var] = bindings[value]\n\n\ndef _trace_unify_start(path, fval1, fval2):\n    if path == ():\n        print('\\nUnification trace:')\n    else:\n        fullname = '.'.join(\"%s\" % n for n in path)\n        print('  ' + '|   ' * (len(path) - 1) + '|')\n        print('  ' + '|   ' * (len(path) - 1) + '| Unify feature: %s' % fullname)\n    print('  ' + '|   ' * len(path) + ' / ' + _trace_valrepr(fval1))\n    print('  ' + '|   ' * len(path) + '|\\\\ ' + _trace_valrepr(fval2))\n\n\ndef _trace_unify_identity(path, fval1):\n    print('  ' + '|   ' * len(path) + '|')\n    print('  ' + '|   ' * len(path) + '| (identical objects)')\n    print('  ' + '|   ' * len(path) + '|')\n    print('  ' + '|   ' * len(path) + '+-->' + unicode_repr(fval1))\n\n\ndef _trace_unify_fail(path, result):\n    if result is UnificationFailure:\n        resume = ''\n    else:\n        resume = ' (nonfatal)'\n    print('  ' + '|   ' * len(path) + '|   |')\n    print('  ' + 'X   ' * len(path) + 'X   X <-- FAIL' + resume)\n\n\ndef _trace_unify_succeed(path, fval1):\n    print('  ' + '|   ' * len(path) + '|')\n    print('  ' + '|   ' * len(path) + '+-->' + unicode_repr(fval1))\n\n\ndef _trace_bindings(path, bindings):\n    if len(bindings) > 0:\n        binditems = sorted(bindings.items(), key=lambda v: v[0].name)\n        bindstr = '{%s}' % ', '.join(\n            '%s: %s' % (var, _trace_valrepr(val))\n            for (var, val) in binditems)\n        print('  ' + '|   ' * len(path) + '    Bindings: ' + bindstr)\n\n\ndef _trace_valrepr(val):\n    if isinstance(val, Variable):\n        return '%s' % val\n    else:\n        return '%s' % unicode_repr(val)\n\n\ndef subsumes(fstruct1, fstruct2):\n    \"\"\"\n    Return True if ``fstruct1`` subsumes ``fstruct2``.  I.e., return\n    true if unifying ``fstruct1`` with ``fstruct2`` would result in a\n    feature structure equal to ``fstruct2.``\n\n    :rtype: bool\n    \"\"\"\n    return fstruct2 == unify(fstruct1, fstruct2)\n\n\ndef conflicts(fstruct1, fstruct2, trace=0):\n    \"\"\"\n    Return a list of the feature paths of all features which are\n    assigned incompatible values by ``fstruct1`` and ``fstruct2``.\n\n    :rtype: list(tuple)\n    \"\"\"\n    conflict_list = []\n\n    def add_conflict(fval1, fval2, path):\n        conflict_list.append(path)\n        return fval1\n\n    unify(fstruct1, fstruct2, fail=add_conflict, trace=trace)\n    return conflict_list\n\n\n\ndef _is_mapping(v):\n    return hasattr(v, '__contains__') and hasattr(v, 'keys')\n\n\ndef _is_sequence(v):\n    return (hasattr(v, '__iter__') and hasattr(v, '__len__') and\n            not isinstance(v, string_types))\n\n\ndef _default_fs_class(obj):\n    if isinstance(obj, FeatStruct): return FeatStruct\n    if isinstance(obj, (dict, list)):\n        return (dict, list)\n    else:\n        raise ValueError('To unify objects of type %s, you must specify '\n                         'fs_class explicitly.' % obj.__class__.__name__)\n\n\n\nclass SubstituteBindingsSequence(SubstituteBindingsI):\n    \"\"\"\n    A mixin class for sequence clases that distributes variables() and\n    substitute_bindings() over the object's elements.\n    \"\"\"\n\n    def variables(self):\n        return ([elt for elt in self if isinstance(elt, Variable)] +\n                sum([list(elt.variables()) for elt in self\n                     if isinstance(elt, SubstituteBindingsI)], []))\n\n    def substitute_bindings(self, bindings):\n        return self.__class__([self.subst(v, bindings) for v in self])\n\n    def subst(self, v, bindings):\n        if isinstance(v, SubstituteBindingsI):\n            return v.substitute_bindings(bindings)\n        else:\n            return bindings.get(v, v)\n\n\n@python_2_unicode_compatible\nclass FeatureValueTuple(SubstituteBindingsSequence, tuple):\n    \"\"\"\n    A base feature value that is a tuple of other base feature values.\n    FeatureValueTuple implements ``SubstituteBindingsI``, so it any\n    variable substitutions will be propagated to the elements\n    contained by the set.  A ``FeatureValueTuple`` is immutable.\n    \"\"\"\n\n    def __repr__(self):  # [xx] really use %s here?\n        if len(self) == 0: return '()'\n        return '(%s)' % ', '.join('%s' % (b,) for b in self)\n\n\n@python_2_unicode_compatible\nclass FeatureValueSet(SubstituteBindingsSequence, frozenset):\n    \"\"\"\n    A base feature value that is a set of other base feature values.\n    FeatureValueSet implements ``SubstituteBindingsI``, so it any\n    variable substitutions will be propagated to the elements\n    contained by the set.  A ``FeatureValueSet`` is immutable.\n    \"\"\"\n\n    def __repr__(self):  # [xx] really use %s here?\n        if len(self) == 0: return '{/}'  # distinguish from dict.\n        return '{%s}' % ', '.join(sorted('%s' % (b,) for b in self))\n\n    __str__ = __repr__\n\n\n@python_2_unicode_compatible\nclass FeatureValueUnion(SubstituteBindingsSequence, frozenset):\n    \"\"\"\n    A base feature value that represents the union of two or more\n    ``FeatureValueSet`` or ``Variable``.\n    \"\"\"\n\n    def __new__(cls, values):\n        values = _flatten(values, FeatureValueUnion)\n\n        if sum(isinstance(v, Variable) for v in values) == 0:\n            values = _flatten(values, FeatureValueSet)\n            return FeatureValueSet(values)\n\n        if len(values) == 1:\n            return list(values)[0]\n\n        return frozenset.__new__(cls, values)\n\n    def __repr__(self):\n        return '{%s}' % '+'.join(sorted('%s' % (b,) for b in self))\n\n\n@python_2_unicode_compatible\nclass FeatureValueConcat(SubstituteBindingsSequence, tuple):\n    \"\"\"\n    A base feature value that represents the concatenation of two or\n    more ``FeatureValueTuple`` or ``Variable``.\n    \"\"\"\n\n    def __new__(cls, values):\n        values = _flatten(values, FeatureValueConcat)\n\n        if sum(isinstance(v, Variable) for v in values) == 0:\n            values = _flatten(values, FeatureValueTuple)\n            return FeatureValueTuple(values)\n\n        if len(values) == 1:\n            return list(values)[0]\n\n        return tuple.__new__(cls, values)\n\n    def __repr__(self):\n        return '(%s)' % '+'.join('%s' % (b,) for b in self)\n\n\ndef _flatten(lst, cls):\n    \"\"\"\n    Helper function -- return a copy of list, with all elements of\n    type ``cls`` spliced in rather than appended in.\n    \"\"\"\n    result = []\n    for elt in lst:\n        if isinstance(elt, cls):\n            result.extend(elt)\n        else:\n            result.append(elt)\n    return result\n\n\n\n@total_ordering\n@python_2_unicode_compatible\nclass Feature(object):\n    \"\"\"\n    A feature identifier that's specialized to put additional\n    constraints, default values, etc.\n    \"\"\"\n\n    def __init__(self, name, default=None, display=None):\n        assert display in (None, 'prefix', 'slash')\n\n        self._name = name  # [xx] rename to .identifier?\n        self._default = default  # [xx] not implemented yet.\n        self._display = display\n\n        if self._display == 'prefix':\n            self._sortkey = (-1, self._name)\n        elif self._display == 'slash':\n            self._sortkey = (1, self._name)\n        else:\n            self._sortkey = (0, self._name)\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def default(self):\n        return self._default\n\n    @property\n    def display(self):\n        return self._display\n\n    def __repr__(self):\n        return '*%s*' % self.name\n\n    def __lt__(self, other):\n        if isinstance(other, string_types):\n            return True\n        if not isinstance(other, Feature):\n            raise_unorderable_types(\"<\", self, other)\n        return self._sortkey < other._sortkey\n\n    def __eq__(self, other):\n        return type(self) == type(other) and self._name == other._name\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash(self._name)\n\n\n    def read_value(self, s, position, reentrances, parser):\n        return parser.read_value(s, position, reentrances)\n\n    def unify_base_values(self, fval1, fval2, bindings):\n        \"\"\"\n        If possible, return a single value..  If not, return\n        the value ``UnificationFailure``.\n        \"\"\"\n        if fval1 == fval2:\n            return fval1\n        else:\n            return UnificationFailure\n\n\nclass SlashFeature(Feature):\n    def read_value(self, s, position, reentrances, parser):\n        return parser.read_partial(s, position, reentrances)\n\n\nclass RangeFeature(Feature):\n    RANGE_RE = re.compile('(-?\\d+):(-?\\d+)')\n\n    def read_value(self, s, position, reentrances, parser):\n        m = self.RANGE_RE.match(s, position)\n        if not m: raise ValueError('range', position)\n        return (int(m.group(1)), int(m.group(2))), m.end()\n\n    def unify_base_values(self, fval1, fval2, bindings):\n        if fval1 is None: return fval2\n        if fval2 is None: return fval1\n        rng = max(fval1[0], fval2[0]), min(fval1[1], fval2[1])\n        if rng[1] < rng[0]: return UnificationFailure\n        return rng\n\n\nSLASH = SlashFeature('slash', default=False, display='slash')\nTYPE = Feature('type', display='prefix')\n\n\n\n@total_ordering\nclass CustomFeatureValue(object):\n    \"\"\"\n    An abstract base class for base values that define a custom\n    unification method.  The custom unification method of\n    ``CustomFeatureValue`` will be used during unification if:\n\n      - The ``CustomFeatureValue`` is unified with another base value.\n      - The ``CustomFeatureValue`` is not the value of a customized\n        ``Feature`` (which defines its own unification method).\n\n    If two ``CustomFeatureValue`` objects are unified with one another\n    during feature structure unification, then the unified base values\n    they return *must* be equal; otherwise, an ``AssertionError`` will\n    be raised.\n\n    Subclasses must define ``unify()``, ``__eq__()`` and ``__lt__()``.\n    Subclasses may also wish to define ``__hash__()``.\n    \"\"\"\n\n    def unify(self, other):\n        \"\"\"\n        If this base value unifies with ``other``, then return the\n        unified value.  Otherwise, return ``UnificationFailure``.\n        \"\"\"\n        raise NotImplementedError('abstract base class')\n\n    def __eq__(self, other):\n        raise NotImplementedError('abstract base class')\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __lt__(self, other):\n        raise NotImplementedError('abstract base class')\n\n    def __hash__(self):\n        raise TypeError('%s objects or unhashable' % self.__class__.__name__)\n\n\n\nclass FeatStructReader(object):\n    def __init__(self, features=(SLASH, TYPE), fdict_class=FeatStruct,\n                 flist_class=FeatList, logic_parser=None):\n        self._features = dict((f.name, f) for f in features)\n        self._fdict_class = fdict_class\n        self._flist_class = flist_class\n        self._prefix_feature = None\n        self._slash_feature = None\n        for feature in features:\n            if feature.display == 'slash':\n                if self._slash_feature:\n                    raise ValueError('Multiple features w/ display=slash')\n                self._slash_feature = feature\n            if feature.display == 'prefix':\n                if self._prefix_feature:\n                    raise ValueError('Multiple features w/ display=prefix')\n                self._prefix_feature = feature\n        self._features_with_defaults = [feature for feature in features\n                                        if feature.default is not None]\n        if logic_parser is None:\n            logic_parser = LogicParser()\n        self._logic_parser = logic_parser\n\n    def fromstring(self, s, fstruct=None):\n        \"\"\"\n        Convert a string representation of a feature structure (as\n        displayed by repr) into a ``FeatStruct``.  This process\n        imposes the following restrictions on the string\n        representation:\n\n        - Feature names cannot contain any of the following:\n          whitespace, parentheses, quote marks, equals signs,\n          dashes, commas, and square brackets.  Feature names may\n          not begin with plus signs or minus signs.\n        - Only the following basic feature value are supported:\n          strings, integers, variables, None, and unquoted\n          alphanumeric strings.\n        - For reentrant values, the first mention must specify\n          a reentrance identifier and a value; and any subsequent\n          mentions must use arrows (``'->'``) to reference the\n          reentrance identifier.\n        \"\"\"\n        s = s.strip()\n        value, position = self.read_partial(s, 0, {}, fstruct)\n        if position != len(s):\n            self._error(s, 'end of string', position)\n        return value\n\n    _START_FSTRUCT_RE = re.compile(r'\\s*(?:\\((\\d+)\\)\\s*)?(\\??[\\w-]+)?(\\[)')\n    _END_FSTRUCT_RE = re.compile(r'\\s*]\\s*')\n    _SLASH_RE = re.compile(r'/')\n    _FEATURE_NAME_RE = re.compile(r'\\s*([+-]?)([^\\s\\(\\)<>\"\\'\\-=\\[\\],]+)\\s*')\n    _REENTRANCE_RE = re.compile(r'\\s*->\\s*')\n    _TARGET_RE = re.compile(r'\\s*\\((\\d+)\\)\\s*')\n    _ASSIGN_RE = re.compile(r'\\s*=\\s*')\n    _COMMA_RE = re.compile(r'\\s*,\\s*')\n    _BARE_PREFIX_RE = re.compile(r'\\s*(?:\\((\\d+)\\)\\s*)?(\\??[\\w-]+\\s*)()')\n    _START_FDICT_RE = re.compile(r'(%s)|(%s\\s*(%s\\s*(=|->)|[+-]%s|\\]))' % (\n        _BARE_PREFIX_RE.pattern, _START_FSTRUCT_RE.pattern,\n        _FEATURE_NAME_RE.pattern, _FEATURE_NAME_RE.pattern))\n\n    def read_partial(self, s, position=0, reentrances=None, fstruct=None):\n        \"\"\"\n        Helper function that reads in a feature structure.\n\n        :param s: The string to read.\n        :param position: The position in the string to start parsing.\n        :param reentrances: A dictionary from reentrance ids to values.\n            Defaults to an empty dictionary.\n        :return: A tuple (val, pos) of the feature structure created by\n            parsing and the position where the parsed feature structure ends.\n        :rtype: bool\n        \"\"\"\n        if reentrances is None: reentrances = {}\n        try:\n            return self._read_partial(s, position, reentrances, fstruct)\n        except ValueError as e:\n            if len(e.args) != 2: raise\n            self._error(s, *e.args)\n\n    def _read_partial(self, s, position, reentrances, fstruct=None):\n        if fstruct is None:\n            if self._START_FDICT_RE.match(s, position):\n                fstruct = self._fdict_class()\n            else:\n                fstruct = self._flist_class()\n\n        match = self._START_FSTRUCT_RE.match(s, position)\n        if not match:\n            match = self._BARE_PREFIX_RE.match(s, position)\n            if not match:\n                raise ValueError('open bracket or identifier', position)\n        position = match.end()\n\n        if match.group(1):\n            identifier = match.group(1)\n            if identifier in reentrances:\n                raise ValueError('new identifier', match.start(1))\n            reentrances[identifier] = fstruct\n\n        if isinstance(fstruct, FeatDict):\n            fstruct.clear()\n            return self._read_partial_featdict(s, position, match,\n                                               reentrances, fstruct)\n        else:\n            del fstruct[:]\n            return self._read_partial_featlist(s, position, match,\n                                               reentrances, fstruct)\n\n    def _read_partial_featlist(self, s, position, match,\n                               reentrances, fstruct):\n        if match.group(2): raise ValueError('open bracket')\n        if not match.group(3): raise ValueError('open bracket')\n\n        while position < len(s):\n            match = self._END_FSTRUCT_RE.match(s, position)\n            if match is not None:\n                return fstruct, match.end()\n\n            match = self._REENTRANCE_RE.match(s, position)\n            if match:\n                position = match.end()\n                match = self._TARGET_RE.match(s, position)\n                if not match: raise ValueError('identifier', position)\n                target = match.group(1)\n                if target not in reentrances:\n                    raise ValueError('bound identifier', position)\n                position = match.end()\n                fstruct.append(reentrances[target])\n\n            else:\n                value, position = (\n                    self._read_value(0, s, position, reentrances))\n                fstruct.append(value)\n\n            if self._END_FSTRUCT_RE.match(s, position):\n                continue\n\n            match = self._COMMA_RE.match(s, position)\n            if match is None: raise ValueError('comma', position)\n            position = match.end()\n\n        raise ValueError('close bracket', position)\n\n    def _read_partial_featdict(self, s, position, match,\n                               reentrances, fstruct):\n        if match.group(2):\n            if self._prefix_feature is None:\n                raise ValueError('open bracket or identifier', match.start(2))\n            prefixval = match.group(2).strip()\n            if prefixval.startswith('?'):\n                prefixval = Variable(prefixval)\n            fstruct[self._prefix_feature] = prefixval\n\n        if not match.group(3):\n            return self._finalize(s, match.end(), reentrances, fstruct)\n\n        while position < len(s):\n            name = value = None\n\n            match = self._END_FSTRUCT_RE.match(s, position)\n            if match is not None:\n                return self._finalize(s, match.end(), reentrances, fstruct)\n\n            match = self._FEATURE_NAME_RE.match(s, position)\n            if match is None: raise ValueError('feature name', position)\n            name = match.group(2)\n            position = match.end()\n\n            if name[0] == '*' and name[-1] == '*':\n                name = self._features.get(name[1:-1])\n                if name is None:\n                    raise ValueError('known special feature', match.start(2))\n\n            if name in fstruct:\n                raise ValueError('new name', match.start(2))\n\n            if match.group(1) == '+': value = True\n            if match.group(1) == '-': value = False\n\n            if value is None:\n                match = self._REENTRANCE_RE.match(s, position)\n                if match is not None:\n                    position = match.end()\n                    match = self._TARGET_RE.match(s, position)\n                    if not match:\n                        raise ValueError('identifier', position)\n                    target = match.group(1)\n                    if target not in reentrances:\n                        raise ValueError('bound identifier', position)\n                    position = match.end()\n                    value = reentrances[target]\n\n            if value is None:\n                match = self._ASSIGN_RE.match(s, position)\n                if match:\n                    position = match.end()\n                    value, position = (\n                        self._read_value(name, s, position, reentrances))\n                else:\n                    raise ValueError('equals sign', position)\n\n            fstruct[name] = value\n\n            if self._END_FSTRUCT_RE.match(s, position):\n                continue\n\n            match = self._COMMA_RE.match(s, position)\n            if match is None: raise ValueError('comma', position)\n            position = match.end()\n\n        raise ValueError('close bracket', position)\n\n    def _finalize(self, s, pos, reentrances, fstruct):\n        \"\"\"\n        Called when we see the close brace -- checks for a slash feature,\n        and adds in default values.\n        \"\"\"\n        match = self._SLASH_RE.match(s, pos)\n        if match:\n            name = self._slash_feature\n            v, pos = self._read_value(name, s, match.end(), reentrances)\n            fstruct[name] = v\n        return fstruct, pos\n\n    def _read_value(self, name, s, position, reentrances):\n        if isinstance(name, Feature):\n            return name.read_value(s, position, reentrances, self)\n        else:\n            return self.read_value(s, position, reentrances)\n\n    def read_value(self, s, position, reentrances):\n        for (handler, regexp) in self.VALUE_HANDLERS:\n            match = regexp.match(s, position)\n            if match:\n                handler_func = getattr(self, handler)\n                return handler_func(s, position, reentrances, match)\n        raise ValueError('value', position)\n\n    def _error(self, s, expected, position):\n        lines = s.split('\\n')\n        while position > len(lines[0]):\n            position -= len(lines.pop(0)) + 1  # +1 for the newline.\n        estr = ('Error parsing feature structure\\n    ' +\n                lines[0] + '\\n    ' + ' ' * position + '^ ' +\n                'Expected %s' % expected)\n        raise ValueError(estr)\n\n\n    VALUE_HANDLERS = [\n        ('read_fstruct_value', _START_FSTRUCT_RE),\n        ('read_var_value', re.compile(r'\\?[a-zA-Z_][a-zA-Z0-9_]*')),\n        ('read_str_value', re.compile(\"[uU]?[rR]?(['\\\"])\")),\n        ('read_int_value', re.compile(r'-?\\d+')),\n        ('read_sym_value', re.compile(r'[a-zA-Z_][a-zA-Z0-9_]*')),\n        ('read_app_value', re.compile(r'<(app)\\((\\?[a-z][a-z]*)\\s*,'\n                                      r'\\s*(\\?[a-z][a-z]*)\\)>')),\n        ('read_logic_value', re.compile(r'<(.*?)(?<!-)>')),\n        ('read_set_value', re.compile(r'{')),\n        ('read_tuple_value', re.compile(r'\\(')),\n    ]\n\n    def read_fstruct_value(self, s, position, reentrances, match):\n        return self.read_partial(s, position, reentrances)\n\n    def read_str_value(self, s, position, reentrances, match):\n        return read_str(s, position)\n\n    def read_int_value(self, s, position, reentrances, match):\n        return int(match.group()), match.end()\n\n    def read_var_value(self, s, position, reentrances, match):\n        return Variable(match.group()), match.end()\n\n    _SYM_CONSTS = {'None': None, 'True': True, 'False': False}\n\n    def read_sym_value(self, s, position, reentrances, match):\n        val, end = match.group(), match.end()\n        return self._SYM_CONSTS.get(val, val), end\n\n    def read_app_value(self, s, position, reentrances, match):\n        return self._logic_parser.parse('%s(%s)' % match.group(2, 3)), match.end()\n\n    def read_logic_value(self, s, position, reentrances, match):\n        try:\n            try:\n                expr = self._logic_parser.parse(match.group(1))\n            except LogicalExpressionException:\n                raise ValueError()\n            return expr, match.end()\n        except ValueError:\n            raise ValueError('logic expression', match.start(1))\n\n    def read_tuple_value(self, s, position, reentrances, match):\n        return self._read_seq_value(s, position, reentrances, match, ')',\n                                    FeatureValueTuple, FeatureValueConcat)\n\n    def read_set_value(self, s, position, reentrances, match):\n        return self._read_seq_value(s, position, reentrances, match, '}',\n                                    FeatureValueSet, FeatureValueUnion)\n\n    def _read_seq_value(self, s, position, reentrances, match,\n                        close_paren, seq_class, plus_class):\n        \"\"\"\n        Helper function used by read_tuple_value and read_set_value.\n        \"\"\"\n        cp = re.escape(close_paren)\n        position = match.end()\n        m = re.compile(r'\\s*/?\\s*%s' % cp).match(s, position)\n        if m: return seq_class(), m.end()\n        values = []\n        seen_plus = False\n        while True:\n            m = re.compile(r'\\s*%s' % cp).match(s, position)\n            if m:\n                if seen_plus:\n                    return plus_class(values), m.end()\n                else:\n                    return seq_class(values), m.end()\n\n            val, position = self.read_value(s, position, reentrances)\n            values.append(val)\n\n            m = re.compile(r'\\s*(,|\\+|(?=%s))\\s*' % cp).match(s, position)\n            if not m: raise ValueError(\"',' or '+' or '%s'\" % cp, position)\n            if m.group(1) == '+': seen_plus = True\n            position = m.end()\n\n\n\ndef display_unification(fs1, fs2, indent='  '):\n    fs1_lines = (\"%s\" % fs1).split('\\n')\n    fs2_lines = (\"%s\" % fs2).split('\\n')\n    if len(fs1_lines) > len(fs2_lines):\n        blankline = '[' + ' ' * (len(fs2_lines[0]) - 2) + ']'\n        fs2_lines += [blankline] * len(fs1_lines)\n    else:\n        blankline = '[' + ' ' * (len(fs1_lines[0]) - 2) + ']'\n        fs1_lines += [blankline] * len(fs2_lines)\n    for (fs1_line, fs2_line) in zip(fs1_lines, fs2_lines):\n        print(indent + fs1_line + '   ' + fs2_line)\n    print(indent + '-' * len(fs1_lines[0]) + '   ' + '-' * len(fs2_lines[0]))\n\n    linelen = len(fs1_lines[0]) * 2 + 3\n    print(indent + '|               |'.center(linelen))\n    print(indent + '+-----UNIFY-----+'.center(linelen))\n    print(indent + '|'.center(linelen))\n    print(indent + 'V'.center(linelen))\n\n    bindings = {}\n\n    result = fs1.unify(fs2, bindings)\n    if result is None:\n        print(indent + '(FAILED)'.center(linelen))\n    else:\n        print('\\n'.join(indent + l.center(linelen)\n                        for l in (\"%s\" % result).split('\\n')))\n        if bindings and len(bindings.bound_variables()) > 0:\n            print(repr(bindings).center(linelen))\n    return result\n\n\ndef interactive_demo(trace=False):\n    import random, sys\n\n    HELP = '''\n    1-%d: Select the corresponding feature structure\n    q: Quit\n    t: Turn tracing on or off\n    l: List all feature structures\n    ?: Help\n    '''\n\n    print('''\n    This demo will repeatedly present you with a list of feature\n    structures, and ask you to choose two for unification.  Whenever a\n    new feature structure is generated, it is added to the list of\n    choices that you can pick from.  However, since this can be a\n    large number of feature structures, the demo will only print out a\n    random subset for you to choose between at a given time.  If you\n    want to see the complete lists, type \"l\".  For a list of valid\n    commands, type \"?\".\n    ''')\n    print('Press \"Enter\" to continue...')\n    sys.stdin.readline()\n\n    fstruct_strings = [\n        '[agr=[number=sing, gender=masc]]',\n        '[agr=[gender=masc, person=3]]',\n        '[agr=[gender=fem, person=3]]',\n        '[subj=[agr=(1)[]], agr->(1)]',\n        '[obj=?x]', '[subj=?x]',\n        '[/=None]', '[/=NP]',\n        '[cat=NP]', '[cat=VP]', '[cat=PP]',\n        '[subj=[agr=[gender=?y]], obj=[agr=[gender=?y]]]',\n        '[gender=masc, agr=?C]',\n        '[gender=?S, agr=[gender=?S,person=3]]'\n    ]\n\n    all_fstructs = [(i, FeatStruct(fstruct_strings[i]))\n                    for i in range(len(fstruct_strings))]\n\n    def list_fstructs(fstructs):\n        for i, fstruct in fstructs:\n            print()\n            lines = (\"%s\" % fstruct).split('\\n')\n            print('%3d: %s' % (i + 1, lines[0]))\n            for line in lines[1:]: print('     ' + line)\n        print()\n\n    while True:\n        MAX_CHOICES = 5\n        if len(all_fstructs) > MAX_CHOICES:\n            fstructs = sorted(random.sample(all_fstructs, MAX_CHOICES))\n        else:\n            fstructs = all_fstructs\n\n        print('_' * 75)\n\n        print('Choose two feature structures to unify:')\n        list_fstructs(fstructs)\n\n        selected = [None, None]\n        for (nth, i) in (('First', 0), ('Second', 1)):\n            while selected[i] is None:\n                print(('%s feature structure (1-%d,q,t,l,?): '\n                       % (nth, len(all_fstructs))), end=' ')\n                try:\n                    input = sys.stdin.readline().strip()\n                    if input in ('q', 'Q', 'x', 'X'): return\n                    if input in ('t', 'T'):\n                        trace = not trace\n                        print('   Trace = %s' % trace)\n                        continue\n                    if input in ('h', 'H', '?'):\n                        print(HELP % len(fstructs));\n                        continue\n                    if input in ('l', 'L'):\n                        list_fstructs(all_fstructs);\n                        continue\n                    num = int(input) - 1\n                    selected[i] = all_fstructs[num][1]\n                    print()\n                except:\n                    print('Bad sentence number')\n                    continue\n\n        if trace:\n            result = selected[0].unify(selected[1], trace=1)\n        else:\n            result = display_unification(selected[0], selected[1])\n        if result is not None:\n            for i, fstruct in all_fstructs:\n                if repr(result) == repr(fstruct): break\n            else:\n                all_fstructs.append((len(all_fstructs), result))\n\n        print('\\nType \"Enter\" to continue unifying; or \"q\" to quit.')\n        input = sys.stdin.readline().strip()\n        if input in ('q', 'Q', 'x', 'X'): return\n\n\ndef demo(trace=False):\n    \"\"\"\n    Just for testing\n    \"\"\"\n\n    fstruct_strings = [\n        '[agr=[number=sing, gender=masc]]',\n        '[agr=[gender=masc, person=3]]',\n        '[agr=[gender=fem, person=3]]',\n        '[subj=[agr=(1)[]], agr->(1)]',\n        '[obj=?x]', '[subj=?x]',\n        '[/=None]', '[/=NP]',\n        '[cat=NP]', '[cat=VP]', '[cat=PP]',\n        '[subj=[agr=[gender=?y]], obj=[agr=[gender=?y]]]',\n        '[gender=masc, agr=?C]',\n        '[gender=?S, agr=[gender=?S,person=3]]'\n    ]\n    all_fstructs = [FeatStruct(fss) for fss in fstruct_strings]\n\n    for fs1 in all_fstructs:\n        for fs2 in all_fstructs:\n            print(\"\\n*******************\\nfs1 is:\\n%s\\n\\nfs2 is:\\n%s\\n\\nresult is:\\n%s\" % (fs1, fs2, unify(fs1, fs2)))\n\n\nif __name__ == '__main__':\n    demo()\n\n__all__ = ['FeatStruct', 'FeatDict', 'FeatList', 'unify', 'subsumes', 'conflicts',\n           'Feature', 'SlashFeature', 'RangeFeature', 'SLASH', 'TYPE',\n           'FeatStructReader']\n"], "nltk\\grammar": [".py", "\nfrom __future__ import print_function, unicode_literals, division\n\nimport re\nfrom functools import total_ordering\n\nfrom six import string_types\n\nfrom nltk.util import transitive_closure, invert_graph\nfrom nltk.compat import python_2_unicode_compatible, unicode_repr\nfrom nltk.internals import raise_unorderable_types\n\nfrom nltk.probability import ImmutableProbabilisticMixIn\nfrom nltk.featstruct import FeatStruct, FeatDict, FeatStructReader, SLASH, TYPE\n\n\n\n@total_ordering\n@python_2_unicode_compatible\nclass Nonterminal(object):\n\n    def __init__(self, symbol):\n        self._symbol = symbol\n        self._hash = hash(symbol)\n\n    def symbol(self):\n        return self._symbol\n\n    def __eq__(self, other):\n        return type(self) == type(other) and self._symbol == other._symbol\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __lt__(self, other):\n        if not isinstance(other, Nonterminal):\n            raise_unorderable_types(\"<\", self, other)\n        return self._symbol < other._symbol\n\n    def __hash__(self):\n        return self._hash\n\n    def __repr__(self):\n        if isinstance(self._symbol, string_types):\n            return '%s' % self._symbol\n        else:\n            return '%s' % unicode_repr(self._symbol)\n\n    def __str__(self):\n        if isinstance(self._symbol, string_types):\n            return '%s' % self._symbol\n        else:\n            return '%s' % unicode_repr(self._symbol)\n\n    def __div__(self, rhs):\n        return Nonterminal('%s/%s' % (self._symbol, rhs._symbol))\n\n    def __truediv__(self, rhs):\n        return self.__div__(rhs)\n\n\ndef nonterminals(symbols):\n    if ',' in symbols:\n        symbol_list = symbols.split(',')\n    else:\n        symbol_list = symbols.split()\n    return [Nonterminal(s.strip()) for s in symbol_list]\n\n\nclass FeatStructNonterminal(FeatDict, Nonterminal):\n    :return: True if the item is a ``Nonterminal``.\n    :rtype: bool\n    \"\"\"\n    return isinstance(item, Nonterminal)\n\n\n\ndef is_terminal(item):\n    \"\"\"\n    Return True if the item is a terminal, which currently is\n    if it is hashable and not a ``Nonterminal``.\n\n    :rtype: bool\n    \"\"\"\n    return hasattr(item, '__hash__') and not isinstance(item, Nonterminal)\n\n\n\n@total_ordering\n@python_2_unicode_compatible\nclass Production(object):\n    \"\"\"\n    A grammar production.  Each production maps a single symbol\n    on the \"left-hand side\" to a sequence of symbols on the\n    \"right-hand side\".  (In the case of context-free productions,\n    the left-hand side must be a ``Nonterminal``, and the right-hand\n    side is a sequence of terminals and ``Nonterminals``.)\n    \"terminals\" can be any immutable hashable object that is\n    not a ``Nonterminal``.  Typically, terminals are strings\n    representing words, such as ``\"dog\"`` or ``\"under\"``.\n\n    :see: ``CFG``\n    :see: ``DependencyGrammar``\n    :see: ``Nonterminal``\n    :type _lhs: Nonterminal\n    :ivar _lhs: The left-hand side of the production.\n    :type _rhs: tuple(Nonterminal, terminal)\n    :ivar _rhs: The right-hand side of the production.\n    \"\"\"\n\n    def __init__(self, lhs, rhs):\n        \"\"\"\n        Construct a new ``Production``.\n\n        :param lhs: The left-hand side of the new ``Production``.\n        :type lhs: Nonterminal\n        :param rhs: The right-hand side of the new ``Production``.\n        :type rhs: sequence(Nonterminal and terminal)\n        \"\"\"\n        if isinstance(rhs, string_types):\n            raise TypeError('production right hand side should be a list, '\n                            'not a string')\n        self._lhs = lhs\n        self._rhs = tuple(rhs)\n        self._hash = hash((self._lhs, self._rhs))\n\n    def lhs(self):\n        \"\"\"\n        Return the left-hand side of this ``Production``.\n\n        :rtype: Nonterminal\n        \"\"\"\n        return self._lhs\n\n    def rhs(self):\n        \"\"\"\n        Return the right-hand side of this ``Production``.\n\n        :rtype: sequence(Nonterminal and terminal)\n        \"\"\"\n        return self._rhs\n\n    def __len__(self):\n        \"\"\"\n        Return the length of the right-hand side.\n\n        :rtype: int\n        \"\"\"\n        return len(self._rhs)\n\n    def is_nonlexical(self):\n        \"\"\"\n        Return True if the right-hand side only contains ``Nonterminals``\n\n        :rtype: bool\n        \"\"\"\n        return all(is_nonterminal(n) for n in self._rhs)\n\n    def is_lexical(self):\n        \"\"\"\n        Return True if the right-hand contain at least one terminal token.\n\n        :rtype: bool\n        \"\"\"\n        return not self.is_nonlexical()\n\n    def __str__(self):\n        \"\"\"\n        Return a verbose string representation of the ``Production``.\n\n        :rtype: str\n        \"\"\"\n        result = '%s -> ' % unicode_repr(self._lhs)\n        result += \" \".join(unicode_repr(el) for el in self._rhs)\n        return result\n\n    def __repr__(self):\n        \"\"\"\n        Return a concise string representation of the ``Production``.\n\n        :rtype: str\n        \"\"\"\n        return '%s' % self\n\n    def __eq__(self, other):\n        \"\"\"\n        Return True if this ``Production`` is equal to ``other``.\n\n        :rtype: bool\n        \"\"\"\n        return (type(self) == type(other) and\n                self._lhs == other._lhs and\n                self._rhs == other._rhs)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __lt__(self, other):\n        if not isinstance(other, Production):\n            raise_unorderable_types(\"<\", self, other)\n        return (self._lhs, self._rhs) < (other._lhs, other._rhs)\n\n    def __hash__(self):\n        \"\"\"\n        Return a hash value for the ``Production``.\n\n        :rtype: int\n        \"\"\"\n        return self._hash\n\n\n@python_2_unicode_compatible\nclass DependencyProduction(Production):\n    \"\"\"\n    A dependency grammar production.  Each production maps a single\n    head word to an unordered list of one or more modifier words.\n    \"\"\"\n\n    def __str__(self):\n        \"\"\"\n        Return a verbose string representation of the ``DependencyProduction``.\n\n        :rtype: str\n        \"\"\"\n        result = '\\'%s\\' ->' % (self._lhs,)\n        for elt in self._rhs:\n            result += ' \\'%s\\'' % (elt,)\n        return result\n\n\n@python_2_unicode_compatible\nclass ProbabilisticProduction(Production, ImmutableProbabilisticMixIn):\n    \"\"\"\n    A probabilistic context free grammar production.\n    A PCFG ``ProbabilisticProduction`` is essentially just a ``Production`` that\n    has an associated probability, which represents how likely it is that\n    this production will be used.  In particular, the probability of a\n    ``ProbabilisticProduction`` records the likelihood that its right-hand side is\n    the correct instantiation for any given occurrence of its left-hand side.\n\n    :see: ``Production``\n    \"\"\"\n\n    def __init__(self, lhs, rhs, **prob):\n        \"\"\"\n        Construct a new ``ProbabilisticProduction``.\n\n        :param lhs: The left-hand side of the new ``ProbabilisticProduction``.\n        :type lhs: Nonterminal\n        :param rhs: The right-hand side of the new ``ProbabilisticProduction``.\n        :type rhs: sequence(Nonterminal and terminal)\n        :param prob: Probability parameters of the new ``ProbabilisticProduction``.\n        \"\"\"\n        ImmutableProbabilisticMixIn.__init__(self, **prob)\n        Production.__init__(self, lhs, rhs)\n\n    def __str__(self):\n        return Production.__unicode__(self) + \\\n               (' [1.0]' if (self.prob() == 1.0) else ' [%g]' % self.prob())\n\n    def __eq__(self, other):\n        return (type(self) == type(other) and\n                self._lhs == other._lhs and\n                self._rhs == other._rhs and\n                self.prob() == other.prob())\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((self._lhs, self._rhs, self.prob()))\n\n\n\n@python_2_unicode_compatible\nclass CFG(object):\n    \"\"\"\n    A context-free grammar.  A grammar consists of a start state and\n    a set of productions.  The set of terminals and nonterminals is\n    implicitly specified by the productions.\n\n    If you need efficient key-based access to productions, you\n    can use a subclass to implement it.\n    \"\"\"\n\n    def __init__(self, start, productions, calculate_leftcorners=True):\n        \"\"\"\n        Create a new context-free grammar, from the given start state\n        and set of ``Production``s.\n\n        :param start: The start symbol\n        :type start: Nonterminal\n        :param productions: The list of productions that defines the grammar\n        :type productions: list(Production)\n        :param calculate_leftcorners: False if we don't want to calculate the\n            leftcorner relation. In that case, some optimized chart parsers won't work.\n        :type calculate_leftcorners: bool\n        \"\"\"\n        if not is_nonterminal(start):\n            raise TypeError(\"start should be a Nonterminal object,\"\n                            \" not a %s\" % type(start).__name__)\n\n        self._start = start\n        self._productions = productions\n        self._categories = set(prod.lhs() for prod in productions)\n        self._calculate_indexes()\n        self._calculate_grammar_forms()\n        if calculate_leftcorners:\n            self._calculate_leftcorners()\n\n    def _calculate_indexes(self):\n        self._lhs_index = {}\n        self._rhs_index = {}\n        self._empty_index = {}\n        self._lexical_index = {}\n        for prod in self._productions:\n            lhs = prod._lhs\n            if lhs not in self._lhs_index:\n                self._lhs_index[lhs] = []\n            self._lhs_index[lhs].append(prod)\n            if prod._rhs:\n                rhs0 = prod._rhs[0]\n                if rhs0 not in self._rhs_index:\n                    self._rhs_index[rhs0] = []\n                self._rhs_index[rhs0].append(prod)\n            else:\n                self._empty_index[prod.lhs()] = prod\n            for token in prod._rhs:\n                if is_terminal(token):\n                    self._lexical_index.setdefault(token, set()).add(prod)\n\n    def _calculate_leftcorners(self):\n        self._immediate_leftcorner_categories = dict((cat, set([cat])) for cat in self._categories)\n        self._immediate_leftcorner_words = dict((cat, set()) for cat in self._categories)\n        for prod in self.productions():\n            if len(prod) > 0:\n                cat, left = prod.lhs(), prod.rhs()[0]\n                if is_nonterminal(left):\n                    self._immediate_leftcorner_categories[cat].add(left)\n                else:\n                    self._immediate_leftcorner_words[cat].add(left)\n\n        lc = transitive_closure(self._immediate_leftcorner_categories, reflexive=True)\n        self._leftcorners = lc\n        self._leftcorner_parents = invert_graph(lc)\n\n        nr_leftcorner_categories = sum(map(len, self._immediate_leftcorner_categories.values()))\n        nr_leftcorner_words = sum(map(len, self._immediate_leftcorner_words.values()))\n        if nr_leftcorner_words > nr_leftcorner_categories > 10000:\n            self._leftcorner_words = None\n            return\n\n        self._leftcorner_words = {}\n        for cat in self._leftcorners:\n            lefts = self._leftcorners[cat]\n            lc = self._leftcorner_words[cat] = set()\n            for left in lefts:\n                lc.update(self._immediate_leftcorner_words.get(left, set()))\n\n    @classmethod\n    def fromstring(cls, input, encoding=None):\n        \"\"\"\n        Return the grammar instance corresponding to the input string(s).\n\n        :param input: a grammar, either in the form of a string or as a list of strings.\n        \"\"\"\n        start, productions = read_grammar(input, standard_nonterm_parser,\n                                          encoding=encoding)\n        return cls(start, productions)\n\n    def start(self):\n        \"\"\"\n        Return the start symbol of the grammar\n\n        :rtype: Nonterminal\n        \"\"\"\n        return self._start\n\n    def productions(self, lhs=None, rhs=None, empty=False):\n        \"\"\"\n        Return the grammar productions, filtered by the left-hand side\n        or the first item in the right-hand side.\n\n        :param lhs: Only return productions with the given left-hand side.\n        :param rhs: Only return productions with the given first item\n            in the right-hand side.\n        :param empty: Only return productions with an empty right-hand side.\n        :return: A list of productions matching the given constraints.\n        :rtype: list(Production)\n        \"\"\"\n        if rhs and empty:\n            raise ValueError(\"You cannot select empty and non-empty \"\n                             \"productions at the same time.\")\n\n        if not lhs and not rhs:\n            if not empty:\n                return self._productions\n            else:\n                return self._empty_index.values()\n\n        elif lhs and not rhs:\n            if not empty:\n                return self._lhs_index.get(lhs, [])\n            elif lhs in self._empty_index:\n                return [self._empty_index[lhs]]\n            else:\n                return []\n\n        elif rhs and not lhs:\n            return self._rhs_index.get(rhs, [])\n\n        else:\n            return [prod for prod in self._lhs_index.get(lhs, [])\n                    if prod in self._rhs_index.get(rhs, [])]\n\n    def leftcorners(self, cat):\n        \"\"\"\n        Return the set of all nonterminals that the given nonterminal\n        can start with, including itself.\n\n        This is the reflexive, transitive closure of the immediate\n        leftcorner relation:  (A > B)  iff  (A -> B beta)\n\n        :param cat: the parent of the leftcorners\n        :type cat: Nonterminal\n        :return: the set of all leftcorners\n        :rtype: set(Nonterminal)\n        \"\"\"\n        return self._leftcorners.get(cat, set([cat]))\n\n    def is_leftcorner(self, cat, left):\n        \"\"\"\n        True if left is a leftcorner of cat, where left can be a\n        terminal or a nonterminal.\n\n        :param cat: the parent of the leftcorner\n        :type cat: Nonterminal\n        :param left: the suggested leftcorner\n        :type left: Terminal or Nonterminal\n        :rtype: bool\n        \"\"\"\n        if is_nonterminal(left):\n            return left in self.leftcorners(cat)\n        elif self._leftcorner_words:\n            return left in self._leftcorner_words.get(cat, set())\n        else:\n            return any(left in self._immediate_leftcorner_words.get(parent, set())\n                       for parent in self.leftcorners(cat))\n\n    def leftcorner_parents(self, cat):\n        \"\"\"\n        Return the set of all nonterminals for which the given category\n        is a left corner. This is the inverse of the leftcorner relation.\n\n        :param cat: the suggested leftcorner\n        :type cat: Nonterminal\n        :return: the set of all parents to the leftcorner\n        :rtype: set(Nonterminal)\n        \"\"\"\n        return self._leftcorner_parents.get(cat, set([cat]))\n\n    def check_coverage(self, tokens):\n        \"\"\"\n        Check whether the grammar rules cover the given list of tokens.\n        If not, then raise an exception.\n\n        :type tokens: list(str)\n        \"\"\"\n        missing = [tok for tok in tokens\n                   if not self._lexical_index.get(tok)]\n        if missing:\n            missing = ', '.join('%r' % (w,) for w in missing)\n            raise ValueError(\"Grammar does not cover some of the \"\n                             \"input words: %r.\" % missing)\n\n    def _calculate_grammar_forms(self):\n        \"\"\"\n        Pre-calculate of which form(s) the grammar is.\n        \"\"\"\n        prods = self._productions\n        self._is_lexical = all(p.is_lexical() for p in prods)\n        self._is_nonlexical = all(p.is_nonlexical() for p in prods\n                                  if len(p) != 1)\n        self._min_len = min(len(p) for p in prods)\n        self._max_len = max(len(p) for p in prods)\n        self._all_unary_are_lexical = all(p.is_lexical() for p in prods\n                                          if len(p) == 1)\n\n    def is_lexical(self):\n        \"\"\"\n        Return True if all productions are lexicalised.\n        \"\"\"\n        return self._is_lexical\n\n    def is_nonlexical(self):\n        \"\"\"\n        Return True if all lexical rules are \"preterminals\", that is,\n        unary rules which can be separated in a preprocessing step.\n\n        This means that all productions are of the forms\n        A -> B1 ... Bn (n>=0), or A -> \"s\".\n\n        Note: is_lexical() and is_nonlexical() are not opposites.\n        There are grammars which are neither, and grammars which are both.\n        \"\"\"\n        return self._is_nonlexical\n\n    def min_len(self):\n        \"\"\"\n        Return the right-hand side length of the shortest grammar production.\n        \"\"\"\n        return self._min_len\n\n    def max_len(self):\n        \"\"\"\n        Return the right-hand side length of the longest grammar production.\n        \"\"\"\n        return self._max_len\n\n    def is_nonempty(self):\n        \"\"\"\n        Return True if there are no empty productions.\n        \"\"\"\n        return self._min_len > 0\n\n    def is_binarised(self):\n        \"\"\"\n        Return True if all productions are at most binary.\n        Note that there can still be empty and unary productions.\n        \"\"\"\n        return self._max_len <= 2\n\n    def is_flexible_chomsky_normal_form(self):\n        \"\"\"\n        Return True if all productions are of the forms\n        A -> B C, A -> B, or A -> \"s\".\n        \"\"\"\n        return self.is_nonempty() and self.is_nonlexical() and self.is_binarised()\n\n    def is_chomsky_normal_form(self):\n        \"\"\"\n        Return True if the grammar is of Chomsky Normal Form, i.e. all productions\n        are of the form A -> B C, or A -> \"s\".\n        \"\"\"\n        return (self.is_flexible_chomsky_normal_form() and\n                self._all_unary_are_lexical)\n\n    def __repr__(self):\n        return '<Grammar with %d productions>' % len(self._productions)\n\n    def __str__(self):\n        result = 'Grammar with %d productions' % len(self._productions)\n        result += ' (start state = %r)' % self._start\n        for production in self._productions:\n            result += '\\n    %s' % production\n        return result\n\n\nclass FeatureGrammar(CFG):\n    \"\"\"\n    A feature-based grammar.  This is equivalent to a\n    ``CFG`` whose nonterminals are all\n    ``FeatStructNonterminal``.\n\n    A grammar consists of a start state and a set of\n    productions.  The set of terminals and nonterminals\n    is implicitly specified by the productions.\n    \"\"\"\n\n    def __init__(self, start, productions):\n        \"\"\"\n        Create a new feature-based grammar, from the given start\n        state and set of ``Productions``.\n\n        :param start: The start symbol\n        :type start: FeatStructNonterminal\n        :param productions: The list of productions that defines the grammar\n        :type productions: list(Production)\n        \"\"\"\n        CFG.__init__(self, start, productions)\n\n\n    def _calculate_indexes(self):\n        self._lhs_index = {}\n        self._rhs_index = {}\n        self._empty_index = {}\n        self._empty_productions = []\n        self._lexical_index = {}\n        for prod in self._productions:\n            lhs = self._get_type_if_possible(prod._lhs)\n            if lhs not in self._lhs_index:\n                self._lhs_index[lhs] = []\n            self._lhs_index[lhs].append(prod)\n            if prod._rhs:\n                rhs0 = self._get_type_if_possible(prod._rhs[0])\n                if rhs0 not in self._rhs_index:\n                    self._rhs_index[rhs0] = []\n                self._rhs_index[rhs0].append(prod)\n            else:\n                if lhs not in self._empty_index:\n                    self._empty_index[lhs] = []\n                self._empty_index[lhs].append(prod)\n                self._empty_productions.append(prod)\n            for token in prod._rhs:\n                if is_terminal(token):\n                    self._lexical_index.setdefault(token, set()).add(prod)\n\n    @classmethod\n    def fromstring(cls, input, features=None, logic_parser=None, fstruct_reader=None,\n                   encoding=None):\n        \"\"\"\n        Return a feature structure based grammar.\n\n        :param input: a grammar, either in the form of a string or else\n        as a list of strings.\n        :param features: a tuple of features (default: SLASH, TYPE)\n        :param logic_parser: a parser for lambda-expressions,\n        by default, ``LogicParser()``\n        :param fstruct_reader: a feature structure parser\n        (only if features and logic_parser is None)\n        \"\"\"\n        if features is None:\n            features = (SLASH, TYPE)\n\n        if fstruct_reader is None:\n            fstruct_reader = FeatStructReader(features, FeatStructNonterminal,\n                                              logic_parser=logic_parser)\n        elif logic_parser is not None:\n            raise Exception('\\'logic_parser\\' and \\'fstruct_reader\\' must '\n                            'not both be set')\n\n        start, productions = read_grammar(input, fstruct_reader.read_partial,\n                                          encoding=encoding)\n        return cls(start, productions)\n\n    def productions(self, lhs=None, rhs=None, empty=False):\n        \"\"\"\n        Return the grammar productions, filtered by the left-hand side\n        or the first item in the right-hand side.\n\n        :param lhs: Only return productions with the given left-hand side.\n        :param rhs: Only return productions with the given first item\n            in the right-hand side.\n        :param empty: Only return productions with an empty right-hand side.\n        :rtype: list(Production)\n        \"\"\"\n        if rhs and empty:\n            raise ValueError(\"You cannot select empty and non-empty \"\n                             \"productions at the same time.\")\n\n        if not lhs and not rhs:\n            if empty:\n                return self._empty_productions\n            else:\n                return self._productions\n\n        elif lhs and not rhs:\n            if empty:\n                return self._empty_index.get(self._get_type_if_possible(lhs), [])\n            else:\n                return self._lhs_index.get(self._get_type_if_possible(lhs), [])\n\n        elif rhs and not lhs:\n            return self._rhs_index.get(self._get_type_if_possible(rhs), [])\n\n        else:\n            return [prod for prod in self._lhs_index.get(self._get_type_if_possible(lhs), [])\n                    if prod in self._rhs_index.get(self._get_type_if_possible(rhs), [])]\n\n    def leftcorners(self, cat):\n        \"\"\"\n        Return the set of all words that the given category can start with.\n        Also called the \"first set\" in compiler construction.\n        \"\"\"\n        raise NotImplementedError(\"Not implemented yet\")\n\n    def leftcorner_parents(self, cat):\n        \"\"\"\n        Return the set of all categories for which the given category\n        is a left corner.\n        \"\"\"\n        raise NotImplementedError(\"Not implemented yet\")\n\n    def _get_type_if_possible(self, item):\n        \"\"\"\n        Helper function which returns the ``TYPE`` feature of the ``item``,\n        if it exists, otherwise it returns the ``item`` itself\n        \"\"\"\n        if isinstance(item, dict) and TYPE in item:\n            return FeatureValueType(item[TYPE])\n        else:\n            return item\n\n\n@total_ordering\n@python_2_unicode_compatible\nclass FeatureValueType(object):\n    \"\"\"\n    A helper class for ``FeatureGrammars``, designed to be different\n    from ordinary strings.  This is to stop the ``FeatStruct``\n    ``FOO[]`` from being compare equal to the terminal \"FOO\".\n    \"\"\"\n\n    def __init__(self, value):\n        self._value = value\n        self._hash = hash(value)\n\n    def __repr__(self):\n        return '<%s>' % self._value\n\n    def __eq__(self, other):\n        return type(self) == type(other) and self._value == other._value\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __lt__(self, other):\n        if not isinstance(other, FeatureValueType):\n            raise_unorderable_types(\"<\", self, other)\n        return self._value < other._value\n\n    def __hash__(self):\n        return self._hash\n\n\n@python_2_unicode_compatible\nclass DependencyGrammar(object):\n    \"\"\"\n    A dependency grammar.  A DependencyGrammar consists of a set of\n    productions.  Each production specifies a head/modifier relationship\n    between a pair of words.\n    \"\"\"\n\n    def __init__(self, productions):\n        \"\"\"\n        Create a new dependency grammar, from the set of ``Productions``.\n\n        :param productions: The list of productions that defines the grammar\n        :type productions: list(Production)\n        \"\"\"\n        self._productions = productions\n\n    @classmethod\n    def fromstring(cls, input):\n        productions = []\n        for linenum, line in enumerate(input.split('\\n')):\n            line = line.strip()\n            if line.startswith('#') or line == '': continue\n            try:\n                productions += _read_dependency_production(line)\n            except ValueError:\n                raise ValueError('Unable to parse line %s: %s' % (linenum, line))\n        if len(productions) == 0:\n            raise ValueError('No productions found!')\n        return cls(productions)\n\n    def contains(self, head, mod):\n        \"\"\"\n        :param head: A head word.\n        :type head: str\n        :param mod: A mod word, to test as a modifier of 'head'.\n        :type mod: str\n\n        :return: true if this ``DependencyGrammar`` contains a\n            ``DependencyProduction`` mapping 'head' to 'mod'.\n        :rtype: bool\n        \"\"\"\n        for production in self._productions:\n            for possibleMod in production._rhs:\n                if (production._lhs == head and possibleMod == mod):\n                    return True\n        return False\n\n    def __contains__(self, head, mod):\n        \"\"\"\n        Return True if this ``DependencyGrammar`` contains a\n        ``DependencyProduction`` mapping 'head' to 'mod'.\n\n        :param head: A head word.\n        :type head: str\n        :param mod: A mod word, to test as a modifier of 'head'.\n        :type mod: str\n        :rtype: bool\n        \"\"\"\n        for production in self._productions:\n            for possibleMod in production._rhs:\n                if (production._lhs == head and possibleMod == mod):\n                    return True\n        return False\n\n\n    def __str__(self):\n        \"\"\"\n        Return a verbose string representation of the ``DependencyGrammar``\n\n        :rtype: str\n        \"\"\"\n        str = 'Dependency grammar with %d productions' % len(self._productions)\n        for production in self._productions:\n            str += '\\n  %s' % production\n        return str\n\n    def __repr__(self):\n        \"\"\"\n        Return a concise string representation of the ``DependencyGrammar``\n        \"\"\"\n        return 'Dependency grammar with %d productions' % len(self._productions)\n\n\n@python_2_unicode_compatible\nclass ProbabilisticDependencyGrammar(object):\n    \"\"\"\n\n    \"\"\"\n\n    def __init__(self, productions, events, tags):\n        self._productions = productions\n        self._events = events\n        self._tags = tags\n\n    def contains(self, head, mod):\n        \"\"\"\n        Return True if this ``DependencyGrammar`` contains a\n        ``DependencyProduction`` mapping 'head' to 'mod'.\n\n        :param head: A head word.\n        :type head: str\n        :param mod: A mod word, to test as a modifier of 'head'.\n        :type mod: str\n        :rtype: bool\n        \"\"\"\n        for production in self._productions:\n            for possibleMod in production._rhs:\n                if (production._lhs == head and possibleMod == mod):\n                    return True\n        return False\n\n    def __str__(self):\n        \"\"\"\n        Return a verbose string representation of the ``ProbabilisticDependencyGrammar``\n\n        :rtype: str\n        \"\"\"\n        str = 'Statistical dependency grammar with %d productions' % len(self._productions)\n        for production in self._productions:\n            str += '\\n  %s' % production\n        str += '\\nEvents:'\n        for event in self._events:\n            str += '\\n  %d:%s' % (self._events[event], event)\n        str += '\\nTags:'\n        for tag_word in self._tags:\n            str += '\\n %s:\\t(%s)' % (tag_word, self._tags[tag_word])\n        return str\n\n    def __repr__(self):\n        \"\"\"\n        Return a concise string representation of the ``ProbabilisticDependencyGrammar``\n        \"\"\"\n        return 'Statistical Dependency grammar with %d productions' % len(self._productions)\n\n\nclass PCFG(CFG):\n    \"\"\"\n    A probabilistic context-free grammar.  A PCFG consists of a\n    start state and a set of productions with probabilities.  The set of\n    terminals and nonterminals is implicitly specified by the productions.\n\n    PCFG productions use the ``ProbabilisticProduction`` class.\n    ``PCFGs`` impose the constraint that the set of productions with\n    any given left-hand-side must have probabilities that sum to 1\n    (allowing for a small margin of error).\n\n    If you need efficient key-based access to productions, you can use\n    a subclass to implement it.\n\n    :type EPSILON: float\n    :cvar EPSILON: The acceptable margin of error for checking that\n        productions with a given left-hand side have probabilities\n        that sum to 1.\n    \"\"\"\n    EPSILON = 0.01\n\n    def __init__(self, start, productions, calculate_leftcorners=True):\n        \"\"\"\n        Create a new context-free grammar, from the given start state\n        and set of ``ProbabilisticProductions``.\n\n        :param start: The start symbol\n        :type start: Nonterminal\n        :param productions: The list of productions that defines the grammar\n        :type productions: list(Production)\n        :raise ValueError: if the set of productions with any left-hand-side\n            do not have probabilities that sum to a value within\n            EPSILON of 1.\n        :param calculate_leftcorners: False if we don't want to calculate the\n            leftcorner relation. In that case, some optimized chart parsers won't work.\n        :type calculate_leftcorners: bool\n        \"\"\"\n        CFG.__init__(self, start, productions, calculate_leftcorners)\n\n        probs = {}\n        for production in productions:\n            probs[production.lhs()] = (probs.get(production.lhs(), 0) +\n                                       production.prob())\n        for (lhs, p) in probs.items():\n            if not ((1 - PCFG.EPSILON) < p <\n                    (1 + PCFG.EPSILON)):\n                raise ValueError(\"Productions for %r do not sum to 1\" % lhs)\n\n    @classmethod\n    def fromstring(cls, input, encoding=None):\n        \"\"\"\n        Return a probabilistic context-free grammar corresponding to the\n        input string(s).\n\n        :param input: a grammar, either in the form of a string or else\n             as a list of strings.\n        \"\"\"\n        start, productions = read_grammar(input, standard_nonterm_parser,\n                                          probabilistic=True, encoding=encoding)\n        return cls(start, productions)\n\n\n\n\ndef induce_pcfg(start, productions):\n    \"\"\"\n    Induce a PCFG grammar from a list of productions.\n\n    The probability of a production A -> B C in a PCFG is:\n\n    |                count(A -> B C)\n    |  P(B, C | A) = ---------------       where \\* is any right hand side\n    |                 count(A -> \\*)\n\n    :param start: The start symbol\n    :type start: Nonterminal\n    :param productions: The list of productions that defines the grammar\n    :type productions: list(Production)\n    \"\"\"\n    pcount = {}\n\n    lcount = {}\n\n    for prod in productions:\n        lcount[prod.lhs()] = lcount.get(prod.lhs(), 0) + 1\n        pcount[prod] = pcount.get(prod, 0) + 1\n\n    prods = [ProbabilisticProduction(p.lhs(), p.rhs(),\n                                     prob=pcount[p] / lcount[p.lhs()])\n             for p in pcount]\n    return PCFG(start, prods)\n\n\n\ndef _read_cfg_production(input):\n    \"\"\"\n    Return a list of context-free ``Productions``.\n    \"\"\"\n    return _read_production(input, standard_nonterm_parser)\n\n\ndef _read_pcfg_production(input):\n    \"\"\"\n    Return a list of PCFG ``ProbabilisticProductions``.\n    \"\"\"\n    return _read_production(input, standard_nonterm_parser, probabilistic=True)\n\n\ndef _read_fcfg_production(input, fstruct_reader):\n    \"\"\"\n    Return a list of feature-based ``Productions``.\n    \"\"\"\n    return _read_production(input, fstruct_reader)\n\n\n\n_ARROW_RE = re.compile(r'\\s* -> \\s*', re.VERBOSE)\n_PROBABILITY_RE = re.compile(r'( \\[ [\\d\\.]+ \\] ) \\s*', re.VERBOSE)\n_TERMINAL_RE = re.compile(r'( \"[^\"]+\" | \\'[^\\']+\\' ) \\s*', re.VERBOSE)\n_DISJUNCTION_RE = re.compile(r'\\| \\s*', re.VERBOSE)\n\n\ndef _read_production(line, nonterm_parser, probabilistic=False):\n    \"\"\"\n    Parse a grammar rule, given as a string, and return\n    a list of productions.\n    \"\"\"\n    pos = 0\n\n    lhs, pos = nonterm_parser(line, pos)\n\n    m = _ARROW_RE.match(line, pos)\n    if not m: raise ValueError('Expected an arrow')\n    pos = m.end()\n\n    probabilities = [0.0]\n    rhsides = [[]]\n    while pos < len(line):\n        m = _PROBABILITY_RE.match(line, pos)\n        if probabilistic and m:\n            pos = m.end()\n            probabilities[-1] = float(m.group(1)[1:-1])\n            if probabilities[-1] > 1.0:\n                raise ValueError('Production probability %f, '\n                                 'should not be greater than 1.0' %\n                                 (probabilities[-1],))\n\n        elif line[pos] in \"\\'\\\"\":\n            m = _TERMINAL_RE.match(line, pos)\n            if not m: raise ValueError('Unterminated string')\n            rhsides[-1].append(m.group(1)[1:-1])\n            pos = m.end()\n\n        elif line[pos] == '|':\n            m = _DISJUNCTION_RE.match(line, pos)\n            probabilities.append(0.0)\n            rhsides.append([])\n            pos = m.end()\n\n        else:\n            nonterm, pos = nonterm_parser(line, pos)\n            rhsides[-1].append(nonterm)\n\n    if probabilistic:\n        return [ProbabilisticProduction(lhs, rhs, prob=probability)\n                for (rhs, probability) in zip(rhsides, probabilities)]\n    else:\n        return [Production(lhs, rhs) for rhs in rhsides]\n\n\n\ndef read_grammar(input, nonterm_parser, probabilistic=False, encoding=None):\n    \"\"\"\n    Return a pair consisting of a starting category and a list of\n    ``Productions``.\n\n    :param input: a grammar, either in the form of a string or else\n        as a list of strings.\n    :param nonterm_parser: a function for parsing nonterminals.\n        It should take a ``(string, position)`` as argument and\n        return a ``(nonterminal, position)`` as result.\n    :param probabilistic: are the grammar rules probabilistic?\n    :type probabilistic: bool\n    :param encoding: the encoding of the grammar, if it is a binary string\n    :type encoding: str\n    \"\"\"\n    if encoding is not None:\n        input = input.decode(encoding)\n    if isinstance(input, string_types):\n        lines = input.split('\\n')\n    else:\n        lines = input\n\n    start = None\n    productions = []\n    continue_line = ''\n    for linenum, line in enumerate(lines):\n        line = continue_line + line.strip()\n        if line.startswith('#') or line == '': continue\n        if line.endswith('\\\\'):\n            continue_line = line[:-1].rstrip() + ' '\n            continue\n        continue_line = ''\n        try:\n            if line[0] == '%':\n                directive, args = line[1:].split(None, 1)\n                if directive == 'start':\n                    start, pos = nonterm_parser(args, 0)\n                    if pos != len(args):\n                        raise ValueError('Bad argument to start directive')\n                else:\n                    raise ValueError('Bad directive')\n            else:\n                productions += _read_production(line, nonterm_parser, probabilistic)\n        except ValueError as e:\n            raise ValueError('Unable to parse line %s: %s\\n%s' %\n                             (linenum + 1, line, e))\n\n    if not productions:\n        raise ValueError('No productions found!')\n    if not start:\n        start = productions[0].lhs()\n    return (start, productions)\n\n\n_STANDARD_NONTERM_RE = re.compile('( [\\w/][\\w/^<>-]* ) \\s*', re.VERBOSE)\n\n\ndef standard_nonterm_parser(string, pos):\n    m = _STANDARD_NONTERM_RE.match(string, pos)\n    if not m: raise ValueError('Expected a nonterminal, found: '\n                               + string[pos:])\n    return (Nonterminal(m.group(1)), m.end())\n\n\n\n_READ_DG_RE = re.compile(r'''^\\s*                # leading whitespace\n                              ('[^']+')\\s*        # single-quoted lhs\n                              (?:[-=]+>)\\s*        # arrow\n                              (?:(                 # rhs:\n                                   \"[^\"]+\"         # doubled-quoted terminal\n                                 | '[^']+'         # single-quoted terminal\n                                 | \\|              # disjunction\n                                 )\n                                 \\s*)              # trailing space\n                                 *$''',  # zero or more copies\n                         re.VERBOSE)\n_SPLIT_DG_RE = re.compile(r'''('[^']'|[-=]+>|\"[^\"]+\"|'[^']+'|\\|)''')\n\n\ndef _read_dependency_production(s):\n    if not _READ_DG_RE.match(s):\n        raise ValueError('Bad production string')\n    pieces = _SPLIT_DG_RE.split(s)\n    pieces = [p for i, p in enumerate(pieces) if i % 2 == 1]\n    lhside = pieces[0].strip('\\'\\\"')\n    rhsides = [[]]\n    for piece in pieces[2:]:\n        if piece == '|':\n            rhsides.append([])\n        else:\n            rhsides[-1].append(piece.strip('\\'\\\"'))\n    return [DependencyProduction(lhside, rhside) for rhside in rhsides]\n\n\n\ndef cfg_demo():\n    \"\"\"\n    A demonstration showing how ``CFGs`` can be created and used.\n    \"\"\"\n\n    from nltk import nonterminals, Production, CFG\n\n    S, NP, VP, PP = nonterminals('S, NP, VP, PP')\n    N, V, P, Det = nonterminals('N, V, P, Det')\n    VP_slash_NP = VP / NP\n\n    print('Some nonterminals:', [S, NP, VP, PP, N, V, P, Det, VP / NP])\n    print('    S.symbol() =>', repr(S.symbol()))\n    print()\n\n    print(Production(S, [NP]))\n\n    grammar = CFG.fromstring(\"\"\"\n      S -> NP VP\n      PP -> P NP\n      NP -> Det N | NP PP\n      VP -> V NP | VP PP\n      Det -> 'a' | 'the'\n      N -> 'dog' | 'cat'\n      V -> 'chased' | 'sat'\n      P -> 'on' | 'in'\n    \"\"\")\n\n    print('A Grammar:', repr(grammar))\n    print('    grammar.start()       =>', repr(grammar.start()))\n    print('    grammar.productions() =>', end=' ')\n    print(repr(grammar.productions()).replace(',', ',\\n' + ' ' * 25))\n    print()\n\n\ntoy_pcfg1 = PCFG.fromstring(\"\"\"\n    S -> NP VP [1.0]\n    NP -> Det N [0.5] | NP PP [0.25] | 'John' [0.1] | 'I' [0.15]\n    Det -> 'the' [0.8] | 'my' [0.2]\n    N -> 'man' [0.5] | 'telescope' [0.5]\n    VP -> VP PP [0.1] | V NP [0.7] | V [0.2]\n    V -> 'ate' [0.35] | 'saw' [0.65]\n    PP -> P NP [1.0]\n    P -> 'with' [0.61] | 'under' [0.39]\n    \"\"\")\n\ntoy_pcfg2 = PCFG.fromstring(\"\"\"\n    S    -> NP VP         [1.0]\n    VP   -> V NP          [.59]\n    VP   -> V             [.40]\n    VP   -> VP PP         [.01]\n    NP   -> Det N         [.41]\n    NP   -> Name          [.28]\n    NP   -> NP PP         [.31]\n    PP   -> P NP          [1.0]\n    V    -> 'saw'         [.21]\n    V    -> 'ate'         [.51]\n    V    -> 'ran'         [.28]\n    N    -> 'boy'         [.11]\n    N    -> 'cookie'      [.12]\n    N    -> 'table'       [.13]\n    N    -> 'telescope'   [.14]\n    N    -> 'hill'        [.5]\n    Name -> 'Jack'        [.52]\n    Name -> 'Bob'         [.48]\n    P    -> 'with'        [.61]\n    P    -> 'under'       [.39]\n    Det  -> 'the'         [.41]\n    Det  -> 'a'           [.31]\n    Det  -> 'my'          [.28]\n    \"\"\")\n\n\ndef pcfg_demo():\n    \"\"\"\n    A demonstration showing how a ``PCFG`` can be created and used.\n    \"\"\"\n\n    from nltk.corpus import treebank\n    from nltk import treetransforms\n    from nltk import induce_pcfg\n    from nltk.parse import pchart\n\n    pcfg_prods = toy_pcfg1.productions()\n\n    pcfg_prod = pcfg_prods[2]\n    print('A PCFG production:', repr(pcfg_prod))\n    print('    pcfg_prod.lhs()  =>', repr(pcfg_prod.lhs()))\n    print('    pcfg_prod.rhs()  =>', repr(pcfg_prod.rhs()))\n    print('    pcfg_prod.prob() =>', repr(pcfg_prod.prob()))\n    print()\n\n    grammar = toy_pcfg2\n    print('A PCFG grammar:', repr(grammar))\n    print('    grammar.start()       =>', repr(grammar.start()))\n    print('    grammar.productions() =>', end=' ')\n    print(repr(grammar.productions()).replace(',', ',\\n' + ' ' * 26))\n    print()\n\n    print(\"Induce PCFG grammar from treebank data:\")\n\n    productions = []\n    item = treebank._fileids[0]\n    for tree in treebank.parsed_sents(item)[:3]:\n        tree.collapse_unary(collapsePOS=False)\n        tree.chomsky_normal_form(horzMarkov=2)\n\n        productions += tree.productions()\n\n    S = Nonterminal('S')\n    grammar = induce_pcfg(S, productions)\n    print(grammar)\n    print()\n\n    print(\"Parse sentence using induced grammar:\")\n\n    parser = pchart.InsideChartParser(grammar)\n    parser.trace(3)\n\n\n    sent = treebank.parsed_sents(item)[0].leaves()\n    print(sent)\n    for parse in parser.parse(sent):\n        print(parse)\n\n\ndef fcfg_demo():\n    import nltk.data\n    g = nltk.data.load('grammars/book_grammars/feat0.fcfg')\n    print(g)\n    print()\n\n\ndef dg_demo():\n    \"\"\"\n    A demonstration showing the creation and inspection of a\n    ``DependencyGrammar``.\n    \"\"\"\n    grammar = DependencyGrammar.fromstring(\"\"\"\n    'scratch' -> 'cats' | 'walls'\n    'walls' -> 'the'\n    'cats' -> 'the'\n    \"\"\")\n    print(grammar)\n\n\ndef sdg_demo():\n    \"\"\"\n    A demonstration of how to read a string representation of\n    a CoNLL format dependency tree.\n    \"\"\"\n    from nltk.parse import DependencyGraph\n\n    dg = DependencyGraph(\"\"\"\n    1   Ze                ze                Pron  Pron  per|3|evofmv|nom                 2   su      _  _\n    2   had               heb               V     V     trans|ovt|1of2of3|ev             0   ROOT    _  _\n    3   met               met               Prep  Prep  voor                             8   mod     _  _\n    4   haar              haar              Pron  Pron  bez|3|ev|neut|attr               5   det     _  _\n    5   moeder            moeder            N     N     soort|ev|neut                    3   obj1    _  _\n    6   kunnen            kan               V     V     hulp|ott|1of2of3|mv              2   vc      _  _\n    7   gaan              ga                V     V     hulp|inf                         6   vc      _  _\n    8   winkelen          winkel            V     V     intrans|inf                      11  cnj     _  _\n    9   ,                 ,                 Punc  Punc  komma                            8   punct   _  _\n    10  zwemmen           zwem              V     V     intrans|inf                      11  cnj     _  _\n    11  of                of                Conj  Conj  neven                            7   vc      _  _\n    12  terrassen         terras            N     N     soort|mv|neut                    11  cnj     _  _\n    13  .                 .                 Punc  Punc  punt                             12  punct   _  _\n    \"\"\")\n    tree = dg.tree()\n    print(tree.pprint())\n\n\ndef demo():\n    cfg_demo()\n    pcfg_demo()\n    fcfg_demo()\n    dg_demo()\n    sdg_demo()\n\n\nif __name__ == '__main__':\n    demo()\n\n__all__ = ['Nonterminal', 'nonterminals',\n           'CFG', 'Production',\n           'PCFG', 'ProbabilisticProduction',\n           'DependencyGrammar', 'DependencyProduction',\n           'ProbabilisticDependencyGrammar',\n           'induce_pcfg', 'read_grammar']\n"], "nltk\\help": [".py", "\nfrom __future__ import print_function\n\nimport re\nfrom textwrap import wrap\n\nfrom nltk.data import load\n\ndef brown_tagset(tagpattern=None):\n    _format_tagset(\"brown_tagset\", tagpattern)\n\ndef claws5_tagset(tagpattern=None):\n    _format_tagset(\"claws5_tagset\", tagpattern)\n\ndef upenn_tagset(tagpattern=None):\n    _format_tagset(\"upenn_tagset\", tagpattern)\n\n\ndef _print_entries(tags, tagdict):\n    for tag in tags:\n        entry = tagdict[tag]\n        defn = [tag + \": \" + entry[0]]\n        examples = wrap(entry[1], width=75, initial_indent='    ', subsequent_indent='    ')\n        print(\"\\n\".join(defn + examples))\n\ndef _format_tagset(tagset, tagpattern=None):\n    tagdict = load(\"help/tagsets/\" + tagset + \".pickle\")\n    if not tagpattern:\n        _print_entries(sorted(tagdict), tagdict)\n    elif tagpattern in tagdict:\n        _print_entries([tagpattern], tagdict)\n    else:\n        tagpattern = re.compile(tagpattern)\n        tags = [tag for tag in sorted(tagdict) if tagpattern.match(tag)]\n        if tags:\n            _print_entries(tags, tagdict)\n        else:\n            print(\"No matching tags found.\")\n\nif __name__ == '__main__':\n    brown_tagset(r'NN.*')\n    upenn_tagset(r'.*\\$')\n    claws5_tagset('UNDEFINED')\n    brown_tagset(r'NN')\n"], "nltk\\inference\\api": [".py", "\nfrom __future__ import print_function\nfrom abc import ABCMeta, abstractmethod\nfrom six import add_metaclass\n\nimport threading\nimport time\n\n\n@add_metaclass(ABCMeta)\nclass Prover(object):\n    def prove(self, goal=None, assumptions=None, verbose=False):\n        return self._prove(goal, assumptions, verbose)[0]\n\n    @abstractmethod\n    def _prove(self, goal=None, assumptions=None, verbose=False):\n\n\n@add_metaclass(ABCMeta)\nclass ModelBuilder(object):\n    def build_model(self, goal=None, assumptions=None, verbose=False):\n        return self._build_model(goal, assumptions, verbose)[0]\n\n    @abstractmethod\n    def _build_model(self, goal=None, assumptions=None, verbose=False):\n\n\n@add_metaclass(ABCMeta)\nclass TheoremToolCommand(object):\n    @abstractmethod\n    def add_assumptions(self, new_assumptions):\n\n    @abstractmethod\n    def retract_assumptions(self, retracted, debug=False):\n\n    @abstractmethod\n    def assumptions(self):\n\n    @abstractmethod\n    def goal(self):\n\n    @abstractmethod\n    def print_assumptions(self):\n\n\nclass ProverCommand(TheoremToolCommand):\n    @abstractmethod\n    def prove(self, verbose=False):\n\n    @abstractmethod\n    def proof(self, simplify=True):\n\n    @abstractmethod\n    def get_prover(self):\n\n\nclass ModelBuilderCommand(TheoremToolCommand):\n    @abstractmethod\n    def build_model(self, verbose=False):\n\n    @abstractmethod\n    def model(self, format=None):\n\n    @abstractmethod\n    def get_model_builder(self):\n\n\nclass BaseTheoremToolCommand(TheoremToolCommand):\n    def __init__(self, goal=None, assumptions=None):\n        self._goal = goal\n\n        if not assumptions:\n            self._assumptions = []\n        else:\n            self._assumptions = list(assumptions)\n\n        self._result = None\n\n    def add_assumptions(self, new_assumptions):\n        self._assumptions.extend(new_assumptions)\n        self._result = None\n\n    def retract_assumptions(self, retracted, debug=False):\n        retracted = set(retracted)\n        result_list = list(filter(lambda a: a not in retracted, self._assumptions))\n        if debug and result_list == self._assumptions:\n            print(Warning(\"Assumptions list has not been changed:\"))\n            self.print_assumptions()\n\n        self._assumptions = result_list\n\n        self._result = None\n\n    def assumptions(self):\n        return self._assumptions\n\n    def goal(self):\n        return self._goal\n\n    def print_assumptions(self):\n        for a in self.assumptions():\n            print(a)\n\n\nclass BaseProverCommand(BaseTheoremToolCommand, ProverCommand):\n    def __init__(self, prover, goal=None, assumptions=None):\n        self._prover = prover\n\n        BaseTheoremToolCommand.__init__(self, goal, assumptions)\n\n        self._proof = None\n\n    def prove(self, verbose=False):\n        if self._result is None:\n            self._result, self._proof = self._prover._prove(self.goal(),\n                                                            self.assumptions(),\n                                                            verbose)\n        return self._result\n\n    def proof(self, simplify=True):\n        if self._result is None:\n            raise LookupError(\"You have to call prove() first to get a proof!\")\n        else:\n            return self.decorate_proof(self._proof, simplify)\n\n    def decorate_proof(self, proof_string, simplify=True):\n        return proof_string\n\n    def get_prover(self):\n        return self._prover\n\n\nclass BaseModelBuilderCommand(BaseTheoremToolCommand, ModelBuilderCommand):\n    def __init__(self, modelbuilder, goal=None, assumptions=None):\n        self._modelbuilder = modelbuilder\n\n        BaseTheoremToolCommand.__init__(self, goal, assumptions)\n\n        self._model = None\n\n    def build_model(self, verbose=False):\n        if self._result is None:\n            self._result, self._model = \\\n                    self._modelbuilder._build_model(self.goal(),\n                                                    self.assumptions(),\n                                                    verbose)\n        return self._result\n\n    def model(self, format=None):\n        if self._result is None:\n            raise LookupError('You have to call build_model() first to '\n                              'get a model!')\n        else:\n            return self._decorate_model(self._model, format)\n\n    def _decorate_model(self, valuation_str, format=None):\n        return valuation_str\n\n    def get_model_builder(self):\n        return self._modelbuilder\n\n\nclass TheoremToolCommandDecorator(TheoremToolCommand):\n    def __init__(self, command):\n        self._command = command\n\n        self._result = None\n\n    def assumptions(self):\n        return self._command.assumptions()\n\n    def goal(self):\n        return self._command.goal()\n\n    def add_assumptions(self, new_assumptions):\n        self._command.add_assumptions(new_assumptions)\n        self._result = None\n\n    def retract_assumptions(self, retracted, debug=False):\n        self._command.retract_assumptions(retracted, debug)\n        self._result = None\n\n    def print_assumptions(self):\n        self._command.print_assumptions()\n\n\nclass ProverCommandDecorator(TheoremToolCommandDecorator, ProverCommand):\n    def __init__(self, proverCommand):\n        TheoremToolCommandDecorator.__init__(self, proverCommand)\n\n        self._proof = None\n\n    def prove(self, verbose=False):\n        if self._result is None:\n            prover = self.get_prover()\n            self._result, self._proof = prover._prove(self.goal(),\n                                                      self.assumptions(),\n                                                      verbose)\n        return self._result\n\n    def proof(self, simplify=True):\n        if self._result is None:\n            raise LookupError(\"You have to call prove() first to get a proof!\")\n        else:\n            return self.decorate_proof(self._proof, simplify)\n\n    def decorate_proof(self, proof_string, simplify=True):\n        return self._command.decorate_proof(proof_string, simplify)\n\n    def get_prover(self):\n        return self._command.get_prover()\n\n\nclass ModelBuilderCommandDecorator(TheoremToolCommandDecorator, ModelBuilderCommand):\n    def __init__(self, modelBuilderCommand):\n        TheoremToolCommandDecorator.__init__(self, modelBuilderCommand)\n\n        self._model = None\n\n    def build_model(self, verbose=False):\n        if self._result is None:\n            modelbuilder = self.get_model_builder()\n            self._result, self._model = \\\n                            modelbuilder._build_model(self.goal(),\n                                                      self.assumptions(),\n                                                      verbose)\n        return self._result\n\n    def model(self, format=None):\n        if self._result is None:\n            raise LookupError('You have to call build_model() first to '\n                              'get a model!')\n        else:\n            return self._decorate_model(self._model, format)\n\n    def _decorate_model(self, valuation_str, format=None):\n        return self._command._decorate_model(valuation_str, format)\n\n    def get_model_builder(self):\n        return self._command.get_prover()\n\n\nclass ParallelProverBuilder(Prover, ModelBuilder):\n    def __init__(self, prover, modelbuilder):\n        self._prover = prover\n        self._modelbuilder = modelbuilder\n\n    def _prove(self, goal=None, assumptions=None, verbose=False):\n        return self._run(goal, assumptions, verbose), ''\n\n    def _build_model(self, goal=None, assumptions=None, verbose=False):\n        return not self._run(goal, assumptions, verbose), ''\n\n    def _run(self, goal, assumptions, verbose):\n        tp_thread = TheoremToolThread(lambda: self._prover.prove(goal, assumptions, verbose), verbose, 'TP')\n        mb_thread = TheoremToolThread(lambda: self._modelbuilder.build_model(goal, assumptions, verbose), verbose, 'MB')\n\n        tp_thread.start()\n        mb_thread.start()\n\n        while tp_thread.isAlive() and mb_thread.isAlive():\n            pass\n\n        if tp_thread.result is not None:\n            return tp_thread.result\n        elif mb_thread.result is not None:\n            return not mb_thread.result\n        else:\n            return None\n\n\nclass ParallelProverBuilderCommand(BaseProverCommand, BaseModelBuilderCommand):\n    def __init__(self, prover, modelbuilder, goal=None, assumptions=None):\n        BaseProverCommand.__init__(self, prover, goal, assumptions)\n        BaseModelBuilderCommand.__init__(self, modelbuilder, goal, assumptions)\n\n    def prove(self, verbose=False):\n        return self._run(verbose)\n\n    def build_model(self, verbose=False):\n        return not self._run(verbose)\n\n    def _run(self, verbose):\n        tp_thread = TheoremToolThread(lambda: BaseProverCommand.prove(self, verbose), verbose, 'TP')\n        mb_thread = TheoremToolThread(lambda: BaseModelBuilderCommand.build_model(self, verbose), verbose, 'MB')\n\n        tp_thread.start()\n        mb_thread.start()\n\n        while tp_thread.isAlive() and mb_thread.isAlive():\n            pass\n\n        if tp_thread.result is not None:\n            self._result = tp_thread.result\n        elif mb_thread.result is not None:\n            self._result = not mb_thread.result\n        return self._result\n\n\nclass TheoremToolThread(threading.Thread):\n    def __init__(self, command, verbose, name=None):\n        threading.Thread.__init__(self)\n        self._command = command\n        self._result = None\n        self._verbose = verbose\n        self._name = name\n\n    def run(self):\n        try:\n            self._result = self._command()\n            if self._verbose:\n                print('Thread %s finished with result %s at %s' % \\\n                      (self._name, self._result, time.localtime(time.time())))\n        except Exception as e:\n            print(e)\n            print('Thread %s completed abnormally' % (self._name))\n\n    @property\n    def result(self): return self._result\n"], "nltk\\inference\\discourse": [".py", "\nfrom __future__ import print_function\nfrom abc import ABCMeta, abstractmethod\nfrom six import add_metaclass\nimport os\n\nfrom operator import and_, add\nfrom functools import reduce\n\nfrom nltk.data import show_cfg\nfrom nltk.tag import RegexpTagger\nfrom nltk.parse import load_parser\nfrom nltk.parse.malt import MaltParser\nfrom nltk.sem.drt import resolve_anaphora, AnaphoraResolutionException\nfrom nltk.sem.glue import DrtGlue\nfrom nltk.sem.logic import Expression\n\nfrom nltk.inference.mace import MaceCommand\nfrom nltk.inference.prover9 import Prover9Command\n\n\n@add_metaclass(ABCMeta)\nclass ReadingCommand(object):\n    @abstractmethod\n    def parse_to_readings(self, sentence):\n\n    def process_thread(self, sentence_readings):\n        return sentence_readings\n\n    @abstractmethod\n    def combine_readings(self, readings):\n\n    @abstractmethod\n    def to_fol(self, expression):\n\n\nclass CfgReadingCommand(ReadingCommand):\n    def __init__(self, gramfile=None):\n        self._gramfile = (gramfile if gramfile else 'grammars/book_grammars/discourse.fcfg')\n        self._parser = load_parser(self._gramfile)\n\n    def parse_to_readings(self, sentence):\n        from nltk.sem import root_semrep\n        tokens = sentence.split()\n        trees = self._parser.parse(tokens)\n        return [root_semrep(tree) for tree in trees]\n\n    def combine_readings(self, readings):\n        return reduce(and_, readings)\n\n    def to_fol(self, expression):\n        return expression\n\n\nclass DrtGlueReadingCommand(ReadingCommand):\n    def __init__(self, semtype_file=None, remove_duplicates=False,\n                 depparser=None):\n        if semtype_file is None:\n            semtype_file = os.path.join('grammars', 'sample_grammars','drt_glue.semtype')\n        self._glue = DrtGlue(semtype_file=semtype_file,\n                             remove_duplicates=remove_duplicates,\n                             depparser=depparser)\n\n    def parse_to_readings(self, sentence):\n        return self._glue.parse_to_meaning(sentence)\n\n    def process_thread(self, sentence_readings):\n        try:\n            return [self.combine_readings(sentence_readings)]\n        except AnaphoraResolutionException:\n            return []\n\n    def combine_readings(self, readings):\n        thread_reading = reduce(add, readings)\n        return resolve_anaphora(thread_reading.simplify())\n\n    def to_fol(self, expression):\n        return expression.fol()\n\n\nclass DiscourseTester(object):\n    def __init__(self, input, reading_command=None, background=None):\n        self._input = input\n        self._sentences = dict([('s%s' % i, sent) for i, sent in enumerate(input)])\n        self._models = None\n        self._readings = {}\n        self._reading_command = (reading_command if reading_command else CfgReadingCommand())\n        self._threads = {}\n        self._filtered_threads = {}\n        if background is not None:\n            from nltk.sem.logic import Expression\n            for e in background:\n                assert isinstance(e, Expression)\n            self._background = background\n        else:\n            self._background = []\n\n\n    def sentences(self):\n        for id in sorted(self._sentences):\n            print(\"%s: %s\" % (id, self._sentences[id]))\n\n    def add_sentence(self, sentence, informchk=False, consistchk=False,):\n        if informchk:\n            self.readings(verbose=False)\n            for tid in sorted(self._threads):\n                assumptions = [reading for (rid, reading) in self.expand_threads(tid)]\n                assumptions += self._background\n                for sent_reading in self._get_readings(sentence):\n                    tp = Prover9Command(goal=sent_reading, assumptions=assumptions)\n                    if tp.prove():\n                        print(\"Sentence '%s' under reading '%s':\" % (sentence, str(sent_reading)))\n                        print(\"Not informative relative to thread '%s'\" % tid)\n\n        self._input.append(sentence)\n        self._sentences = dict([('s%s' % i, sent) for i, sent in enumerate(self._input)])\n        if consistchk:\n            self.readings(verbose=False)\n            self.models(show=False)\n\n    def retract_sentence(self, sentence, verbose=True):\n        try:\n            self._input.remove(sentence)\n        except ValueError:\n            print(\"Retraction failed. The sentence '%s' is not part of the current discourse:\" % sentence)\n            self.sentences()\n            return None\n        self._sentences = dict([('s%s' % i, sent) for i, sent in enumerate(self._input)])\n        self.readings(verbose=False)\n        if verbose:\n            print(\"Current sentences are \")\n            self.sentences()\n\n    def grammar(self):\n        show_cfg(self._reading_command._gramfile)\n\n\n    def _get_readings(self, sentence):\n        return self._reading_command.parse_to_readings(sentence)\n\n    def _construct_readings(self):\n        self._readings = {}\n        for sid in sorted(self._sentences):\n            sentence = self._sentences[sid]\n            readings = self._get_readings(sentence)\n            self._readings[sid] = dict([(\"%s-r%s\" % (sid, rid), reading.simplify())\n                                                        for rid, reading in enumerate(sorted(readings, key=str))])\n\n    def _construct_threads(self):\n        thread_list = [[]]\n        for sid in sorted(self._readings):\n            thread_list = self.multiply(thread_list, sorted(self._readings[sid]))\n        self._threads = dict([(\"d%s\" % tid, thread) for tid, thread in enumerate(thread_list)])\n        self._filtered_threads = {}\n        consistency_checked = self._check_consistency(self._threads)\n        for (tid, thread) in self._threads.items():\n            if (tid, True) in consistency_checked:\n                self._filtered_threads[tid] = thread\n\n    def _show_readings(self, sentence=None):\n        if sentence is not None:\n            print(\"The sentence '%s' has these readings:\" % sentence)\n            for r in [str(reading) for reading in (self._get_readings(sentence))]:\n                print(\"    %s\" % r)\n        else:\n            for sid in sorted(self._readings):\n                print()\n                print('%s readings:' % sid)\n                print() #'-' * 30\n                for rid in sorted(self._readings[sid]):\n                    lf = self._readings[sid][rid]\n                    print(\"%s: %s\" % (rid, lf.normalize()))\n\n    def _show_threads(self, filter=False, show_thread_readings=False):\n        threads = (self._filtered_threads if filter else self._threads)\n        for tid in sorted(threads):\n            if show_thread_readings:\n                readings = [self._readings[rid.split('-')[0]][rid]\n                            for rid in self._threads[tid]]\n                try:\n                    thread_reading = \": %s\" % \\\n                              self._reading_command.combine_readings(readings).normalize()\n                except Exception as e:\n                    thread_reading = ': INVALID: %s' % e.__class__.__name__\n            else:\n                thread_reading = ''\n\n            print(\"%s:\" % tid, self._threads[tid], thread_reading)\n\n\n    def readings(self, sentence=None, threaded=False, verbose=True,\n                 filter=False, show_thread_readings=False):\n        self._construct_readings()\n        self._construct_threads()\n\n        if filter or show_thread_readings:\n            threaded = True\n\n        if verbose:\n            if not threaded:\n                self._show_readings(sentence=sentence)\n            else:\n                self._show_threads(filter=filter,\n                                   show_thread_readings=show_thread_readings)\n\n    def expand_threads(self, thread_id, threads=None):\n        if threads is None:\n            threads = self._threads\n        return [(rid, self._readings[sid][rid]) for rid in threads[thread_id] for sid in rid.split('-')[:1]]\n\n\n\n    def _check_consistency(self, threads, show=False, verbose=False):\n        results = []\n        for tid in sorted(threads):\n            assumptions = [reading for (rid, reading) in self.expand_threads(tid, threads=threads)]\n            assumptions = list(map(self._reading_command.to_fol, self._reading_command.process_thread(assumptions)))\n            if assumptions:\n                assumptions += self._background\n                mb = MaceCommand(None, assumptions, max_models=20)\n                modelfound = mb.build_model()\n            else:\n                modelfound = False\n            results.append((tid, modelfound))\n            if show:\n                spacer(80)\n                print(\"Model for Discourse Thread %s\" % tid)\n                spacer(80)\n                if verbose:\n                    for a in assumptions:\n                        print(a)\n                    spacer(80)\n                if modelfound:\n                    print(mb.model(format='cooked'))\n                else:\n                    print(\"No model found!\\n\")\n        return results\n\n    def models(self, thread_id=None, show=True, verbose=False):\n        self._construct_readings()\n        self._construct_threads()\n        threads = ({thread_id: self._threads[thread_id]} if thread_id else self._threads)\n\n        for (tid, modelfound) in self._check_consistency(threads, show=show, verbose=verbose):\n            idlist = [rid for rid in threads[tid]]\n\n            if not modelfound:\n                print(\"Inconsistent discourse: %s %s:\" % (tid, idlist))\n                for rid, reading in self.expand_threads(tid):\n                    print(\"    %s: %s\" % (rid, reading.normalize()))\n                print()\n            else:\n                print(\"Consistent discourse: %s %s:\" % (tid, idlist))\n                for rid, reading in self.expand_threads(tid):\n                    print(\"    %s: %s\" % (rid, reading.normalize()))\n                print()\n\n    def add_background(self, background, verbose=False):\n        from nltk.sem.logic import Expression\n        for (count, e) in enumerate(background):\n            assert isinstance(e, Expression)\n            if verbose:\n                print(\"Adding assumption %s to background\" % count)\n            self._background.append(e)\n\n        self._construct_readings()\n        self._construct_threads()\n\n    def background(self):\n        for e in self._background:\n            print(str(e))\n\n\n    @staticmethod\n    def multiply(discourse, readings):\n        result = []\n        for sublist in discourse:\n            for r in readings:\n                new = []\n                new += sublist\n                new.append(r)\n                result.append(new)\n        return result\n\n\n\ndef load_fol(s):\n    statements = []\n    for linenum, line in enumerate(s.splitlines()):\n        line = line.strip()\n        if line.startswith('#') or line == '':\n            continue\n        try:\n            statements.append(Expression.fromstring(line))\n        except Exception:\n            raise ValueError('Unable to parse line %s: %s' % (linenum, line))\n    return statements\n\n\ndef discourse_demo(reading_command=None):\n    dt = DiscourseTester(['A boxer walks', 'Every boxer chases a girl'],\n                         reading_command)\n    dt.models()\n    print()\n    print()\n    dt.sentences()\n    print()\n    dt.readings()\n    print()\n    dt.readings(threaded=True)\n    print()\n    dt.models('d1')\n    dt.add_sentence('John is a boxer')\n    print()\n    dt.sentences()\n    print()\n    dt.readings(threaded=True)\n    print()\n    dt = DiscourseTester(['A student dances', 'Every student is a person'],\n                         reading_command)\n    print()\n    dt.add_sentence('No person dances', consistchk=True)\n    print()\n    dt.readings()\n    print()\n    dt.retract_sentence('No person dances', verbose=True)\n    print()\n    dt.models()\n    print()\n    dt.readings('A person dances')\n    print()\n    dt.add_sentence('A person dances', informchk=True)\n    dt = DiscourseTester(['Vincent is a boxer', 'Fido is a boxer',\n                          'Vincent is married', 'Fido barks'],\n                         reading_command)\n    dt.readings(filter=True)\n    import nltk.data\n    background_file = os.path.join('grammars', 'book_grammars', 'background.fol')\n    background = nltk.data.load(background_file)\n\n    print()\n    dt.add_background(background, verbose=False)\n    dt.background()\n    print()\n    dt.readings(filter=True)\n    print()\n    dt.models()\n\n\ndef drt_discourse_demo(reading_command=None):\n    dt = DiscourseTester(['every dog chases a boy', 'he runs'],\n                         reading_command)\n    dt.models()\n    print()\n    dt.sentences()\n    print()\n    dt.readings()\n    print()\n    dt.readings(show_thread_readings=True)\n    print()\n    dt.readings(filter=True, show_thread_readings=True)\n\n\ndef spacer(num=30):\n    print('-' * num)\n\n\ndef demo():\n    discourse_demo()\n\n    tagger = RegexpTagger([('^(chases|runs)$', 'VB'),\n                           ('^(a)$', 'ex_quant'),\n                           ('^(every)$', 'univ_quant'),\n                           ('^(dog|boy)$', 'NN'),\n                           ('^(he)$', 'PRP')])\n    depparser = MaltParser(tagger=tagger)\n    drt_discourse_demo(DrtGlueReadingCommand(remove_duplicates=False,\n                                             depparser=depparser))\n\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\inference\\mace": [".py", "\n\nfrom __future__ import print_function\n\nimport os\nimport tempfile\n\nfrom nltk.sem.logic import is_indvar\nfrom nltk.sem import Valuation, Expression\n\nfrom nltk.inference.api import ModelBuilder, BaseModelBuilderCommand\nfrom nltk.inference.prover9 import Prover9CommandParent, Prover9Parent\n\n\nclass MaceCommand(Prover9CommandParent, BaseModelBuilderCommand):\n    _interpformat_bin = None\n\n    def __init__(self, goal=None, assumptions=None, max_models=500, model_builder=None):\n        if model_builder is not None:\n            assert isinstance(model_builder, Mace)\n        else:\n            model_builder = Mace(max_models)\n\n        BaseModelBuilderCommand.__init__(self, model_builder, goal, assumptions)\n\n    @property\n    def valuation(mbc): return mbc.model('valuation')\n\n    def _convert2val(self, valuation_str):\n        valuation_standard_format = self._transform_output(valuation_str, 'standard')\n\n        val = []\n        for line in valuation_standard_format.splitlines(False):\n            l = line.strip()\n\n            if l.startswith('interpretation'):\n                num_entities = int(l[l.index('(')+1:l.index(',')].strip())\n\n            elif l.startswith('function') and l.find('_') == -1:\n                name = l[l.index('(')+1:l.index(',')].strip()\n                if is_indvar(name):\n                    name = name.upper()\n                value = int(l[l.index('[')+1:l.index(']')].strip())\n                val.append((name, MaceCommand._make_model_var(value)))\n\n            elif l.startswith('relation'):\n                l = l[l.index('(')+1:]\n                if '(' in l:\n                    name = l[:l.index('(')].strip()\n                    values = [int(v.strip()) for v in l[l.index('[')+1:l.index(']')].split(',')]\n                    val.append((name, MaceCommand._make_relation_set(num_entities, values)))\n                else:\n                    name = l[:l.index(',')].strip()\n                    value = int(l[l.index('[')+1:l.index(']')].strip())\n                    val.append((name, value == 1))\n\n        return Valuation(val)\n\n    @staticmethod\n    def _make_relation_set(num_entities, values):\n        r = set()\n        for position in [pos for (pos,v) in enumerate(values) if v == 1]:\n            r.add(tuple(MaceCommand._make_relation_tuple(position, values, num_entities)))\n        return r\n\n    @staticmethod\n    def _make_relation_tuple(position, values, num_entities):\n        if len(values) == 1:\n            return []\n        else:\n            sublist_size = len(values) // num_entities\n            sublist_start = position // sublist_size\n            sublist_position = int(position % sublist_size)\n\n            sublist = values[sublist_start*sublist_size:(sublist_start+1)*sublist_size]\n            return [MaceCommand._make_model_var(sublist_start)] + \\\n                   MaceCommand._make_relation_tuple(sublist_position,\n                                                    sublist,\n                                                    num_entities)\n\n    @staticmethod\n    def _make_model_var(value):\n        letter = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n',\n                  'o','p','q','r','s','t','u','v','w','x','y','z'][value]\n        num = value // 26\n        return (letter + str(num) if num > 0 else letter)\n\n    def _decorate_model(self, valuation_str, format):\n        if not format:\n            return valuation_str\n        elif format == 'valuation':\n            return self._convert2val(valuation_str)\n        else:\n            return self._transform_output(valuation_str, format)\n\n    def _transform_output(self, valuation_str, format):\n        if format in ['standard', 'standard2', 'portable', 'tabular',\n                      'raw', 'cooked', 'xml', 'tex']:\n            return self._call_interpformat(valuation_str, [format])[0]\n        else:\n            raise LookupError(\"The specified format does not exist\")\n\n    def _call_interpformat(self, input_str, args=[], verbose=False):\n        if self._interpformat_bin is None:\n            self._interpformat_bin = self._modelbuilder._find_binary(\n                                                'interpformat', verbose)\n\n        return self._modelbuilder._call(input_str, self._interpformat_bin,\n                                        args, verbose)\n\n\nclass Mace(Prover9Parent, ModelBuilder):\n    _mace4_bin = None\n\n    def __init__(self, end_size=500):\n        self._end_size = end_size\n        Use Mace4 to build a first order model.\n\n        :return: ``True`` if a model was found (i.e. Mace returns value of 0),\n        else ``False``\n        \"\"\"\n        if not assumptions:\n            assumptions = []\n\n        stdout, returncode = self._call_mace4(self.prover9_input(goal, assumptions),\n                                              verbose=verbose)\n        return (returncode == 0, stdout)\n\n    def _call_mace4(self, input_str, args=[], verbose=False):\n        \"\"\"\n        Call the ``mace4`` binary with the given input.\n\n        :param input_str: A string whose contents are used as stdin.\n        :param args: A list of command-line arguments.\n        :return: A tuple (stdout, returncode)\n        :see: ``config_prover9``\n        \"\"\"\n        if self._mace4_bin is None:\n            self._mace4_bin = self._find_binary('mace4', verbose)\n\n        updated_input_str = ''\n        if self._end_size > 0:\n            updated_input_str += 'assign(end_size, %d).\\n\\n' % self._end_size\n        updated_input_str += input_str\n\n        return self._call(updated_input_str, self._mace4_bin, args, verbose)\n\n\ndef spacer(num=30):\n    print('-' * num)\n\ndef decode_result(found):\n    \"\"\"\n    Decode the result of model_found()\n\n    :param found: The output of model_found()\n    :type found: bool\n    \"\"\"\n    return {True: 'Countermodel found', False: 'No countermodel found', None: 'None'}[found]\n\ndef test_model_found(arguments):\n    \"\"\"\n    Try some proofs and exhibit the results.\n    \"\"\"\n    for (goal, assumptions) in arguments:\n        g = Expression.fromstring(goal)\n        alist = [lp.parse(a) for a in assumptions]\n        m = MaceCommand(g, assumptions=alist, max_models=50)\n        found = m.build_model()\n        for a in alist:\n            print('   %s' % a)\n        print('|- %s: %s\\n' % (g, decode_result(found)))\n\n\ndef test_build_model(arguments):\n    \"\"\"\n    Try to build a ``nltk.sem.Valuation``.\n    \"\"\"\n    g = Expression.fromstring('all x.man(x)')\n    alist = [Expression.fromstring(a) for a in ['man(John)',\n                                   'man(Socrates)',\n                                   'man(Bill)',\n                                   'some x.(-(x = John) & man(x) & sees(John,x))',\n                                   'some x.(-(x = Bill) & man(x))',\n                                   'all x.some y.(man(x) -> gives(Socrates,x,y))']]\n\n    m = MaceCommand(g, assumptions=alist)\n    m.build_model()\n    spacer()\n    print(\"Assumptions and Goal\")\n    spacer()\n    for a in alist:\n        print('   %s' % a)\n    print('|- %s: %s\\n' % (g, decode_result(m.build_model())))\n    spacer()\n    print(\"Valuation\")\n    spacer()\n    print(m.valuation, '\\n')\n\ndef test_transform_output(argument_pair):\n    \"\"\"\n    Transform the model into various Mace4 ``interpformat`` formats.\n    \"\"\"\n    g = Expression.fromstring(argument_pair[0])\n    alist = [lp.parse(a) for a in argument_pair[1]]\n    m = MaceCommand(g, assumptions=alist)\n    m.build_model()\n    for a in alist:\n        print('   %s' % a)\n    print('|- %s: %s\\n' % (g, m.build_model()))\n    for format in ['standard', 'portable', 'xml', 'cooked']:\n        spacer()\n        print(\"Using '%s' format\" % format)\n        spacer()\n        print(m.model(format=format))\n\ndef test_make_relation_set():\n    print(MaceCommand._make_relation_set(num_entities=3, values=[1,0,1]) == set([('c',), ('a',)]))\n    print(MaceCommand._make_relation_set(num_entities=3, values=[0,0,0,0,0,0,1,0,0]) == set([('c', 'a')]))\n    print(MaceCommand._make_relation_set(num_entities=2, values=[0,0,1,0,0,0,1,0]) == set([('a', 'b', 'a'), ('b', 'b', 'a')]))\n\narguments = [\n    ('mortal(Socrates)', ['all x.(man(x) -> mortal(x))', 'man(Socrates)']),\n    ('(not mortal(Socrates))', ['all x.(man(x) -> mortal(x))', 'man(Socrates)'])\n]\n\ndef demo():\n    test_model_found(arguments)\n    test_build_model(arguments)\n    test_transform_output(arguments[1])\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\inference\\nonmonotonic": [".py", "\nfrom __future__ import print_function, unicode_literals\n\nfrom nltk.inference.prover9 import Prover9, Prover9Command\nfrom collections import defaultdict\nfrom functools import reduce\n\nfrom nltk.sem.logic import (VariableExpression, EqualityExpression,\n                            ApplicationExpression, Expression,\n                            AbstractVariableExpression, AllExpression,\n                            BooleanExpression, NegatedExpression,\n                            ExistsExpression, Variable, ImpExpression,\n                            AndExpression, unique_variable, operator)\n\nfrom nltk.inference.api import Prover, ProverCommandDecorator\nfrom nltk.compat import python_2_unicode_compatible\n\nclass ProverParseError(Exception): pass\n\ndef get_domain(goal, assumptions):\n    if goal is None:\n        all_expressions = assumptions\n    else:\n        all_expressions = assumptions + [-goal]\n    return reduce(operator.or_, (a.constants() for a in all_expressions), set())\n\nclass ClosedDomainProver(ProverCommandDecorator):\n    def assumptions(self):\n        assumptions = [a for a in self._command.assumptions()]\n        goal = self._command.goal()\n        domain = get_domain(goal, assumptions)\n        return [self.replace_quants(ex, domain) for ex in assumptions]\n\n    def goal(self):\n        goal = self._command.goal()\n        domain = get_domain(goal, self._command.assumptions())\n        return self.replace_quants(goal, domain)\n\n    def replace_quants(self, ex, domain):\n        if isinstance(ex, AllExpression):\n            conjuncts = [ex.term.replace(ex.variable, VariableExpression(d))\n                         for d in domain]\n            conjuncts = [self.replace_quants(c, domain) for c in conjuncts]\n            return reduce(lambda x,y: x&y, conjuncts)\n        elif isinstance(ex, BooleanExpression):\n            return ex.__class__(self.replace_quants(ex.first, domain),\n                                self.replace_quants(ex.second, domain) )\n        elif isinstance(ex, NegatedExpression):\n            return -self.replace_quants(ex.term, domain)\n        elif isinstance(ex, ExistsExpression):\n            disjuncts = [ex.term.replace(ex.variable, VariableExpression(d))\n                         for d in domain]\n            disjuncts = [self.replace_quants(d, domain) for d in disjuncts]\n            return reduce(lambda x,y: x|y, disjuncts)\n        else:\n            return ex\n\nclass UniqueNamesProver(ProverCommandDecorator):\n    def assumptions(self):\n        assumptions = self._command.assumptions()\n\n        domain = list(get_domain(self._command.goal(), assumptions))\n\n        eq_sets = SetHolder()\n        for a in assumptions:\n            if isinstance(a, EqualityExpression):\n                av = a.first.variable\n                bv = a.second.variable\n                eq_sets[av].add(bv)\n\n        new_assumptions = []\n        for i,a in enumerate(domain):\n            for b in domain[i+1:]:\n                if b not in eq_sets[a]:\n                    newEqEx = EqualityExpression(VariableExpression(a),\n                                                 VariableExpression(b))\n                    if Prover9().prove(newEqEx, assumptions):\n                        eq_sets[a].add(b)\n                    else:\n                        new_assumptions.append(-newEqEx)\n\n        return assumptions + new_assumptions\n\nclass SetHolder(list):\n    def __getitem__(self, item):\n        assert isinstance(item, Variable)\n        for s in self:\n            if item in s:\n                return s\n        new = set([item])\n        self.append(new)\n        return new\n\nclass ClosedWorldProver(ProverCommandDecorator):\n    def assumptions(self):\n        assumptions = self._command.assumptions()\n\n        predicates = self._make_predicate_dict(assumptions)\n\n        new_assumptions = []\n        for p in predicates:\n            predHolder = predicates[p]\n            new_sig = self._make_unique_signature(predHolder)\n            new_sig_exs = [VariableExpression(v) for v in new_sig]\n\n            disjuncts = []\n\n            for sig in predHolder.signatures:\n                equality_exs = []\n                for v1,v2 in zip(new_sig_exs, sig):\n                    equality_exs.append(EqualityExpression(v1,v2))\n                disjuncts.append(reduce(lambda x,y: x&y, equality_exs))\n\n            for prop in predHolder.properties:\n                bindings = {}\n                for v1,v2 in zip(new_sig_exs, prop[0]):\n                    bindings[v2] = v1\n                disjuncts.append(prop[1].substitute_bindings(bindings))\n\n            if disjuncts:\n                antecedent = self._make_antecedent(p, new_sig)\n                consequent = reduce(lambda x,y: x|y, disjuncts)\n                accum = ImpExpression(antecedent, consequent)\n            else:\n                accum = NegatedExpression(self._make_antecedent(p, new_sig))\n\n            for new_sig_var in new_sig[::-1]:\n                accum = AllExpression(new_sig_var, accum)\n            new_assumptions.append(accum)\n\n        return assumptions + new_assumptions\n\n    def _make_unique_signature(self, predHolder):\n        return tuple(unique_variable() for i in range(predHolder.signature_len))\n\n    def _make_antecedent(self, predicate, signature):\n        antecedent = predicate\n        for v in signature:\n            antecedent = antecedent(VariableExpression(v))\n        return antecedent\n\n    def _make_predicate_dict(self, assumptions):\n        predicates = defaultdict(PredHolder)\n        for a in assumptions:\n            self._map_predicates(a, predicates)\n        return predicates\n\n    def _map_predicates(self, expression, predDict):\n        if isinstance(expression, ApplicationExpression):\n            func, args = expression.uncurry()\n            if isinstance(func, AbstractVariableExpression):\n                predDict[func].append_sig(tuple(args))\n        elif isinstance(expression, AndExpression):\n            self._map_predicates(expression.first, predDict)\n            self._map_predicates(expression.second, predDict)\n        elif isinstance(expression, AllExpression):\n            sig = [expression.variable]\n            term = expression.term\n            while isinstance(term, AllExpression):\n                sig.append(term.variable)\n                term = term.term\n            if isinstance(term, ImpExpression):\n                if isinstance(term.first, ApplicationExpression) and \\\n                   isinstance(term.second, ApplicationExpression):\n                    func1, args1 = term.first.uncurry()\n                    func2, args2 = term.second.uncurry()\n                    if isinstance(func1, AbstractVariableExpression) and \\\n                       isinstance(func2, AbstractVariableExpression) and \\\n                       sig == [v.variable for v in args1] and \\\n                       sig == [v.variable for v in args2]:\n                        predDict[func2].append_prop((tuple(sig), term.first))\n                        predDict[func1].validate_sig_len(sig)\n\n@python_2_unicode_compatible\nclass PredHolder(object):\n    def __init__(self):\n        self.signatures = []\n        self.properties = []\n        self.signature_len = None\n\n    def append_sig(self, new_sig):\n        self.validate_sig_len(new_sig)\n        self.signatures.append(new_sig)\n\n    def append_prop(self, new_prop):\n        self.validate_sig_len(new_prop[0])\n        self.properties.append(new_prop)\n\n    def validate_sig_len(self, new_sig):\n        if self.signature_len is None:\n            self.signature_len = len(new_sig)\n        elif self.signature_len != len(new_sig):\n            raise Exception(\"Signature lengths do not match\")\n\n    def __str__(self):\n        return '(%s,%s,%s)' % (self.signatures, self.properties,\n                               self.signature_len)\n\n    def __repr__(self):\n        return \"%s\" % self\n\ndef closed_domain_demo():\n    lexpr = Expression.fromstring\n\n    p1 = lexpr(r'exists x.walk(x)')\n    p2 = lexpr(r'man(Socrates)')\n    c = lexpr(r'walk(Socrates)')\n    prover = Prover9Command(c, [p1,p2])\n    print(prover.prove())\n    cdp = ClosedDomainProver(prover)\n    print('assumptions:')\n    for a in cdp.assumptions(): print('   ', a)\n    print('goal:', cdp.goal())\n    print(cdp.prove())\n\n    p1 = lexpr(r'exists x.walk(x)')\n    p2 = lexpr(r'man(Socrates)')\n    p3 = lexpr(r'-walk(Bill)')\n    c = lexpr(r'walk(Socrates)')\n    prover = Prover9Command(c, [p1,p2,p3])\n    print(prover.prove())\n    cdp = ClosedDomainProver(prover)\n    print('assumptions:')\n    for a in cdp.assumptions(): print('   ', a)\n    print('goal:', cdp.goal())\n    print(cdp.prove())\n\n    p1 = lexpr(r'exists x.walk(x)')\n    p2 = lexpr(r'man(Socrates)')\n    p3 = lexpr(r'-walk(Bill)')\n    c = lexpr(r'walk(Socrates)')\n    prover = Prover9Command(c, [p1,p2,p3])\n    print(prover.prove())\n    cdp = ClosedDomainProver(prover)\n    print('assumptions:')\n    for a in cdp.assumptions(): print('   ', a)\n    print('goal:', cdp.goal())\n    print(cdp.prove())\n\n    p1 = lexpr(r'walk(Socrates)')\n    p2 = lexpr(r'walk(Bill)')\n    c = lexpr(r'all x.walk(x)')\n    prover = Prover9Command(c, [p1,p2])\n    print(prover.prove())\n    cdp = ClosedDomainProver(prover)\n    print('assumptions:')\n    for a in cdp.assumptions(): print('   ', a)\n    print('goal:', cdp.goal())\n    print(cdp.prove())\n\n    p1 = lexpr(r'girl(mary)')\n    p2 = lexpr(r'dog(rover)')\n    p3 = lexpr(r'all x.(girl(x) -> -dog(x))')\n    p4 = lexpr(r'all x.(dog(x) -> -girl(x))')\n    p5 = lexpr(r'chase(mary, rover)')\n    c = lexpr(r'exists y.(dog(y) & all x.(girl(x) -> chase(x,y)))')\n    prover = Prover9Command(c, [p1,p2,p3,p4,p5])\n    print(prover.prove())\n    cdp = ClosedDomainProver(prover)\n    print('assumptions:')\n    for a in cdp.assumptions(): print('   ', a)\n    print('goal:', cdp.goal())\n    print(cdp.prove())\n\ndef unique_names_demo():\n    lexpr = Expression.fromstring\n\n    p1 = lexpr(r'man(Socrates)')\n    p2 = lexpr(r'man(Bill)')\n    c = lexpr(r'exists x.exists y.(x != y)')\n    prover = Prover9Command(c, [p1,p2])\n    print(prover.prove())\n    unp = UniqueNamesProver(prover)\n    print('assumptions:')\n    for a in unp.assumptions(): print('   ', a)\n    print('goal:', unp.goal())\n    print(unp.prove())\n\n    p1 = lexpr(r'all x.(walk(x) -> (x = Socrates))')\n    p2 = lexpr(r'Bill = William')\n    p3 = lexpr(r'Bill = Billy')\n    c = lexpr(r'-walk(William)')\n    prover = Prover9Command(c, [p1,p2,p3])\n    print(prover.prove())\n    unp = UniqueNamesProver(prover)\n    print('assumptions:')\n    for a in unp.assumptions(): print('   ', a)\n    print('goal:', unp.goal())\n    print(unp.prove())\n\ndef closed_world_demo():\n    lexpr = Expression.fromstring\n\n    p1 = lexpr(r'walk(Socrates)')\n    p2 = lexpr(r'(Socrates != Bill)')\n    c = lexpr(r'-walk(Bill)')\n    prover = Prover9Command(c, [p1,p2])\n    print(prover.prove())\n    cwp = ClosedWorldProver(prover)\n    print('assumptions:')\n    for a in cwp.assumptions(): print('   ', a)\n    print('goal:', cwp.goal())\n    print(cwp.prove())\n\n    p1 = lexpr(r'see(Socrates, John)')\n    p2 = lexpr(r'see(John, Mary)')\n    p3 = lexpr(r'(Socrates != John)')\n    p4 = lexpr(r'(John != Mary)')\n    c = lexpr(r'-see(Socrates, Mary)')\n    prover = Prover9Command(c, [p1,p2,p3,p4])\n    print(prover.prove())\n    cwp = ClosedWorldProver(prover)\n    print('assumptions:')\n    for a in cwp.assumptions(): print('   ', a)\n    print('goal:', cwp.goal())\n    print(cwp.prove())\n\n    p1 = lexpr(r'all x.(ostrich(x) -> bird(x))')\n    p2 = lexpr(r'bird(Tweety)')\n    p3 = lexpr(r'-ostrich(Sam)')\n    p4 = lexpr(r'Sam != Tweety')\n    c = lexpr(r'-bird(Sam)')\n    prover = Prover9Command(c, [p1,p2,p3,p4])\n    print(prover.prove())\n    cwp = ClosedWorldProver(prover)\n    print('assumptions:')\n    for a in cwp.assumptions(): print('   ', a)\n    print('goal:', cwp.goal())\n    print(cwp.prove())\n\ndef combination_prover_demo():\n    lexpr = Expression.fromstring\n\n    p1 = lexpr(r'see(Socrates, John)')\n    p2 = lexpr(r'see(John, Mary)')\n    c = lexpr(r'-see(Socrates, Mary)')\n    prover = Prover9Command(c, [p1,p2])\n    print(prover.prove())\n    command = ClosedDomainProver(\n                  UniqueNamesProver(\n                      ClosedWorldProver(prover)))\n    for a in command.assumptions(): print(a)\n    print(command.prove())\n\ndef default_reasoning_demo():\n    lexpr = Expression.fromstring\n\n    premises = []\n\n    premises.append(lexpr(r'all x.(elephant(x)        -> animal(x))'))\n    premises.append(lexpr(r'all x.(bird(x)            -> animal(x))'))\n    premises.append(lexpr(r'all x.(dove(x)            -> bird(x))'))\n    premises.append(lexpr(r'all x.(ostrich(x)         -> bird(x))'))\n    premises.append(lexpr(r'all x.(flying_ostrich(x)  -> ostrich(x))'))\n\n    premises.append(lexpr(r'all x.((animal(x)  & -Ab1(x)) -> -fly(x))')) #normal animals don't fly\n    premises.append(lexpr(r'all x.((bird(x)    & -Ab2(x)) -> fly(x))')) #normal birds fly\n    premises.append(lexpr(r'all x.((ostrich(x) & -Ab3(x)) -> -fly(x))')) #normal ostriches don't fly\n\n    premises.append(lexpr(r'all x.(bird(x)           -> Ab1(x))')) #flight\n    premises.append(lexpr(r'all x.(ostrich(x)        -> Ab2(x))')) #non-flying bird\n    premises.append(lexpr(r'all x.(flying_ostrich(x) -> Ab3(x))')) #flying ostrich\n\n    premises.append(lexpr(r'elephant(E)'))\n    premises.append(lexpr(r'dove(D)'))\n    premises.append(lexpr(r'ostrich(O)'))\n\n    prover = Prover9Command(None, premises)\n    command = UniqueNamesProver(ClosedWorldProver(prover))\n    for a in command.assumptions(): print(a)\n\n    print_proof('-fly(E)', premises)\n    print_proof('fly(D)', premises)\n    print_proof('-fly(O)', premises)\n\ndef print_proof(goal, premises):\n    lexpr = Expression.fromstring\n    prover = Prover9Command(lexpr(goal), premises)\n    command = UniqueNamesProver(ClosedWorldProver(prover))\n    print(goal, prover.prove(), command.prove())\n\ndef demo():\n    closed_domain_demo()\n    unique_names_demo()\n    closed_world_demo()\n    combination_prover_demo()\n    default_reasoning_demo()\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\inference\\prover9": [".py", "from __future__ import print_function\n\nimport os\nimport subprocess\n\nimport nltk\nfrom nltk.sem.logic import Expression, ExistsExpression, AllExpression, \\\n    NegatedExpression, AndExpression, IffExpression, OrExpression, \\\n    EqualityExpression, ImpExpression\nfrom nltk.inference.api import BaseProverCommand, Prover\n\np9_return_codes = {\n    0: True,\n    1:  \"(FATAL)\",      #A fatal error occurred (user's syntax error).\n    2: False,           # (SOS_EMPTY) Prover9 ran out of things to do\n    3: \"(MAX_MEGS)\",    # The max_megs (memory limit) parameter was exceeded.\n    4: \"(MAX_SECONDS)\", # The max_seconds parameter was exceeded.\n    5: \"(MAX_GIVEN)\",   # The max_given parameter was exceeded.\n    6: \"(MAX_KEPT)\",    # The max_kept parameter was exceeded.\n    7: \"(ACTION)\",      # A Prover9 action terminated the search.\n    101: \"(SIGSEGV)\",   # Prover9 crashed, most probably due to a bug.\n }\n\n\nclass Prover9CommandParent(object):\n    def print_assumptions(self, output_format='nltk'):\n        if output_format.lower() == 'nltk':\n            for a in self.assumptions():\n                print(a)\n        elif output_format.lower() == 'prover9':\n            for a in convert_to_prover9(self.assumptions()):\n                print(a)\n        else:\n            raise NameError(\"Unrecognized value for 'output_format': %s\" %\n                            output_format)\n\nclass Prover9Command(Prover9CommandParent, BaseProverCommand):\n    def __init__(self, goal=None, assumptions=None, timeout=60, prover=None):\n        if not assumptions:\n            assumptions = []\n\n        if prover is not None:\n            assert isinstance(prover, Prover9)\n        else:\n            prover = Prover9(timeout)\n\n        BaseProverCommand.__init__(self, prover, goal, assumptions)\n\n    def decorate_proof(self, proof_string, simplify=True):\n        if simplify:\n            return self._prover._call_prooftrans(proof_string, ['striplabels'])[0].rstrip()\n        else:\n            return proof_string.rstrip()\n\n\nclass Prover9Parent(object):\n\n    _binary_location = None\n\n    def config_prover9(self, binary_location, verbose=False):\n        if binary_location is None:\n            self._binary_location = None\n            self._prover9_bin = None\n        else:\n            name = 'prover9'\n            self._prover9_bin = nltk.internals.find_binary(\n                                  name,\n                                  path_to_bin=binary_location,\n                                  env_vars=['PROVER9'],\n                                  url='http://www.cs.unm.edu/~mccune/prover9/',\n                                  binary_names=[name, name + '.exe'],\n                                  verbose=verbose)\n            self._binary_location = self._prover9_bin.rsplit(os.path.sep, 1)\n\n    def prover9_input(self, goal, assumptions):\n        s = ''\n\n        if assumptions:\n            s += 'formulas(assumptions).\\n'\n            for p9_assumption in convert_to_prover9(assumptions):\n                s += '    %s.\\n' % p9_assumption\n            s += 'end_of_list.\\n\\n'\n\n        if goal:\n            s += 'formulas(goals).\\n'\n            s += '    %s.\\n' % convert_to_prover9(goal)\n            s += 'end_of_list.\\n\\n'\n\n        return s\n\n    def binary_locations(self):\n        return ['/usr/local/bin/prover9',\n                '/usr/local/bin/prover9/bin',\n                '/usr/local/bin',\n                '/usr/bin',\n                '/usr/local/prover9',\n                '/usr/local/share/prover9']\n\n    def _find_binary(self, name, verbose=False):\n        binary_locations = self.binary_locations()\n        if self._binary_location is not None:\n            binary_locations += [self._binary_location]\n        return nltk.internals.find_binary(name,\n            searchpath=binary_locations,\n            env_vars=['PROVER9'],\n            url='http://www.cs.unm.edu/~mccune/prover9/',\n            binary_names=[name, name + '.exe'],\n            verbose=verbose)\n\n    def _call(self, input_str, binary, args=[], verbose=False):\n        if verbose:\n            print('Calling:', binary)\n            print('Args:', args)\n            print('Input:\\n', input_str, '\\n')\n\n        cmd = [binary] + args\n        try:\n            input_str = input_str.encode(\"utf8\")\n        except AttributeError:\n            pass\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n                             stderr=subprocess.STDOUT,\n                             stdin=subprocess.PIPE)\n        (stdout, stderr) = p.communicate(input=input_str)\n\n        if verbose:\n            print('Return code:', p.returncode)\n            if stdout: print('stdout:\\n', stdout, '\\n')\n            if stderr: print('stderr:\\n', stderr, '\\n')\n\n        return (stdout.decode(\"utf-8\"), p.returncode)\n\n\ndef convert_to_prover9(input):\n    if isinstance(input, list):\n        result = []\n        for s in input:\n            try:\n                result.append(_convert_to_prover9(s.simplify()))\n            except:\n                print('input %s cannot be converted to Prover9 input syntax' % input)\n                raise\n        return result\n    else:\n        try:\n            return _convert_to_prover9(input.simplify())\n        except:\n            print('input %s cannot be converted to Prover9 input syntax' % input)\n            raise\n\ndef _convert_to_prover9(expression):\n    if isinstance(expression, ExistsExpression):\n        return 'exists ' + str(expression.variable) + ' ' + _convert_to_prover9(expression.term)\n    elif isinstance(expression, AllExpression):\n        return 'all ' + str(expression.variable) + ' ' + _convert_to_prover9(expression.term)\n    elif isinstance(expression, NegatedExpression):\n        return '-(' + _convert_to_prover9(expression.term) + ')'\n    elif isinstance(expression, AndExpression):\n        return '(' + _convert_to_prover9(expression.first) + ' & ' + \\\n                     _convert_to_prover9(expression.second) + ')'\n    elif isinstance(expression, OrExpression):\n        return '(' + _convert_to_prover9(expression.first) + ' | ' + \\\n                     _convert_to_prover9(expression.second) + ')'\n    elif isinstance(expression, ImpExpression):\n        return '(' + _convert_to_prover9(expression.first) + ' -> ' + \\\n                     _convert_to_prover9(expression.second) + ')'\n    elif isinstance(expression, IffExpression):\n        return '(' + _convert_to_prover9(expression.first) + ' <-> ' + \\\n                     _convert_to_prover9(expression.second) + ')'\n    elif isinstance(expression, EqualityExpression):\n        return '(' + _convert_to_prover9(expression.first) + ' = ' + \\\n                     _convert_to_prover9(expression.second) + ')'\n    else:\n        return str(expression)\n\n\nclass Prover9(Prover9Parent, Prover):\n    _prover9_bin = None\n    _prooftrans_bin = None\n\n    def __init__(self, timeout=60):\n        self._timeout = timeout\n        Use Prover9 to prove a theorem.\n        :return: A pair whose first element is a boolean indicating if the\n        proof was successful (i.e. returns value of 0) and whose second element\n        is the output of the prover.\n        \"\"\"\n        if not assumptions:\n            assumptions = []\n\n        stdout, returncode = self._call_prover9(self.prover9_input(goal, assumptions),\n                                                verbose=verbose)\n        return (returncode == 0, stdout)\n\n    def prover9_input(self, goal, assumptions):\n        \"\"\"\n        :see: Prover9Parent.prover9_input\n        \"\"\"\n        s = 'clear(auto_denials).\\n' #only one proof required\n        return s + Prover9Parent.prover9_input(self, goal, assumptions)\n\n    def _call_prover9(self, input_str, args=[], verbose=False):\n        \"\"\"\n        Call the ``prover9`` binary with the given input.\n\n        :param input_str: A string whose contents are used as stdin.\n        :param args: A list of command-line arguments.\n        :return: A tuple (stdout, returncode)\n        :see: ``config_prover9``\n        \"\"\"\n        if self._prover9_bin is None:\n            self._prover9_bin = self._find_binary('prover9', verbose)\n\n        updated_input_str = ''\n        if self._timeout > 0:\n            updated_input_str += 'assign(max_seconds, %d).\\n\\n' % self._timeout\n        updated_input_str += input_str\n\n        stdout, returncode = self._call(updated_input_str, self._prover9_bin, args, verbose)\n\n        if returncode not in [0,2]:\n            errormsgprefix = '%%ERROR:'\n            if errormsgprefix in stdout:\n                msgstart = stdout.index(errormsgprefix)\n                errormsg = stdout[msgstart:].strip()\n            else:\n                errormsg = None\n            if returncode in [3,4,5,6]:\n                raise Prover9LimitExceededException(returncode, errormsg)\n            else:\n                raise Prover9FatalException(returncode, errormsg)\n\n        return stdout, returncode\n\n    def _call_prooftrans(self, input_str, args=[], verbose=False):\n        \"\"\"\n        Call the ``prooftrans`` binary with the given input.\n\n        :param input_str: A string whose contents are used as stdin.\n        :param args: A list of command-line arguments.\n        :return: A tuple (stdout, returncode)\n        :see: ``config_prover9``\n        \"\"\"\n        if self._prooftrans_bin is None:\n            self._prooftrans_bin = self._find_binary('prooftrans', verbose)\n\n        return self._call(input_str, self._prooftrans_bin, args, verbose)\n\n\nclass Prover9Exception(Exception):\n    def __init__(self, returncode, message):\n        msg = p9_return_codes[returncode]\n        if message:\n            msg += '\\n%s' % message\n        Exception.__init__(self, msg)\n\nclass Prover9FatalException(Prover9Exception):\n    pass\n\nclass Prover9LimitExceededException(Prover9Exception):\n    pass\n\n\n\n\ndef test_config():\n\n    a = Expression.fromstring('(walk(j) & sing(j))')\n    g = Expression.fromstring('walk(j)')\n    p = Prover9Command(g, assumptions=[a])\n    p._executable_path = None\n    p.prover9_search=[]\n    p.prove()\n    print(p.prove())\n    print(p.proof())\n\ndef test_convert_to_prover9(expr):\n    \"\"\"\n    Test that parsing works OK.\n    \"\"\"\n    for t in expr:\n        e = Expression.fromstring(t)\n        print(convert_to_prover9(e))\n\ndef test_prove(arguments):\n    \"\"\"\n    Try some proofs and exhibit the results.\n    \"\"\"\n    for (goal, assumptions) in arguments:\n        g = Expression.fromstring(goal)\n        alist = [Expression.fromstring(a) for a in assumptions]\n        p = Prover9Command(g, assumptions=alist).prove()\n        for a in alist:\n            print('   %s' % a)\n        print('|- %s: %s\\n' % (g, p))\n\narguments = [\n    ('(man(x) <-> (not (not man(x))))', []),\n    ('(not (man(x) & (not man(x))))', []),\n    ('(man(x) | (not man(x)))', []),\n    ('(man(x) & (not man(x)))', []),\n    ('(man(x) -> man(x))', []),\n    ('(not (man(x) & (not man(x))))', []),\n    ('(man(x) | (not man(x)))', []),\n    ('(man(x) -> man(x))', []),\n    ('(man(x) <-> man(x))', []),\n    ('(not (man(x) <-> (not man(x))))', []),\n    ('mortal(Socrates)', ['all x.(man(x) -> mortal(x))', 'man(Socrates)']),\n    ('((all x.(man(x) -> walks(x)) & man(Socrates)) -> some y.walks(y))', []),\n    ('(all x.man(x) -> all x.man(x))', []),\n    ('some x.all y.sees(x,y)', []),\n    ('some e3.(walk(e3) & subj(e3, mary))',\n        ['some e1.(see(e1) & subj(e1, john) & some e2.(pred(e1, e2) & walk(e2) & subj(e2, mary)))']),\n    ('some x e1.(see(e1) & subj(e1, x) & some e2.(pred(e1, e2) & walk(e2) & subj(e2, mary)))',\n       ['some e1.(see(e1) & subj(e1, john) & some e2.(pred(e1, e2) & walk(e2) & subj(e2, mary)))'])\n]\n\nexpressions = [r'some x y.sees(x,y)',\n               r'some x.(man(x) & walks(x))',\n               r'\\x.(man(x) & walks(x))',\n               r'\\x y.sees(x,y)',\n               r'walks(john)',\n               r'\\x.big(x, \\y.mouse(y))',\n               r'(walks(x) & (runs(x) & (threes(x) & fours(x))))',\n               r'(walks(x) -> runs(x))',\n               r'some x.(PRO(x) & sees(John, x))',\n               r'some x.(man(x) & (not walks(x)))',\n               r'all x.(man(x) -> walks(x))']\n\ndef spacer(num=45):\n    print('-' * num)\n\ndef demo():\n    print(\"Testing configuration\")\n    spacer()\n    test_config()\n    print()\n    print(\"Testing conversion to Prover9 format\")\n    spacer()\n    test_convert_to_prover9(expressions)\n    print()\n    print(\"Testing proofs\")\n    spacer()\n    test_prove(arguments)\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\inference\\resolution": [".py", "\nfrom __future__ import print_function, unicode_literals\n\nimport operator\nfrom collections import defaultdict\nfrom functools import reduce\n\nfrom nltk.sem import skolemize\nfrom nltk.sem.logic import (VariableExpression, EqualityExpression,\n                            ApplicationExpression, Expression,\n                            NegatedExpression, Variable,\n                            AndExpression, unique_variable, OrExpression,\n                            is_indvar, IndividualVariableExpression, Expression)\n\nfrom nltk.inference.api import Prover, BaseProverCommand\nfrom nltk.compat import python_2_unicode_compatible\n\nclass ProverParseError(Exception): pass\n\nclass ResolutionProver(Prover):\n    ANSWER_KEY = 'ANSWER'\n    _assume_false=True\n\n    def _prove(self, goal=None, assumptions=None, verbose=False):\n        if not assumptions:\n            assumptions = []\n\n        result = None\n        try:\n            clauses = []\n            if goal:\n                clauses.extend(clausify(-goal))\n            for a in assumptions:\n                clauses.extend(clausify(a))\n            result, clauses = self._attempt_proof(clauses)\n            if verbose:\n                print(ResolutionProverCommand._decorate_clauses(clauses))\n        except RuntimeError as e:\n            if self._assume_false and str(e).startswith('maximum recursion depth exceeded'):\n                result = False\n                clauses = []\n            else:\n                if verbose:\n                    print(e)\n                else:\n                    raise e\n        return (result, clauses)\n\n    def _attempt_proof(self, clauses):\n        tried = defaultdict(list)\n\n        i = 0\n        while i < len(clauses):\n            if not clauses[i].is_tautology():\n                if tried[i]:\n                    j = tried[i][-1] + 1\n                else:\n                    j = i + 1 #nothing tried yet for 'i', so start with the next\n\n                while j < len(clauses):\n                    if i != j and j and not clauses[j].is_tautology():\n                        tried[i].append(j)\n                        newclauses = clauses[i].unify(clauses[j])\n                        if newclauses:\n                            for newclause in newclauses:\n                                newclause._parents = (i+1, j+1)\n                                clauses.append(newclause)\n                                if not len(newclause): #if there's an empty clause\n                                    return (True, clauses)\n                            i=-1 #since we added a new clause, restart from the top\n                            break\n                    j += 1\n            i += 1\n        return (False, clauses)\n\nclass ResolutionProverCommand(BaseProverCommand):\n    def __init__(self, goal=None, assumptions=None, prover=None):\n        if prover is not None:\n            assert isinstance(prover, ResolutionProver)\n        else:\n            prover = ResolutionProver()\n\n        BaseProverCommand.__init__(self, prover, goal, assumptions)\n        self._clauses = None\n\n    def prove(self, verbose=False):\n        if self._result is None:\n            self._result, clauses = self._prover._prove(self.goal(),\n                                                        self.assumptions(),\n                                                        verbose)\n            self._clauses = clauses\n            self._proof = ResolutionProverCommand._decorate_clauses(clauses)\n        return self._result\n\n    def find_answers(self, verbose=False):\n        self.prove(verbose)\n\n        answers = set()\n        answer_ex = VariableExpression(Variable(ResolutionProver.ANSWER_KEY))\n        for clause in self._clauses:\n            for term in clause:\n                if isinstance(term, ApplicationExpression) and\\\n                   term.function == answer_ex and\\\n                   not isinstance(term.argument, IndividualVariableExpression):\n                    answers.add(term.argument)\n        return answers\n\n    @staticmethod\n    def _decorate_clauses(clauses):\n        out = ''\n        max_clause_len = max([len(str(clause)) for clause in clauses])\n        max_seq_len = len(str(len(clauses)))\n        for i in range(len(clauses)):\n            parents = 'A'\n            taut = ''\n            if clauses[i].is_tautology():\n                taut = 'Tautology'\n            if clauses[i]._parents:\n                parents = str(clauses[i]._parents)\n            parents = ' '*(max_clause_len-len(str(clauses[i]))+1) + parents\n            seq = ' '*(max_seq_len-len(str(i+1))) + str(i+1)\n            out += '[%s] %s %s %s\\n' % (seq, clauses[i], parents, taut)\n        return out\n\n@python_2_unicode_compatible\nclass Clause(list):\n    def __init__(self, data):\n        list.__init__(self, data)\n        self._is_tautology = None\n        self._parents = None\n\n    def unify(self, other, bindings=None, used=None, skipped=None, debug=False):\n        if bindings is None: bindings = BindingDict()\n        if used is None: used = ([],[])\n        if skipped is None: skipped = ([],[])\n        if isinstance(debug, bool): debug = DebugObject(debug)\n\n        newclauses = _iterate_first(self, other, bindings, used, skipped, _complete_unify_path, debug)\n\n        subsumed = []\n        for i, c1 in enumerate(newclauses):\n            if i not in subsumed:\n                for j, c2 in enumerate(newclauses):\n                    if i!=j and j not in subsumed and c1.subsumes(c2):\n                        subsumed.append(j)\n        result = []\n        for i in range(len(newclauses)):\n            if i not in subsumed:\n                result.append(newclauses[i])\n\n        return result\n\n    def isSubsetOf(self, other):\n        for a in self:\n            if a not in other:\n                return False\n        return True\n\n    def subsumes(self, other):\n        negatedother = []\n        for atom in other:\n            if isinstance(atom, NegatedExpression):\n                negatedother.append(atom.term)\n            else:\n                negatedother.append(-atom)\n\n        negatedotherClause = Clause(negatedother)\n\n        bindings = BindingDict()\n        used = ([],[])\n        skipped = ([],[])\n        debug = DebugObject(False)\n\n        return len(_iterate_first(self, negatedotherClause, bindings, used,\n                                      skipped, _subsumes_finalize,\n                                      debug)) > 0\n\n    def __getslice__(self, start, end):\n        return Clause(list.__getslice__(self, start, end))\n\n    def __sub__(self, other):\n        return Clause([a for a in self if a not in other])\n\n    def __add__(self, other):\n        return Clause(list.__add__(self, other))\n\n    def is_tautology(self):\n        if self._is_tautology is not None:\n            return self._is_tautology\n        for i,a in enumerate(self):\n            if not isinstance(a, EqualityExpression):\n                j = len(self)-1\n                while j > i:\n                    b = self[j]\n                    if isinstance(a, NegatedExpression):\n                        if a.term == b:\n                            self._is_tautology = True\n                            return True\n                    elif isinstance(b, NegatedExpression):\n                        if a == b.term:\n                            self._is_tautology = True\n                            return True\n                    j -= 1\n        self._is_tautology = False\n        return False\n\n    def free(self):\n        return reduce(operator.or_, ((atom.free() | atom.constants()) for atom in self))\n\n    def replace(self, variable, expression):\n        return Clause([atom.replace(variable, expression) for atom in self])\n\n    def substitute_bindings(self, bindings):\n        return Clause([atom.substitute_bindings(bindings) for atom in self])\n\n    def __str__(self):\n        return '{' + ', '.join(\"%s\" % item for item in self) + '}'\n\n    def __repr__(self):\n        return \"%s\" % self\n\ndef _iterate_first(first, second, bindings, used, skipped, finalize_method, debug):\n    debug.line('unify(%s,%s) %s'%(first, second, bindings))\n\n    if not len(first) or not len(second): #if no more recursions can be performed\n        return finalize_method(first, second, bindings, used, skipped, debug)\n    else:\n        result = _iterate_second(first, second, bindings, used, skipped, finalize_method, debug+1)\n\n        newskipped = (skipped[0]+[first[0]], skipped[1])\n        result += _iterate_first(first[1:], second, bindings, used, newskipped, finalize_method, debug+1)\n\n        try:\n            newbindings, newused, unused = _unify_terms(first[0], second[0], bindings, used)\n            newfirst = first[1:] + skipped[0] + unused[0]\n            newsecond = second[1:] + skipped[1] + unused[1]\n            result += _iterate_first(newfirst, newsecond, newbindings, newused, ([],[]), finalize_method, debug+1)\n        except BindingException:\n            pass\n\n        return result\n\ndef _iterate_second(first, second, bindings, used, skipped, finalize_method, debug):\n    debug.line('unify(%s,%s) %s'%(first, second, bindings))\n\n    if not len(first) or not len(second): #if no more recursions can be performed\n        return finalize_method(first, second, bindings, used, skipped, debug)\n    else:\n        newskipped = (skipped[0], skipped[1]+[second[0]])\n        result = _iterate_second(first, second[1:], bindings, used, newskipped, finalize_method, debug+1)\n\n        try:\n            newbindings, newused, unused = _unify_terms(first[0], second[0], bindings, used)\n            newfirst = first[1:] + skipped[0] + unused[0]\n            newsecond = second[1:] + skipped[1] + unused[1]\n            result += _iterate_second(newfirst, newsecond, newbindings, newused, ([],[]), finalize_method, debug+1)\n        except BindingException:\n            pass\n\n        return result\n\ndef _unify_terms(a, b, bindings=None, used=None):\n    assert isinstance(a, Expression)\n    assert isinstance(b, Expression)\n\n    if bindings is None: bindings = BindingDict()\n    if used is None: used = ([],[])\n\n    if isinstance(a, NegatedExpression) and isinstance(b, ApplicationExpression):\n        newbindings = most_general_unification(a.term, b, bindings)\n        newused = (used[0]+[a], used[1]+[b])\n        unused = ([],[])\n    elif isinstance(a, ApplicationExpression) and isinstance(b, NegatedExpression):\n        newbindings = most_general_unification(a, b.term, bindings)\n        newused = (used[0]+[a], used[1]+[b])\n        unused = ([],[])\n\n    elif isinstance(a, EqualityExpression):\n        newbindings = BindingDict([(a.first.variable, a.second)])\n        newused = (used[0]+[a], used[1])\n        unused = ([],[b])\n    elif isinstance(b, EqualityExpression):\n        newbindings = BindingDict([(b.first.variable, b.second)])\n        newused = (used[0], used[1]+[b])\n        unused = ([a],[])\n\n    else:\n        raise BindingException((a, b))\n\n    return newbindings, newused, unused\n\ndef _complete_unify_path(first, second, bindings, used, skipped, debug):\n    if used[0] or used[1]: #if bindings were made along the path\n        newclause = Clause(skipped[0] + skipped[1] + first + second)\n        debug.line('  -> New Clause: %s' % newclause)\n        return [newclause.substitute_bindings(bindings)]\n    else: #no bindings made means no unification occurred.  so no result\n        debug.line('  -> End')\n        return []\n\ndef _subsumes_finalize(first, second, bindings, used, skipped, debug):\n    if not len(skipped[0]) and not len(first):\n        return [True]\n    else:\n        return []\n\ndef clausify(expression):\n    clause_list = []\n    for clause in _clausify(skolemize(expression)):\n        for free in clause.free():\n            if is_indvar(free.name):\n                newvar = VariableExpression(unique_variable())\n                clause = clause.replace(free, newvar)\n        clause_list.append(clause)\n    return clause_list\n\ndef _clausify(expression):\n    if isinstance(expression, AndExpression):\n        return _clausify(expression.first) + _clausify(expression.second)\n    elif isinstance(expression, OrExpression):\n        first = _clausify(expression.first)\n        second = _clausify(expression.second)\n        assert len(first) == 1\n        assert len(second) == 1\n        return [first[0] + second[0]]\n    elif isinstance(expression, EqualityExpression):\n        return [Clause([expression])]\n    elif isinstance(expression, ApplicationExpression):\n        return [Clause([expression])]\n    elif isinstance(expression, NegatedExpression):\n        if isinstance(expression.term, ApplicationExpression):\n            return [Clause([expression])]\n        elif isinstance(expression.term, EqualityExpression):\n            return [Clause([expression])]\n    raise ProverParseError()\n\n\n@python_2_unicode_compatible\nclass BindingDict(object):\n    def __init__(self, binding_list=None):\n        self.d = {}\n\n        if binding_list:\n            for (v, b) in binding_list:\n                self[v] = b\n\n    def __setitem__(self, variable, binding):\n        assert isinstance(variable, Variable)\n        assert isinstance(binding, Expression)\n\n        try:\n            existing = self[variable]\n        except KeyError:\n            existing = None\n\n        if not existing or binding == existing:\n            self.d[variable] = binding\n        elif isinstance(binding, IndividualVariableExpression):\n            try:\n                existing = self[binding.variable]\n            except KeyError:\n                existing = None\n\n            binding2 = VariableExpression(variable)\n\n            if not existing or binding2 == existing:\n                self.d[binding.variable] = binding2\n            else:\n                raise BindingException('Variable %s already bound to another '\n                                       'value' % (variable))\n        else:\n            raise BindingException('Variable %s already bound to another '\n                                   'value' % (variable))\n\n    def __getitem__(self, variable):\n        assert isinstance(variable, Variable)\n\n        intermediate = self.d[variable]\n        while intermediate:\n            try:\n                intermediate = self.d[intermediate]\n            except KeyError:\n                return intermediate\n\n    def __contains__(self, item):\n        return item in self.d\n\n    def __add__(self, other):\n        try:\n            combined = BindingDict()\n            for v in self.d:\n                combined[v] = self.d[v]\n            for v in other.d:\n                combined[v] = other.d[v]\n            return combined\n        except BindingException:\n            raise BindingException(\"Attempting to add two contradicting \"\n                                   \"BindingDicts: '%s' and '%s'\"\n                                   % (self, other))\n\n    def __len__(self):\n        return len(self.d)\n\n    def __str__(self):\n        data_str = ', '.join('%s: %s' % (v, self.d[v]) for v in sorted(self.d.keys()))\n        return '{' + data_str + '}'\n\n    def __repr__(self):\n        return \"%s\" % self\n\n\ndef most_general_unification(a, b, bindings=None):\n    if bindings is None:\n        bindings = BindingDict()\n\n    if a == b:\n        return bindings\n    elif isinstance(a, IndividualVariableExpression):\n        return _mgu_var(a, b, bindings)\n    elif isinstance(b, IndividualVariableExpression):\n        return _mgu_var(b, a, bindings)\n    elif isinstance(a, ApplicationExpression) and\\\n         isinstance(b, ApplicationExpression):\n        return most_general_unification(a.function, b.function, bindings) +\\\n               most_general_unification(a.argument, b.argument, bindings)\n    raise BindingException((a, b))\n\ndef _mgu_var(var, expression, bindings):\n    if var.variable in expression.free()|expression.constants():\n        raise BindingException((var, expression))\n    else:\n        return BindingDict([(var.variable, expression)]) + bindings\n\n\nclass BindingException(Exception):\n    def __init__(self, arg):\n        if isinstance(arg, tuple):\n            Exception.__init__(self, \"'%s' cannot be bound to '%s'\" % arg)\n        else:\n            Exception.__init__(self, arg)\n\nclass UnificationException(Exception):\n    def __init__(self, a, b):\n        Exception.__init__(self, \"'%s' cannot unify with '%s'\" % (a,b))\n\n\nclass DebugObject(object):\n    def __init__(self, enabled=True, indent=0):\n        self.enabled = enabled\n        self.indent = indent\n\n    def __add__(self, i):\n        return DebugObject(self.enabled, self.indent+i)\n\n    def line(self, line):\n        if self.enabled:\n            print('    '*self.indent + line)\n\n\ndef testResolutionProver():\n    resolution_test(r'man(x)')\n    resolution_test(r'(man(x) -> man(x))')\n    resolution_test(r'(man(x) -> --man(x))')\n    resolution_test(r'-(man(x) and -man(x))')\n    resolution_test(r'(man(x) or -man(x))')\n    resolution_test(r'(man(x) -> man(x))')\n    resolution_test(r'-(man(x) and -man(x))')\n    resolution_test(r'(man(x) or -man(x))')\n    resolution_test(r'(man(x) -> man(x))')\n    resolution_test(r'(man(x) iff man(x))')\n    resolution_test(r'-(man(x) iff -man(x))')\n    resolution_test('all x.man(x)')\n    resolution_test('-all x.some y.F(x,y) & some x.all y.(-F(x,y))')\n    resolution_test('some x.all y.sees(x,y)')\n\n    p1 = Expression.fromstring(r'all x.(man(x) -> mortal(x))')\n    p2 = Expression.fromstring(r'man(Socrates)')\n    c = Expression.fromstring(r'mortal(Socrates)')\n    print('%s, %s |- %s: %s' % (p1, p2, c, ResolutionProver().prove(c, [p1,p2])))\n\n    p1 = Expression.fromstring(r'all x.(man(x) -> walks(x))')\n    p2 = Expression.fromstring(r'man(John)')\n    c = Expression.fromstring(r'some y.walks(y)')\n    print('%s, %s |- %s: %s' % (p1, p2, c, ResolutionProver().prove(c, [p1,p2])))\n\n    p = Expression.fromstring(r'some e1.some e2.(believe(e1,john,e2) & walk(e2,mary))')\n    c = Expression.fromstring(r'some e0.walk(e0,mary)')\n    print('%s |- %s: %s' % (p, c, ResolutionProver().prove(c, [p])))\n\ndef resolution_test(e):\n    f = Expression.fromstring(e)\n    t = ResolutionProver().prove(f)\n    print('|- %s: %s' % (f, t))\n\ndef test_clausify():\n    lexpr = Expression.fromstring\n\n    print(clausify(lexpr('P(x) | Q(x)')))\n    print(clausify(lexpr('(P(x) & Q(x)) | R(x)')))\n    print(clausify(lexpr('P(x) | (Q(x) & R(x))')))\n    print(clausify(lexpr('(P(x) & Q(x)) | (R(x) & S(x))')))\n\n    print(clausify(lexpr('P(x) | Q(x) | R(x)')))\n    print(clausify(lexpr('P(x) | (Q(x) & R(x)) | S(x)')))\n\n    print(clausify(lexpr('exists x.P(x) | Q(x)')))\n\n    print(clausify(lexpr('-(-P(x) & Q(x))')))\n    print(clausify(lexpr('P(x) <-> Q(x)')))\n    print(clausify(lexpr('-(P(x) <-> Q(x))')))\n    print(clausify(lexpr('-(all x.P(x))')))\n    print(clausify(lexpr('-(some x.P(x))')))\n\n    print(clausify(lexpr('some x.P(x)')))\n    print(clausify(lexpr('some x.all y.P(x,y)')))\n    print(clausify(lexpr('all y.some x.P(x,y)')))\n    print(clausify(lexpr('all z.all y.some x.P(x,y,z)')))\n    print(clausify(lexpr('all x.(all y.P(x,y) -> -all y.(Q(x,y) -> R(x,y)))')))\n\n\ndef demo():\n    test_clausify()\n    print()\n    testResolutionProver()\n    print()\n\n    p = Expression.fromstring('man(x)')\n    print(ResolutionProverCommand(p, [p]).prove())\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\inference\\tableau": [".py", "\nfrom __future__ import print_function, unicode_literals\n\nfrom nltk.internals import Counter\n\nfrom nltk.sem.logic import (VariableExpression, EqualityExpression,\n                            ApplicationExpression, Expression,\n                            AbstractVariableExpression, AllExpression,\n                            NegatedExpression,\n                            ExistsExpression, Variable, ImpExpression,\n                            AndExpression, unique_variable,\n                            LambdaExpression, IffExpression,\n                            OrExpression, FunctionVariableExpression)\n\nfrom nltk.inference.api import Prover, BaseProverCommand\n\n_counter = Counter()\n\nclass ProverParseError(Exception): pass\n\nclass TableauProver(Prover):\n    _assume_false=False\n\n    def _prove(self, goal=None, assumptions=None, verbose=False):\n        if not assumptions:\n            assumptions = []\n\n        result = None\n        try:\n            agenda = Agenda()\n            if goal:\n                agenda.put(-goal)\n            agenda.put_all(assumptions)\n            debugger = Debug(verbose)\n            result = self._attempt_proof(agenda, set(), set(), debugger)\n        except RuntimeError as e:\n            if self._assume_false and str(e).startswith('maximum recursion depth exceeded'):\n                result = False\n            else:\n                if verbose:\n                    print(e)\n                else:\n                    raise e\n        return (result, '\\n'.join(debugger.lines))\n\n    def _attempt_proof(self, agenda, accessible_vars, atoms, debug):\n        (current, context), category = agenda.pop_first()\n\n        if not current:\n            debug.line('AGENDA EMPTY')\n            return False\n\n        proof_method = { Categories.ATOM:     self._attempt_proof_atom,\n                         Categories.PROP:     self._attempt_proof_prop,\n                         Categories.N_ATOM:   self._attempt_proof_n_atom,\n                         Categories.N_PROP:   self._attempt_proof_n_prop,\n                         Categories.APP:      self._attempt_proof_app,\n                         Categories.N_APP:    self._attempt_proof_n_app,\n                         Categories.N_EQ:     self._attempt_proof_n_eq,\n                         Categories.D_NEG:    self._attempt_proof_d_neg,\n                         Categories.N_ALL:    self._attempt_proof_n_all,\n                         Categories.N_EXISTS: self._attempt_proof_n_some,\n                         Categories.AND:      self._attempt_proof_and,\n                         Categories.N_OR:     self._attempt_proof_n_or,\n                         Categories.N_IMP:    self._attempt_proof_n_imp,\n                         Categories.OR:       self._attempt_proof_or,\n                         Categories.IMP:      self._attempt_proof_imp,\n                         Categories.N_AND:    self._attempt_proof_n_and,\n                         Categories.IFF:      self._attempt_proof_iff,\n                         Categories.N_IFF:    self._attempt_proof_n_iff,\n                         Categories.EQ:       self._attempt_proof_eq,\n                         Categories.EXISTS:   self._attempt_proof_some,\n                         Categories.ALL:      self._attempt_proof_all,\n                        }[category]\n\n        debug.line((current, context))\n        return proof_method(current, context, agenda, accessible_vars, atoms, debug)\n\n    def _attempt_proof_atom(self, current, context, agenda, accessible_vars, atoms, debug):\n        if (current, True) in atoms:\n            debug.line('CLOSED', 1)\n            return True\n\n        if context:\n            if isinstance(context.term, NegatedExpression):\n                current = current.negate()\n            agenda.put(context(current).simplify())\n            return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)\n        else:\n            agenda.mark_alls_fresh();\n            return self._attempt_proof(agenda, accessible_vars|set(current.args), atoms|set([(current, False)]), debug+1)\n\n    def _attempt_proof_n_atom(self, current, context, agenda, accessible_vars, atoms, debug):\n        if (current.term, False) in atoms:\n            debug.line('CLOSED', 1)\n            return True\n\n        if context:\n            if isinstance(context.term, NegatedExpression):\n                current = current.negate()\n            agenda.put(context(current).simplify())\n            return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)\n        else:\n            agenda.mark_alls_fresh();\n            return self._attempt_proof(agenda, accessible_vars|set(current.term.args), atoms|set([(current.term, True)]), debug+1)\n\n    def _attempt_proof_prop(self, current, context, agenda, accessible_vars, atoms, debug):\n        if (current, True) in atoms:\n            debug.line('CLOSED', 1)\n            return True\n\n        agenda.mark_alls_fresh();\n        return self._attempt_proof(agenda, accessible_vars, atoms|set([(current, False)]), debug+1)\n\n    def _attempt_proof_n_prop(self, current, context, agenda, accessible_vars, atoms, debug):\n        if (current.term, False) in atoms:\n            debug.line('CLOSED', 1)\n            return True\n\n        agenda.mark_alls_fresh();\n        return self._attempt_proof(agenda, accessible_vars, atoms|set([(current.term, True)]), debug+1)\n\n    def _attempt_proof_app(self, current, context, agenda, accessible_vars, atoms, debug):\n        f, args = current.uncurry()\n        for i, arg in enumerate(args):\n            if not TableauProver.is_atom(arg):\n                ctx = f\n                nv = Variable('X%s' % _counter.get())\n                for j,a in enumerate(args):\n                    ctx = (ctx(VariableExpression(nv)) if i == j else ctx(a))\n                if context:\n                    ctx = context(ctx).simplify()\n                ctx = LambdaExpression(nv, ctx)\n                agenda.put(arg, ctx)\n                return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)\n        raise Exception('If this method is called, there must be a non-atomic argument')\n\n    def _attempt_proof_n_app(self, current, context, agenda, accessible_vars, atoms, debug):\n        f, args = current.term.uncurry()\n        for i, arg in enumerate(args):\n            if not TableauProver.is_atom(arg):\n                ctx = f\n                nv = Variable('X%s' % _counter.get())\n                for j,a in enumerate(args):\n                    ctx = (ctx(VariableExpression(nv)) if i == j else ctx(a))\n                if context:\n                    ctx = context(ctx).simplify()\n                ctx = LambdaExpression(nv, -ctx)\n                agenda.put(-arg, ctx)\n                return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)\n        raise Exception('If this method is called, there must be a non-atomic argument')\n\n    def _attempt_proof_n_eq(self, current, context, agenda, accessible_vars, atoms, debug):\n        if current.term.first == current.term.second:\n            debug.line('CLOSED', 1)\n            return True\n\n        agenda[Categories.N_EQ].add((current,context))\n        current._exhausted = True\n        return self._attempt_proof(agenda, accessible_vars|set([current.term.first, current.term.second]), atoms, debug+1)\n\n    def _attempt_proof_d_neg(self, current, context, agenda, accessible_vars, atoms, debug):\n        agenda.put(current.term.term, context)\n        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)\n\n    def _attempt_proof_n_all(self, current, context, agenda, accessible_vars, atoms, debug):\n        agenda[Categories.EXISTS].add((ExistsExpression(current.term.variable, -current.term.term), context))\n        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)\n\n    def _attempt_proof_n_some(self, current, context, agenda, accessible_vars, atoms, debug):\n        agenda[Categories.ALL].add((AllExpression(current.term.variable, -current.term.term), context))\n        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)\n\n    def _attempt_proof_and(self, current, context, agenda, accessible_vars, atoms, debug):\n        agenda.put(current.first, context)\n        agenda.put(current.second, context)\n        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)\n\n    def _attempt_proof_n_or(self, current, context, agenda, accessible_vars, atoms, debug):\n        agenda.put(-current.term.first, context)\n        agenda.put(-current.term.second, context)\n        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)\n\n    def _attempt_proof_n_imp(self, current, context, agenda, accessible_vars, atoms, debug):\n        agenda.put(current.term.first, context)\n        agenda.put(-current.term.second, context)\n        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)\n\n    def _attempt_proof_or(self, current, context, agenda, accessible_vars, atoms, debug):\n        new_agenda = agenda.clone()\n        agenda.put(current.first, context)\n        new_agenda.put(current.second, context)\n        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1) and \\\n                self._attempt_proof(new_agenda, accessible_vars, atoms, debug+1)\n\n    def _attempt_proof_imp(self, current, context, agenda, accessible_vars, atoms, debug):\n        new_agenda = agenda.clone()\n        agenda.put(-current.first, context)\n        new_agenda.put(current.second, context)\n        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1) and \\\n                self._attempt_proof(new_agenda, accessible_vars, atoms, debug+1)\n\n    def _attempt_proof_n_and(self, current, context, agenda, accessible_vars, atoms, debug):\n        new_agenda = agenda.clone()\n        agenda.put(-current.term.first, context)\n        new_agenda.put(-current.term.second, context)\n        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1) and \\\n                self._attempt_proof(new_agenda, accessible_vars, atoms, debug+1)\n\n    def _attempt_proof_iff(self, current, context, agenda, accessible_vars, atoms, debug):\n        new_agenda = agenda.clone()\n        agenda.put(current.first, context)\n        agenda.put(current.second, context)\n        new_agenda.put(-current.first, context)\n        new_agenda.put(-current.second, context)\n        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1) and \\\n                self._attempt_proof(new_agenda, accessible_vars, atoms, debug+1)\n\n    def _attempt_proof_n_iff(self, current, context, agenda, accessible_vars, atoms, debug):\n        new_agenda = agenda.clone()\n        agenda.put(current.term.first, context)\n        agenda.put(-current.term.second, context)\n        new_agenda.put(-current.term.first, context)\n        new_agenda.put(current.term.second, context)\n        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1) and \\\n                self._attempt_proof(new_agenda, accessible_vars, atoms, debug+1)\n\n    def _attempt_proof_eq(self, current, context, agenda, accessible_vars, atoms, debug):\n        agenda.put_atoms(atoms)\n        agenda.replace_all(current.first, current.second)\n        accessible_vars.discard(current.first)\n        agenda.mark_neqs_fresh();\n        return self._attempt_proof(agenda, accessible_vars, set(), debug+1)\n\n    def _attempt_proof_some(self, current, context, agenda, accessible_vars, atoms, debug):\n        new_unique_variable = VariableExpression(unique_variable())\n        agenda.put(current.term.replace(current.variable, new_unique_variable), context)\n        agenda.mark_alls_fresh()\n        return self._attempt_proof(agenda, accessible_vars|set([new_unique_variable]), atoms, debug+1)\n\n    def _attempt_proof_all(self, current, context, agenda, accessible_vars, atoms, debug):\n        try:\n            current._used_vars\n        except AttributeError:\n            current._used_vars = set()\n\n        if accessible_vars:\n            bv_available = accessible_vars - current._used_vars\n\n            if bv_available:\n                variable_to_use = list(bv_available)[0]\n                debug.line('--> Using \\'%s\\'' % variable_to_use, 2)\n                current._used_vars |= set([variable_to_use])\n                agenda.put(current.term.replace(current.variable, variable_to_use), context)\n                agenda[Categories.ALL].add((current,context))\n                return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)\n\n            else:\n                debug.line('--> Variables Exhausted', 2)\n                current._exhausted = True\n                agenda[Categories.ALL].add((current,context))\n                return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)\n\n        else:\n            new_unique_variable = VariableExpression(unique_variable())\n            debug.line('--> Using \\'%s\\'' % new_unique_variable, 2)\n            current._used_vars |= set([new_unique_variable])\n            agenda.put(current.term.replace(current.variable, new_unique_variable), context)\n            agenda[Categories.ALL].add((current,context))\n            agenda.mark_alls_fresh()\n            return self._attempt_proof(agenda, accessible_vars|set([new_unique_variable]), atoms, debug+1)\n\n    @staticmethod\n    def is_atom(e):\n        if isinstance(e, NegatedExpression):\n            e = e.term\n\n        if isinstance(e, ApplicationExpression):\n            for arg in e.args:\n                if not TableauProver.is_atom(arg):\n                    return False\n            return True\n        elif isinstance(e, AbstractVariableExpression) or \\\n             isinstance(e, LambdaExpression):\n            return True\n        else:\n            return False\n\n\nclass TableauProverCommand(BaseProverCommand):\n    def __init__(self, goal=None, assumptions=None, prover=None):\n        if prover is not None:\n            assert isinstance(prover, TableauProver)\n        else:\n            prover = TableauProver()\n\n        BaseProverCommand.__init__(self, prover, goal, assumptions)\n\n\nclass Agenda(object):\n    def __init__(self):\n        self.sets = tuple(set() for i in range(21))\n\n    def clone(self):\n        new_agenda = Agenda()\n        set_list = [s.copy() for s in self.sets]\n\n        new_allExs = set()\n        for allEx,_ in set_list[Categories.ALL]:\n            new_allEx = AllExpression(allEx.variable, allEx.term)\n            try:\n                new_allEx._used_vars = set(used for used in allEx._used_vars)\n            except AttributeError:\n                new_allEx._used_vars = set()\n            new_allExs.add((new_allEx,None))\n        set_list[Categories.ALL] = new_allExs\n\n        set_list[Categories.N_EQ] = set((NegatedExpression(n_eq.term),ctx)\n                                        for (n_eq,ctx) in set_list[Categories.N_EQ])\n\n        new_agenda.sets = tuple(set_list)\n        return new_agenda\n\n    def __getitem__(self, index):\n        return self.sets[index]\n\n    def put(self, expression, context=None):\n        if isinstance(expression, AllExpression):\n            ex_to_add = AllExpression(expression.variable, expression.term)\n            try:\n                ex_to_add._used_vars = set(used for used in expression._used_vars)\n            except AttributeError:\n                ex_to_add._used_vars = set()\n        else:\n            ex_to_add = expression\n        self.sets[self._categorize_expression(ex_to_add)].add((ex_to_add, context))\n\n    def put_all(self, expressions):\n        for expression in expressions:\n            self.put(expression)\n\n    def put_atoms(self, atoms):\n        for atom, neg in atoms:\n            if neg:\n                self[Categories.N_ATOM].add((-atom,None))\n            else:\n                self[Categories.ATOM].add((atom,None))\n\n    def pop_first(self):\n        for i,s in enumerate(self.sets):\n            if s:\n                if i in [Categories.N_EQ, Categories.ALL]:\n                    for ex in s:\n                        try:\n                            if not ex[0]._exhausted:\n                                s.remove(ex)\n                                return (ex, i)\n                        except AttributeError:\n                            s.remove(ex)\n                            return (ex, i)\n                else:\n                    return (s.pop(), i)\n        return ((None, None), None)\n\n    def replace_all(self, old, new):\n        for s in self.sets:\n            for ex,ctx in s:\n                ex.replace(old.variable, new)\n                if ctx is not None:\n                    ctx.replace(old.variable, new)\n\n    def mark_alls_fresh(self):\n        for u,_ in self.sets[Categories.ALL]:\n            u._exhausted = False\n\n    def mark_neqs_fresh(self):\n        for neq,_ in self.sets[Categories.N_EQ]:\n            neq._exhausted = False\n\n    def _categorize_expression(self, current):\n        if isinstance(current, NegatedExpression):\n            return self._categorize_NegatedExpression(current)\n        elif isinstance(current, FunctionVariableExpression):\n            return Categories.PROP\n        elif TableauProver.is_atom(current):\n            return Categories.ATOM\n        elif isinstance(current, AllExpression):\n            return Categories.ALL\n        elif isinstance(current, AndExpression):\n            return Categories.AND\n        elif isinstance(current, OrExpression):\n            return Categories.OR\n        elif isinstance(current, ImpExpression):\n            return Categories.IMP\n        elif isinstance(current, IffExpression):\n            return Categories.IFF\n        elif isinstance(current, EqualityExpression):\n            return Categories.EQ\n        elif isinstance(current, ExistsExpression):\n            return Categories.EXISTS\n        elif isinstance(current, ApplicationExpression):\n            return Categories.APP\n        else:\n            raise ProverParseError(\"cannot categorize %s\" % \\\n                                   current.__class__.__name__)\n\n    def _categorize_NegatedExpression(self, current):\n        negated = current.term\n\n        if isinstance(negated, NegatedExpression):\n            return Categories.D_NEG\n        elif isinstance(negated, FunctionVariableExpression):\n            return Categories.N_PROP\n        elif TableauProver.is_atom(negated):\n            return Categories.N_ATOM\n        elif isinstance(negated, AllExpression):\n            return Categories.N_ALL\n        elif isinstance(negated, AndExpression):\n            return Categories.N_AND\n        elif isinstance(negated, OrExpression):\n            return Categories.N_OR\n        elif isinstance(negated, ImpExpression):\n            return Categories.N_IMP\n        elif isinstance(negated, IffExpression):\n            return Categories.N_IFF\n        elif isinstance(negated, EqualityExpression):\n            return Categories.N_EQ\n        elif isinstance(negated, ExistsExpression):\n            return Categories.N_EXISTS\n        elif isinstance(negated, ApplicationExpression):\n            return Categories.N_APP\n        else:\n            raise ProverParseError(\"cannot categorize %s\" % \\\n                                   negated.__class__.__name__)\n\n\nclass Debug(object):\n    def __init__(self, verbose, indent=0, lines=None):\n        self.verbose = verbose\n        self.indent = indent\n\n        if not lines: lines = []\n        self.lines = lines\n\n    def __add__(self, increment):\n        return Debug(self.verbose, self.indent+1, self.lines)\n\n    def line(self, data, indent=0):\n        if isinstance(data, tuple):\n            ex, ctx = data\n            if ctx:\n                data = '%s, %s' % (ex, ctx)\n            else:\n                data = '%s' % ex\n\n            if isinstance(ex, AllExpression):\n                try:\n                    used_vars = \"[%s]\" % (\",\".join(\"%s\" % ve.variable.name for ve in ex._used_vars))\n                    data += ':   %s' % used_vars\n                except AttributeError:\n                    data += ':   []'\n\n        newline = '%s%s' % ('   '*(self.indent+indent), data)\n        self.lines.append(newline)\n\n        if self.verbose:\n            print(newline)\n\n\nclass Categories(object):\n    ATOM     = 0\n    PROP     = 1\n    N_ATOM   = 2\n    N_PROP   = 3\n    APP      = 4\n    N_APP    = 5\n    N_EQ     = 6\n    D_NEG    = 7\n    N_ALL    = 8\n    N_EXISTS = 9\n    AND      = 10\n    N_OR     = 11\n    N_IMP    = 12\n    OR       = 13\n    IMP      = 14\n    N_AND    = 15\n    IFF      = 16\n    N_IFF    = 17\n    EQ       = 18\n    EXISTS   = 19\n    ALL      = 20\n\n\ndef testTableauProver():\n    tableau_test('P | -P')\n    tableau_test('P & -P')\n    tableau_test('Q', ['P', '(P -> Q)'])\n    tableau_test('man(x)')\n    tableau_test('(man(x) -> man(x))')\n    tableau_test('(man(x) -> --man(x))')\n    tableau_test('-(man(x) and -man(x))')\n    tableau_test('(man(x) or -man(x))')\n    tableau_test('(man(x) -> man(x))')\n    tableau_test('-(man(x) and -man(x))')\n    tableau_test('(man(x) or -man(x))')\n    tableau_test('(man(x) -> man(x))')\n    tableau_test('(man(x) iff man(x))')\n    tableau_test('-(man(x) iff -man(x))')\n    tableau_test('all x.man(x)')\n    tableau_test('all x.all y.((x = y) -> (y = x))')\n    tableau_test('all x.all y.all z.(((x = y) & (y = z)) -> (x = z))')\n\n    p1 = 'all x.(man(x) -> mortal(x))'\n    p2 = 'man(Socrates)'\n    c = 'mortal(Socrates)'\n    tableau_test(c, [p1, p2])\n\n    p1 = 'all x.(man(x) -> walks(x))'\n    p2 = 'man(John)'\n    c = 'some y.walks(y)'\n    tableau_test(c, [p1, p2])\n\n    p = '((x = y) & walks(y))'\n    c = 'walks(x)'\n    tableau_test(c, [p])\n\n    p = '((x = y) & ((y = z) & (z = w)))'\n    c = '(x = w)'\n    tableau_test(c, [p])\n\n    p = 'some e1.some e2.(believe(e1,john,e2) & walk(e2,mary))'\n    c = 'some e0.walk(e0,mary)'\n    tableau_test(c, [p])\n\n    c = '(exists x.exists z3.((x = Mary) & ((z3 = John) & sees(z3,x))) <-> exists x.exists z4.((x = John) & ((z4 = Mary) & sees(x,z4))))'\n    tableau_test(c)\n\n\n\ndef testHigherOrderTableauProver():\n    tableau_test('believe(j, -lie(b))', ['believe(j, -lie(b) & -cheat(b))'])\n    tableau_test('believe(j, lie(b) & cheat(b))', ['believe(j, lie(b))'])\n    tableau_test('believe(j, lie(b))', ['lie(b)']) #how do we capture that John believes all things that are true\n    tableau_test('believe(j, know(b, cheat(b)))', ['believe(j, know(b, lie(b)) & know(b, steals(b) & cheat(b)))'])\n    tableau_test('P(Q(y), R(y) & R(z))', ['P(Q(x) & Q(y), R(y) & R(z))'])\n\n    tableau_test('believe(j, cheat(b) & lie(b))', ['believe(j, lie(b) & cheat(b))'])\n    tableau_test('believe(j, -cheat(b) & -lie(b))', ['believe(j, -lie(b) & -cheat(b))'])\n\n\ndef tableau_test(c, ps=None, verbose=False):\n    pc = Expression.fromstring(c)\n    pps = ([Expression.fromstring(p) for p in ps] if ps else [])\n    if not ps:\n        ps = []\n    print('%s |- %s: %s' % (', '.join(ps), pc, TableauProver().prove(pc, pps, verbose=verbose)))\n\ndef demo():\n    testTableauProver()\n    testHigherOrderTableauProver()\n\nif __name__ == '__main__':\n    demo()\n\n"], "nltk\\inference\\__init__": [".py", "\n\nfrom nltk.inference.api import ParallelProverBuilder, ParallelProverBuilderCommand\nfrom nltk.inference.mace import Mace, MaceCommand\nfrom nltk.inference.prover9 import Prover9, Prover9Command\nfrom nltk.inference.resolution import ResolutionProver, ResolutionProverCommand\nfrom nltk.inference.tableau import TableauProver, TableauProverCommand\nfrom nltk.inference.discourse import (ReadingCommand, CfgReadingCommand,\n                       DrtGlueReadingCommand, DiscourseTester)\n", 1], "nltk\\internals": [".py", "from __future__ import print_function\n\nimport subprocess\nimport os\nimport fnmatch\nimport re\nimport warnings\nimport textwrap\nimport types\nimport sys\nimport stat\nimport locale\n\ntry:\n    from xml.etree import cElementTree as ElementTree\nexcept ImportError:\n    from xml.etree import ElementTree\n\nfrom six import string_types\n\nfrom nltk import __file__\nfrom nltk import compat\n\n\n_java_bin = None\n_java_options = []\ndef config_java(bin=None, options=None, verbose=False):\n    global _java_bin, _java_options\n    _java_bin = find_binary('java', bin, env_vars=['JAVAHOME', 'JAVA_HOME'], verbose=verbose, binary_names=['java.exe'])\n\n    if options is not None:\n        if isinstance(options, string_types):\n            options = options.split()\n        _java_options = list(options)\n\ndef java(cmd, classpath=None, stdin=None, stdout=None, stderr=None,\n         blocking=True):\n    if stdin == 'pipe': stdin = subprocess.PIPE\n    if stdout == 'pipe': stdout = subprocess.PIPE\n    if stderr == 'pipe': stderr = subprocess.PIPE\n    if isinstance(cmd, string_types):\n        raise TypeError('cmd should be a list of strings')\n\n    if _java_bin is None:\n        config_java()\n\n    if isinstance(classpath, string_types):\n        classpaths=[classpath]\n    else:\n        classpaths=list(classpath)\n    classpath=os.path.pathsep.join(classpaths)\n\n    cmd = list(cmd)\n    cmd = ['-cp', classpath] + cmd\n    cmd = [_java_bin] + _java_options + cmd\n\n    p = subprocess.Popen(cmd, stdin=stdin, stdout=stdout, stderr=stderr)\n    if not blocking: return p\n    (stdout, stderr) = p.communicate()\n\n    if p.returncode != 0:\n        print(_decode_stdoutdata(stderr))\n        raise OSError('Java command failed : ' + str(cmd))\n\n    return (stdout, stderr)\n\nif 0:\n    (a,b) = java(['weka.classifiers.bayes.NaiveBayes',\n                  '-l', '/tmp/names.model', '-T', '/tmp/test.arff',\n                  '-p', '0'],#, '-distribution'],\n                 classpath='/Users/edloper/Desktop/weka/weka.jar')\n\n\n\nclass ReadError(ValueError):\n    def __init__(self, expected, position):\n        ValueError.__init__(self, expected, position)\n        self.expected = expected\n        self.position = position\n    def __str__(self):\n        return 'Expected %s at %s' % (self.expected, self.position)\n\n_STRING_START_RE = re.compile(r\"[uU]?[rR]?(\\\"\\\"\\\"|\\'\\'\\'|\\\"|\\')\")\ndef read_str(s, start_position):\n    m = _STRING_START_RE.match(s, start_position)\n    if not m: raise ReadError('open quote', start_position)\n    quotemark = m.group(1)\n\n    _STRING_END_RE = re.compile(r'\\\\|%s' % quotemark)\n    position = m.end()\n    while True:\n        match = _STRING_END_RE.search(s, position)\n        if not match: raise ReadError('close quote', position)\n        if match.group(0) == '\\\\': position = match.end()+1\n        else: break\n\n    try:\n        return eval(s[start_position:match.end()]), match.end()\n    except ValueError as e:\n        raise ReadError('invalid string (%s)' % e)\n\n_READ_INT_RE = re.compile(r'-?\\d+')\ndef read_int(s, start_position):\n    m = _READ_INT_RE.match(s, start_position)\n    if not m: raise ReadError('integer', start_position)\n    return int(m.group()), m.end()\n\n_READ_NUMBER_VALUE = re.compile(r'-?(\\d*)([.]?\\d*)?')\ndef read_number(s, start_position):\n    m = _READ_NUMBER_VALUE.match(s, start_position)\n    if not m or not (m.group(1) or m.group(2)):\n        raise ReadError('number', start_position)\n    if m.group(2): return float(m.group()), m.end()\n    else: return int(m.group()), m.end()\n\n\n\n\ndef overridden(method):\n    if isinstance(method, types.MethodType) and compat.get_im_class(method) is not None:\n        name = method.__name__\n        funcs = [cls.__dict__[name]\n                 for cls in _mro(compat.get_im_class(method))\n                 if name in cls.__dict__]\n        return len(funcs) > 1\n    else:\n        raise TypeError('Expected an instance method.')\n\ndef _mro(cls):\n    if isinstance(cls, type):\n        return cls.__mro__\n    else:\n        mro = [cls]\n        for base in cls.__bases__: mro.extend(_mro(base))\n        return mro\n\n\ndef _add_epytext_field(obj, field, message):\n    indent = ''\n    if obj.__doc__:\n        obj.__doc__ = obj.__doc__.rstrip()+'\\n\\n'\n        indents = re.findall(r'(?<=\\n)[ ]+(?!\\s)', obj.__doc__.expandtabs())\n        if indents: indent = min(indents)\n    else:\n        obj.__doc__ = ''\n\n    obj.__doc__ += textwrap.fill('@%s: %s' % (field, message),\n                                 initial_indent=indent,\n                                 subsequent_indent=indent+'    ')\n\ndef deprecated(message):\n\n    def decorator(func):\n        msg = (\"Function %s() has been deprecated.  %s\"\n               % (func.__name__, message))\n        msg = '\\n' + textwrap.fill(msg, initial_indent='  ',\n                                   subsequent_indent='  ')\n        def newFunc(*args, **kwargs):\n            warnings.warn(msg, category=DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n\n        newFunc.__dict__.update(func.__dict__)\n        newFunc.__name__ = func.__name__\n        newFunc.__doc__ = func.__doc__\n        newFunc.__deprecated__ = True\n        _add_epytext_field(newFunc, 'deprecated', message)\n        return newFunc\n    return decorator\n\nclass Deprecated(object):\n    def __new__(cls, *args, **kwargs):\n        dep_cls = None\n        for base in _mro(cls):\n            if Deprecated in base.__bases__:\n                dep_cls = base; break\n        assert dep_cls, 'Unable to determine which base is deprecated.'\n\n        doc = dep_cls.__doc__ or ''.strip()\n        doc = re.sub(r'\\A\\s*@deprecated:', r'', doc)\n        doc = re.sub(r'(?m)^\\s*', '', doc)\n        name = 'Class %s' % dep_cls.__name__\n        if cls != dep_cls:\n            name += ' (base class for %s)' % cls.__name__\n        msg = '%s has been deprecated.  %s' % (name, doc)\n        msg = '\\n' + textwrap.fill(msg, initial_indent='    ',\n                                   subsequent_indent='    ')\n        warnings.warn(msg, category=DeprecationWarning, stacklevel=2)\n        return object.__new__(cls)\n\n\nclass Counter:\n    def __init__(self, initial_value=0):\n        self._value = initial_value\n    def get(self):\n        self._value += 1\n        return self._value\n\n\ndef find_file_iter(filename, env_vars=(), searchpath=(),\n    file_names=None, url=None, verbose=False, finding_dir=False):\n    file_names = [filename] + (file_names or [])\n    assert isinstance(filename, string_types)\n    assert not isinstance(file_names, string_types)\n    assert not isinstance(searchpath, string_types)\n    if isinstance(env_vars, string_types):\n        env_vars = env_vars.split()\n    yielded = False\n\n    for alternative in file_names:\n        path_to_file = os.path.join(filename, alternative)\n        if os.path.isfile(path_to_file):\n            if verbose:\n                print('[Found %s: %s]' % (filename, path_to_file))\n            yielded = True\n            yield path_to_file\n        if os.path.isfile(alternative):\n            if verbose:\n                print('[Found %s: %s]' % (filename, alternative))\n            yielded = True\n            yield alternative\n        path_to_file = os.path.join(filename, 'file', alternative)\n        if os.path.isfile(path_to_file):\n            if verbose:\n                print('[Found %s: %s]' % (filename, path_to_file))\n            yielded = True\n            yield path_to_file\n\n    for env_var in env_vars:\n        if env_var in os.environ:\n            if finding_dir: # This is to file a directory instead of file\n                yielded = True\n                yield os.environ[env_var]\n\n            for env_dir in os.environ[env_var].split(os.pathsep):\n                if os.path.isfile(env_dir):\n                    if verbose:\n                        print('[Found %s: %s]'%(filename, env_dir))\n                    yielded = True\n                    yield env_dir\n                for alternative in file_names:\n                    path_to_file = os.path.join(env_dir, alternative)\n                    if os.path.isfile(path_to_file):\n                        if verbose:\n                            print('[Found %s: %s]'%(filename, path_to_file))\n                        yielded = True\n                        yield path_to_file\n\n                    path_to_file = os.path.join(env_dir, 'bin', alternative)\n\n                    if os.path.isfile(path_to_file):\n                        if verbose:\n                            print('[Found %s: %s]' % (filename, path_to_file))\n                        yielded = True\n                        yield path_to_file\n\n    for directory in searchpath:\n        for alternative in file_names:\n            path_to_file = os.path.join(directory, alternative)\n            if os.path.isfile(path_to_file):\n                yielded = True\n                yield path_to_file\n\n    if os.name == 'posix':\n        for alternative in file_names:\n            try:\n                p = subprocess.Popen(['which', alternative],\n                        stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n                stdout, stderr = p.communicate()\n                path = _decode_stdoutdata(stdout).strip()\n                if path.endswith(alternative) and os.path.exists(path):\n                    if verbose:\n                        print('[Found %s: %s]' % (filename, path))\n                    yielded = True\n                    yield path\n            except (KeyboardInterrupt, SystemExit, OSError):\n                raise\n            except:\n                pass\n\n    if not yielded:\n        msg = (\"NLTK was unable to find the %s file!\" \"\\nUse software specific \"\n               \"configuration paramaters\" % filename)\n        if env_vars: msg += ' or set the %s environment variable' % env_vars[0]\n        msg += '.'\n        if searchpath:\n            msg += '\\n\\n  Searched in:'\n            msg += ''.join('\\n    - %s' % d for d in searchpath)\n        if url: msg += ('\\n\\n  For more information on %s, see:\\n    <%s>' %\n                        (filename, url))\n        div = '='*75\n        raise LookupError('\\n\\n%s\\n%s\\n%s' % (div, msg, div))\n\n\ndef find_file(filename, env_vars=(), searchpath=(),\n        file_names=None, url=None, verbose=False):\n    return next(find_file_iter(filename, env_vars, searchpath,\n                               file_names, url, verbose))\n\n\ndef find_dir(filename, env_vars=(), searchpath=(),\n        file_names=None, url=None, verbose=False):\n    return next(find_file_iter(filename, env_vars, searchpath,\n                               file_names, url, verbose, finding_dir=True))\n\n\ndef find_binary_iter(name, path_to_bin=None, env_vars=(), searchpath=(),\n                binary_names=None, url=None, verbose=False):\n    for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,\n                     url, verbose):\n        yield file\n\ndef find_binary(name, path_to_bin=None, env_vars=(), searchpath=(),\n                binary_names=None, url=None, verbose=False):\n    return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,\n                                 binary_names, url, verbose))\n\ndef find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),\n        searchpath=(), url=None, verbose=False, is_regex=False):\n\n    assert isinstance(name_pattern, string_types)\n    assert not isinstance(searchpath, string_types)\n    if isinstance(env_vars, string_types):\n        env_vars = env_vars.split()\n    yielded = False\n\n    env_vars = ['CLASSPATH'] + list(env_vars)\n\n    if path_to_jar is not None:\n        if os.path.isfile(path_to_jar):\n            yielded = True\n            yield path_to_jar\n        else:\n            raise LookupError('Could not find %s jar file at %s' %\n                            (name_pattern, path_to_jar))\n\n    for env_var in env_vars:\n        if env_var in os.environ:\n            if env_var == 'CLASSPATH':\n                classpath = os.environ['CLASSPATH']\n                for cp in classpath.split(os.path.pathsep):\n                    if os.path.isfile(cp):\n                        filename=os.path.basename(cp)\n                        if is_regex and re.match(name_pattern, filename) or \\\n                                (not is_regex and filename == name_pattern):\n                            if verbose:\n                                print('[Found %s: %s]' % (name_pattern, cp))\n                            yielded = True\n                            yield cp\n                    if os.path.isdir(cp):\n                        if not is_regex:\n                            if os.path.isfile(os.path.join(cp,name_pattern)):\n                                if verbose:\n                                    print('[Found %s: %s]' % (name_pattern, cp))\n                                yielded = True\n                                yield os.path.join(cp,name_pattern)\n                        else:\n                            for file_name in os.listdir(cp):\n                                if re.match(name_pattern,file_name):\n                                    if verbose:\n                                        print('[Found %s: %s]' % (name_pattern, os.path.join(cp,file_name)))\n                                    yielded = True\n                                    yield os.path.join(cp,file_name)\n\n            else:\n                jar_env = os.environ[env_var]\n                jar_iter = ((os.path.join(jar_env, path_to_jar) for path_to_jar in os.listdir(jar_env))\n                            if os.path.isdir(jar_env) else (jar_env,))\n                for path_to_jar in jar_iter:\n                    if os.path.isfile(path_to_jar):\n                        filename=os.path.basename(path_to_jar)\n                        if is_regex and re.match(name_pattern, filename) or \\\n                                (not is_regex and filename == name_pattern):\n                            if verbose:\n                                print('[Found %s: %s]' % (name_pattern, path_to_jar))\n                            yielded = True\n                            yield path_to_jar\n\n    for directory in searchpath:\n        if is_regex:\n            for filename in os.listdir(directory):\n                path_to_jar = os.path.join(directory, filename)\n                if os.path.isfile(path_to_jar):\n                    if re.match(name_pattern, filename):\n                        if verbose:\n                            print('[Found %s: %s]' % (filename, path_to_jar))\n                yielded = True\n                yield path_to_jar\n        else:\n            path_to_jar = os.path.join(directory, name_pattern)\n            if os.path.isfile(path_to_jar):\n                if verbose:\n                    print('[Found %s: %s]' % (name_pattern, path_to_jar))\n                yielded = True\n                yield path_to_jar\n\n    if not yielded:\n        msg = (\"NLTK was unable to find %s!\" % name_pattern)\n        if env_vars: msg += ' Set the %s environment variable' % env_vars[0]\n        msg = textwrap.fill(msg+'.', initial_indent='  ',\n                            subsequent_indent='  ')\n        if searchpath:\n            msg += '\\n\\n  Searched in:'\n            msg += ''.join('\\n    - %s' % d for d in searchpath)\n        if url:\n            msg += ('\\n\\n  For more information, on %s, see:\\n    <%s>' %\n                    (name_pattern, url))\n        div = '='*75\n        raise LookupError('\\n\\n%s\\n%s\\n%s' % (div, msg, div))\n\ndef find_jar(name_pattern, path_to_jar=None, env_vars=(),\n        searchpath=(), url=None, verbose=False, is_regex=False):\n    return next(find_jar_iter(name_pattern, path_to_jar, env_vars,\n                         searchpath, url, verbose, is_regex))\n\n\ndef find_jars_within_path(path_to_jars):\n\treturn [os.path.join(root, filename)\n\t\t\tfor root, dirnames, filenames in os.walk(path_to_jars)\n\t\t\tfor filename in fnmatch.filter(filenames, '*.jar')]\n\ndef _decode_stdoutdata(stdoutdata):\n    if not isinstance(stdoutdata, bytes):\n        return stdoutdata\n\n    encoding = getattr(sys.__stdout__, \"encoding\", locale.getpreferredencoding())\n    if encoding is None:\n        return stdoutdata.decode()\n    return stdoutdata.decode(encoding)\n\n\ndef import_from_stdlib(module):\n    old_path = sys.path\n    sys.path = [d for d in sys.path if d not in ('', '.')]\n    m = __import__(module)\n    sys.path = old_path\n    return m\n\n\n\n@compat.python_2_unicode_compatible\nclass ElementWrapper(object):\n\n    def __new__(cls, etree):\n        if isinstance(etree, ElementWrapper):\n            return etree\n        else:\n            return object.__new__(ElementWrapper)\n\n    def __init__(self, etree):\n        r\"\"\"\n        Initialize a new Element wrapper for ``etree``.\n\n        If ``etree`` is a string, then it will be converted to an\n        Element object using ``ElementTree.fromstring()`` first:\n\n            >>> ElementWrapper(\"<test></test>\")\n            <Element \"<?xml version='1.0' encoding='utf8'?>\\n<test />\">\n\n        \"\"\"\n        if isinstance(etree, string_types):\n            etree = ElementTree.fromstring(etree)\n        self.__dict__['_etree'] = etree\n\n    def unwrap(self):\n        \"\"\"\n        Return the Element object wrapped by this wrapper.\n        \"\"\"\n        return self._etree\n\n\n    def __repr__(self):\n        s = ElementTree.tostring(self._etree, encoding='utf8').decode('utf8')\n        if len(s) > 60:\n            e = s.rfind('<')\n            if (len(s)-e) > 30: e = -20\n            s = '%s...%s' % (s[:30], s[e:])\n        return '<Element %r>' % s\n\n    def __str__(self):\n        \"\"\"\n        :return: the result of applying ``ElementTree.tostring()`` to\n        the wrapped Element object.\n        \"\"\"\n        return ElementTree.tostring(self._etree, encoding='utf8').decode('utf8').rstrip()\n\n\n    def __getattr__(self, attrib):\n        return getattr(self._etree, attrib)\n\n    def __setattr__(self, attr, value):\n        return setattr(self._etree, attr, value)\n\n    def __delattr__(self, attr):\n        return delattr(self._etree, attr)\n\n    def __setitem__(self, index, element):\n        self._etree[index] = element\n\n    def __delitem__(self, index):\n        del self._etree[index]\n\n    def __setslice__(self, start, stop, elements):\n        self._etree[start:stop] = elements\n\n    def __delslice__(self, start, stop):\n        del self._etree[start:stop]\n\n    def __len__(self):\n        return len(self._etree)\n\n\n    def __getitem__(self, index):\n        return ElementWrapper(self._etree[index])\n\n    def __getslice__(self, start, stop):\n        return [ElementWrapper(elt) for elt in self._etree[start:stop]]\n\n    def getchildren(self):\n        return [ElementWrapper(elt) for elt in self._etree]\n\n    def getiterator(self, tag=None):\n        return (ElementWrapper(elt)\n                for elt in self._etree.getiterator(tag))\n\n    def makeelement(self, tag, attrib):\n        return ElementWrapper(self._etree.makeelement(tag, attrib))\n\n    def find(self, path):\n        elt = self._etree.find(path)\n        if elt is None: return elt\n        else: return ElementWrapper(elt)\n\n    def findall(self, path):\n        return [ElementWrapper(elt) for elt in self._etree.findall(path)]\n\n\ndef slice_bounds(sequence, slice_obj, allow_step=False):\n    \"\"\"\n    Given a slice, return the corresponding (start, stop) bounds,\n    taking into account None indices and negative indices.  The\n    following guarantees are made for the returned start and stop values:\n\n      - 0 <= start <= len(sequence)\n      - 0 <= stop <= len(sequence)\n      - start <= stop\n\n    :raise ValueError: If ``slice_obj.step`` is not None.\n    :param allow_step: If true, then the slice object may have a\n        non-None step.  If it does, then return a tuple\n        (start, stop, step).\n    \"\"\"\n    start, stop = (slice_obj.start, slice_obj.stop)\n\n    if allow_step:\n        step = slice_obj.step\n        if step is None: step = 1\n        if step < 0:\n            start, stop = slice_bounds(sequence, slice(stop, start))\n        else:\n            start, stop = slice_bounds(sequence, slice(start, stop))\n        return start, stop, step\n\n    elif slice_obj.step not in (None, 1):\n        raise ValueError('slices with steps are not supported by %s' %\n                         sequence.__class__.__name__)\n\n    if start is None: start = 0\n    if stop is None: stop = len(sequence)\n\n    if start < 0: start = max(0, len(sequence)+start)\n    if stop < 0: stop = max(0, len(sequence)+stop)\n\n    if stop > 0:\n        try: sequence[stop-1]\n        except IndexError: stop = len(sequence)\n\n    start = min(start, stop)\n\n    return start, stop\n\n\ndef is_writable(path):\n    if not os.path.exists(path):\n        return False\n\n    if hasattr(os, 'getuid'):\n        statdata = os.stat(path)\n        perm = stat.S_IMODE(statdata.st_mode)\n        if (perm & 0o002):\n            return True\n        elif statdata.st_uid == os.getuid() and (perm & 0o200):\n            return True\n        elif (statdata.st_gid in [os.getgid()] + os.getgroups()) \\\n            and (perm & 0o020):\n            return True\n        else:\n            return False\n\n    return True\n\n\ndef raise_unorderable_types(ordering, a, b):\n    raise TypeError(\"unorderable types: %s() %s %s()\" % (type(a).__name__, ordering, type(b).__name__))\n"], "nltk\\jsontags": [".py", "\n\nimport json\n\njson_tags = {}\n\nTAG_PREFIX = '!'\n\ndef register_tag(cls):\n    json_tags[TAG_PREFIX+getattr(cls, 'json_tag')] = cls\n    return cls\n\nclass JSONTaggedEncoder(json.JSONEncoder):\n    def default(self, obj):\n        obj_tag = getattr(obj, 'json_tag', None)\n        if obj_tag is None:\n            return super(JSONTaggedEncoder, self).default(obj)\n        obj_tag = TAG_PREFIX + obj_tag\n        obj = obj.encode_json_obj()\n        return {obj_tag: obj}\n\nclass JSONTaggedDecoder(json.JSONDecoder):\n    def decode(self, s):\n        return self.decode_obj(super(JSONTaggedDecoder, self).decode(s))\n\n    @classmethod\n    def decode_obj(cls, obj):\n        if isinstance(obj, dict):\n            obj = dict((key, cls.decode_obj(val)) for (key, val) in obj.items())\n        elif isinstance(obj, list):\n            obj = list(cls.decode_obj(val) for val in obj)\n        if not isinstance(obj, dict) or len(obj) != 1:\n            return obj\n        obj_tag = next(iter(obj.keys()))\n        if not obj_tag.startswith('!'):\n            return obj\n        if obj_tag not in json_tags:\n            raise ValueError('Unknown tag', obj_tag)\n        obj_cls = json_tags[obj_tag]\n        return obj_cls.decode_json_obj(obj[obj_tag])\n\n__all__ = ['register_tag', 'json_tags',\n           'JSONTaggedEncoder', 'JSONTaggedDecoder']\n"], "nltk\\lazyimport": [".py", "\nfrom __future__ import print_function\n\n\n_debug = 0\n\n\nclass LazyModule:\n\n    __lazymodule_init = 0\n\n    __lazymodule_name = ''\n\n    __lazymodule_loaded = 0\n\n    __lazymodule_locals = None\n\n    __lazymodule_globals = None\n\n    def __init__(self, name, locals, globals=None):\n\n        self.__lazymodule_locals = locals\n        if globals is None:\n            globals = locals\n        self.__lazymodule_globals = globals\n        mainname = globals.get('__name__', '')\n        if mainname:\n            self.__name__ = mainname + '.' + name\n            self.__lazymodule_name = name\n        else:\n            self.__name__ = self.__lazymodule_name = name\n        self.__lazymodule_init = 1\n\n    def __lazymodule_import(self):\n\n        name = self.__lazymodule_name\n        if self.__lazymodule_loaded:\n            return self.__lazymodule_locals[name]\n        if _debug:\n            print('LazyModule: Loading module %r' % name)\n        self.__lazymodule_locals[name] \\\n             = module \\\n             = __import__(name,\n                          self.__lazymodule_locals,\n                          self.__lazymodule_globals,\n                          '*')\n\n        self.__dict__.update(module.__dict__)\n\n        self.__dict__['__lazymodule_loaded'] = 1\n\n        if _debug:\n            print('LazyModule: Module %r loaded' % name)\n        return module\n\n    def __getattr__(self, name):\n\n        if self.__lazymodule_loaded:\n            raise AttributeError(name)\n        if _debug:\n            print('LazyModule: ' \\\n                  'Module load triggered by attribute %r read access' % name)\n        module = self.__lazymodule_import()\n        return getattr(module, name)\n\n    def __setattr__(self, name, value):\n\n        if not self.__lazymodule_init:\n            self.__dict__[name] = value\n            return\n        if self.__lazymodule_loaded:\n            self.__lazymodule_locals[self.__lazymodule_name] = value\n            self.__dict__[name] = value\n            return\n        if _debug:\n            print('LazyModule: ' \\\n                  'Module load triggered by attribute %r write access' % name)\n        module = self.__lazymodule_import()\n        setattr(module, name, value)\n\n    def __repr__(self):\n        return \"<LazyModule '%s'>\" % self.__name__\n"], "nltk\\metrics\\agreement": [".py", "\nfrom __future__ import print_function, unicode_literals, division\n\nimport logging\nfrom itertools import groupby\nfrom operator import itemgetter\n\nfrom six import iteritems\n\nfrom nltk.probability import FreqDist, ConditionalFreqDist\nfrom nltk.internals import deprecated\nfrom nltk.compat import python_2_unicode_compatible\n\nfrom nltk.metrics.distance import binary_distance\n\nlog = logging.getLogger(__file__)\n\n@python_2_unicode_compatible\nclass AnnotationTask(object):\n\n    def __init__(self, data=None, distance=binary_distance):\n        self.distance = distance\n        self.I = set()\n        self.K = set()\n        self.C = set()\n        self.data = []\n        if data is not None:\n            self.load_array(data)\n\n    def __str__(self):\n        return \"\\r\\n\".join(map(lambda x:\"%s\\t%s\\t%s\" %\n                               (x['coder'], x['item'].replace('_', \"\\t\"),\n                                \",\".join(x['labels'])), self.data))\n\n    def load_array(self, array):\n        for coder, item, labels in array:\n            self.C.add(coder)\n            self.K.add(labels)\n            self.I.add(item)\n            self.data.append({'coder':coder, 'labels':labels, 'item':item})\n\n    def agr(self, cA, cB, i, data=None):\n        data = data or self.data\n        k1 = next((x for x in data if x['coder'] in (cA,cB) and x['item']==i))\n        if k1['coder'] == cA:\n            k2 = next((x for x in data if x['coder']==cB and x['item']==i))\n        else:\n            k2 = next((x for x in data if x['coder']==cA and x['item']==i))\n\n        ret = 1.0 - float(self.distance(k1['labels'], k2['labels']))\n        log.debug(\"Observed agreement between %s and %s on %s: %f\",\n                      cA, cB, i, ret)\n        log.debug(\"Distance between \\\"%r\\\" and \\\"%r\\\": %f\",\n                      k1['labels'], k2['labels'], 1.0 - ret)\n        return ret\n\n    def Nk(self, k):\n        return float(sum(1 for x in self.data if x['labels'] == k))\n\n    def Nik(self, i, k):\n        return float(sum(1 for x in self.data if x['item'] == i and x['labels'] == k))\n\n    def Nck(self, c, k):\n        return float(sum(1 for x in self.data if x['coder'] == c and x['labels'] == k))\n\n    @deprecated('Use Nk, Nik or Nck instead')\n    def N(self, k=None, i=None, c=None):\n        if k is not None and i is None and c is None:\n            ret = self.Nk(k)\n        elif k is not None and i is not None and c is None:\n            ret = self.Nik(i, k)\n        elif k is not None and c is not None and i is None:\n            ret = self.Nck(c, k)\n        else:\n            raise ValueError(\"You must pass either i or c, not both! (k=%r,i=%r,c=%r)\" % (k, i, c))\n        log.debug(\"Count on N[%s,%s,%s]: %d\", k, i, c, ret)\n        return ret\n\n    def _grouped_data(self, field, data=None):\n        data = data or self.data\n        return groupby(sorted(data, key=itemgetter(field)), itemgetter(field))\n\n    def Ao(self, cA, cB):\n        data = self._grouped_data('item', (x for x in self.data if x['coder'] in (cA, cB)))\n        ret = sum(self.agr(cA, cB, item, item_data) for item, item_data in data) / len(self.I)\n        log.debug(\"Observed agreement between %s and %s: %f\", cA, cB, ret)\n        return ret\n\n    def _pairwise_average(self, function):\n        total = 0\n        n = 0\n        s = self.C.copy()\n        for cA in self.C:\n            s.remove(cA)\n            for cB in s:\n                total += function(cA, cB)\n                n += 1\n        ret = total / n\n        return ret\n\n    def avg_Ao(self):\n        ret = self._pairwise_average(self.Ao)\n        log.debug(\"Average observed agreement: %f\", ret)\n        return ret\n\n    def Do_alpha(self):\n        total = 0.0\n        for i, itemdata in self._grouped_data('item'):\n            label_freqs = FreqDist(x['labels'] for x in itemdata)\n\n            for j, nj in iteritems(label_freqs):\n                for l, nl in iteritems(label_freqs):\n                    total += float(nj * nl) * self.distance(l, j)\n        ret = (1.0 / (len(self.I) * len(self.C) * (len(self.C) - 1))) * total\n        log.debug(\"Observed disagreement: %f\", ret)\n        return ret\n\n    def Do_Kw_pairwise(self,cA,cB,max_distance=1.0):\n        total = 0.0\n        data = (x for x in self.data if x['coder'] in (cA, cB))\n        for i, itemdata in self._grouped_data('item', data):\n            total += self.distance(next(itemdata)['labels'],\n                                   next(itemdata)['labels'])\n\n        ret = total / (len(self.I) * max_distance)\n        log.debug(\"Observed disagreement between %s and %s: %f\", cA, cB, ret)\n        return ret\n\n    def Do_Kw(self, max_distance=1.0):\n        ret = self._pairwise_average(lambda cA, cB: self.Do_Kw_pairwise(cA, cB, max_distance))\n        log.debug(\"Observed disagreement: %f\", ret)\n        return ret\n\n    def S(self):\n        Ae = 1.0 / len(self.K)\n        ret = (self.avg_Ao() - Ae) / (1.0 - Ae)\n        return ret\n\n    def pi(self):\n        total = 0.0\n        label_freqs = FreqDist(x['labels'] for x in self.data)\n        for k, f in iteritems(label_freqs):\n            total += f ** 2\n        Ae = total / ((len(self.I) * len(self.C)) ** 2)\n        return (self.avg_Ao() - Ae) / (1 - Ae)\n\n    def Ae_kappa(self, cA, cB):\n        Ae = 0.0\n        nitems = float(len(self.I))\n        label_freqs = ConditionalFreqDist((x['labels'], x['coder']) for x in self.data)\n        for k in label_freqs.conditions():\n            Ae += (label_freqs[k][cA] / nitems) * (label_freqs[k][cB] / nitems)\n        return Ae\n\n    def kappa_pairwise(self, cA, cB):\n        Ae = self.Ae_kappa(cA, cB)\n        ret = (self.Ao(cA, cB) - Ae) / (1.0 - Ae)\n        log.debug(\"Expected agreement between %s and %s: %f\", cA, cB, Ae)\n        return ret\n\n    def kappa(self):\n        return self._pairwise_average(self.kappa_pairwise)\n\n    def multi_kappa(self):\n        Ae = self._pairwise_average(self.Ae_kappa)\n        return (self.avg_Ao() - Ae) / (1.0 - Ae)\n\n    def alpha(self):\n        if len(self.K)==0:\n            raise ValueError(\"Cannot calculate alpha, no data present!\")\n        if len(self.K) == 1:\n            log.debug(\"Only one annotation value, allpha returning 1.\")\n            return 1\n        if len(self.C)==1 and len(self.I) == 1:\n            raise ValueError(\"Cannot calculate alpha, only one coder and item present!\")\n\n        De = 0.0\n\n        label_freqs = FreqDist(x['labels'] for x in self.data)\n        for j in self.K:\n            nj = label_freqs[j]\n            for l in self.K:\n                De += float(nj * label_freqs[l]) * self.distance(j, l)\n        try:\n            De = (1.0 / (len(self.I) * len(self.C) * (len(self.I) * len(self.C) - 1))) * De\n            log.debug(\"Expected disagreement: %f\", De)\n            ret = 1.0 - (self.Do_alpha() / De)\n        except ZeroDivisionError:\n            raise ValueError(\"Cannot calculate alpha, expected disagreement zero, check the distance function!\")\n        return ret\n\n    def weighted_kappa_pairwise(self, cA, cB, max_distance=1.0):\n        total = 0.0\n        label_freqs = ConditionalFreqDist((x['coder'], x['labels'])\n                for x in self.data\n                if x['coder'] in (cA, cB))\n        for j in self.K:\n            for l in self.K:\n                total += label_freqs[cA][j] * label_freqs[cB][l] * self.distance(j, l)\n        De = total / (max_distance * pow(len(self.I), 2))\n        log.debug(\"Expected disagreement between %s and %s: %f\", cA, cB, De)\n        Do = self.Do_Kw_pairwise(cA, cB)\n        ret = 1.0 - (Do / De)\n        return ret\n\n    def weighted_kappa(self, max_distance=1.0):\n        return self._pairwise_average(lambda cA, cB: self.weighted_kappa_pairwise(cA, cB, max_distance))\n\n\nif __name__ == '__main__':\n\n    import re\n    import optparse\n    from nltk.metrics import distance\n\n    parser = optparse.OptionParser()\n    parser.add_option(\"-d\", \"--distance\", dest=\"distance\", default=\"binary_distance\",\n                      help=\"distance metric to use\")\n    parser.add_option(\"-a\", \"--agreement\", dest=\"agreement\", default=\"kappa\",\n                      help=\"agreement coefficient to calculate\")\n    parser.add_option(\"-e\", \"--exclude\", dest=\"exclude\", action=\"append\",\n                      default=[], help=\"coder names to exclude (may be specified multiple times)\")\n    parser.add_option(\"-i\", \"--include\", dest=\"include\", action=\"append\", default=[],\n                      help=\"coder names to include, same format as exclude\")\n    parser.add_option(\"-f\", \"--file\", dest=\"file\",\n                      help=\"file to read labelings from, each line with three columns: 'labeler item labels'\")\n    parser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", default='0',\n                      help=\"how much debugging to print on stderr (0-4)\")\n    parser.add_option(\"-c\", \"--columnsep\", dest=\"columnsep\", default=\"\\t\",\n                      help=\"char/string that separates the three columns in the file, defaults to tab\")\n    parser.add_option(\"-l\", \"--labelsep\", dest=\"labelsep\", default=\",\",\n                      help=\"char/string that separates labels (if labelers can assign more than one), defaults to comma\")\n    parser.add_option(\"-p\", \"--presence\", dest=\"presence\", default=None,\n                      help=\"convert each labeling into 1 or 0, based on presence of LABEL\")\n    parser.add_option(\"-T\", \"--thorough\", dest=\"thorough\", default=False, action=\"store_true\",\n                      help=\"calculate agreement for every subset of the annotators\")\n    (options, remainder) = parser.parse_args()\n\n    if not options.file:\n        parser.print_help()\n        exit()\n\n    logging.basicConfig(level=50 - 10 * int(options.verbose))\n\n    data = []\n    with open(options.file, 'r') as infile:\n        for l in infile:\n            toks = l.split(options.columnsep)\n            coder, object_, labels = toks[0], str(toks[1:-1]), frozenset(toks[-1].strip().split(options.labelsep))\n            if ((options.include == options.exclude) or\n                (len(options.include) > 0 and coder in options.include) or\n                (len(options.exclude) > 0 and coder not in options.exclude)):\n                data.append((coder, object_, labels))\n\n    if options.presence:\n        task = AnnotationTask(data, getattr(distance, options.distance)(options.presence))\n    else:\n        task = AnnotationTask(data, getattr(distance, options.distance))\n\n    if options.thorough:\n        pass\n    else:\n        print(getattr(task, options.agreement)())\n\n    logging.shutdown()\n"], "nltk\\metrics\\aline": [".py", "\n\nfrom __future__ import unicode_literals\n\ntry:\n    import numpy as np\nexcept ImportError:\n    np = None\n\n\ninf = float('inf')\n\nC_skip = 10 # Indels\nC_sub  = 35  # Substitutions\nC_exp  = 45  # Expansions/compressions\nC_vwl  = 5  # Vowel/consonant relative weight (decreased from 10)\n\nconsonants = ['B', 'N', 'R', 'b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm',\n              'n', 'p', 'q', 'r', 's', 't', 'v', 'x', 'z', '\u00e7', '\u00f0', '\u0127',\n              '\u014b', '\u0256', '\u025f', '\u0262', '\u0263', '\u0266', '\u026c', '\u026e', '\u0270', '\u0271', '\u0272', '\u0273', '\u0274',\n              '\u0278', '\u0279', '\u027b', '\u027d', '\u027e', '\u0280', '\u0281', '\u0282', '\u0283', '\u0288', '\u028b', '\u0290 ', '\u0292',\n              '\u0294', '\u0295', '\u0299', '\u029d', '\u03b2', '\u03b8', '\u03c7', '\u0290', 'w']\n\nR_c = ['aspirated', 'lateral', 'manner', 'nasal', 'place', 'retroflex',\n       'syllabic', 'voice']\nR_v = ['back', 'lateral', 'long', 'manner', 'nasal', 'place',\n       'retroflex', 'round', 'syllabic', 'voice']\n\nsimilarity_matrix = {\n   'bilabial': 1.0, 'labiodental': 0.95, 'dental': 0.9,\n   'alveolar': 0.85, 'retroflex': 0.8, 'palato-alveolar': 0.75,\n   'palatal': 0.7, 'velar': 0.6, 'uvular': 0.5, 'pharyngeal': 0.3,\n   'glottal': 0.1, 'labiovelar': 1.0, 'vowel': -1.0, # added 'vowel'\n   'stop': 1.0, 'affricate': 0.9, 'fricative': 0.85, # increased fricative from 0.8\n   'trill': 0.7, 'tap': 0.65, 'approximant': 0.6, 'high vowel': 0.4,\n   'mid vowel': 0.2, 'low vowel': 0.0, 'vowel2': 0.5, # added vowel\n   'high': 1.0, 'mid': 0.5, 'low': 0.0,\n   'front': 1.0, 'central': 0.5, 'back': 0.0,\n   'plus': 1.0, 'minus': 0.0\n}\n\nsalience = {\n   'syllabic': 5,\n   'place': 40,\n   'manner': 50,\n   'voice': 5, # decreased from 10\n   'nasal': 20, # increased from 10\n   'retroflex': 10,\n   'lateral': 10,\n   'aspirated': 5,\n   'long': 0, # decreased from 1\n   'high': 3, # decreased from 5\n   'back': 2, # decreased from 5\n   'round': 2 # decreased from 5\n}\n\nfeature_matrix = {\n'p': {'place': 'bilabial', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'b': {'place': 'bilabial', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n't': {'place': 'alveolar', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'd': {'place': 'alveolar', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0288': {'place': 'retroflex', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'plus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0256': {'place': 'retroflex', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'plus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'c': {'place': 'palatal', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u025f': {'place': 'palatal', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'k': {'place': 'velar', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'g': {'place': 'velar', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'q': {'place': 'uvular', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0262': {'place': 'uvular', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0294': {'place': 'glottal', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'm': {'place': 'bilabial', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'plus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0271': {'place': 'labiodental', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'plus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'n': {'place': 'alveolar', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'plus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0273': {'place': 'retroflex', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'plus', 'retroflex': 'plus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0272': {'place': 'palatal', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'plus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u014b': {'place': 'velar', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'plus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0274': {'place': 'uvular', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'plus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'N': {'place': 'uvular', 'manner': 'stop', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'plus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0299': {'place': 'bilabial', 'manner': 'trill', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'B': {'place': 'bilabial', 'manner': 'trill', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'r': {'place': 'alveolar', 'manner': 'trill', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'plus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0280': {'place': 'uvular', 'manner': 'trill', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'R': {'place': 'uvular', 'manner': 'trill', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u027e': {'place': 'alveolar', 'manner': 'tap', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u027d': {'place': 'retroflex', 'manner': 'tap', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'plus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0278': {'place': 'bilabial', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u03b2': {'place': 'bilabial', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'f': {'place': 'labiodental', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'v': {'place': 'labiodental', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u03b8': {'place': 'dental', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u00f0': {'place': 'dental', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n's': {'place': 'alveolar', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'z': {'place': 'alveolar', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0283': {'place': 'palato-alveolar', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0292': {'place': 'palato-alveolar', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0282': {'place': 'retroflex', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'plus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0290': {'place': 'retroflex', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'plus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u00e7': {'place': 'palatal', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u029d': {'place': 'palatal', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'x': {'place': 'velar', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0263': {'place': 'velar', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u03c7': {'place': 'uvular', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0281': {'place': 'uvular', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0127': {'place': 'pharyngeal', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0295': {'place': 'pharyngeal', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'h': {'place': 'glottal', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0266': {'place': 'glottal', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u026c': {'place': 'alveolar', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'minus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'plus', 'aspirated': 'minus'},\n\n'\u026e': {'place': 'alveolar', 'manner': 'fricative', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'plus', 'aspirated': 'minus'},\n\n'\u028b': {'place': 'labiodental', 'manner': 'approximant', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0279': {'place': 'alveolar', 'manner': 'approximant', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u027b': {'place': 'retroflex', 'manner': 'approximant', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'plus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'j': {'place': 'palatal', 'manner': 'approximant', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'\u0270': {'place': 'velar', 'manner': 'approximant', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n'l': {'place': 'alveolar', 'manner': 'approximant', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'plus', 'aspirated': 'minus'},\n\n'w': {'place': 'labiovelar', 'manner': 'approximant', 'syllabic': 'minus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'aspirated': 'minus'},\n\n\n'i': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'high',\n'back': 'front','round': 'minus', 'long': 'minus', 'aspirated': 'minus'},\n\n'y': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'high',\n'back': 'front','round': 'plus', 'long': 'minus', 'aspirated': 'minus'},\n\n'e': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'mid',\n'back': 'front','round': 'minus', 'long': 'minus', 'aspirated': 'minus'},\n\n'E': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'mid',\n'back': 'front','round': 'minus', 'long': 'plus', 'aspirated': 'minus'},\n\n'\u00f8': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'mid',\n'back': 'front','round': 'plus', 'long': 'minus', 'aspirated': 'minus'},\n\n'\u025b': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'mid',\n'back': 'front','round': 'minus', 'long': 'minus', 'aspirated': 'minus'},\n\n'\u0153': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'mid',\n'back': 'front','round': 'plus', 'long': 'minus', 'aspirated': 'minus'},\n\n'\u00e6': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'low',\n'back': 'front','round': 'minus', 'long': 'minus', 'aspirated': 'minus'},\n\n'a': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'low',\n'back': 'front','round': 'minus', 'long': 'minus', 'aspirated': 'minus'},\n\n'A': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'low',\n'back': 'front','round': 'minus', 'long': 'plus', 'aspirated': 'minus'},\n\n'\u0268': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'high',\n'back': 'central','round': 'minus', 'long': 'minus', 'aspirated': 'minus'},\n\n'\u0289': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'high',\n'back': 'central','round': 'plus', 'long': 'minus', 'aspirated': 'minus'},\n\n'\u0259': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'mid',\n'back': 'central','round': 'minus', 'long': 'minus', 'aspirated': 'minus'},\n\n'u': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'high',\n'back': 'back','round': 'plus', 'long': 'minus', 'aspirated': 'minus'},\n\n'U': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'high',\n'back': 'back','round': 'plus', 'long': 'plus', 'aspirated': 'minus'},\n\n'o': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'mid',\n'back': 'back','round': 'plus', 'long': 'minus', 'aspirated': 'minus'},\n\n'O': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'mid',\n'back': 'back','round': 'plus', 'long': 'plus', 'aspirated': 'minus'},\n\n'\u0254': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'mid',\n'back': 'back','round': 'plus', 'long': 'minus', 'aspirated': 'minus'},\n\n'\u0252': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'low',\n'back': 'back','round': 'minus', 'long': 'minus', 'aspirated': 'minus'},\n\n'I': {'place': 'vowel', 'manner': 'vowel2', 'syllabic': 'plus', 'voice': 'plus',\n'nasal': 'minus', 'retroflex': 'minus', 'lateral': 'minus', 'high': 'high',\n'back': 'front','round': 'minus', 'long': 'plus', 'aspirated': 'minus'},\n\n}\n\n\ndef align(str1, str2, epsilon=0):\n    if np == None:\n      raise ImportError('You need numpy in order to use the align function')\n\n    assert 0.0 <= epsilon <= 1.0, \"Epsilon must be between 0.0 and 1.0.\"\n    m = len(str1)\n    n = len(str2)\n    S = np.zeros((m+1, n+1), dtype=float)\n\n    for i in range(1, m+1):\n        for j in range(1, n+1):\n            edit1 = S[i-1, j] + sigma_skip(str1[i-1])\n            edit2 = S[i, j-1] + sigma_skip(str2[j-1])\n            edit3 = S[i-1, j-1] + sigma_sub(str1[i-1], str2[j-1])\n            if i > 1:\n                edit4 = S[i-2, j-1] + sigma_exp(str2[j-1], str1[i-2:i])\n            else:\n                edit4 = -inf\n            if j > 1:\n                edit5 = S[i-1, j-2] + sigma_exp(str1[i-1], str2[j-2:j])\n            else:\n                edit5 = -inf\n            S[i, j] = max(edit1, edit2, edit3, edit4, edit5, 0)\n\n    T = (1-epsilon)*np.amax(S) # Threshold score for near-optimal alignments\n\n    alignments = []\n    for i in range(1, m+1):\n        for j in range(1, n+1):\n            if S[i,j] >= T:\n                alignments.append(_retrieve(i, j, 0, S, T, str1, str2, []))\n    return alignments\n\ndef _retrieve(i, j, s, S, T, str1, str2, out):\n    if S[i, j] == 0:\n        return out\n    else:\n        if j > 1 and S[i-1, j-2] + sigma_exp(str1[i-1], str2[j-2:j]) + s >= T:\n            out.insert(0, (str1[i-1], str2[j-2:j]))\n            _retrieve(i-1, j-2, s+sigma_exp(str1[i-1], str2[j-2:j]), S, T, str1, str2, out)\n        elif i > 1 and S[i-2, j-1] + sigma_exp(str2[j-1], str1[i-2:i]) + s >= T:\n            out.insert(0, (str1[i-2:i], str2[j-1]))\n            _retrieve(i-2, j-1, s+sigma_exp(str2[j-1], str1[i-2:i]), S, T, str1, str2, out)\n        elif S[i, j-1] + sigma_skip(str2[j-1]) + s >= T:\n            out.insert(0, ('-', str2[j-1]))\n            _retrieve(i, j-1, s+sigma_skip(str2[j-1]), S, T, str1, str2, out)\n        elif S[i-1, j] + sigma_skip(str1[i-1]) + s >= T:\n            out.insert(0, (str1[i-1], '-'))\n            _retrieve(i-1, j, s+sigma_skip(str1[i-1]), S, T, str1, str2, out)\n        elif S[i-1, j-1] + sigma_sub(str1[i-1], str2[j-1]) + s >= T:\n            out.insert(0, (str1[i-1], str2[j-1]))\n            _retrieve(i-1, j-1, s+sigma_sub(str1[i-1], str2[j-1]), S, T, str1, str2, out)\n    return out\n\ndef sigma_skip(p):\n    return C_skip\n\ndef sigma_sub(p, q):\n    return C_sub - delta(p, q) - V(p) - V(q)\n\ndef sigma_exp(p, q):\n    q1 = q[0]\n    q2 = q[1]\n    return C_exp - delta(p, q1) - delta(p, q2) - V(p) - max(V(q1), V(q2))\n\ndef delta(p, q):\n    features = R(p, q)\n    total = 0\n    for f in features:\n        total += diff(p, q, f) * salience[f]\n    return total\n\ndef diff(p, q, f):\n    p_features, q_features = feature_matrix[p], feature_matrix[q]\n    return abs(similarity_matrix[p_features[f]] - similarity_matrix[q_features[f]])\n\ndef R(p, q):\n    if p in consonants or q in consonants:\n        return R_c\n    return R_v\n\ndef V(p):\n    if p in consonants:\n        return 0\n    return C_vwl\n\n\ndef demo():\n    data = [pair.split(',') for pair in cognate_data.split('\\n')]\n    for pair in data:\n        alignment = align(pair[0], pair[1])[0]\n        alignment = ['({}, {})'.format(a[0], a[1]) for a in alignment]\n        alignment = ' '.join(alignment)\n        print('{} ~ {} : {}'.format(pair[0], pair[1], alignment))\n\ncognate_data = \"\"\"jo,\u0292\u0259\ntu,ty\nnosotros,nu\nkjen,ki\nke,kwa\ntodos,tu\nuna,\u0259n\ndos,d\u00f8\ntres,trwa\nombre,om\narbol,arbr\u0259\npluma,plym\nkabe\u03b8a,kap\nboka,bu\u0283\npje,pje\nkora\u03b8on,k\u0153r\nber,vwar\nbenir,v\u0259nir\nde\u03b8ir,dir\npobre,povr\u0259\n\u00f0is,dIzes\n\u00f0\u00e6t,das\nwat,vas\nnat,nixt\nlo\u014b,la\u014b\nm\u00e6n,man\nfle\u0283,flaj\u0283\nbl\u0259d,blyt\nfe\u00f0\u0259r,fEd\u0259r\nh\u00e6r,hAr\nir,Or\naj,awg\u0259\nnowz,nAz\u0259\nmaw\u03b8,munt\nt\u0259\u014b,tsu\u014b\u0259\nfut,fys\nnij,knI\nh\u00e6nd,hant\nhart,herts\nliv\u0259r,lEb\u0259r\n\u00e6nd,ante\n\u00e6t,ad\nblow,flAre\nir,awris\nijt,edere\nfi\u0283,pi\u0283kis\nflow,fluere\nsta\u027e,stella\nful,plenus\ngr\u00e6s,gramen\nhart,kordis\nhorn,korny\naj,ego\nnij,genU\nm\u0259\u00f0\u0259r,mAter\nmawnt\u0259n,mons\nnejm,nomen\nnjuw,nowus\nw\u0259n,unus\nrawnd,rotundus\nsow,suere\nsit,sedere\n\u03b8rij,tres\ntuw\u03b8,dentis\n\u03b8in,tenwis\nkinwawa,kenua\u0294\nnina,nenah\nnapewa,nap\u025bw\nwapimini,wapemen\nnamesa,nam\u025b\u0294s\nokimawa,okemaw\n\u0283i\u0283ipa,se\u0294sep\nahkohkwa,ahk\u025bh\npematesiweni,pematesewen\nasenja,a\u0294s\u025bn\"\"\"\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\metrics\\association": [".py", "\n\nfrom __future__ import division\nfrom abc import ABCMeta, abstractmethod\nfrom six import add_metaclass\nimport math as _math\nfrom functools import reduce\n_log2 = lambda x: _math.log(x, 2.0)\n_ln = _math.log\n\n_product = lambda s: reduce(lambda x, y: x * y, s)\n\n_SMALL = 1e-20\n\ntry:\n    from scipy.stats import fisher_exact\nexcept ImportError:\n    def fisher_exact(*_args, **_kwargs):\n        raise NotImplementedError\n\n\nNGRAM = 0\n\nUNIGRAMS = -2\n\nTOTAL = -1\n\n\n@add_metaclass(ABCMeta)\nclass NgramAssocMeasures(object):\n\n    _n = 0\n\n    @staticmethod\n    @abstractmethod\n    def _contingency(*marginals):\n        raise NotImplementedError(\"The contingency table is not available\"\n                                  \"in the general ngram case\")\n\n    @staticmethod\n    @abstractmethod\n    def _marginals(*contingency):\n        raise NotImplementedError(\"The contingency table is not available\"\n                                  \"in the general ngram case\")\n\n    @classmethod\n    def _expected_values(cls, cont):\n        n_all = sum(cont)\n        bits = [1 << i for i in range(cls._n)]\n\n        for i in range(len(cont)):\n            yield (_product(sum(cont[x] for x in range(2 ** cls._n)\n                                if (x & j) == (i & j))\n                            for j in bits) /\n                   (n_all ** (cls._n - 1)))\n\n    @staticmethod\n    def raw_freq(*marginals):\n        return marginals[NGRAM] / marginals[TOTAL]\n\n    @classmethod\n    def student_t(cls, *marginals):\n        return ((marginals[NGRAM] -\n                  _product(marginals[UNIGRAMS]) /\n                  (marginals[TOTAL] ** (cls._n - 1))) /\n                (marginals[NGRAM] + _SMALL) ** .5)\n\n    @classmethod\n    def chi_sq(cls, *marginals):\n        cont = cls._contingency(*marginals)\n        exps = cls._expected_values(cont)\n        return sum((obs - exp) ** 2 / (exp + _SMALL)\n                   for obs, exp in zip(cont, exps))\n\n    @staticmethod\n    def mi_like(*marginals, **kwargs):\n        return (marginals[NGRAM] ** kwargs.get('power', 3) /\n                _product(marginals[UNIGRAMS]))\n\n    @classmethod\n    def pmi(cls, *marginals):\n        return (_log2(marginals[NGRAM] * marginals[TOTAL] ** (cls._n - 1)) -\n                _log2(_product(marginals[UNIGRAMS])))\n\n    @classmethod\n    def likelihood_ratio(cls, *marginals):\n        cont = cls._contingency(*marginals)\n        return (cls._n *\n                sum(obs * _ln(obs / (exp + _SMALL) + _SMALL)\n                    for obs, exp in zip(cont, cls._expected_values(cont))))\n\n    @classmethod\n    def poisson_stirling(cls, *marginals):\n        exp = (_product(marginals[UNIGRAMS]) /\n               (marginals[TOTAL] ** (cls._n - 1)))\n        return marginals[NGRAM] * (_log2(marginals[NGRAM] / exp) - 1)\n\n    @classmethod\n    def jaccard(cls, *marginals):\n        cont = cls._contingency(*marginals)\n        return cont[0] / sum(cont[:-1])\n\n\nclass BigramAssocMeasures(NgramAssocMeasures):\n\n    _n = 2\n\n    @staticmethod\n    def _contingency(n_ii, n_ix_xi_tuple, n_xx):\n        (n_ix, n_xi) = n_ix_xi_tuple\n        n_oi = n_xi - n_ii\n        n_io = n_ix - n_ii\n        return (n_ii, n_oi, n_io, n_xx - n_ii - n_oi - n_io)\n\n    @staticmethod\n    def _marginals(n_ii, n_oi, n_io, n_oo):\n        return (n_ii, (n_oi + n_ii, n_io + n_ii), n_oo + n_oi + n_io + n_ii)\n\n    @staticmethod\n    def _expected_values(cont):\n        n_xx = sum(cont)\n        for i in range(4):\n            yield (cont[i] + cont[i ^ 1]) * (cont[i] + cont[i ^ 2]) / n_xx\n\n    @classmethod\n    def phi_sq(cls, *marginals):\n        n_ii, n_io, n_oi, n_oo = cls._contingency(*marginals)\n\n        return ((n_ii*n_oo - n_io*n_oi)**2 /\n                ((n_ii + n_io) * (n_ii + n_oi) * (n_io + n_oo) * (n_oi + n_oo)))\n\n    @classmethod\n    def chi_sq(cls, n_ii, n_ix_xi_tuple, n_xx):\n        (n_ix, n_xi) = n_ix_xi_tuple\n        return n_xx * cls.phi_sq(n_ii, (n_ix, n_xi), n_xx)\n\n    @classmethod\n    def fisher(cls, *marginals):\n\n        n_ii, n_io, n_oi, n_oo = cls._contingency(*marginals)\n\n        (odds, pvalue) = fisher_exact([[n_ii, n_io], [n_oi, n_oo]], alternative='less')\n        return pvalue\n\n    @staticmethod\n    def dice(n_ii, n_ix_xi_tuple, n_xx):\n        (n_ix, n_xi) = n_ix_xi_tuple\n        return 2 * n_ii / (n_ix + n_xi)\n\n\nclass TrigramAssocMeasures(NgramAssocMeasures):\n\n    _n = 3\n\n    @staticmethod\n    def _contingency(n_iii, n_iix_tuple, n_ixx_tuple, n_xxx):\n        (n_iix, n_ixi, n_xii) = n_iix_tuple\n        (n_ixx, n_xix, n_xxi) = n_ixx_tuple\n        n_oii = n_xii - n_iii\n        n_ioi = n_ixi - n_iii\n        n_iio = n_iix - n_iii\n        n_ooi = n_xxi - n_iii - n_oii - n_ioi\n        n_oio = n_xix - n_iii - n_oii - n_iio\n        n_ioo = n_ixx - n_iii - n_ioi - n_iio\n        n_ooo = n_xxx - n_iii - n_oii - n_ioi - n_iio - n_ooi - n_oio - n_ioo\n\n        return (n_iii, n_oii, n_ioi, n_ooi,\n                n_iio, n_oio, n_ioo, n_ooo)\n\n    @staticmethod\n    def _marginals(*contingency):\n        n_iii, n_oii, n_ioi, n_ooi, n_iio, n_oio, n_ioo, n_ooo = contingency\n        return (n_iii,\n                (n_iii + n_iio, n_iii + n_ioi, n_iii + n_oii),\n                (n_iii + n_ioi + n_iio + n_ioo,\n                 n_iii + n_oii + n_iio + n_oio,\n                 n_iii + n_oii + n_ioi + n_ooi),\n                sum(contingency))\n\n\nclass QuadgramAssocMeasures(NgramAssocMeasures):\n\n    _n = 4\n\n    @staticmethod\n    def _contingency(n_iiii, n_iiix_tuple, n_iixx_tuple, n_ixxx_tuple, n_xxxx):\n        (n_iiix, n_iixi, n_ixii, n_xiii) = n_iiix_tuple\n        (n_iixx, n_ixix, n_ixxi, n_xixi, n_xxii, n_xiix) = n_iixx_tuple\n        (n_ixxx, n_xixx, n_xxix, n_xxxi) = n_ixxx_tuple\n        n_oiii = n_xiii - n_iiii\n        n_ioii = n_ixii - n_iiii\n        n_iioi = n_iixi - n_iiii\n        n_ooii = n_xxii - n_iiii - n_oiii - n_ioii\n        n_oioi = n_xixi - n_iiii - n_oiii - n_iioi\n        n_iooi = n_ixxi - n_iiii - n_ioii - n_iioi\n        n_oooi = n_xxxi - n_iiii - n_oiii - n_ioii - n_iioi - n_ooii - n_iooi - n_oioi\n        n_iiio = n_iiix - n_iiii\n        n_oiio = n_xiix - n_iiii - n_oiii - n_iiio\n        n_ioio = n_ixix - n_iiii - n_ioii - n_iiio\n        n_ooio = n_xxix - n_iiii - n_oiii - n_ioii - n_iiio - n_ooii - n_ioio - n_oiio\n        n_iioo = n_iixx - n_iiii - n_iioi - n_iiio\n        n_oioo = n_xixx - n_iiii - n_oiii - n_iioi - n_iiio - n_oioi - n_oiio - n_iioo\n        n_iooo = n_ixxx - n_iiii - n_ioii - n_iioi - n_iiio - n_iooi - n_iioo - n_ioio\n        n_oooo = n_xxxx - n_iiii - n_oiii - n_ioii - n_iioi - n_ooii - n_oioi - n_iooi - \\\n                 n_oooi - n_iiio - n_oiio - n_ioio - n_ooio - n_iioo - n_oioo - n_iooo\n\n        return (n_iiii, n_oiii, n_ioii, n_ooii, n_iioi,\n                n_oioi, n_iooi, n_oooi, n_iiio, n_oiio,\n                n_ioio, n_ooio, n_iioo, n_oioo, n_iooo, n_oooo)\n\n    @staticmethod\n    def _marginals(*contingency):\n        n_iiii, n_oiii, n_ioii, n_ooii, n_iioi, n_oioi, n_iooi, n_oooi, n_iiio, n_oiio, n_ioio, n_ooio, \\\n        n_iioo, n_oioo, n_iooo, n_oooo = contingency\n\n        n_iiix = n_iiii + n_iiio\n        n_iixi = n_iiii + n_iioi\n        n_ixii = n_iiii + n_ioii\n        n_xiii = n_iiii + n_oiii\n\n        n_iixx = n_iiii + n_iioi + n_iiio + n_iioo\n        n_ixix = n_iiii + n_ioii + n_iiio + n_ioio\n        n_ixxi = n_iiii + n_ioii + n_iioi + n_iooi\n        n_xixi = n_iiii + n_oiii + n_iioi + n_oioi\n        n_xxii = n_iiii + n_oiii + n_ioii + n_ooii\n        n_xiix = n_iiii + n_oiii + n_iiio + n_oiio\n\n        n_ixxx = n_iiii + n_ioii + n_iioi + n_iiio + n_iooi + n_iioo + n_ioio + n_iooo\n        n_xixx = n_iiii + n_oiii + n_iioi + n_iiio + n_oioi + n_oiio + n_iioo + n_oioo\n        n_xxix = n_iiii + n_oiii + n_ioii + n_iiio + n_ooii + n_ioio + n_oiio + n_ooio\n        n_xxxi = n_iiii + n_oiii + n_ioii + n_iioi + n_ooii + n_iooi + n_oioi + n_oooi\n\n        n_all = sum(contingency)\n\n        return (n_iiii,\n                (n_iiix, n_iixi, n_ixii, n_xiii),\n                (n_iixx, n_ixix, n_ixxi, n_xixi, n_xxii, n_xiix),\n                (n_ixxx, n_xixx, n_xxix, n_xxxi),\n                n_all)\n\n\nclass ContingencyMeasures(object):\n\n    def __init__(self, measures):\n        self.__class__.__name__ = 'Contingency' + measures.__class__.__name__\n        for k in dir(measures):\n            if k.startswith('__'):\n                continue\n            v = getattr(measures, k)\n            if not k.startswith('_'):\n                v = self._make_contingency_fn(measures, v)\n            setattr(self, k, v)\n\n    @staticmethod\n    def _make_contingency_fn(measures, old_fn):\n        def res(*contingency):\n            return old_fn(*measures._marginals(*contingency))\n        res.__doc__ = old_fn.__doc__\n        res.__name__ = old_fn.__name__\n        return res\n"], "nltk\\metrics\\confusionmatrix": [".py", "from __future__ import print_function, unicode_literals\nfrom nltk.probability import FreqDist\nfrom nltk.compat import python_2_unicode_compatible\n\n@python_2_unicode_compatible\nclass ConfusionMatrix(object):\n\n    def __init__(self, reference, test, sort_by_count=False):\n        if len(reference) != len(test):\n            raise ValueError('Lists must have the same length.')\n\n        if sort_by_count:\n            ref_fdist = FreqDist(reference)\n            test_fdist = FreqDist(test)\n            def key(v): return -(ref_fdist[v]+test_fdist[v])\n            values = sorted(set(reference+test), key=key)\n        else:\n            values = sorted(set(reference+test))\n\n        indices = dict((val,i) for (i,val) in enumerate(values))\n\n        confusion = [[0 for val in values] for val in values]\n        max_conf = 0 # Maximum confusion\n        for w,g in zip(reference, test):\n            confusion[indices[w]][indices[g]] += 1\n            max_conf = max(max_conf, confusion[indices[w]][indices[g]])\n\n        self._values = values\n        self._indices = indices\n        self._confusion = confusion\n        self._max_conf = max_conf\n        self._total = len(reference)\n        self._correct = sum(confusion[i][i] for i in range(len(values)))\n\n    def __getitem__(self, li_lj_tuple):\n        (li, lj) = li_lj_tuple\n        i = self._indices[li]\n        j = self._indices[lj]\n        return self._confusion[i][j]\n\n    def __repr__(self):\n        return '<ConfusionMatrix: %s/%s correct>' % (self._correct,\n                                                     self._total)\n\n    def __str__(self):\n        return self.pretty_format()\n\n    def pretty_format(self, show_percents=False, values_in_chart=True,\n           truncate=None, sort_by_count=False):\n        confusion = self._confusion\n\n        values = self._values\n        if sort_by_count:\n            values = sorted(values, key=lambda v:\n                            -sum(self._confusion[self._indices[v]]))\n\n        if truncate:\n            values = values[:truncate]\n\n        if values_in_chart:\n            value_strings = [\"%s\" % val for val in values]\n        else:\n            value_strings = [str(n+1) for n in range(len(values))]\n\n        valuelen = max(len(val) for val in value_strings)\n        value_format = '%' + repr(valuelen) + 's | '\n        if show_percents:\n            entrylen = 6\n            entry_format = '%5.1f%%'\n            zerostr = '     .'\n        else:\n            entrylen = len(repr(self._max_conf))\n            entry_format = '%' + repr(entrylen) + 'd'\n            zerostr = ' '*(entrylen-1) + '.'\n\n        s = ''\n        for i in range(valuelen):\n            s += (' '*valuelen)+' |'\n            for val in value_strings:\n                if i >= valuelen-len(val):\n                    s += val[i-valuelen+len(val)].rjust(entrylen+1)\n                else:\n                    s += ' '*(entrylen+1)\n            s += ' |\\n'\n\n        s += '%s-+-%s+\\n' % ('-'*valuelen, '-'*((entrylen+1)*len(values)))\n\n        for val, li in zip(value_strings, values):\n            i = self._indices[li]\n            s += value_format % val\n            for lj in values:\n                j = self._indices[lj]\n                if confusion[i][j] == 0:\n                    s += zerostr\n                elif show_percents:\n                    s += entry_format % (100.0*confusion[i][j]/self._total)\n                else:\n                    s += entry_format % confusion[i][j]\n                if i == j:\n                    prevspace = s.rfind(' ')\n                    s = s[:prevspace] + '<' + s[prevspace+1:] + '>'\n                else: s += ' '\n            s += '|\\n'\n\n        s += '%s-+-%s+\\n' % ('-'*valuelen, '-'*((entrylen+1)*len(values)))\n\n        s += '(row = reference; col = test)\\n'\n        if not values_in_chart:\n            s += 'Value key:\\n'\n            for i, value in enumerate(values):\n                s += '%6d: %s\\n' % (i+1, value)\n\n        return s\n\n    def key(self):\n        values = self._values\n        str = 'Value key:\\n'\n        indexlen = len(repr(len(values)-1))\n        key_format = '  %'+repr(indexlen)+'d: %s\\n'\n        for i in range(len(values)):\n            str += key_format % (i, values[i])\n\n        return str\n\ndef demo():\n    reference = 'DET NN VB DET JJ NN NN IN DET NN'.split()\n    test    = 'DET VB VB DET NN NN NN IN DET NN'.split()\n    print('Reference =', reference)\n    print('Test    =', test)\n    print('Confusion matrix:')\n    print(ConfusionMatrix(reference, test))\n    print(ConfusionMatrix(reference, test).pretty_format(sort_by_count=True))\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\metrics\\distance": [".py", "\n\nfrom __future__ import print_function\nfrom __future__ import division\n\n\ndef _edit_dist_init(len1, len2):\n    lev = []\n    for i in range(len1):\n        lev.append([0] * len2)  # initialize 2D array to zero\n    for i in range(len1):\n        lev[i][0] = i           # column 0: 0,1,2,3,4,...\n    for j in range(len2):\n        lev[0][j] = j           # row 0: 0,1,2,3,4,...\n    return lev\n\n\ndef _edit_dist_step(lev, i, j, s1, s2, substitution_cost=1, transpositions=False):\n    c1 = s1[i - 1]\n    c2 = s2[j - 1]\n\n    a = lev[i - 1][j] + 1\n    b = lev[i][j - 1] + 1\n    c = lev[i - 1][j - 1] + (substitution_cost if c1 != c2 else 0)\n\n    d = c + 1  # never picked by default\n    if transpositions and i > 1 and j > 1:\n        if s1[i - 2] == c2 and s2[j - 2] == c1:\n            d = lev[i - 2][j - 2] + 1\n\n    lev[i][j] = min(a, b, c, d)\n\n\ndef edit_distance(s1, s2, substitution_cost=1, transpositions=False):\n    len1 = len(s1)\n    len2 = len(s2)\n    lev = _edit_dist_init(len1 + 1, len2 + 1)\n\n    for i in range(len1):\n        for j in range(len2):\n            _edit_dist_step(lev, i + 1, j + 1, s1, s2,\n                            substitution_cost=substitution_cost, transpositions=transpositions)\n    return lev[len1][len2]\n\n\ndef binary_distance(label1, label2):\n\n    return 0.0 if label1 == label2 else 1.0\n\n\ndef jaccard_distance(label1, label2):\n    return (len(label1.union(label2)) - len(label1.intersection(label2)))/len(label1.union(label2))\n\n\ndef masi_distance(label1, label2):\n\n    len_intersection = len(label1.intersection(label2))\n    len_union = len(label1.union(label2))\n    len_label1 = len(label1)\n    len_label2 = len(label2)\n    if len_label1 == len_label2 and len_label1 == len_intersection:\n        m = 1\n    elif len_intersection == min(len_label1, len_label2):\n        m = 0.67\n    elif len_intersection > 0:\n        m = 0.33\n    else:\n        m = 0\n\n    return 1 - len_intersection / len_union * m\n\n\ndef interval_distance(label1,label2):\n\n    try:\n        return pow(label1 - label2, 2)\n    except:\n        print(\"non-numeric labels not supported with interval distance\")\n\n\ndef presence(label):\n\n    return lambda x, y: 1.0 * ((label in x) == (label in y))\n\n\ndef fractional_presence(label):\n    return lambda x, y:\\\n        abs(((1.0 / len(x)) - (1.0 / len(y)))) * (label in x and label in y) \\\n        or 0.0 * (label not in x and label not in y) \\\n        or abs((1.0 / len(x))) * (label in x and label not in y) \\\n        or ((1.0 / len(y))) * (label not in x and label in y)\n\n\ndef custom_distance(file):\n    data = {}\n    with open(file, 'r') as infile:\n        for l in infile:\n            labelA, labelB, dist = l.strip().split(\"\\t\")\n            labelA = frozenset([labelA])\n            labelB = frozenset([labelB])\n            data[frozenset([labelA,labelB])] = float(dist)\n    return lambda x,y:data[frozenset([x,y])]\n\n\ndef demo():\n    edit_distance_examples = [\n        (\"rain\", \"shine\"), (\"abcdef\", \"acbdef\"), (\"language\", \"lnaguaeg\"),\n        (\"language\", \"lnaugage\"), (\"language\", \"lngauage\")]\n    for s1, s2 in edit_distance_examples:\n        print(\"Edit distance between '%s' and '%s':\" % (s1, s2), edit_distance(s1, s2))\n    for s1, s2 in edit_distance_examples:\n        print(\"Edit distance with transpositions between '%s' and '%s':\" % (s1, s2), edit_distance(s1, s2, transpositions=True))\n\n    s1 = set([1, 2, 3, 4])\n    s2 = set([3, 4, 5])\n    print(\"s1:\", s1)\n    print(\"s2:\", s2)\n    print(\"Binary distance:\", binary_distance(s1, s2))\n    print(\"Jaccard distance:\", jaccard_distance(s1, s2))\n    print(\"MASI distance:\", masi_distance(s1, s2))\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\metrics\\paice": [".py", "\n\nfrom math import sqrt\n\n\ndef get_words_from_dictionary(lemmas):\n    '''\n    Get original set of words used for analysis.\n\n    :param lemmas: A dictionary where keys are lemmas and values are sets\n    or lists of words corresponding to that lemma.\n    :type lemmas: dict(str): list(str)\n    :return: Set of words that exist as values in the dictionary\n    :rtype: set(str)\n    '''\n    words = set()\n    for lemma in lemmas:\n        words.update(set(lemmas[lemma]))\n    return words\n\n\ndef _truncate(words, cutlength):\n    '''Group words by stems defined by truncating them at given length.\n\n    :param words: Set of words used for analysis\n    :param cutlength: Words are stemmed by cutting at this length.\n    :type words: set(str) or list(str)\n    :type cutlength: int\n    :return: Dictionary where keys are stems and values are sets of words\n    corresponding to that stem.\n    :rtype: dict(str): set(str)\n    '''\n    stems = {}\n    for word in words:\n        stem = word[:cutlength]\n        try:\n            stems[stem].update([word])\n        except KeyError:\n            stems[stem] = set([word])\n    return stems\n\n\ndef _count_intersection(l1, l2):\n    '''Count intersection between two line segments defined by coordinate pairs.\n\n    :param l1: Tuple of two coordinate pairs defining the first line segment\n    :param l2: Tuple of two coordinate pairs defining the second line segment\n    :type l1: tuple(float, float)\n    :type l2: tuple(float, float)\n    :return: Coordinates of the intersection\n    :rtype: tuple(float, float)\n    '''\n    x1, y1 = l1[0]\n    x2, y2 = l1[1]\n    x3, y3 = l2[0]\n    x4, y4 = l2[1]\n\n    denominator = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)\n\n    if denominator == 0.0: # lines are parallel\n        if x1 == x2 == x3 == x4 == 0.0:\n            return (0.0, y4)\n\n    x = ((x1 * y2 - y1 * x2) * (x3 - x4) - (x1 - x2) * (x3 * y4 - y3 * x4)) / denominator\n    y = ((x1 * y2 - y1 * x2) * (y3 - y4) - (y1 - y2) * (x3 * y4 - y3 * x4)) / denominator\n    return (x, y)\n\n\ndef _get_derivative(coordinates):\n    '''Get derivative of the line from (0,0) to given coordinates.\n\n    :param coordinates: A coordinate pair\n    :type coordinates: tuple(float, float)\n    :return: Derivative; inf if x is zero\n    :rtype: float\n    '''\n    try:\n        return coordinates[1] / coordinates[0]\n    except ZeroDivisionError:\n        return float('inf')\n\n\ndef _calculate_cut(lemmawords, stems):\n    '''Count understemmed and overstemmed pairs for (lemma, stem) pair with common words.\n\n    :param lemmawords: Set or list of words corresponding to certain lemma.\n    :param stems: A dictionary where keys are stems and values are sets\n    or lists of words corresponding to that stem.\n    :type lemmawords: set(str) or list(str)\n    :type stems: dict(str): set(str)\n    :return: Amount of understemmed and overstemmed pairs contributed by words\n    existing in both lemmawords and stems.\n    :rtype: tuple(float, float)\n    '''\n    umt, wmt = 0.0, 0.0\n    for stem in stems:\n        cut = set(lemmawords) & set(stems[stem])\n        if cut:\n            cutcount = len(cut)\n            stemcount = len(stems[stem])\n            umt += cutcount * (len(lemmawords) - cutcount)\n            wmt += cutcount * (stemcount - cutcount)\n    return (umt, wmt)\n\n\ndef _calculate(lemmas, stems):\n    '''Calculate actual and maximum possible amounts of understemmed and overstemmed word pairs.\n\n    :param lemmas: A dictionary where keys are lemmas and values are sets\n    or lists of words corresponding to that lemma.\n    :param stems: A dictionary where keys are stems and values are sets\n    or lists of words corresponding to that stem.\n    :type lemmas: dict(str): list(str)\n    :type stems: dict(str): set(str)\n    :return: Global unachieved merge total (gumt),\n    global desired merge total (gdmt),\n    global wrongly merged total (gwmt) and\n    global desired non-merge total (gdnt).\n    :rtype: tuple(float, float, float, float)\n    '''\n\n    n = sum(len(lemmas[word]) for word in lemmas)\n\n    gdmt, gdnt, gumt, gwmt = (0.0, 0.0, 0.0, 0.0)\n\n    for lemma in lemmas:\n        lemmacount = len(lemmas[lemma])\n\n        gdmt += lemmacount * (lemmacount - 1)\n\n        gdnt += lemmacount * (n - lemmacount)\n\n        umt, wmt = _calculate_cut(lemmas[lemma], stems)\n\n        gumt += umt\n        gwmt += wmt\n\n    return (gumt / 2, gdmt / 2, gwmt / 2, gdnt / 2)\n\n\ndef _indexes(gumt, gdmt, gwmt, gdnt):\n    '''Count Understemming Index (UI), Overstemming Index (OI) and Stemming Weight (SW).\n\n    :param gumt, gdmt, gwmt, gdnt: Global unachieved merge total (gumt),\n    global desired merge total (gdmt),\n    global wrongly merged total (gwmt) and\n    global desired non-merge total (gdnt).\n    :type gumt, gdmt, gwmt, gdnt: float\n    :return: Understemming Index (UI),\n    Overstemming Index (OI) and\n    Stemming Weight (SW).\n    :rtype: tuple(float, float, float)\n    '''\n    try:\n        ui = gumt / gdmt\n    except ZeroDivisionError:\n        ui = 0.0\n    try:\n        oi = gwmt / gdnt\n    except ZeroDivisionError:\n        oi = 0.0\n    try:\n        sw = oi / ui\n    except ZeroDivisionError:\n        if oi == 0.0:\n            sw = float('nan')\n        else:\n            sw = float('inf')\n    return (ui, oi, sw)\n\n\nclass Paice(object):\n    '''Class for storing lemmas, stems and evaluation metrics.'''\n    def __init__(self, lemmas, stems):\n        '''\n        :param lemmas: A dictionary where keys are lemmas and values are sets\n        or lists of words corresponding to that lemma.\n        :param stems: A dictionary where keys are stems and values are sets\n        or lists of words corresponding to that stem.\n        :type lemmas: dict(str): list(str)\n        :type stems: dict(str): set(str)\n        '''\n        self.lemmas = lemmas\n        self.stems = stems\n        self.coords = []\n        self.gumt, self.gdmt, self.gwmt, self.gdnt = (None, None, None, None)\n        self.ui, self.oi, self.sw = (None, None, None)\n        self.errt = None\n        self.update()\n\n    def __str__(self):\n        text = ['Global Unachieved Merge Total (GUMT): %s\\n' % self.gumt]\n        text.append('Global Desired Merge Total (GDMT): %s\\n' % self.gdmt)\n        text.append('Global Wrongly-Merged Total (GWMT): %s\\n' % self.gwmt)\n        text.append('Global Desired Non-merge Total (GDNT): %s\\n' % self.gdnt)\n        text.append('Understemming Index (GUMT / GDMT): %s\\n' % self.ui)\n        text.append('Overstemming Index (GWMT / GDNT): %s\\n' % self.oi)\n        text.append('Stemming Weight (OI / UI): %s\\n' % self.sw)\n        text.append('Error-Rate Relative to Truncation (ERRT): %s\\r\\n' % self.errt)\n        coordinates = ' '.join(['(%s, %s)' % item for item in self.coords])\n        text.append('Truncation line: %s' % coordinates)\n        return ''.join(text)\n\n    def _get_truncation_indexes(self, words, cutlength):\n        '''Count (UI, OI) when stemming is done by truncating words at \\'cutlength\\'.\n\n        :param words: Words used for the analysis\n        :param cutlength: Words are stemmed by cutting them at this length\n        :type words: set(str) or list(str)\n        :type cutlength: int\n        :return: Understemming and overstemming indexes\n        :rtype: tuple(int, int)\n        '''\n\n        truncated = _truncate(words, cutlength)\n        gumt, gdmt, gwmt, gdnt = _calculate(self.lemmas, truncated)\n        ui, oi = _indexes(gumt, gdmt, gwmt, gdnt)[:2]\n        return (ui, oi)\n\n    def _get_truncation_coordinates(self, cutlength=0):\n        '''Count (UI, OI) pairs for truncation points until we find the segment where (ui, oi) crosses the truncation line.\n\n        :param cutlength: Optional parameter to start counting from (ui, oi)\n        coordinates gotten by stemming at this length. Useful for speeding up\n        the calculations when you know the approximate location of the\n        intersection.\n        :type cutlength: int\n        :return: List of coordinate pairs that define the truncation line\n        :rtype: list(tuple(float, float))\n        '''\n        words = get_words_from_dictionary(self.lemmas)\n        maxlength = max(len(word) for word in words)\n\n        coords = []\n        while cutlength <= maxlength:\n            pair = self._get_truncation_indexes(words, cutlength)\n\n            if pair not in coords:\n                coords.append(pair)\n            if pair == (0.0, 0.0):\n                return coords\n            if len(coords) >= 2 and pair[0] > 0.0:\n                derivative1 = _get_derivative(coords[-2])\n                derivative2 = _get_derivative(coords[-1])\n                if derivative1 >= self.sw >= derivative2:\n                    return coords\n            cutlength += 1\n        return coords\n\n    def _errt(self):\n        '''Count Error-Rate Relative to Truncation (ERRT).\n\n        :return: ERRT, length of the line from origo to (UI, OI) divided by\n        the length of the line from origo to the point defined by the same\n        line when extended until the truncation line.\n        :rtype: float\n        '''\n        self.coords = self._get_truncation_coordinates()\n        if (0.0, 0.0) in self.coords:\n            if (self.ui, self.oi) != (0.0, 0.0):\n                return float('inf')\n            else:\n                return float('nan')\n        if (self.ui, self.oi) == (0.0, 0.0):\n            return 0.0\n        intersection = _count_intersection(((0, 0), (self.ui, self.oi)),\n                                           self.coords[-2:]\n                                           )\n        op = sqrt(self.ui ** 2 + self.oi ** 2)\n        ot = sqrt(intersection[0] ** 2 + intersection[1] ** 2)\n        return op / ot\n\n    def update(self):\n        '''Update statistics after lemmas and stems have been set.'''\n        self.gumt, self.gdmt, self.gwmt, self.gdnt = _calculate(self.lemmas, self.stems)\n        self.ui, self.oi, self.sw = _indexes(self.gumt, self.gdmt, self.gwmt, self.gdnt)\n        self.errt = self._errt()\n\n\ndef demo():\n    '''Demonstration of the module.'''\n    lemmas = {'kneel': ['kneel', 'knelt'],\n              'range': ['range', 'ranged'],\n              'ring': ['ring', 'rang', 'rung']\n              }\n    stems = {'kneel': ['kneel'],\n             'knelt': ['knelt'],\n             'rang': ['rang', 'range', 'ranged'],\n             'ring': ['ring'],\n             'rung': ['rung']\n             }\n    print('Words grouped by their lemmas:')\n    for lemma in sorted(lemmas):\n        print('%s => %s' % (lemma, ' '.join(lemmas[lemma])))\n    print()\n    print('Same words grouped by a stemming algorithm:')\n    for stem in sorted(stems):\n        print('%s => %s' % (stem, ' '.join(stems[stem])))\n    print()\n    p = Paice(lemmas, stems)\n    print(p)\n    print()\n    stems = {'kneel': ['kneel'],\n             'knelt': ['knelt'],\n             'rang': ['rang'],\n             'range': ['range', 'ranged'],\n             'ring': ['ring'],\n             'rung': ['rung']\n             }\n    print('Counting stats after changing stemming results:')\n    for stem in sorted(stems):\n        print('%s => %s' % (stem, ' '.join(stems[stem])))\n    print()\n    p.stems = stems\n    p.update()\n    print(p)\n\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\metrics\\scores": [".py", "from __future__ import print_function, division\n\nfrom math import fabs\nimport operator\nfrom random import shuffle\nfrom functools import reduce\n\nfrom six.moves import range, zip\n\ntry:\n    from scipy.stats.stats import betai\nexcept ImportError:\n    betai = None\n\nfrom nltk.util import LazyConcatenation, LazyMap\n\ndef accuracy(reference, test):\n    if len(reference) != len(test):\n        raise ValueError(\"Lists must have the same length.\")\n    return sum(x == y for x, y in zip(reference, test)) / len(test)\n\ndef precision(reference, test):\n    if (not hasattr(reference, 'intersection') or\n        not hasattr(test, 'intersection')):\n        raise TypeError('reference and test should be sets')\n\n    if len(test) == 0:\n        return None\n    else:\n        return len(reference.intersection(test)) / len(test)\n\ndef recall(reference, test):\n    if (not hasattr(reference, 'intersection') or\n        not hasattr(test, 'intersection')):\n        raise TypeError('reference and test should be sets')\n\n    if len(reference) == 0:\n        return None\n    else:\n        return len(reference.intersection(test)) / len(reference)\n\ndef f_measure(reference, test, alpha=0.5):\n    p = precision(reference, test)\n    r = recall(reference, test)\n    if p is None or r is None:\n        return None\n    if p == 0 or r == 0:\n        return 0\n    return 1.0 / (alpha / p + (1-alpha) / r)\n\ndef log_likelihood(reference, test):\n    if len(reference) != len(test):\n        raise ValueError(\"Lists must have the same length.\")\n\n    total_likelihood = sum(dist.logprob(val)\n                            for (val, dist) in zip(reference, test))\n    return total_likelihood / len(reference)\n\ndef approxrand(a, b, **kwargs):\n    shuffles = kwargs.get('shuffles', 999)\n    shuffles = \\\n        min(shuffles, reduce(operator.mul, range(1, len(a) + len(b) + 1)))\n    stat = kwargs.get('statistic', lambda lst: sum(lst) / len(lst))\n    verbose = kwargs.get('verbose', False)\n\n    if verbose:\n        print('shuffles: %d' % shuffles)\n\n    actual_stat = fabs(stat(a) - stat(b))\n\n    if verbose:\n        print('actual statistic: %f' % actual_stat)\n        print('-' * 60)\n\n    c = 1e-100\n    lst = LazyConcatenation([a, b])\n    indices = list(range(len(a) + len(b)))\n\n    for i in range(shuffles):\n        if verbose and i % 10 == 0:\n            print('shuffle: %d' % i)\n\n        shuffle(indices)\n\n        pseudo_stat_a = stat(LazyMap(lambda i: lst[i], indices[:len(a)]))\n        pseudo_stat_b = stat(LazyMap(lambda i: lst[i], indices[len(a):]))\n        pseudo_stat = fabs(pseudo_stat_a - pseudo_stat_b)\n\n        if pseudo_stat >= actual_stat:\n            c += 1\n\n        if verbose and i % 10 == 0:\n            print('pseudo-statistic: %f' % pseudo_stat)\n            print('significance: %f' % ((c + 1) / (i + 1)))\n            print('-' * 60)\n\n    significance = (c + 1) / (shuffles + 1)\n\n    if verbose:\n        print('significance: %f' % significance)\n        if betai:\n            for phi in [0.01, 0.05, 0.10, 0.15, 0.25, 0.50]:\n                print(\"prob(phi<=%f): %f\" % (phi, betai(c, shuffles, phi)))\n\n    return (significance, c, shuffles)\n\n\ndef demo():\n    print('-'*75)\n    reference = 'DET NN VB DET JJ NN NN IN DET NN'.split()\n    test    = 'DET VB VB DET NN NN NN IN DET NN'.split()\n    print('Reference =', reference)\n    print('Test    =', test)\n    print('Accuracy:', accuracy(reference, test))\n\n    print('-'*75)\n    reference_set = set(reference)\n    test_set = set(test)\n    print('Reference =', reference_set)\n    print('Test =   ', test_set)\n    print('Precision:', precision(reference_set, test_set))\n    print('   Recall:', recall(reference_set, test_set))\n    print('F-Measure:', f_measure(reference_set, test_set))\n    print('-'*75)\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\metrics\\segmentation": [".py", "\n\n\n\ntry:\n    import numpy as np\nexcept ImportError:\n    pass\n\nfrom six.moves import range\n\n\ndef windowdiff(seg1, seg2, k, boundary=\"1\", weighted=False):\n\n    if len(seg1) != len(seg2):\n        raise ValueError(\"Segmentations have unequal length\")\n    if k > len(seg1):\n        raise ValueError(\"Window width k should be smaller or equal than segmentation lengths\")\n    wd = 0\n    for i in range(len(seg1) - k + 1):\n        ndiff = abs(seg1[i:i+k].count(boundary) - seg2[i:i+k].count(boundary))\n        if weighted:\n            wd += ndiff\n        else:\n            wd += min(1, ndiff)\n    return wd / (len(seg1) - k + 1.)\n\n\n\n\ndef _init_mat(nrows, ncols, ins_cost, del_cost):\n    mat = np.empty((nrows, ncols))\n    mat[0, :] = ins_cost * np.arange(ncols)\n    mat[:, 0] = del_cost * np.arange(nrows)\n    return mat\n\n\ndef _ghd_aux(mat, rowv, colv, ins_cost, del_cost, shift_cost_coeff):\n    for i, rowi in enumerate(rowv):\n        for j, colj in enumerate(colv):\n            shift_cost = shift_cost_coeff * abs(rowi - colj) + mat[i, j]\n            if rowi == colj:\n                tcost = mat[i, j]\n            elif rowi > colj:\n                tcost = del_cost + mat[i, j + 1]\n            else:\n                tcost = ins_cost + mat[i + 1, j]\n            mat[i + 1, j + 1] = min(tcost, shift_cost)\n\n\ndef ghd(ref, hyp, ins_cost=2.0, del_cost=2.0, shift_cost_coeff=1.0, boundary='1'):\n\n    ref_idx = [i for (i, val) in enumerate(ref) if val == boundary]\n    hyp_idx = [i for (i, val) in enumerate(hyp) if val == boundary]\n\n    nref_bound = len(ref_idx)\n    nhyp_bound = len(hyp_idx)\n\n    if nref_bound == 0 and nhyp_bound == 0:\n        return 0.0\n    elif nref_bound > 0 and nhyp_bound == 0:\n        return nref_bound * ins_cost\n    elif nref_bound == 0 and nhyp_bound > 0:\n        return nhyp_bound * del_cost\n\n    mat = _init_mat(nhyp_bound + 1, nref_bound + 1, ins_cost, del_cost)\n    _ghd_aux(mat, hyp_idx, ref_idx, ins_cost, del_cost, shift_cost_coeff)\n    return mat[-1, -1]\n\n\n\ndef pk(ref, hyp, k=None, boundary='1'):\n\n    if k is None:\n        k = int(round(len(ref) / (ref.count(boundary) * 2.)))\n\n    err = 0\n    for i in range(len(ref)-k +1):\n        r = ref[i:i+k].count(boundary) > 0\n        h = hyp[i:i+k].count(boundary) > 0\n        if r != h:\n           err += 1\n    return err / (len(ref)-k +1.)\n\n\ndef setup_module(module):\n    from nose import SkipTest\n    try:\n        import numpy\n    except ImportError:\n        raise SkipTest(\"numpy is required for nltk.metrics.segmentation\")\n"], "nltk\\metrics\\spearman": [".py", "from __future__ import division\n\n\ndef _rank_dists(ranks1, ranks2):\n    ranks1 = dict(ranks1)\n    ranks2 = dict(ranks2)\n    for k in ranks1:\n        try:\n            yield k, ranks1[k] - ranks2[k]\n        except KeyError:\n            pass\n\n\ndef spearman_correlation(ranks1, ranks2):\n    return ((k, i) for i, k in enumerate(seq))\n\n\ndef ranks_from_scores(scores, rank_gap=1e-15):\n    \"\"\"Given a sequence of (key, score) tuples, yields each key with an\n    increasing rank, tying with previous key's rank if the difference between\n    their scores is less than rank_gap. Suitable for use as an argument to\n    ``spearman_correlation``.\n    \"\"\"\n    prev_score = None\n    rank = 0\n    for i, (key, score) in enumerate(scores):\n        try:\n            if abs(score - prev_score) > rank_gap:\n                rank = i\n        except TypeError:\n            pass\n\n        yield key, rank\n        prev_score = score\n\n"], "nltk\\metrics\\__init__": [".py", "\n\nfrom nltk.metrics.scores import          (accuracy, precision, recall, f_measure,\n                                          log_likelihood, approxrand)\nfrom nltk.metrics.confusionmatrix import ConfusionMatrix\nfrom nltk.metrics.distance        import (edit_distance, binary_distance,\n                                          jaccard_distance, masi_distance,\n                                          interval_distance, custom_distance,\n                                          presence, fractional_presence)\nfrom nltk.metrics.paice           import Paice\nfrom nltk.metrics.segmentation    import windowdiff, ghd, pk\nfrom nltk.metrics.agreement       import AnnotationTask\nfrom nltk.metrics.association     import (NgramAssocMeasures, BigramAssocMeasures,\n                                          TrigramAssocMeasures, ContingencyMeasures)\nfrom nltk.metrics.spearman        import (spearman_correlation, ranks_from_sequence,\n                                          ranks_from_scores)\nfrom nltk.metrics.aline           import align\n", 1], "nltk\\misc\\babelfish": [".py", "from __future__ import print_function\n\ndef babelize_shell():\n    print(\"Babelfish online translation service is no longer available.\")\n"], "nltk\\misc\\chomsky": [".py", "\nfrom __future__ import print_function\n\nleadins = \"\"\"To characterize a linguistic level L,\n    On the other hand,\n    This suggests that\n    It appears that\n    Furthermore,\n    We will bring evidence in favor of the following thesis:\n    To provide a constituent structure for T(Z,K),\n    From C1, it follows that\n    For any transformation which is sufficiently diversified in \\\napplication to be of any interest,\n    Analogously,\n    Clearly,\n    Note that\n    Of course,\n    Suppose, for instance, that\n    Thus\n    With this clarification,\n    Conversely,\n    We have already seen that\n    By combining adjunctions and certain deformations,\n    I suggested that these results would follow from the assumption that\n    If the position of the trace in (99c) were only relatively \\\ninaccessible to movement,\n    However, this assumption is not correct, since\n    Comparing these examples with their parasitic gap counterparts in \\\n(96) and (97), we see that\n    In the discussion of resumptive pronouns following (81),\n    So far,\n    Nevertheless,\n    For one thing,\n    Summarizing, then, we assume that\n    A consequence of the approach just outlined is that\n    Presumably,\n    On our assumptions,\n    It may be, then, that\n    It must be emphasized, once again, that\n    Let us continue to suppose that\n    Notice, incidentally, that \"\"\"\n\nsubjects = \"\"\" the notion of level of grammaticalness\n    a case of semigrammaticalness of a different sort\n    most of the methodological work in modern linguistics\n    a subset of English sentences interesting on quite independent grounds\n    the natural general principle that will subsume this case\n    an important property of these three types of EC\n    any associated supporting element\n    the appearance of parasitic gaps in domains relatively inaccessible \\\nto ordinary extraction\n    the speaker-hearer's linguistic intuition\n    the descriptive power of the base component\n    the earlier discussion of deviance\n    this analysis of a formative as a pair of sets of features\n    this selectionally introduced contextual feature\n    a descriptively adequate grammar\n    the fundamental error of regarding functional notions as categorial\n    relational information\n    the systematic use of complex symbols\n    the theory of syntactic features developed earlier\"\"\"\n\nverbs = \"\"\"can be defined in such a way as to impose\n    delimits\n    suffices to account for\n    cannot be arbitrary in\n    is not subject to\n    does not readily tolerate\n    raises serious doubts about\n    is not quite equivalent to\n    does not affect the structure of\n    may remedy and, at the same time, eliminate\n    is not to be considered in determining\n    is to be regarded as\n    is unspecified with respect to\n    is, apparently, determined by\n    is necessary to impose an interpretation on\n    appears to correlate rather closely with\n    is rather different from\"\"\"\n\nobjects = \"\"\" problems of phonemic and morphological analysis.\n    a corpus of utterance tokens upon which conformity has been defined \\\nby the paired utterance test.\n    the traditional practice of grammarians.\n    the levels of acceptability from fairly high (e.g. (99a)) to virtual \\\ngibberish (e.g. (98d)).\n    a stipulation to place the constructions into these various categories.\n    a descriptive fact.\n    a parasitic gap construction.\n    the extended c-command discussed in connection with (34).\n    the ultimate standard that determines the accuracy of any proposed grammar.\n    the system of base rules exclusive of the lexicon.\n    irrelevant intervening contexts in selectional rules.\n    nondistinctness in the sense of distinctive feature theory.\n    a general convention regarding the forms of the grammar.\n    an abstract underlying order.\n    an important distinction in language use.\n    the requirement that branching is not tolerated within the dominance \\\nscope of a complex symbol.\n    the strong generative capacity of the theory.\"\"\"\n\nimport textwrap, random\nfrom itertools import chain, islice\n\nfrom six.moves import zip\n\n\ndef generate_chomsky(times=5, line_length=72):\n    parts = []\n    for part in (leadins, subjects, verbs, objects):\n        phraselist = list(map(str.strip, part.splitlines()))\n        random.shuffle(phraselist)\n        parts.append(phraselist)\n    output = chain(*islice(zip(*parts), 0, times))\n    print(textwrap.fill(\" \".join(output), line_length))\n\nif __name__ == '__main__':\n    generate_chomsky()\n"], "nltk\\misc\\minimalset": [".py", "\nfrom collections import defaultdict\n\nclass MinimalSet(object):\n    def __init__(self, parameters=None):\n        self._targets = set()  # the contrastive information\n        self._contexts = set() # what we are controlling for\n        self._seen = defaultdict(set)  # to record what we have seen\n        self._displays = {}    # what we will display\n\n        if parameters:\n            for context, target, display in parameters:\n                self.add(context, target, display)\n\n    def add(self, context, target, display):\n        self._seen[context].add(target)\n\n        self._contexts.add(context)\n        self._targets.add(target)\n\n        self._displays[(context, target)] = display\n\n    def contexts(self, minimum=2):\n        return [c for c in self._contexts if len(self._seen[c]) >= minimum]\n\n    def display(self, context, target, default=\"\"):\n        if (context, target) in self._displays:\n            return self._displays[(context, target)]\n        else:\n            return default\n\n    def display_all(self, context):\n        result = []\n        for target in self._targets:\n            x = self.display(context, target)\n            if x: result.append(x)\n        return result\n\n    def targets(self):\n        return self._targets\n\n"], "nltk\\misc\\sort": [".py", "\nfrom __future__ import print_function, division\n\n\n\ndef selection(a):\n    count = 0\n\n    for i in range(len(a) - 1):\n        min = i\n\n        for j in range(i+1, len(a)):\n            if a[j] < a[min]:\n                min = j\n\n            count += 1\n\n        a[min],a[i] = a[i],a[min]\n\n    return count\n\n\ndef bubble(a):\n    count = 0\n    for i in range(len(a)-1):\n        for j in range(len(a)-i-1):\n            if a[j+1] < a[j]:\n                a[j],a[j+1] = a[j+1],a[j]\n                count += 1\n    return count\n\n\n\ndef _merge_lists(b, c):\n    count = 0\n    i = j = 0\n    a = []\n    while (i < len(b) and j < len(c)):\n        count += 1\n        if b[i] <= c[j]:\n            a.append(b[i])\n            i += 1\n        else:\n            a.append(c[j])\n            j += 1\n    if i == len(b):\n        a += c[j:]\n    else:\n        a += b[i:]\n    return a, count\n\ndef merge(a):\n    count = 0\n    if len(a) > 1:\n        midpoint = len(a) // 2\n        b = a[:midpoint]\n        c = a[midpoint:]\n        count_b = merge(b)\n        count_c = merge(c)\n        result, count_a = _merge_lists(b, c)\n        a[:] = result # copy the result back into a.\n        count = count_a + count_b + count_c\n    return count\n\n\ndef _partition(a, l, r):\n    p = a[l]; i = l; j = r+1\n    count = 0\n    while True:\n        while i < r:\n            i += 1\n            if a[i] >= p: break\n        while j > l:\n            j -= 1\n            if j < l or a[j] <= p: break\n        a[i],a[j] = a[j],a[i]               # swap\n        count += 1\n        if i >= j: break\n    a[i],a[j] = a[j],a[i]                   # undo last swap\n    a[l],a[j] = a[j],a[l]\n    return j, count\n\ndef _quick(a, l, r):\n    count = 0\n    if l<r:\n        s, count = _partition(a, l, r)\n        count += _quick(a, l, s-1)\n        count += _quick(a, s+1, r)\n    return count\n\ndef quick(a):\n    return _quick(a, 0, len(a)-1)\n\n\ndef demo():\n    from random import shuffle\n\n    for size in (10, 20, 50, 100, 200, 500, 1000):\n        a = list(range(size))\n\n        shuffle(a); count_selection = selection(a)\n        shuffle(a); count_bubble    = bubble(a)\n        shuffle(a); count_merge     = merge(a)\n        shuffle(a); count_quick     = quick(a)\n\n        print(((\"size=%5d:  selection=%8d,  bubble=%8d,  \"\n                \"merge=%6d,  quick=%6d\") %\n               (size, count_selection, count_bubble,\n                count_merge, count_quick)))\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\misc\\wordfinder": [".py", "\nfrom __future__ import print_function\n\nimport random\n\n\ndef revword(word):\n    if random.randint(1,2) == 1:\n        return word[::-1]\n    return word\n\ndef step(word, x, xf, y, yf, grid):\n    for i in range(len(word)):\n        if grid[xf(i)][yf(i)] != \"\" and grid[xf(i)][yf(i)] != word[i]:\n            return False\n    for i in range(len(word)):\n        grid[xf(i)][yf(i)] = word[i]\n    return True\n\ndef check(word, dir, x, y, grid, rows, cols):\n    if dir==1:\n        if x-len(word)<0 or y-len(word)<0:\n            return False\n        return step(word, x, lambda i:x-i, y, lambda i:y-i, grid)\n    elif dir==2:\n        if x-len(word)<0:\n            return False\n        return step(word, x, lambda i:x-i, y, lambda i:y, grid)\n    elif dir==3:\n        if x-len(word)<0 or y+(len(word)-1)>=cols:\n            return False\n        return step(word, x, lambda i:x-i, y, lambda i:y+i, grid)\n    elif dir==4:\n        if y-len(word)<0:\n            return False\n        return step(word, x, lambda i:x, y, lambda i:y-i, grid)\n\ndef wordfinder(words, rows=20, cols=20, attempts=50,\n               alph='ABCDEFGHIJKLMNOPQRSTUVWXYZ'):\n\n    words = sorted(words, key=len, reverse=True)\n\n    grid = []  # the letter grid\n    used = []  # the words we used\n\n    for i in range(rows):\n        grid.append([\"\"] * cols)\n\n    for word in words:\n        word = word.strip().upper()  # normalize\n        save = word                  # keep a record of the word\n        word = revword(word)\n        for attempt in range(attempts):\n            r = random.randint(0, len(word))\n            dir = random.choice([1,2,3,4])\n            x = random.randint(0,rows)\n            y = random.randint(0,cols)\n            if   dir==1: x+=r; y+=r\n            elif dir==2: x+=r\n            elif dir==3: x+=r; y-=r\n            elif dir==4: y+=r\n            if 0<=x<rows and 0<=y<cols:\n                if check(word, dir, x, y, grid, rows, cols):\n                    used.append(save)\n                    break\n\n    for i in range(rows):\n        for j in range(cols):\n            if grid[i][j] == '':\n                grid[i][j] = random.choice(alph)\n\n    return grid, used\n\ndef word_finder():\n    from nltk.corpus import words\n    wordlist = words.words()\n    random.shuffle(wordlist)\n    wordlist = wordlist[:200]\n    wordlist = [w for w in wordlist if 3 <= len(w) <= 12]\n    grid, used = wordfinder(wordlist)\n\n    print(\"Word Finder\\n\")\n    for i in range(len(grid)):\n        for j in range(len(grid[i])):\n            print(grid[i][j], end=' ')\n        print()\n    print()\n\n    for i in range(len(used)):\n        print(\"%d:\" % (i+1), used[i])\n\nif __name__ == '__main__':\n    word_finder()\n"], "nltk\\misc\\__init__": [".py", "\nfrom nltk.misc.chomsky import generate_chomsky\nfrom nltk.misc.wordfinder import word_finder\nfrom nltk.misc.minimalset import MinimalSet\nfrom nltk.misc.babelfish import babelize_shell\n", 1], "nltk\\parse\\api": [".py", "\nimport itertools\n\nfrom nltk.internals import overridden\n\nclass ParserI(object):\n    def grammar(self):\n        raise NotImplementedError()\n\n    def parse(self, sent, *args, **kwargs):\n        if overridden(self.parse_sents):\n            return next(self.parse_sents([sent], *args, **kwargs))\n        elif overridden(self.parse_one):\n            return (tree for tree in [self.parse_one(sent, *args, **kwargs)] if tree is not None)\n        elif overridden(self.parse_all):\n            return iter(self.parse_all(sent, *args, **kwargs))\n        else:\n            raise NotImplementedError()\n\n    def parse_sents(self, sents, *args, **kwargs):\n        return (self.parse(sent, *args, **kwargs) for sent in sents)\n\n    def parse_all(self, sent, *args, **kwargs):\n        return list(self.parse(sent, *args, **kwargs))\n\n    def parse_one(self, sent, *args, **kwargs):\n        return next(self.parse(sent, *args, **kwargs), None)\n"], "nltk\\parse\\bllip": [".py", "\nfrom __future__ import print_function\n\nfrom nltk.parse.api import ParserI\nfrom nltk.tree import Tree\n\n\n__all__ = ['BllipParser']\n\ntry:\n    from bllipparser import RerankingParser\n    from bllipparser.RerankingParser import get_unified_model_parameters\n\n    def _ensure_bllip_import_or_error():\n        pass\nexcept ImportError as ie:\n    def _ensure_bllip_import_or_error(ie=ie):\n        raise ImportError(\"Couldn't import bllipparser module: %s\" % ie)\n\ndef _ensure_ascii(words):\n    try:\n        for i, word in enumerate(words):\n            word.decode('ascii')\n    except UnicodeDecodeError:\n        raise ValueError(\"Token %d (%r) is non-ASCII. BLLIP Parser \"\n                         \"currently doesn't support non-ASCII inputs.\" %\n                         (i, word))\n\ndef _scored_parse_to_nltk_tree(scored_parse):\n    return Tree.fromstring(str(scored_parse.ptb_parse))\n\nclass BllipParser(ParserI):\n    def __init__(self, parser_model=None, reranker_features=None,\n                 reranker_weights=None, parser_options=None,\n                 reranker_options=None):\n        _ensure_bllip_import_or_error()\n\n        parser_options = parser_options or {}\n        reranker_options = reranker_options or {}\n\n        self.rrp = RerankingParser()\n        self.rrp.load_parser_model(parser_model, **parser_options)\n        if reranker_features and reranker_weights:\n            self.rrp.load_reranker_model(features_filename=reranker_features,\n                                         weights_filename=reranker_weights,\n                                         **reranker_options)\n\n    def parse(self, sentence):\n        _ensure_ascii(sentence)\n        nbest_list = self.rrp.parse(sentence)\n        for scored_parse in nbest_list:\n            yield _scored_parse_to_nltk_tree(scored_parse)\n\n    def tagged_parse(self, word_and_tag_pairs):\n        words = []\n        tag_map = {}\n        for i, (word, tag) in enumerate(word_and_tag_pairs):\n            words.append(word)\n            if tag is not None:\n                tag_map[i] = tag\n\n        _ensure_ascii(words)\n        nbest_list = self.rrp.parse_tagged(words, tag_map)\n        for scored_parse in nbest_list:\n            yield _scored_parse_to_nltk_tree(scored_parse)\n\n    @classmethod\n    def from_unified_model_dir(this_class, model_dir, parser_options=None,\n                               reranker_options=None):\n        (parser_model_dir, reranker_features_filename,\n         reranker_weights_filename) = get_unified_model_parameters(model_dir)\n        return this_class(parser_model_dir, reranker_features_filename,\n                          reranker_weights_filename, parser_options,\n                          reranker_options)\n\ndef demo():\n\n\n    from nltk.data import find\n    model_dir = find('models/bllip_wsj_no_aux').path\n\n    print('Loading BLLIP Parsing models...')\n    bllip = BllipParser.from_unified_model_dir(model_dir)\n    print('Done.')\n\n    sentence1 = 'British left waffles on Falklands .'.split()\n    sentence2 = 'I saw the man with the telescope .'.split()\n    fail1 = '# ! ? : -'.split()\n    for sentence in (sentence1, sentence2, fail1):\n        print('Sentence: %r' % ' '.join(sentence))\n        try:\n            tree = next(bllip.parse(sentence))\n            print(tree)\n        except StopIteration:\n            print(\"(parse failed)\")\n\n    for i, parse in enumerate(bllip.parse(sentence1)):\n        print('parse %d:\\n%s' % (i, parse))\n\n    print(\"forcing 'tree' to be 'NN':\",\n          next(bllip.tagged_parse([('A', None), ('tree', 'NN')])))\n    print(\"forcing 'A' to be 'DT' and 'tree' to be 'NNP':\",\n          next(bllip.tagged_parse([('A', 'DT'), ('tree', 'NNP')])))\n    print(\"forcing 'A' to be 'NNP':\",\n          next(bllip.tagged_parse([('A', 'NNP'), ('tree', None)])))\n\ndef setup_module(module):\n    from nose import SkipTest\n\n    try:\n        _ensure_bllip_import_or_error()\n    except ImportError:\n        raise SkipTest('doctests from nltk.parse.bllip are skipped because '\n                       'the bllipparser module is not installed')\n\n\n"], "nltk\\parse\\chart": [".py", "\nfrom __future__ import print_function, division, unicode_literals\n\nimport itertools\nimport re\nimport warnings\nfrom functools import total_ordering\n\nfrom six.moves import range\n\nfrom nltk.tree import Tree\nfrom nltk.grammar import PCFG, is_nonterminal, is_terminal\nfrom nltk.util import OrderedDict\nfrom nltk.internals import raise_unorderable_types\nfrom nltk.compat import python_2_unicode_compatible, unicode_repr\n\nfrom nltk.parse.api import ParserI\n\n\n\n@total_ordering\nclass EdgeI(object):\n    def __init__(self):\n        if self.__class__ == EdgeI:\n            raise TypeError('Edge is an abstract interface')\n\n\n    def span(self):\n        raise NotImplementedError()\n\n    def start(self):\n        raise NotImplementedError()\n\n    def end(self):\n        raise NotImplementedError()\n\n    def length(self):\n        raise NotImplementedError()\n\n\n    def lhs(self):\n        raise NotImplementedError()\n\n\n    def rhs(self):\n        raise NotImplementedError()\n\n    def dot(self):\n        raise NotImplementedError()\n\n    def nextsym(self):\n        raise NotImplementedError()\n\n    def is_complete(self):\n        raise NotImplementedError()\n\n    def is_incomplete(self):\n        raise NotImplementedError()\n\n\n    def __eq__(self, other):\n        return (self.__class__ is other.__class__ and\n                self._comparison_key == other._comparison_key)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __lt__(self, other):\n        if not isinstance(other, EdgeI):\n            raise_unorderable_types(\"<\", self, other)\n        if self.__class__ is other.__class__:\n            return self._comparison_key < other._comparison_key\n        else:\n            return self.__class__.__name__ < other.__class__.__name__\n\n    def __hash__(self):\n        try:\n            return self._hash\n        except AttributeError:\n            self._hash = hash(self._comparison_key)\n            return self._hash\n\n\n@python_2_unicode_compatible\nclass TreeEdge(EdgeI):\n    def __init__(self, span, lhs, rhs, dot=0):\n        self._span = span\n        self._lhs = lhs\n        rhs = tuple(rhs)\n        self._rhs = rhs\n        self._dot = dot\n        self._comparison_key = (span, lhs, rhs, dot)\n\n    @staticmethod\n    def from_production(production, index):\n        return TreeEdge(span=(index, index), lhs=production.lhs(),\n                        rhs=production.rhs(), dot=0)\n\n    def move_dot_forward(self, new_end):\n        return TreeEdge(span=(self._span[0], new_end),\n                        lhs=self._lhs, rhs=self._rhs,\n                        dot=self._dot+1)\n\n    def lhs(self): return self._lhs\n    def span(self): return self._span\n    def start(self): return self._span[0]\n    def end(self): return self._span[1]\n    def length(self): return self._span[1] - self._span[0]\n    def rhs(self): return self._rhs\n    def dot(self): return self._dot\n    def is_complete(self): return self._dot == len(self._rhs)\n    def is_incomplete(self): return self._dot != len(self._rhs)\n    def nextsym(self):\n        if self._dot >= len(self._rhs): return None\n        else: return self._rhs[self._dot]\n\n    def __str__(self):\n        str = '[%s:%s] ' % (self._span[0], self._span[1])\n        str += '%-2r ->' % (self._lhs,)\n\n        for i in range(len(self._rhs)):\n            if i == self._dot: str += ' *'\n            str += ' %s' % unicode_repr(self._rhs[i])\n        if len(self._rhs) == self._dot: str += ' *'\n        return str\n\n    def __repr__(self):\n        return '[Edge: %s]' % self\n\n\n@python_2_unicode_compatible\nclass LeafEdge(EdgeI):\n    def __init__(self, leaf, index):\n        self._leaf = leaf\n        self._index = index\n        self._comparison_key = (leaf, index)\n\n    def lhs(self): return self._leaf\n    def span(self): return (self._index, self._index+1)\n    def start(self): return self._index\n    def end(self): return self._index+1\n    def length(self): return 1\n    def rhs(self): return ()\n    def dot(self): return 0\n    def is_complete(self): return True\n    def is_incomplete(self): return False\n    def nextsym(self): return None\n\n    def __str__(self):\n        return '[%s:%s] %s' % (self._index, self._index+1, unicode_repr(self._leaf))\n    def __repr__(self):\n        return '[Edge: %s]' % (self)\n\n\nclass Chart(object):\n    def __init__(self, tokens):\n        self._tokens = tuple(tokens)\n        self._num_leaves = len(self._tokens)\n\n        self.initialize()\n\n    def initialize(self):\n        self._edges = []\n\n        self._edge_to_cpls = {}\n\n        self._indexes = {}\n\n\n    def num_leaves(self):\n        return self._num_leaves\n\n    def leaf(self, index):\n        return self._tokens[index]\n\n    def leaves(self):\n        return self._tokens\n\n\n    def edges(self):\n        return self._edges[:]\n\n    def iteredges(self):\n        return iter(self._edges)\n\n    __iter__ = iteredges\n\n    def num_edges(self):\n        return len(self._edge_to_cpls)\n\n    def select(self, **restrictions):\n        if restrictions=={}: return iter(self._edges)\n\n        restr_keys = sorted(restrictions.keys())\n        restr_keys = tuple(restr_keys)\n\n        if restr_keys not in self._indexes:\n            self._add_index(restr_keys)\n\n        vals = tuple(restrictions[key] for key in restr_keys)\n        return iter(self._indexes[restr_keys].get(vals, []))\n\n    def _add_index(self, restr_keys):\n        for key in restr_keys:\n            if not hasattr(EdgeI, key):\n                raise ValueError('Bad restriction: %s' % key)\n\n        index = self._indexes[restr_keys] = {}\n\n        for edge in self._edges:\n            vals = tuple(getattr(edge, key)() for key in restr_keys)\n            index.setdefault(vals, []).append(edge)\n\n    def _register_with_indexes(self, edge):\n        for (restr_keys, index) in self._indexes.items():\n            vals = tuple(getattr(edge, key)() for key in restr_keys)\n            index.setdefault(vals, []).append(edge)\n\n\n    def insert_with_backpointer(self, new_edge, previous_edge, child_edge):\n        cpls = self.child_pointer_lists(previous_edge)\n        new_cpls = [cpl+(child_edge,) for cpl in cpls]\n        return self.insert(new_edge, *new_cpls)\n\n    def insert(self, edge, *child_pointer_lists):\n        if edge not in self._edge_to_cpls:\n            self._append_edge(edge)\n            self._register_with_indexes(edge)\n\n        cpls = self._edge_to_cpls.setdefault(edge, OrderedDict())\n        chart_was_modified = False\n        for child_pointer_list in child_pointer_lists:\n            child_pointer_list = tuple(child_pointer_list)\n            if child_pointer_list not in cpls:\n                cpls[child_pointer_list] = True\n                chart_was_modified = True\n        return chart_was_modified\n\n    def _append_edge(self, edge):\n        self._edges.append(edge)\n\n\n    def parses(self, root, tree_class=Tree):\n        for edge in self.select(start=0, end=self._num_leaves, lhs=root):\n            for tree in self.trees(edge, tree_class=tree_class, complete=True):\n                yield tree\n\n    def trees(self, edge, tree_class=Tree, complete=False):\n        return iter(self._trees(edge, complete, memo={}, tree_class=tree_class))\n\n    def _trees(self, edge, complete, memo, tree_class):\n        if edge in memo:\n            return memo[edge]\n\n        if complete and edge.is_incomplete():\n            return []\n\n        if isinstance(edge, LeafEdge):\n            leaf = self._tokens[edge.start()]\n            memo[edge] = [leaf]\n            return [leaf]\n\n        memo[edge] = []\n        trees = []\n        lhs = edge.lhs().symbol()\n\n        for cpl in self.child_pointer_lists(edge):\n            child_choices = [self._trees(cp, complete, memo, tree_class)\n                             for cp in cpl]\n\n            for children in itertools.product(*child_choices):\n                trees.append(tree_class(lhs, children))\n\n        if edge.is_incomplete():\n            unexpanded = [tree_class(elt,[])\n                          for elt in edge.rhs()[edge.dot():]]\n            for tree in trees:\n                tree.extend(unexpanded)\n\n        memo[edge] = trees\n\n        return trees\n\n    def child_pointer_lists(self, edge):\n        return self._edge_to_cpls.get(edge, {}).keys()\n\n    def pretty_format_edge(self, edge, width=None):\n        if width is None: width = 50 // (self.num_leaves()+1)\n        (start, end) = (edge.start(), edge.end())\n\n        str = '|' + ('.'+' '*(width-1))*start\n\n        if start == end:\n            if edge.is_complete(): str += '#'\n            else: str += '>'\n\n        elif edge.is_complete() and edge.span() == (0,self._num_leaves):\n            str += '['+('='*width)*(end-start-1) + '='*(width-1)+']'\n        elif edge.is_complete():\n            str += '['+('-'*width)*(end-start-1) + '-'*(width-1)+']'\n        else:\n            str += '['+('-'*width)*(end-start-1) + '-'*(width-1)+'>'\n\n        str += (' '*(width-1)+'.')*(self._num_leaves-end)\n        return str + '| %s' % edge\n\n    def pretty_format_leaves(self, width=None):\n        if width is None: width = 50 // (self.num_leaves()+1)\n\n        if self._tokens is not None and width>1:\n            header = '|.'\n            for tok in self._tokens:\n                header += tok[:width-1].center(width-1)+'.'\n            header += '|'\n        else:\n            header = ''\n\n        return header\n\n    def pretty_format(self, width=None):\n        if width is None: width = 50 // (self.num_leaves()+1)\n        edges = sorted([(e.length(), e.start(), e) for e in self])\n        edges = [e for (_,_,e) in edges]\n\n        return (self.pretty_format_leaves(width) + '\\n' +\n                '\\n'.join(self.pretty_format_edge(edge, width) for edge in edges))\n\n\n    def dot_digraph(self):\n        s = 'digraph nltk_chart {\\n'\n        s += '  rankdir=LR;\\n'\n        s += '  node [height=0.1,width=0.1];\\n'\n        s += '  node [style=filled, color=\"lightgray\"];\\n'\n\n        for y in range(self.num_edges(), -1, -1):\n            if y == 0:\n                s += '  node [style=filled, color=\"black\"];\\n'\n            for x in range(self.num_leaves()+1):\n                if y == 0 or (x <= self._edges[y-1].start() or\n                              x >= self._edges[y-1].end()):\n                    s += '  %04d.%04d [label=\"\"];\\n' % (x,y)\n\n        s += '  x [style=invis]; x->0000.0000 [style=invis];\\n'\n\n        for x in range(self.num_leaves()+1):\n            s += '  {rank=same;'\n            for y in range(self.num_edges()+1):\n                if y == 0 or (x <= self._edges[y-1].start() or\n                              x >= self._edges[y-1].end()):\n                    s += ' %04d.%04d' % (x,y)\n            s += '}\\n'\n\n        s += '  edge [style=invis, weight=100];\\n'\n        s += '  node [shape=plaintext]\\n'\n        s += '  0000.0000'\n        for x in range(self.num_leaves()):\n            s += '->%s->%04d.0000' % (self.leaf(x), x+1)\n        s += ';\\n\\n'\n\n        s += '  edge [style=solid, weight=1];\\n'\n        for y, edge in enumerate(self):\n            for x in range(edge.start()):\n                s += ('  %04d.%04d -> %04d.%04d [style=\"invis\"];\\n' %\n                      (x, y+1, x+1, y+1))\n            s += ('  %04d.%04d -> %04d.%04d [label=\"%s\"];\\n' %\n                  (edge.start(), y+1, edge.end(), y+1, edge))\n            for x in range(edge.end(), self.num_leaves()):\n                s += ('  %04d.%04d -> %04d.%04d [style=\"invis\"];\\n' %\n                      (x, y+1, x+1, y+1))\n        s += '}\\n'\n        return s\n\n\nclass ChartRuleI(object):\n    def apply(self, chart, grammar, *edges):\n        raise NotImplementedError()\n\n    def apply_everywhere(self, chart, grammar):\n        raise NotImplementedError()\n\n\n@python_2_unicode_compatible\nclass AbstractChartRule(ChartRuleI):\n\n    def apply(self, chart, grammar, *edges):\n        raise NotImplementedError()\n\n    def apply_everywhere(self, chart, grammar):\n        if self.NUM_EDGES == 0:\n            for new_edge in self.apply(chart, grammar):\n                yield new_edge\n\n        elif self.NUM_EDGES == 1:\n            for e1 in chart:\n                for new_edge in self.apply(chart, grammar, e1):\n                    yield new_edge\n\n        elif self.NUM_EDGES == 2:\n            for e1 in chart:\n                for e2 in chart:\n                    for new_edge in self.apply(chart, grammar, e1, e2):\n                        yield new_edge\n\n        elif self.NUM_EDGES == 3:\n            for e1 in chart:\n                for e2 in chart:\n                    for e3 in chart:\n                        for new_edge in self.apply(chart,grammar,e1,e2,e3):\n                            yield new_edge\n\n        else:\n            raise AssertionError('NUM_EDGES>3 is not currently supported')\n\n    def __str__(self):\n        return re.sub('([a-z])([A-Z])', r'\\1 \\2', self.__class__.__name__)\n\n\nclass FundamentalRule(AbstractChartRule):\n    NUM_EDGES = 2\n    def apply(self, chart, grammar, left_edge, right_edge):\n        if not (left_edge.is_incomplete() and\n                right_edge.is_complete() and\n                left_edge.end() == right_edge.start() and\n                left_edge.nextsym() == right_edge.lhs()):\n            return\n\n        new_edge = left_edge.move_dot_forward(right_edge.end())\n\n        if chart.insert_with_backpointer(new_edge, left_edge, right_edge):\n            yield new_edge\n\nclass SingleEdgeFundamentalRule(FundamentalRule):\n    NUM_EDGES = 1\n\n    def apply(self, chart, grammar, edge):\n        if edge.is_incomplete():\n            for new_edge in self._apply_incomplete(chart, grammar, edge):\n                yield new_edge\n        else:\n            for new_edge in self._apply_complete(chart, grammar, edge):\n                yield new_edge\n\n    def _apply_complete(self, chart, grammar, right_edge):\n        for left_edge in chart.select(end=right_edge.start(),\n                                      is_complete=False,\n                                      nextsym=right_edge.lhs()):\n            new_edge = left_edge.move_dot_forward(right_edge.end())\n            if chart.insert_with_backpointer(new_edge, left_edge, right_edge):\n                yield new_edge\n\n    def _apply_incomplete(self, chart, grammar, left_edge):\n        for right_edge in chart.select(start=left_edge.end(),\n                                       is_complete=True,\n                                       lhs=left_edge.nextsym()):\n            new_edge = left_edge.move_dot_forward(right_edge.end())\n            if chart.insert_with_backpointer(new_edge, left_edge, right_edge):\n                yield new_edge\n\n\nclass LeafInitRule(AbstractChartRule):\n    NUM_EDGES=0\n    def apply(self, chart, grammar):\n        for index in range(chart.num_leaves()):\n            new_edge = LeafEdge(chart.leaf(index), index)\n            if chart.insert(new_edge, ()):\n                yield new_edge\n\n\nclass TopDownInitRule(AbstractChartRule):\n    NUM_EDGES = 0\n    def apply(self, chart, grammar):\n        for prod in grammar.productions(lhs=grammar.start()):\n            new_edge = TreeEdge.from_production(prod, 0)\n            if chart.insert(new_edge, ()):\n                yield new_edge\n\nclass TopDownPredictRule(AbstractChartRule):\n    NUM_EDGES = 1\n    def apply(self, chart, grammar, edge):\n        if edge.is_complete(): return\n        for prod in grammar.productions(lhs=edge.nextsym()):\n            new_edge = TreeEdge.from_production(prod, edge.end())\n            if chart.insert(new_edge, ()):\n                yield new_edge\n\nclass CachedTopDownPredictRule(TopDownPredictRule):\n    def __init__(self):\n        TopDownPredictRule.__init__(self)\n        self._done = {}\n\n    def apply(self, chart, grammar, edge):\n        if edge.is_complete(): return\n        nextsym, index = edge.nextsym(), edge.end()\n        if not is_nonterminal(nextsym): return\n\n        done = self._done.get((nextsym, index), (None,None))\n        if done[0] is chart and done[1] is grammar: return\n\n        for prod in grammar.productions(lhs=nextsym):\n            if prod.rhs():\n                first = prod.rhs()[0]\n                if is_terminal(first):\n                    if index >= chart.num_leaves() or first != chart.leaf(index): continue\n\n            new_edge = TreeEdge.from_production(prod, index)\n            if chart.insert(new_edge, ()):\n                yield new_edge\n\n        self._done[nextsym, index] = (chart, grammar)\n\n\nclass BottomUpPredictRule(AbstractChartRule):\n    NUM_EDGES = 1\n    def apply(self, chart, grammar, edge):\n        if edge.is_incomplete(): return\n        for prod in grammar.productions(rhs=edge.lhs()):\n            new_edge = TreeEdge.from_production(prod, edge.start())\n            if chart.insert(new_edge, ()):\n                yield new_edge\n\nclass BottomUpPredictCombineRule(BottomUpPredictRule):\n    NUM_EDGES = 1\n    def apply(self, chart, grammar, edge):\n        if edge.is_incomplete(): return\n        for prod in grammar.productions(rhs=edge.lhs()):\n            new_edge = TreeEdge(edge.span(), prod.lhs(), prod.rhs(), 1)\n            if chart.insert(new_edge, (edge,)):\n                yield new_edge\n\nclass EmptyPredictRule(AbstractChartRule):\n    NUM_EDGES = 0\n    def apply(self, chart, grammar):\n        for prod in grammar.productions(empty=True):\n            for index in range(chart.num_leaves() + 1):\n                new_edge = TreeEdge.from_production(prod, index)\n                if chart.insert(new_edge, ()):\n                    yield new_edge\n\n\n\nclass FilteredSingleEdgeFundamentalRule(SingleEdgeFundamentalRule):\n    def _apply_complete(self, chart, grammar, right_edge):\n        end = right_edge.end()\n        nexttoken = end < chart.num_leaves() and chart.leaf(end)\n        for left_edge in chart.select(end=right_edge.start(),\n                                      is_complete=False,\n                                      nextsym=right_edge.lhs()):\n            if _bottomup_filter(grammar, nexttoken, left_edge.rhs(), left_edge.dot()):\n                new_edge = left_edge.move_dot_forward(right_edge.end())\n                if chart.insert_with_backpointer(new_edge, left_edge, right_edge):\n                    yield new_edge\n\n    def _apply_incomplete(self, chart, grammar, left_edge):\n        for right_edge in chart.select(start=left_edge.end(),\n                                       is_complete=True,\n                                       lhs=left_edge.nextsym()):\n            end = right_edge.end()\n            nexttoken = end < chart.num_leaves() and chart.leaf(end)\n            if _bottomup_filter(grammar, nexttoken, left_edge.rhs(), left_edge.dot()):\n                new_edge = left_edge.move_dot_forward(right_edge.end())\n                if chart.insert_with_backpointer(new_edge, left_edge, right_edge):\n                    yield new_edge\n\nclass FilteredBottomUpPredictCombineRule(BottomUpPredictCombineRule):\n    def apply(self, chart, grammar, edge):\n        if edge.is_incomplete():\n            return\n\n        end = edge.end()\n        nexttoken = end < chart.num_leaves() and chart.leaf(end)\n        for prod in grammar.productions(rhs=edge.lhs()):\n            if _bottomup_filter(grammar, nexttoken, prod.rhs()):\n                new_edge = TreeEdge(edge.span(), prod.lhs(), prod.rhs(), 1)\n                if chart.insert(new_edge, (edge,)):\n                    yield new_edge\n\ndef _bottomup_filter(grammar, nexttoken, rhs, dot=0):\n    if len(rhs) <= dot + 1:\n        return True\n    _next = rhs[dot + 1]\n    if is_terminal(_next):\n        return nexttoken == _next\n    else:\n        return grammar.is_leftcorner(_next, nexttoken)\n\n\n\nTD_STRATEGY = [LeafInitRule(),\n               TopDownInitRule(),\n               CachedTopDownPredictRule(),\n               SingleEdgeFundamentalRule()]\nBU_STRATEGY = [LeafInitRule(),\n               EmptyPredictRule(),\n               BottomUpPredictRule(),\n               SingleEdgeFundamentalRule()]\nBU_LC_STRATEGY = [LeafInitRule(),\n                  EmptyPredictRule(),\n                  BottomUpPredictCombineRule(),\n                  SingleEdgeFundamentalRule()]\n\nLC_STRATEGY = [LeafInitRule(),\n               FilteredBottomUpPredictCombineRule(),\n               FilteredSingleEdgeFundamentalRule()]\n\nclass ChartParser(ParserI):\n    def __init__(self, grammar, strategy=BU_LC_STRATEGY, trace=0,\n                 trace_chart_width=50, use_agenda=True, chart_class=Chart):\n        self._grammar = grammar\n        self._strategy = strategy\n        self._trace = trace\n        self._trace_chart_width = trace_chart_width\n        self._use_agenda = use_agenda\n        self._chart_class = chart_class\n\n        self._axioms = []\n        self._inference_rules = []\n        for rule in strategy:\n            if rule.NUM_EDGES == 0:\n                self._axioms.append(rule)\n            elif rule.NUM_EDGES == 1:\n                self._inference_rules.append(rule)\n            else:\n                self._use_agenda = False\n\n    def grammar(self):\n        return self._grammar\n\n    def _trace_new_edges(self, chart, rule, new_edges, trace, edge_width):\n        if not trace: return\n        print_rule_header = trace > 1\n        for edge in new_edges:\n            if print_rule_header:\n                print('%s:' % rule)\n                print_rule_header = False\n            print(chart.pretty_format_edge(edge, edge_width))\n\n    def chart_parse(self, tokens, trace=None):\n        if trace is None: trace = self._trace\n        trace_new_edges = self._trace_new_edges\n\n        tokens = list(tokens)\n        self._grammar.check_coverage(tokens)\n        chart = self._chart_class(tokens)\n        grammar = self._grammar\n\n        trace_edge_width = self._trace_chart_width // (chart.num_leaves() + 1)\n        if trace: print(chart.pretty_format_leaves(trace_edge_width))\n\n        if self._use_agenda:\n            for axiom in self._axioms:\n                new_edges = list(axiom.apply(chart, grammar))\n                trace_new_edges(chart, axiom, new_edges, trace, trace_edge_width)\n\n            inference_rules = self._inference_rules\n            agenda = chart.edges()\n            agenda.reverse()\n            while agenda:\n                edge = agenda.pop()\n                for rule in inference_rules:\n                    new_edges = list(rule.apply(chart, grammar, edge))\n                    if trace:\n                        trace_new_edges(chart, rule, new_edges, trace, trace_edge_width)\n                    agenda += new_edges\n\n        else:\n            edges_added = True\n            while edges_added:\n                edges_added = False\n                for rule in self._strategy:\n                    new_edges = list(rule.apply_everywhere(chart, grammar))\n                    edges_added = len(new_edges)\n                    trace_new_edges(chart, rule, new_edges, trace, trace_edge_width)\n\n        return chart\n\n    def parse(self, tokens, tree_class=Tree):\n        chart = self.chart_parse(tokens)\n        return iter(chart.parses(self._grammar.start(), tree_class=tree_class))\n\nclass TopDownChartParser(ChartParser):\n    def __init__(self, grammar, **parser_args):\n        ChartParser.__init__(self, grammar, TD_STRATEGY, **parser_args)\n\nclass BottomUpChartParser(ChartParser):\n    def __init__(self, grammar, **parser_args):\n        if isinstance(grammar, PCFG):\n            warnings.warn(\"BottomUpChartParser only works for CFG, \"\n                          \"use BottomUpProbabilisticChartParser instead\",\n                          category=DeprecationWarning)\n        ChartParser.__init__(self, grammar, BU_STRATEGY, **parser_args)\n\nclass BottomUpLeftCornerChartParser(ChartParser):\n    def __init__(self, grammar, **parser_args):\n        ChartParser.__init__(self, grammar, BU_LC_STRATEGY, **parser_args)\n\nclass LeftCornerChartParser(ChartParser):\n    def __init__(self, grammar, **parser_args):\n        if not grammar.is_nonempty():\n            raise ValueError(\"LeftCornerParser only works for grammars \"\n                             \"without empty productions.\")\n        ChartParser.__init__(self, grammar, LC_STRATEGY, **parser_args)\n\n\nclass SteppingChartParser(ChartParser):\n    def __init__(self, grammar, strategy=[], trace=0):\n        self._chart = None\n        self._current_chartrule = None\n        self._restart = False\n        ChartParser.__init__(self, grammar, strategy, trace)\n\n\n    def initialize(self, tokens):\n        \"Begin parsing the given tokens.\"\n        self._chart = Chart(list(tokens))\n        self._restart = True\n\n\n    def step(self):\n        if self._chart is None:\n            raise ValueError('Parser must be initialized first')\n        while True:\n            self._restart = False\n            w = 50 // (self._chart.num_leaves()+1)\n\n            for e in self._parse():\n                if self._trace > 1: print(self._current_chartrule)\n                if self._trace > 0: print(self._chart.pretty_format_edge(e,w))\n                yield e\n                if self._restart: break\n            else:\n                yield None # No more edges.\n\n    def _parse(self):\n        chart = self._chart\n        grammar = self._grammar\n        edges_added = 1\n        while edges_added > 0:\n            edges_added = 0\n            for rule in self._strategy:\n                self._current_chartrule = rule\n                for e in rule.apply_everywhere(chart, grammar):\n                    edges_added += 1\n                    yield e\n\n\n    def strategy(self):\n        \"Return the strategy used by this parser.\"\n        return self._strategy\n\n    def grammar(self):\n        \"Return the grammar used by this parser.\"\n        return self._grammar\n\n    def chart(self):\n        \"Return the chart that is used by this parser.\"\n        return self._chart\n\n    def current_chartrule(self):\n        \"Return the chart rule used to generate the most recent edge.\"\n        return self._current_chartrule\n\n    def parses(self, tree_class=Tree):\n        \"Return the parse trees currently contained in the chart.\"\n        return self._chart.parses(self._grammar.start(), tree_class)\n\n\n    def set_strategy(self, strategy):\n        if strategy == self._strategy: return\n        self._strategy = strategy[:] # Make a copy.\n        self._restart = True\n\n    def set_grammar(self, grammar):\n        \"Change the grammar used by the parser.\"\n        if grammar is self._grammar: return\n        self._grammar = grammar\n        self._restart = True\n\n    def set_chart(self, chart):\n        \"Load a given chart into the chart parser.\"\n        if chart is self._chart: return\n        self._chart = chart\n        self._restart = True\n\n\n    def parse(self, tokens, tree_class=Tree):\n        tokens = list(tokens)\n        self._grammar.check_coverage(tokens)\n\n        self.initialize(tokens)\n\n        for e in self.step():\n            if e is None: break\n\n        return self.parses(tree_class=tree_class)\n\n\ndef demo_grammar():\n    from nltk.grammar import CFG\n    return CFG.fromstring(\"\"\"\nS  -> NP VP\nPP -> \"with\" NP\nNP -> NP PP\nVP -> VP PP\nVP -> Verb NP\nVP -> Verb\nNP -> Det Noun\nNP -> \"John\"\nNP -> \"I\"\nDet -> \"the\"\nDet -> \"my\"\nDet -> \"a\"\nNoun -> \"dog\"\nNoun -> \"cookie\"\nVerb -> \"ate\"\nVerb -> \"saw\"\nPrep -> \"with\"\nPrep -> \"under\"\n\"\"\")\n\ndef demo(choice=None,\n         print_times=True, print_grammar=False,\n         print_trees=True, trace=2,\n         sent='I saw John with a dog with my cookie', numparses=5):\n    \"\"\"\n    A demonstration of the chart parsers.\n    \"\"\"\n    import sys, time\n    from nltk import nonterminals, Production, CFG\n\n    grammar = demo_grammar()\n    if print_grammar:\n        print(\"* Grammar\")\n        print(grammar)\n\n    print(\"* Sentence:\")\n    print(sent)\n    tokens = sent.split()\n    print(tokens)\n    print()\n\n    if choice is None:\n        print('  1: Top-down chart parser')\n        print('  2: Bottom-up chart parser')\n        print('  3: Bottom-up left-corner chart parser')\n        print('  4: Left-corner chart parser with bottom-up filter')\n        print('  5: Stepping chart parser (alternating top-down & bottom-up)')\n        print('  6: All parsers')\n        print('\\nWhich parser (1-6)? ', end=' ')\n        choice = sys.stdin.readline().strip()\n        print()\n\n    choice = str(choice)\n    if choice not in \"123456\":\n        print('Bad parser number')\n        return\n\n    times = {}\n\n    strategies = {'1': ('Top-down', TD_STRATEGY),\n                  '2': ('Bottom-up', BU_STRATEGY),\n                  '3': ('Bottom-up left-corner', BU_LC_STRATEGY),\n                  '4': ('Filtered left-corner', LC_STRATEGY)}\n    choices = []\n    if choice in strategies: choices = [choice]\n    if choice=='6': choices = \"1234\"\n\n    for strategy in choices:\n        print(\"* Strategy: \" + strategies[strategy][0])\n        print()\n        cp = ChartParser(grammar, strategies[strategy][1], trace=trace)\n        t = time.time()\n        chart = cp.chart_parse(tokens)\n        parses = list(chart.parses(grammar.start()))\n\n        times[strategies[strategy][0]] = time.time()-t\n        print(\"Nr edges in chart:\", len(chart.edges()))\n        if numparses:\n            assert len(parses)==numparses, 'Not all parses found'\n        if print_trees:\n            for tree in parses: print(tree)\n        else:\n            print(\"Nr trees:\", len(parses))\n        print()\n\n    if choice in \"56\":\n        print(\"* Strategy: Stepping (top-down vs bottom-up)\")\n        print()\n        t = time.time()\n        cp = SteppingChartParser(grammar, trace=trace)\n        cp.initialize(tokens)\n        for i in range(5):\n            print('*** SWITCH TO TOP DOWN')\n            cp.set_strategy(TD_STRATEGY)\n            for j, e in enumerate(cp.step()):\n                if j>20 or e is None: break\n            print('*** SWITCH TO BOTTOM UP')\n            cp.set_strategy(BU_STRATEGY)\n            for j, e in enumerate(cp.step()):\n                if j>20 or e is None: break\n        times['Stepping'] = time.time()-t\n        print(\"Nr edges in chart:\", len(cp.chart().edges()))\n        if numparses:\n            assert len(list(cp.parses()))==numparses, 'Not all parses found'\n        if print_trees:\n            for tree in cp.parses(): print(tree)\n        else:\n            print(\"Nr trees:\", len(list(cp.parses())))\n        print()\n\n    if not (print_times and times): return\n    print(\"* Parsing times\")\n    print()\n    maxlen = max(len(key) for key in times)\n    format = '%' + repr(maxlen) + 's parser: %6.3fsec'\n    times_items = times.items()\n    for (parser, t) in sorted(times_items, key=lambda a:a[1]):\n        print(format % (parser, t))\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\parse\\corenlp": [".py", "\nfrom __future__ import unicode_literals\n\nimport re\nimport json\nimport time\nimport socket\n\nfrom nltk.internals import find_jar_iter, config_java, java, _java_options\n\nfrom nltk.tag.api import TaggerI\nfrom nltk.parse.api import ParserI\nfrom nltk.tokenize.api import TokenizerI\nfrom nltk.parse.dependencygraph import DependencyGraph\nfrom nltk.tree import Tree\n\n_stanford_url = 'http://stanfordnlp.github.io/CoreNLP/'\n\n\nclass CoreNLPServerError(EnvironmentError):\n\n\ndef try_port(port=0):\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.bind(('', port))\n\n    p = sock.getsockname()[1]\n    sock.close()\n\n    return p\n\n\nclass CoreNLPServer(object):\n\n    _MODEL_JAR_PATTERN = r'stanford-corenlp-(\\d+)\\.(\\d+)\\.(\\d+)-models\\.jar'\n    _JAR = r'stanford-corenlp-(\\d+)\\.(\\d+)\\.(\\d+)\\.jar'\n\n    def __init__(\n        self, path_to_jar=None, path_to_models_jar=None, verbose=False,\n        java_options=None, corenlp_options=None, port=None,\n    ):\n\n        if corenlp_options is None:\n            corenlp_options = [\n                '-preload', 'tokenize,ssplit,pos,lemma,parse,depparse',\n            ]\n\n        jars = list(find_jar_iter(\n            self._JAR,\n            path_to_jar,\n            env_vars=('CORENLP', ),\n            searchpath=(),\n            url=_stanford_url,\n            verbose=verbose,\n            is_regex=True,\n        ))\n\n        stanford_jar = max(\n            jars,\n            key=lambda model_name: re.match(self._JAR, model_name)\n        )\n\n        if port is None:\n            try:\n                port = try_port(9000)\n            except socket.error:\n                port = try_port()\n                corenlp_options.append(str(port))\n        else:\n            try_port(port)\n\n        self.url = 'http://localhost:{}'.format(port)\n\n        model_jar = max(\n            find_jar_iter(\n                self._MODEL_JAR_PATTERN,\n                path_to_models_jar,\n                env_vars=('CORENLP_MODELS', ),\n                searchpath=(),\n                url=_stanford_url,\n                verbose=verbose,\n                is_regex=True,\n            ),\n            key=lambda model_name: re.match(self._MODEL_JAR_PATTERN, model_name)\n        )\n\n        self.verbose = verbose\n\n        self._classpath = stanford_jar, model_jar\n\n        self.corenlp_options = corenlp_options\n        self.java_options = java_options or ['-mx2g']\n\n    def start(self):\n        import requests\n\n        cmd = ['edu.stanford.nlp.pipeline.StanfordCoreNLPServer']\n\n        if self.corenlp_options:\n            cmd.extend(self.corenlp_options)\n\n        default_options = ' '.join(_java_options)\n        config_java(options=self.java_options, verbose=self.verbose)\n\n        try:\n            self.popen = java(\n                cmd,\n                classpath=self._classpath,\n                blocking=False,\n                stdout='pipe',\n                stderr='pipe',\n            )\n        finally:\n            config_java(options=default_options, verbose=self.verbose)\n\n        returncode = self.popen.poll()\n        if returncode is not None:\n            _, stderrdata = self.popen.communicate()\n            raise CoreNLPServerError(\n                returncode,\n                'Could not start the server. '\n                'The error was: {}'.format(stderrdata.decode('ascii'))\n            )\n\n        for i in range(30):\n            try:\n                response = requests.get(requests.compat.urljoin(self.url, 'live'))\n            except requests.exceptions.ConnectionError:\n                time.sleep(1)\n            else:\n                if response.ok:\n                    break\n        else:\n            raise CoreNLPServerError(\n                'Could not connect to the server.'\n            )\n\n        for i in range(60):\n            try:\n                response = requests.get(requests.compat.urljoin(self.url, 'ready'))\n            except requests.exceptions.ConnectionError:\n                time.sleep(1)\n            else:\n                if response.ok:\n                    break\n        else:\n            raise CoreNLPServerError(\n                'The server is not ready.'\n            )\n\n    def stop(self):\n        self.popen.terminate()\n        self.popen.wait()\n\n    def __enter__(self):\n        self.start()\n\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.stop()\n        return False\n\n\nclass GenericCoreNLPParser(ParserI, TokenizerI, TaggerI):\n\n    def __init__(self, url='http://localhost:9000', encoding='utf8', tagtype=None):\n        import requests\n\n        self.url = url\n        self.encoding = encoding\n\n        if tagtype not in ['pos', 'ner', None]:\n            raise ValueError(\"tagtype must be either 'pos', 'ner' or None\")\n            \n        self.tagtype = tagtype\n\n        self.session = requests.Session()\n\n    def parse_sents(self, sentences, *args, **kwargs):\n        sentences = (' '.join(words) for words in sentences)\n        return self.raw_parse_sents(sentences, *args, **kwargs)\n\n    def raw_parse(self, sentence, properties=None, *args, **kwargs):\n        default_properties = {\n            'tokenize.whitespace': 'false',\n        }\n        default_properties.update(properties or {})\n\n        return next(\n            self.raw_parse_sents(\n                [sentence],\n                properties=default_properties,\n                *args,\n                **kwargs\n            )\n        )\n\n    def api_call(self, data, properties=None):\n        default_properties = {\n            'outputFormat': 'json',\n            'annotators': 'tokenize,pos,lemma,ssplit,{parser_annotator}'.format(\n                parser_annotator=self.parser_annotator,\n            ),\n        }\n\n        default_properties.update(properties or {})\n\n        response = self.session.post(\n            self.url,\n            params={\n                'properties': json.dumps(default_properties),\n            },\n            data=data.encode(self.encoding),\n            timeout=60,\n        )\n\n        response.raise_for_status()\n\n        return response.json()\n\n    def raw_parse_sents(\n        self,\n        sentences,\n        verbose=False,\n        properties=None,\n        *args,\n        **kwargs\n    ):\n        default_properties = {\n            'ssplit.ssplit.eolonly': 'true',\n        }\n\n        default_properties.update(properties or {})\n\n        parsed_data = self.api_call('\\n'.join(sentences), properties=default_properties)\n        for parsed_sent in parsed_data['sentences']:\n            tree = self.make_tree(parsed_sent)\n            yield iter([tree])\n\n\n    def parse_text(self, text, *args, **kwargs):\n        parsed_data = self.api_call(text, *args, **kwargs)\n\n        for parse in parsed_data['sentences']:\n            yield self.make_tree(parse)\n\n    def tokenize(self, text, properties=None):\n        default_properties = {\n            'annotators': 'tokenize,ssplit',\n\n        }\n\n        default_properties.update(properties or {})\n\n        result = self.api_call(text, properties=default_properties)\n\n        for sentence in result['sentences']:\n            for token in sentence['tokens']:\n                yield token['originalText'] or token['word']\n\n    def tag_sents(self, sentences):\n        sentences = (' '.join(words) for words in sentences)\n        return [sentences[0] for sentences in self.raw_tag_sents(sentences)]\n\n    def tag(self, sentence):\n        return self.tag_sents([sentence])[0]\n\n    def raw_tag_sents(self, sentences):\n        default_properties = {'ssplit.isOneSentence': 'true',\n                              'annotators': 'tokenize,ssplit,' }\n                              \n        assert self.tagtype in ['pos', 'ner']\n        default_properties['annotators'] += self.tagtype\n        for sentence in sentences:\n            tagged_data = self.api_call(sentence, properties=default_properties)\n            yield [[(token['word'], token[self.tagtype]) for token in tagged_sentence['tokens']]\n                    for tagged_sentence in tagged_data['sentences']]\n\nclass CoreNLPParser(GenericCoreNLPParser):\n\n    _OUTPUT_FORMAT = 'penn'\n    parser_annotator = 'parse'\n\n    def make_tree(self, result):\n        return Tree.fromstring(result['parse'])\n\n\nclass CoreNLPDependencyParser(GenericCoreNLPParser):\n\n    _OUTPUT_FORMAT = 'conll2007'\n    parser_annotator = 'depparse'\n\n    def make_tree(self, result):\n\n        return DependencyGraph(\n            (\n                ' '.join(n_items[1:])  # NLTK expects an iterable of strings...\n                for n_items in sorted(transform(result))\n            ),\n            cell_separator=' ',  # To make sure that a non-breaking space is kept inside of a token.\n        )\n\n\ndef transform(sentence):\n    for dependency in sentence['basicDependencies']:\n\n        dependent_index = dependency['dependent']\n        token = sentence['tokens'][dependent_index - 1]\n\n        yield (\n            dependent_index,\n            '_',\n            token['word'],\n            token['lemma'],\n            token['pos'],\n            token['pos'],\n            '_',\n            str(dependency['governor']),\n            dependency['dep'],\n            '_',\n            '_',\n        )\n\n\ndef setup_module(module):\n    from nose import SkipTest\n    raise SkipTest('Skipping all CoreNLP tests.')\n    global server\n    \n    try:\n        server = CoreNLPServer(port=9000)\n    except LookupError as e:\n        raise SkipTest('Could not instantiate CoreNLPServer.')\n\n    try:\n        server.start()\n    except CoreNLPServerError as e:\n        raise SkipTest(\n            'Skipping CoreNLP tests because the server could not be started. '\n            'Make sure that the 9000 port is free. '\n            '{}'.format(e.strerror)\n        )\n\n\ndef teardown_module(module):\n    return\n    server.stop()\n"], "nltk\\parse\\dependencygraph": [".py", "\nfrom __future__ import print_function, unicode_literals\n\nfrom collections import defaultdict\nfrom itertools import chain\nfrom pprint import pformat\nimport subprocess\nimport warnings\n\nfrom six import string_types\n\nfrom nltk.tree import Tree\nfrom nltk.compat import python_2_unicode_compatible\n\n\n\n\n@python_2_unicode_compatible\nclass DependencyGraph(object):\n\n    def __init__(self, tree_str=None, cell_extractor=None, zero_based=False, cell_separator=None, top_relation_label='ROOT'):\n        self.nodes = defaultdict(lambda:  {'address': None,\n                                           'word': None,\n                                           'lemma': None,\n                                           'ctag': None,\n                                           'tag': None,\n                                           'feats': None,\n                                           'head': None,\n                                           'deps': defaultdict(list),\n                                           'rel': None,\n                                           })\n\n        self.nodes[0].update(\n            {\n                'ctag': 'TOP',\n                'tag': 'TOP',\n                'address': 0,\n            }\n        )\n\n        self.root = None\n\n        if tree_str:\n            self._parse(\n                tree_str,\n                cell_extractor=cell_extractor,\n                zero_based=zero_based,\n                cell_separator=cell_separator,\n                top_relation_label=top_relation_label,\n            )\n\n    def remove_by_address(self, address):\n        del self.nodes[address]\n\n    def redirect_arcs(self, originals, redirect):\n        for node in self.nodes.values():\n            new_deps = []\n            for dep in node['deps']:\n                if dep in originals:\n                    new_deps.append(redirect)\n                else:\n                    new_deps.append(dep)\n            node['deps'] = new_deps\n\n    def add_arc(self, head_address, mod_address):\n        relation = self.nodes[mod_address]['rel']\n        self.nodes[head_address]['deps'].setdefault(relation, [])\n        self.nodes[head_address]['deps'][relation].append(mod_address)\n\n\n    def connect_graph(self):\n        for node1 in self.nodes.values():\n            for node2 in self.nodes.values():\n                if node1['address'] != node2['address'] and node2['rel'] != 'TOP':\n                    relation = node2['rel']\n                    node1['deps'].setdefault(relation, [])\n                    node1['deps'][relation].append(node2['address'])\n\n    def get_by_address(self, node_address):\n        return self.nodes[node_address]\n\n    def contains_address(self, node_address):\n        return node_address in self.nodes\n\n    def to_dot(self):\n        s = 'digraph G{\\n'\n        s += 'edge [dir=forward]\\n'\n        s += 'node [shape=plaintext]\\n'\n\n        for node in sorted(self.nodes.values(), key=lambda v: v['address']):\n            s += '\\n%s [label=\"%s (%s)\"]' % (node['address'], node['address'], node['word'])\n            for rel, deps in node['deps'].items():\n                for dep in deps:\n                    if rel is not None:\n                        s += '\\n%s -> %s [label=\"%s\"]' % (node['address'], dep, rel)\n                    else:\n                        s += '\\n%s -> %s ' % (node['address'], dep)\n        s += \"\\n}\"\n\n        return s\n\n    def _repr_svg_(self):\n        dot_string = self.to_dot()\n\n        try:\n            process = subprocess.Popen(\n                ['dot', '-Tsvg'],\n                stdin=subprocess.PIPE,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                universal_newlines=True,\n            )\n        except OSError:\n            raise Exception('Cannot find the dot binary from Graphviz package')\n        out, err = process.communicate(dot_string)\n        if err:\n            raise Exception(\n                'Cannot create svg representation by running dot from string: {}'\n                ''.format(dot_string))\n        return out\n\n    def __str__(self):\n        return pformat(self.nodes)\n\n    def __repr__(self):\n        return \"<DependencyGraph with {0} nodes>\".format(len(self.nodes))\n\n    @staticmethod\n    def load(filename, zero_based=False, cell_separator=None, top_relation_label='ROOT'):\n        with open(filename) as infile:\n            return [\n                DependencyGraph(\n                    tree_str,\n                    zero_based=zero_based,\n                    cell_separator=cell_separator,\n                    top_relation_label=top_relation_label,\n                )\n                for tree_str in infile.read().split('\\n\\n')\n            ]\n\n    def left_children(self, node_index):\n        children = chain.from_iterable(self.nodes[node_index]['deps'].values())\n        index = self.nodes[node_index]['address']\n        return sum(1 for c in children if c < index)\n\n    def right_children(self, node_index):\n        children = chain.from_iterable(self.nodes[node_index]['deps'].values())\n        index = self.nodes[node_index]['address']\n        return sum(1 for c in children if c > index)\n\n    def add_node(self, node):\n        if not self.contains_address(node['address']):\n            self.nodes[node['address']].update(node)\n\n    def _parse(self, input_, cell_extractor=None, zero_based=False, cell_separator=None, top_relation_label='ROOT'):\n\n        def extract_3_cells(cells, index):\n            word, tag, head = cells\n            return index, word, word, tag, tag, '', head, ''\n\n        def extract_4_cells(cells, index):\n            word, tag, head, rel = cells\n            return index, word, word, tag, tag, '', head, rel\n\n        def extract_7_cells(cells, index):\n            line_index, word, lemma, tag, _, head, rel = cells\n            try:\n                index = int(line_index)\n            except ValueError:\n                pass\n            return index, word, lemma, tag, tag, '', head, rel\n\n        def extract_10_cells(cells, index):\n            line_index, word, lemma, ctag, tag, feats, head, rel, _, _ = cells\n            try:\n                index = int(line_index)\n            except ValueError:\n                pass\n            return index, word, lemma, ctag, tag, feats, head, rel\n\n        extractors = {\n            3: extract_3_cells,\n            4: extract_4_cells,\n            7: extract_7_cells,\n            10: extract_10_cells,\n        }\n\n        if isinstance(input_, string_types):\n            input_ = (line for line in input_.split('\\n'))\n\n        lines = (l.rstrip() for l in input_)\n        lines = (l for l in lines if l)\n\n        cell_number = None\n        for index, line in enumerate(lines, start=1):\n            cells = line.split(cell_separator)\n            if cell_number is None:\n                cell_number = len(cells)\n            else:\n                assert cell_number == len(cells)\n\n            if cell_extractor is None:\n                try:\n                    cell_extractor = extractors[cell_number]\n                except KeyError:\n                    raise ValueError(\n                        'Number of tab-delimited fields ({0}) not supported by '\n                        'CoNLL(10) or Malt-Tab(4) format'.format(cell_number)\n                    )\n\n            try:\n                index, word, lemma, ctag, tag, feats, head, rel = cell_extractor(cells, index)\n            except (TypeError, ValueError):\n                word, lemma, ctag, tag, feats, head, rel = cell_extractor(cells)\n\n            if head == '_':\n                continue\n\n            head = int(head)\n            if zero_based:\n                head += 1\n\n            self.nodes[index].update(\n                {\n                    'address': index,\n                    'word': word,\n                    'lemma': lemma,\n                    'ctag': ctag,\n                    'tag': tag,\n                    'feats': feats,\n                    'head': head,\n                    'rel': rel,\n                }\n            )\n\n            if (cell_number == 3) and (head == 0):\n                rel = top_relation_label\n            self.nodes[head]['deps'][rel].append(index)\n\n        if self.nodes[0]['deps'][top_relation_label]:\n            root_address = self.nodes[0]['deps'][top_relation_label][0]\n            self.root = self.nodes[root_address]\n            self.top_relation_label = top_relation_label\n        else:\n            warnings.warn(\n                \"The graph doesn't contain a node \"\n                \"that depends on the root element.\"\n            )\n\n    def _word(self, node, filter=True):\n        w = node['word']\n        if filter:\n            if w != ',':\n                return w\n        return w\n\n    def _tree(self, i):\n        node = self.get_by_address(i)\n        word = node['word']\n        deps = sorted(chain.from_iterable(node['deps'].values()))\n\n        if deps:\n            return Tree(word, [self._tree(dep) for dep in deps])\n        else:\n            return word\n\n    def tree(self):\n        node = self.root\n\n        word = node['word']\n        deps = sorted(chain.from_iterable(node['deps'].values()))\n        return Tree(word, [self._tree(dep) for dep in deps])\n\n    def triples(self, node=None):\n\n        if not node:\n            node = self.root\n\n        head = (node['word'], node['ctag'])\n        for i in sorted(chain.from_iterable(node['deps'].values())):\n            dep = self.get_by_address(i)\n            yield (head, dep['rel'], (dep['word'], dep['ctag']))\n            for triple in self.triples(node=dep):\n                yield triple\n\n    def _hd(self, i):\n        try:\n            return self.nodes[i]['head']\n        except IndexError:\n            return None\n\n    def _rel(self, i):\n        try:\n            return self.nodes[i]['rel']\n        except IndexError:\n            return None\n\n    def contains_cycle(self):\n        distances = {}\n\n        for node in self.nodes.values():\n            for dep in node['deps']:\n                key = tuple([node['address'], dep])\n                distances[key] = 1\n\n        for _ in self.nodes:\n            new_entries = {}\n\n            for pair1 in distances:\n                for pair2 in distances:\n                    if pair1[1] == pair2[0]:\n                        key = tuple([pair1[0], pair2[1]])\n                        new_entries[key] = distances[pair1] + distances[pair2]\n\n            for pair in new_entries:\n                distances[pair] = new_entries[pair]\n                if pair[0] == pair[1]:\n                    path = self.get_cycle_path(self.get_by_address(pair[0]), pair[0])\n                    return path\n\n        return False  # return []?\n\n    def get_cycle_path(self, curr_node, goal_node_index):\n        for dep in curr_node['deps']:\n            if dep == goal_node_index:\n                return [curr_node['address']]\n        for dep in curr_node['deps']:\n            path = self.get_cycle_path(self.get_by_address(dep), goal_node_index)\n            if len(path) > 0:\n                path.insert(0, curr_node['address'])\n                return path\n        return []\n\n    def to_conll(self, style):\n\n        if style == 3:\n            template = '{word}\\t{tag}\\t{head}\\n'\n        elif style == 4:\n            template = '{word}\\t{tag}\\t{head}\\t{rel}\\n'\n        elif style == 10:\n            template = '{i}\\t{word}\\t{lemma}\\t{ctag}\\t{tag}\\t{feats}\\t{head}\\t{rel}\\t_\\t_\\n'\n        else:\n            raise ValueError(\n                'Number of tab-delimited fields ({0}) not supported by '\n                'CoNLL(10) or Malt-Tab(4) format'.format(style)\n            )\n\n        return ''.join(template.format(i=i, **node) for i, node in sorted(self.nodes.items()) if node['tag'] != 'TOP')\n\n    def nx_graph(self):\n        import networkx\n\n        nx_nodelist = list(range(1, len(self.nodes)))\n        nx_edgelist = [\n            (n, self._hd(n), self._rel(n))\n            for n in nx_nodelist if self._hd(n)\n        ]\n        self.nx_labels = {}\n        for n in nx_nodelist:\n            self.nx_labels[n] = self.nodes[n]['word']\n\n        g = networkx.MultiDiGraph()\n        g.add_nodes_from(nx_nodelist)\n        g.add_edges_from(nx_edgelist)\n\n        return g\n\n\nclass DependencyGraphError(Exception):\n\n\ndef demo():\n    malt_demo()\n    conll_demo()\n    conll_file_demo()\n    cycle_finding_demo()\n\n\ndef malt_demo(nx=False):\n    dg = DependencyGraph(\"\"\"Pierre  NNP     2       NMOD\nVinken  NNP     8       SUB\n,       ,       2       P\n61      CD      5       NMOD\nyears   NNS     6       AMOD\nold     JJ      2       NMOD\n,       ,       2       P\nwill    MD      0       ROOT\njoin    VB      8       VC\nthe     DT      11      NMOD\nboard   NN      9       OBJ\nas      IN      9       VMOD\na       DT      15      NMOD\nnonexecutive    JJ      15      NMOD\ndirector        NN      12      PMOD\nNov.    NNP     9       VMOD\n29      CD      16      NMOD\n.       .       9       VMOD\n\"\"\")\n    tree = dg.tree()\n    tree.pprint()\n    if nx:\n        import networkx\n        from matplotlib import pylab\n\n        g = dg.nx_graph()\n        g.info()\n        pos = networkx.spring_layout(g, dim=1)\n        networkx.draw_networkx_nodes(g, pos, node_size=50)\n        networkx.draw_networkx_labels(g, pos, dg.nx_labels)\n        pylab.xticks([])\n        pylab.yticks([])\n        pylab.savefig('tree.png')\n        pylab.show()\n\n\ndef conll_demo():\n    \"\"\"\n    A demonstration of how to read a string representation of\n    a CoNLL format dependency tree.\n    \"\"\"\n    dg = DependencyGraph(conll_data1)\n    tree = dg.tree()\n    tree.pprint()\n    print(dg)\n    print(dg.to_conll(4))\n\n\ndef conll_file_demo():\n    print('Mass conll_read demo...')\n    graphs = [DependencyGraph(entry)\n              for entry in conll_data2.split('\\n\\n') if entry]\n    for graph in graphs:\n        tree = graph.tree()\n        print('\\n')\n        tree.pprint()\n\n\ndef cycle_finding_demo():\n    dg = DependencyGraph(treebank_data)\n    print(dg.contains_cycle())\n    cyclic_dg = DependencyGraph()\n    cyclic_dg.add_node({'word': None, 'deps': [1], 'rel': 'TOP', 'address': 0})\n    cyclic_dg.add_node({'word': None, 'deps': [2], 'rel': 'NTOP', 'address': 1})\n    cyclic_dg.add_node({'word': None, 'deps': [4], 'rel': 'NTOP', 'address': 2})\n    cyclic_dg.add_node({'word': None, 'deps': [1], 'rel': 'NTOP', 'address': 3})\n    cyclic_dg.add_node({'word': None, 'deps': [3], 'rel': 'NTOP', 'address': 4})\n    print(cyclic_dg.contains_cycle())\n\ntreebank_data = \"\"\"Pierre  NNP     2       NMOD\nVinken  NNP     8       SUB\n,       ,       2       P\n61      CD      5       NMOD\nyears   NNS     6       AMOD\nold     JJ      2       NMOD\n,       ,       2       P\nwill    MD      0       ROOT\njoin    VB      8       VC\nthe     DT      11      NMOD\nboard   NN      9       OBJ\nas      IN      9       VMOD\na       DT      15      NMOD\nnonexecutive    JJ      15      NMOD\ndirector        NN      12      PMOD\nNov.    NNP     9       VMOD\n29      CD      16      NMOD\n.       .       9       VMOD\n\"\"\"\n\nconll_data1 = \"\"\"\n1   Ze                ze                Pron  Pron  per|3|evofmv|nom                 2   su      _  _\n2   had               heb               V     V     trans|ovt|1of2of3|ev             0   ROOT    _  _\n3   met               met               Prep  Prep  voor                             8   mod     _  _\n4   haar              haar              Pron  Pron  bez|3|ev|neut|attr               5   det     _  _\n5   moeder            moeder            N     N     soort|ev|neut                    3   obj1    _  _\n6   kunnen            kan               V     V     hulp|ott|1of2of3|mv              2   vc      _  _\n7   gaan              ga                V     V     hulp|inf                         6   vc      _  _\n8   winkelen          winkel            V     V     intrans|inf                      11  cnj     _  _\n9   ,                 ,                 Punc  Punc  komma                            8   punct   _  _\n10  zwemmen           zwem              V     V     intrans|inf                      11  cnj     _  _\n11  of                of                Conj  Conj  neven                            7   vc      _  _\n12  terrassen         terras            N     N     soort|mv|neut                    11  cnj     _  _\n13  .                 .                 Punc  Punc  punt                             12  punct   _  _\n\"\"\"\n\nconll_data2 = \"\"\"1   Cathy             Cathy             N     N     eigen|ev|neut                    2   su      _  _\n2   zag               zie               V     V     trans|ovt|1of2of3|ev             0   ROOT    _  _\n3   hen               hen               Pron  Pron  per|3|mv|datofacc                2   obj1    _  _\n4   wild              wild              Adj   Adj   attr|stell|onverv                5   mod     _  _\n5   zwaaien           zwaai             N     N     soort|mv|neut                    2   vc      _  _\n6   .                 .                 Punc  Punc  punt                             5   punct   _  _\n\n1   Ze                ze                Pron  Pron  per|3|evofmv|nom                 2   su      _  _\n2   had               heb               V     V     trans|ovt|1of2of3|ev             0   ROOT    _  _\n3   met               met               Prep  Prep  voor                             8   mod     _  _\n4   haar              haar              Pron  Pron  bez|3|ev|neut|attr               5   det     _  _\n5   moeder            moeder            N     N     soort|ev|neut                    3   obj1    _  _\n6   kunnen            kan               V     V     hulp|ott|1of2of3|mv              2   vc      _  _\n7   gaan              ga                V     V     hulp|inf                         6   vc      _  _\n8   winkelen          winkel            V     V     intrans|inf                      11  cnj     _  _\n9   ,                 ,                 Punc  Punc  komma                            8   punct   _  _\n10  zwemmen           zwem              V     V     intrans|inf                      11  cnj     _  _\n11  of                of                Conj  Conj  neven                            7   vc      _  _\n12  terrassen         terras            N     N     soort|mv|neut                    11  cnj     _  _\n13  .                 .                 Punc  Punc  punt                             12  punct   _  _\n\n1   Dat               dat               Pron  Pron  aanw|neut|attr                   2   det     _  _\n2   werkwoord         werkwoord         N     N     soort|ev|neut                    6   obj1    _  _\n3   had               heb               V     V     hulp|ovt|1of2of3|ev              0   ROOT    _  _\n4   ze                ze                Pron  Pron  per|3|evofmv|nom                 6   su      _  _\n5   zelf              zelf              Pron  Pron  aanw|neut|attr|wzelf             3   predm   _  _\n6   uitgevonden       vind              V     V     trans|verldw|onverv              3   vc      _  _\n7   .                 .                 Punc  Punc  punt                             6   punct   _  _\n\n1   Het               het               Pron  Pron  onbep|neut|zelfst                2   su      _  _\n2   hoorde            hoor              V     V     trans|ovt|1of2of3|ev             0   ROOT    _  _\n3   bij               bij               Prep  Prep  voor                             2   ld      _  _\n4   de                de                Art   Art   bep|zijdofmv|neut                6   det     _  _\n5   warme             warm              Adj   Adj   attr|stell|vervneut              6   mod     _  _\n6   zomerdag          zomerdag          N     N     soort|ev|neut                    3   obj1    _  _\n7   die               die               Pron  Pron  betr|neut|zelfst                 6   mod     _  _\n8   ze                ze                Pron  Pron  per|3|evofmv|nom                 12  su      _  _\n9   ginds             ginds             Adv   Adv   gew|aanw                         12  mod     _  _\n10  achter            achter            Adv   Adv   gew|geenfunc|stell|onverv        12  svp     _  _\n11  had               heb               V     V     hulp|ovt|1of2of3|ev              7   body    _  _\n12  gelaten           laat              V     V     trans|verldw|onverv              11  vc      _  _\n13  .                 .                 Punc  Punc  punt                             12  punct   _  _\n\n1   Ze                ze                Pron  Pron  per|3|evofmv|nom                 2   su      _  _\n2   hadden            heb               V     V     trans|ovt|1of2of3|mv             0   ROOT    _  _\n3   languit           languit           Adv   Adv   gew|geenfunc|stell|onverv        11  mod     _  _\n4   naast             naast             Prep  Prep  voor                             11  mod     _  _\n5   elkaar            elkaar            Pron  Pron  rec|neut                         4   obj1    _  _\n6   op                op                Prep  Prep  voor                             11  ld      _  _\n7   de                de                Art   Art   bep|zijdofmv|neut                8   det     _  _\n8   strandstoelen     strandstoel       N     N     soort|mv|neut                    6   obj1    _  _\n9   kunnen            kan               V     V     hulp|inf                         2   vc      _  _\n10  gaan              ga                V     V     hulp|inf                         9   vc      _  _\n11  liggen            lig               V     V     intrans|inf                      10  vc      _  _\n12  .                 .                 Punc  Punc  punt                             11  punct   _  _\n\n1   Zij               zij               Pron  Pron  per|3|evofmv|nom                 2   su      _  _\n2   zou               zal               V     V     hulp|ovt|1of2of3|ev              7   cnj     _  _\n3   mams              mams              N     N     soort|ev|neut                    4   det     _  _\n4   rug               rug               N     N     soort|ev|neut                    5   obj1    _  _\n5   ingewreven        wrijf             V     V     trans|verldw|onverv              6   vc      _  _\n6   hebben            heb               V     V     hulp|inf                         2   vc      _  _\n7   en                en                Conj  Conj  neven                            0   ROOT    _  _\n8   mam               mam               V     V     trans|ovt|1of2of3|ev             7   cnj     _  _\n9   de                de                Art   Art   bep|zijdofmv|neut                10  det     _  _\n10  hare              hare              Pron  Pron  bez|3|ev|neut|attr               8   obj1    _  _\n11  .                 .                 Punc  Punc  punt                             10  punct   _  _\n\n1   Of                of                Conj  Conj  onder|metfin                     0   ROOT    _  _\n2   ze                ze                Pron  Pron  per|3|evofmv|nom                 3   su      _  _\n3   had               heb               V     V     hulp|ovt|1of2of3|ev              0   ROOT    _  _\n4   gewoon            gewoon            Adj   Adj   adv|stell|onverv                 10  mod     _  _\n5   met               met               Prep  Prep  voor                             10  mod     _  _\n6   haar              haar              Pron  Pron  bez|3|ev|neut|attr               7   det     _  _\n7   vriendinnen       vriendin          N     N     soort|mv|neut                    5   obj1    _  _\n8   rond              rond              Adv   Adv   deelv                            10  svp     _  _\n9   kunnen            kan               V     V     hulp|inf                         3   vc      _  _\n10  slenteren         slenter           V     V     intrans|inf                      9   vc      _  _\n11  in                in                Prep  Prep  voor                             10  mod     _  _\n12  de                de                Art   Art   bep|zijdofmv|neut                13  det     _  _\n13  buurt             buurt             N     N     soort|ev|neut                    11  obj1    _  _\n14  van               van               Prep  Prep  voor                             13  mod     _  _\n15  Trafalgar_Square  Trafalgar_Square  MWU   N_N   eigen|ev|neut_eigen|ev|neut      14  obj1    _  _\n16  .                 .                 Punc  Punc  punt                             15  punct   _  _\n\"\"\"\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\parse\\earleychart": [".py", "\nfrom __future__ import print_function, division\n\nfrom six.moves import range\n\nfrom nltk.parse.chart import (Chart, ChartParser, EdgeI, LeafEdge, LeafInitRule,\n                              BottomUpPredictRule, BottomUpPredictCombineRule,\n                              TopDownInitRule, SingleEdgeFundamentalRule,\n                              EmptyPredictRule,\n                              CachedTopDownPredictRule,\n                              FilteredSingleEdgeFundamentalRule,\n                              FilteredBottomUpPredictCombineRule)\nfrom nltk.parse.featurechart import (FeatureChart, FeatureChartParser,\n                                     FeatureTopDownInitRule,\n                                     FeatureTopDownPredictRule,\n                                     FeatureEmptyPredictRule,\n                                     FeatureBottomUpPredictRule,\n                                     FeatureBottomUpPredictCombineRule,\n                                     FeatureSingleEdgeFundamentalRule)\n\n\nclass IncrementalChart(Chart):\n    def initialize(self):\n        self._edgelists = tuple([] for x in self._positions())\n\n        self._edge_to_cpls = {}\n\n        self._indexes = {}\n\n    def edges(self):\n        return list(self.iteredges())\n\n    def iteredges(self):\n        return (edge for edgelist in self._edgelists for edge in edgelist)\n\n    def select(self, end, **restrictions):\n        edgelist = self._edgelists[end]\n\n        if restrictions=={}: return iter(edgelist)\n\n        restr_keys = sorted(restrictions.keys())\n        restr_keys = tuple(restr_keys)\n\n        if restr_keys not in self._indexes:\n            self._add_index(restr_keys)\n\n        vals = tuple(restrictions[key] for key in restr_keys)\n        return iter(self._indexes[restr_keys][end].get(vals, []))\n\n    def _add_index(self, restr_keys):\n        for key in restr_keys:\n            if not hasattr(EdgeI, key):\n                raise ValueError('Bad restriction: %s' % key)\n\n        index = self._indexes[restr_keys] = tuple({} for x in self._positions())\n\n        for end, edgelist in enumerate(self._edgelists):\n            this_index = index[end]\n            for edge in edgelist:\n                vals = tuple(getattr(edge, key)() for key in restr_keys)\n                this_index.setdefault(vals, []).append(edge)\n\n    def _register_with_indexes(self, edge):\n        end = edge.end()\n        for (restr_keys, index) in self._indexes.items():\n            vals = tuple(getattr(edge, key)() for key in restr_keys)\n            index[end].setdefault(vals, []).append(edge)\n\n    def _append_edge(self, edge):\n        self._edgelists[edge.end()].append(edge)\n\n    def _positions(self):\n        return range(self.num_leaves() + 1)\n\n\nclass FeatureIncrementalChart(IncrementalChart, FeatureChart):\n    def select(self, end, **restrictions):\n        edgelist = self._edgelists[end]\n\n        if restrictions=={}: return iter(edgelist)\n\n        restr_keys = sorted(restrictions.keys())\n        restr_keys = tuple(restr_keys)\n\n        if restr_keys not in self._indexes:\n            self._add_index(restr_keys)\n\n        vals = tuple(self._get_type_if_possible(restrictions[key])\n                     for key in restr_keys)\n        return iter(self._indexes[restr_keys][end].get(vals, []))\n\n    def _add_index(self, restr_keys):\n        for key in restr_keys:\n            if not hasattr(EdgeI, key):\n                raise ValueError('Bad restriction: %s' % key)\n\n        index = self._indexes[restr_keys] = tuple({} for x in self._positions())\n\n        for end, edgelist in enumerate(self._edgelists):\n            this_index = index[end]\n            for edge in edgelist:\n                vals = tuple(self._get_type_if_possible(getattr(edge, key)())\n                             for key in restr_keys)\n                this_index.setdefault(vals, []).append(edge)\n\n    def _register_with_indexes(self, edge):\n        end = edge.end()\n        for (restr_keys, index) in self._indexes.items():\n            vals = tuple(self._get_type_if_possible(getattr(edge, key)())\n                         for key in restr_keys)\n            index[end].setdefault(vals, []).append(edge)\n\n\nclass CompleteFundamentalRule(SingleEdgeFundamentalRule):\n    def _apply_incomplete(self, chart, grammar, left_edge):\n        end = left_edge.end()\n        for right_edge in chart.select(start=end, end=end,\n                                       is_complete=True,\n                                       lhs=left_edge.nextsym()):\n            new_edge = left_edge.move_dot_forward(right_edge.end())\n            if chart.insert_with_backpointer(new_edge, left_edge, right_edge):\n                yield new_edge\n\nclass CompleterRule(CompleteFundamentalRule):\n    _fundamental_rule = CompleteFundamentalRule()\n    def apply(self, chart, grammar, edge):\n        if not isinstance(edge, LeafEdge):\n            for new_edge in self._fundamental_rule.apply(chart, grammar, edge):\n                yield new_edge\n\nclass ScannerRule(CompleteFundamentalRule):\n    _fundamental_rule = CompleteFundamentalRule()\n    def apply(self, chart, grammar, edge):\n        if isinstance(edge, LeafEdge):\n            for new_edge in self._fundamental_rule.apply(chart, grammar, edge):\n                yield new_edge\n\nclass PredictorRule(CachedTopDownPredictRule):\n    pass\n\nclass FilteredCompleteFundamentalRule(FilteredSingleEdgeFundamentalRule):\n    def apply(self, chart, grammar, edge):\n        if edge.is_complete():\n            for new_edge in self._apply_complete(chart, grammar, edge):\n                yield new_edge\n\n\nclass FeatureCompleteFundamentalRule(FeatureSingleEdgeFundamentalRule):\n    def _apply_incomplete(self, chart, grammar, left_edge):\n        fr = self._fundamental_rule\n        end = left_edge.end()\n        for right_edge in chart.select(start=end, end=end,\n                                       is_complete=True,\n                                       lhs=left_edge.nextsym()):\n            for new_edge in fr.apply(chart, grammar, left_edge, right_edge):\n                yield new_edge\n\nclass FeatureCompleterRule(CompleterRule):\n    _fundamental_rule = FeatureCompleteFundamentalRule()\n\nclass FeatureScannerRule(ScannerRule):\n    _fundamental_rule = FeatureCompleteFundamentalRule()\n\nclass FeaturePredictorRule(FeatureTopDownPredictRule):\n    pass\n\n\nEARLEY_STRATEGY = [LeafInitRule(),\n                   TopDownInitRule(),\n                   CompleterRule(),\n                   ScannerRule(),\n                   PredictorRule()]\nTD_INCREMENTAL_STRATEGY = [LeafInitRule(),\n                           TopDownInitRule(),\n                           CachedTopDownPredictRule(),\n                           CompleteFundamentalRule()]\nBU_INCREMENTAL_STRATEGY = [LeafInitRule(),\n                           EmptyPredictRule(),\n                           BottomUpPredictRule(),\n                           CompleteFundamentalRule()]\nBU_LC_INCREMENTAL_STRATEGY = [LeafInitRule(),\n                              EmptyPredictRule(),\n                              BottomUpPredictCombineRule(),\n                              CompleteFundamentalRule()]\n\nLC_INCREMENTAL_STRATEGY = [LeafInitRule(),\n                           FilteredBottomUpPredictCombineRule(),\n                           FilteredCompleteFundamentalRule()]\n\nclass IncrementalChartParser(ChartParser):\n    def __init__(self, grammar, strategy=BU_LC_INCREMENTAL_STRATEGY,\n                 trace=0, trace_chart_width=50,\n                 chart_class=IncrementalChart):\n        self._grammar = grammar\n        self._trace = trace\n        self._trace_chart_width = trace_chart_width\n        self._chart_class = chart_class\n\n        self._axioms = []\n        self._inference_rules = []\n        for rule in strategy:\n            if rule.NUM_EDGES == 0:\n                self._axioms.append(rule)\n            elif rule.NUM_EDGES == 1:\n                self._inference_rules.append(rule)\n            else:\n                raise ValueError(\"Incremental inference rules must have \"\n                                 \"NUM_EDGES == 0 or 1\")\n\n    def chart_parse(self, tokens, trace=None):\n        if trace is None: trace = self._trace\n        trace_new_edges = self._trace_new_edges\n\n        tokens = list(tokens)\n        self._grammar.check_coverage(tokens)\n        chart = self._chart_class(tokens)\n        grammar = self._grammar\n\n        trace_edge_width = self._trace_chart_width // (chart.num_leaves() + 1)\n        if trace: print(chart.pretty_format_leaves(trace_edge_width))\n\n        for axiom in self._axioms:\n            new_edges = list(axiom.apply(chart, grammar))\n            trace_new_edges(chart, axiom, new_edges, trace, trace_edge_width)\n\n        inference_rules = self._inference_rules\n        for end in range(chart.num_leaves()+1):\n            if trace > 1: print(\"\\n* Processing queue:\", end, \"\\n\")\n            agenda = list(chart.select(end=end))\n            while agenda:\n                edge = agenda.pop()\n                for rule in inference_rules:\n                    new_edges = list(rule.apply(chart, grammar, edge))\n                    trace_new_edges(chart, rule, new_edges, trace, trace_edge_width)\n                    for new_edge in new_edges:\n                        if new_edge.end()==end:\n                            agenda.append(new_edge)\n\n        return chart\n\nclass EarleyChartParser(IncrementalChartParser):\n    def __init__(self, grammar, **parser_args):\n        IncrementalChartParser.__init__(self, grammar, EARLEY_STRATEGY, **parser_args)\n    pass\n\nclass IncrementalTopDownChartParser(IncrementalChartParser):\n    def __init__(self, grammar, **parser_args):\n        IncrementalChartParser.__init__(self, grammar, TD_INCREMENTAL_STRATEGY, **parser_args)\n\nclass IncrementalBottomUpChartParser(IncrementalChartParser):\n    def __init__(self, grammar, **parser_args):\n        IncrementalChartParser.__init__(self, grammar, BU_INCREMENTAL_STRATEGY, **parser_args)\n\nclass IncrementalBottomUpLeftCornerChartParser(IncrementalChartParser):\n    def __init__(self, grammar, **parser_args):\n        IncrementalChartParser.__init__(self, grammar, BU_LC_INCREMENTAL_STRATEGY, **parser_args)\n\nclass IncrementalLeftCornerChartParser(IncrementalChartParser):\n    def __init__(self, grammar, **parser_args):\n        if not grammar.is_nonempty():\n            raise ValueError(\"IncrementalLeftCornerParser only works for grammars \"\n                             \"without empty productions.\")\n        IncrementalChartParser.__init__(self, grammar, LC_INCREMENTAL_STRATEGY, **parser_args)\n\n\nEARLEY_FEATURE_STRATEGY = [LeafInitRule(),\n                           FeatureTopDownInitRule(),\n                           FeatureCompleterRule(),\n                           FeatureScannerRule(),\n                           FeaturePredictorRule()]\nTD_INCREMENTAL_FEATURE_STRATEGY = [LeafInitRule(),\n                                   FeatureTopDownInitRule(),\n                                   FeatureTopDownPredictRule(),\n                                   FeatureCompleteFundamentalRule()]\nBU_INCREMENTAL_FEATURE_STRATEGY = [LeafInitRule(),\n                                   FeatureEmptyPredictRule(),\n                                   FeatureBottomUpPredictRule(),\n                                   FeatureCompleteFundamentalRule()]\nBU_LC_INCREMENTAL_FEATURE_STRATEGY = [LeafInitRule(),\n                                      FeatureEmptyPredictRule(),\n                                      FeatureBottomUpPredictCombineRule(),\n                                      FeatureCompleteFundamentalRule()]\n\nclass FeatureIncrementalChartParser(IncrementalChartParser, FeatureChartParser):\n    def __init__(self, grammar,\n                 strategy=BU_LC_INCREMENTAL_FEATURE_STRATEGY,\n                 trace_chart_width=20,\n                 chart_class=FeatureIncrementalChart,\n                 **parser_args):\n        IncrementalChartParser.__init__(self, grammar,\n                                        strategy=strategy,\n                                        trace_chart_width=trace_chart_width,\n                                        chart_class=chart_class,\n                                        **parser_args)\n\nclass FeatureEarleyChartParser(FeatureIncrementalChartParser):\n    def __init__(self, grammar, **parser_args):\n        FeatureIncrementalChartParser.__init__(self, grammar, EARLEY_FEATURE_STRATEGY, **parser_args)\n\nclass FeatureIncrementalTopDownChartParser(FeatureIncrementalChartParser):\n    def __init__(self, grammar, **parser_args):\n        FeatureIncrementalChartParser.__init__(self, grammar, TD_INCREMENTAL_FEATURE_STRATEGY, **parser_args)\n\nclass FeatureIncrementalBottomUpChartParser(FeatureIncrementalChartParser):\n    def __init__(self, grammar, **parser_args):\n        FeatureIncrementalChartParser.__init__(self, grammar, BU_INCREMENTAL_FEATURE_STRATEGY, **parser_args)\n\nclass FeatureIncrementalBottomUpLeftCornerChartParser(FeatureIncrementalChartParser):\n    def __init__(self, grammar, **parser_args):\n        FeatureIncrementalChartParser.__init__(self, grammar, BU_LC_INCREMENTAL_FEATURE_STRATEGY, **parser_args)\n\n\n\ndef demo(print_times=True, print_grammar=False,\n         print_trees=True, trace=2,\n         sent='I saw John with a dog with my cookie', numparses=5):\n    import sys, time\n    from nltk.parse.chart import demo_grammar\n\n    grammar = demo_grammar()\n    if print_grammar:\n        print(\"* Grammar\")\n        print(grammar)\n\n    print(\"* Sentence:\")\n    print(sent)\n    tokens = sent.split()\n    print(tokens)\n    print()\n\n    earley = EarleyChartParser(grammar, trace=trace)\n    t = time.clock()\n    chart = earley.chart_parse(tokens)\n    parses = list(chart.parses(grammar.start()))\n    t = time.clock()-t\n\n    if numparses:\n        assert len(parses)==numparses, 'Not all parses found'\n    if print_trees:\n        for tree in parses: print(tree)\n    else:\n        print(\"Nr trees:\", len(parses))\n    if print_times:\n        print(\"Time:\", t)\n\nif __name__ == '__main__': demo()\n"], "nltk\\parse\\evaluate": [".py", "\nfrom __future__ import division\n\nimport unicodedata\n\n\nclass DependencyEvaluator(object):\n\n    def __init__(self, parsed_sents, gold_sents):\n        \"\"\"\n        :param parsed_sents: the list of parsed_sents as the output of parser\n        :type parsed_sents: list(DependencyGraph)\n        \"\"\"\n        self._parsed_sents = parsed_sents\n        self._gold_sents = gold_sents\n\n    def _remove_punct(self, inStr):\n        \"\"\"\n        Function to remove punctuation from Unicode string.\n        :param input: the input string\n        :return: Unicode string after remove all punctuation\n        \"\"\"\n        punc_cat = set([\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"])\n        return \"\".join(x for x in inStr if unicodedata.category(x) not in punc_cat)\n\n    def eval(self):\n        \"\"\"\n        Return the Labeled Attachment Score (LAS) and Unlabeled Attachment Score (UAS)\n\n        :return : tuple(float,float)\n        \"\"\"\n        if (len(self._parsed_sents) != len(self._gold_sents)):\n            raise ValueError(\" Number of parsed sentence is different with number of gold sentence.\")\n\n        corr = 0\n        corrL = 0\n        total = 0\n\n        for i in range(len(self._parsed_sents)):\n            parsed_sent_nodes = self._parsed_sents[i].nodes\n            gold_sent_nodes = self._gold_sents[i].nodes\n\n            if (len(parsed_sent_nodes) != len(gold_sent_nodes)):\n                raise ValueError(\"Sentences must have equal length.\")\n\n            for parsed_node_address, parsed_node in parsed_sent_nodes.items():\n                gold_node = gold_sent_nodes[parsed_node_address]\n\n                if parsed_node[\"word\"] is None:\n                    continue\n                if parsed_node[\"word\"] != gold_node[\"word\"]:\n                    raise ValueError(\"Sentence sequence is not matched.\")\n\n                if self._remove_punct(parsed_node[\"word\"]) == \"\":\n                    continue\n\n                total += 1\n                if parsed_node[\"head\"] == gold_node[\"head\"]:\n                    corr += 1\n                    if parsed_node[\"rel\"] == gold_node[\"rel\"]:\n                        corrL += 1\n\n        return corr / total, corrL / total\n\n\n"], "nltk\\parse\\featurechart": [".py", "\nfrom __future__ import print_function, unicode_literals\n\nfrom six.moves import range\n\nfrom nltk.compat import python_2_unicode_compatible\nfrom nltk.featstruct import FeatStruct, unify, TYPE, find_variables\nfrom nltk.sem import logic\nfrom nltk.tree import Tree\nfrom nltk.grammar import (Nonterminal, Production, CFG,\n                          FeatStructNonterminal, is_nonterminal,\n                          is_terminal)\nfrom nltk.parse.chart import (TreeEdge, Chart, ChartParser, EdgeI,\n                              FundamentalRule, LeafInitRule,\n                              EmptyPredictRule, BottomUpPredictRule,\n                              SingleEdgeFundamentalRule,\n                              BottomUpPredictCombineRule,\n                              CachedTopDownPredictRule,\n                              TopDownInitRule)\n\n\n@python_2_unicode_compatible\nclass FeatureTreeEdge(TreeEdge):\n    def __init__(self, span, lhs, rhs, dot=0, bindings=None):\n        if bindings is None: bindings = {}\n\n        if dot == len(rhs) and bindings:\n            lhs = self._bind(lhs, bindings)\n            rhs = [self._bind(elt, bindings) for elt in rhs]\n            bindings = {}\n\n        TreeEdge.__init__(self, span, lhs, rhs, dot)\n        self._bindings = bindings\n        self._comparison_key = (self._comparison_key, tuple(sorted(bindings.items())))\n\n    @staticmethod\n    def from_production(production, index):\n        return FeatureTreeEdge(span=(index, index), lhs=production.lhs(),\n                               rhs=production.rhs(), dot=0)\n\n    def move_dot_forward(self, new_end, bindings=None):\n        return FeatureTreeEdge(span=(self._span[0], new_end),\n                               lhs=self._lhs, rhs=self._rhs,\n                               dot=self._dot+1, bindings=bindings)\n\n    def _bind(self, nt, bindings):\n        if not isinstance(nt, FeatStructNonterminal): return nt\n        return nt.substitute_bindings(bindings)\n\n    def next_with_bindings(self):\n        return self._bind(self.nextsym(), self._bindings)\n\n    def bindings(self):\n        return self._bindings.copy()\n\n    def variables(self):\n        return find_variables([self._lhs] + list(self._rhs) +\n                              list(self._bindings.keys()) +\n                              list(self._bindings.values()),\n                              fs_class=FeatStruct)\n\n    def __str__(self):\n        if self.is_complete():\n            return TreeEdge.__unicode__(self)\n        else:\n            bindings = '{%s}' % ', '.join('%s: %r' % item for item in\n                                           sorted(self._bindings.items()))\n            return '%s %s' % (TreeEdge.__unicode__(self), bindings)\n\n\n\n\nclass FeatureChart(Chart):\n\n    def select(self, **restrictions):\n        if restrictions=={}: return iter(self._edges)\n\n        restr_keys = sorted(restrictions.keys())\n        restr_keys = tuple(restr_keys)\n\n        if restr_keys not in self._indexes:\n            self._add_index(restr_keys)\n\n        vals = tuple(self._get_type_if_possible(restrictions[key])\n                     for key in restr_keys)\n        return iter(self._indexes[restr_keys].get(vals, []))\n\n    def _add_index(self, restr_keys):\n        for key in restr_keys:\n            if not hasattr(EdgeI, key):\n                raise ValueError('Bad restriction: %s' % key)\n\n        index = self._indexes[restr_keys] = {}\n\n        for edge in self._edges:\n            vals = tuple(self._get_type_if_possible(getattr(edge, key)())\n                         for key in restr_keys)\n            index.setdefault(vals, []).append(edge)\n\n    def _register_with_indexes(self, edge):\n        for (restr_keys, index) in self._indexes.items():\n            vals = tuple(self._get_type_if_possible(getattr(edge, key)())\n                         for key in restr_keys)\n            index.setdefault(vals, []).append(edge)\n\n    def _get_type_if_possible(self, item):\n        if isinstance(item, dict) and TYPE in item:\n            return item[TYPE]\n        else:\n            return item\n\n    def parses(self, start, tree_class=Tree):\n        for edge in self.select(start=0, end=self._num_leaves):\n            if ((isinstance(edge, FeatureTreeEdge)) and\n                (edge.lhs()[TYPE] == start[TYPE]) and\n                (unify(edge.lhs(), start, rename_vars=True))\n                ):\n                for tree in self.trees(edge, complete=True, tree_class=tree_class):\n                    yield tree\n\n\n\nclass FeatureFundamentalRule(FundamentalRule):\n    def apply(self, chart, grammar, left_edge, right_edge):\n        if not (left_edge.end() == right_edge.start() and\n                left_edge.is_incomplete() and\n                right_edge.is_complete() and\n                isinstance(left_edge, FeatureTreeEdge)):\n            return\n        found = right_edge.lhs()\n        nextsym = left_edge.nextsym()\n        if isinstance(right_edge, FeatureTreeEdge):\n            if not is_nonterminal(nextsym): return\n            if left_edge.nextsym()[TYPE] != right_edge.lhs()[TYPE]: return\n            bindings = left_edge.bindings()\n            found = found.rename_variables(used_vars=left_edge.variables())\n            result = unify(nextsym, found, bindings, rename_vars=False)\n            if result is None: return\n        else:\n            if nextsym != found: return\n            bindings = left_edge.bindings()\n\n        new_edge = left_edge.move_dot_forward(right_edge.end(), bindings)\n\n        if chart.insert_with_backpointer(new_edge, left_edge, right_edge):\n            yield new_edge\n\nclass FeatureSingleEdgeFundamentalRule(SingleEdgeFundamentalRule):\n    _fundamental_rule = FeatureFundamentalRule()\n\n    def _apply_complete(self, chart, grammar, right_edge):\n        fr = self._fundamental_rule\n        for left_edge in chart.select(end=right_edge.start(),\n                                      is_complete=False,\n                                      nextsym=right_edge.lhs()):\n            for new_edge in fr.apply(chart, grammar, left_edge, right_edge):\n                yield new_edge\n\n    def _apply_incomplete(self, chart, grammar, left_edge):\n        fr = self._fundamental_rule\n        for right_edge in chart.select(start=left_edge.end(),\n                                       is_complete=True,\n                                       lhs=left_edge.nextsym()):\n            for new_edge in fr.apply(chart, grammar, left_edge, right_edge):\n                yield new_edge\n\n\n\nclass FeatureTopDownInitRule(TopDownInitRule):\n    def apply(self, chart, grammar):\n        for prod in grammar.productions(lhs=grammar.start()):\n            new_edge = FeatureTreeEdge.from_production(prod, 0)\n            if chart.insert(new_edge, ()):\n                yield new_edge\n\nclass FeatureTopDownPredictRule(CachedTopDownPredictRule):\n    def apply(self, chart, grammar, edge):\n        if edge.is_complete(): return\n        nextsym, index = edge.nextsym(), edge.end()\n        if not is_nonterminal(nextsym): return\n\n        nextsym_with_bindings = edge.next_with_bindings()\n        done = self._done.get((nextsym_with_bindings, index), (None, None))\n        if done[0] is chart and done[1] is grammar:\n            return\n\n        for prod in grammar.productions(lhs=nextsym):\n            if prod.rhs():\n                first = prod.rhs()[0]\n                if is_terminal(first):\n                    if index >= chart.num_leaves(): continue\n                    if first != chart.leaf(index): continue\n\n            if unify(prod.lhs(), nextsym_with_bindings, rename_vars=True):\n                new_edge = FeatureTreeEdge.from_production(prod, edge.end())\n                if chart.insert(new_edge, ()):\n                    yield new_edge\n\n        self._done[nextsym_with_bindings, index] = (chart, grammar)\n\n\n\nclass FeatureBottomUpPredictRule(BottomUpPredictRule):\n    def apply(self, chart, grammar, edge):\n        if edge.is_incomplete(): return\n        for prod in grammar.productions(rhs=edge.lhs()):\n            if isinstance(edge, FeatureTreeEdge):\n                _next = prod.rhs()[0]\n                if not is_nonterminal(_next): continue\n\n            new_edge = FeatureTreeEdge.from_production(prod, edge.start())\n            if chart.insert(new_edge, ()):\n                yield new_edge\n\nclass FeatureBottomUpPredictCombineRule(BottomUpPredictCombineRule):\n    def apply(self, chart, grammar, edge):\n        if edge.is_incomplete(): return\n        found = edge.lhs()\n        for prod in grammar.productions(rhs=found):\n            bindings = {}\n            if isinstance(edge, FeatureTreeEdge):\n                _next = prod.rhs()[0]\n                if not is_nonterminal(_next): continue\n\n                used_vars = find_variables((prod.lhs(),) + prod.rhs(),\n                                           fs_class=FeatStruct)\n                found = found.rename_variables(used_vars=used_vars)\n\n                result = unify(_next, found, bindings, rename_vars=False)\n                if result is None: continue\n\n            new_edge = (FeatureTreeEdge.from_production(prod, edge.start())\n                        .move_dot_forward(edge.end(), bindings))\n            if chart.insert(new_edge, (edge,)):\n                yield new_edge\n\nclass FeatureEmptyPredictRule(EmptyPredictRule):\n    def apply(self, chart, grammar):\n        for prod in grammar.productions(empty=True):\n            for index in range(chart.num_leaves() + 1):\n                new_edge = FeatureTreeEdge.from_production(prod, index)\n                if chart.insert(new_edge, ()):\n                    yield new_edge\n\n\n\nTD_FEATURE_STRATEGY = [LeafInitRule(),\n                       FeatureTopDownInitRule(),\n                       FeatureTopDownPredictRule(),\n                       FeatureSingleEdgeFundamentalRule()]\nBU_FEATURE_STRATEGY = [LeafInitRule(),\n                       FeatureEmptyPredictRule(),\n                       FeatureBottomUpPredictRule(),\n                       FeatureSingleEdgeFundamentalRule()]\nBU_LC_FEATURE_STRATEGY = [LeafInitRule(),\n                          FeatureEmptyPredictRule(),\n                          FeatureBottomUpPredictCombineRule(),\n                          FeatureSingleEdgeFundamentalRule()]\n\nclass FeatureChartParser(ChartParser):\n    def __init__(self, grammar,\n                 strategy=BU_LC_FEATURE_STRATEGY,\n                 trace_chart_width=20,\n                 chart_class=FeatureChart,\n                 **parser_args):\n        ChartParser.__init__(self, grammar,\n                             strategy=strategy,\n                             trace_chart_width=trace_chart_width,\n                             chart_class=chart_class,\n                             **parser_args)\n\nclass FeatureTopDownChartParser(FeatureChartParser):\n    def __init__(self, grammar, **parser_args):\n        FeatureChartParser.__init__(self, grammar, TD_FEATURE_STRATEGY, **parser_args)\n\nclass FeatureBottomUpChartParser(FeatureChartParser):\n    def __init__(self, grammar, **parser_args):\n        FeatureChartParser.__init__(self, grammar, BU_FEATURE_STRATEGY, **parser_args)\n\nclass FeatureBottomUpLeftCornerChartParser(FeatureChartParser):\n    def __init__(self, grammar, **parser_args):\n        FeatureChartParser.__init__(self, grammar, BU_LC_FEATURE_STRATEGY, **parser_args)\n\n\n\nclass InstantiateVarsChart(FeatureChart):\n    def __init__(self, tokens):\n        FeatureChart.__init__(self, tokens)\n\n    def initialize(self):\n        self._instantiated = set()\n        FeatureChart.initialize(self)\n\n    def insert(self, edge, child_pointer_list):\n        if edge in self._instantiated: return False\n        self.instantiate_edge(edge)\n        return FeatureChart.insert(self, edge, child_pointer_list)\n\n    def instantiate_edge(self, edge):\n        if not isinstance(edge, FeatureTreeEdge): return\n        if not edge.is_complete(): return\n        if edge in self._edge_to_cpls: return\n\n        inst_vars = self.inst_vars(edge)\n        if not inst_vars: return\n\n        self._instantiated.add(edge)\n        edge._lhs = edge.lhs().substitute_bindings(inst_vars)\n\n    def inst_vars(self, edge):\n        return dict((var, logic.unique_variable())\n                    for var in edge.lhs().variables()\n                    if var.name.startswith('@'))\n\n\n\ndef demo_grammar():\n    from nltk.grammar import FeatureGrammar\n    return FeatureGrammar.fromstring(\"\"\"\nS  -> NP VP\nPP -> Prep NP\nNP -> NP PP\nVP -> VP PP\nVP -> Verb NP\nVP -> Verb\nNP -> Det[pl=?x] Noun[pl=?x]\nNP -> \"John\"\nNP -> \"I\"\nDet -> \"the\"\nDet -> \"my\"\nDet[-pl] -> \"a\"\nNoun[-pl] -> \"dog\"\nNoun[-pl] -> \"cookie\"\nVerb -> \"ate\"\nVerb -> \"saw\"\nPrep -> \"with\"\nPrep -> \"under\"\n\"\"\")\n\ndef demo(print_times=True, print_grammar=True,\n         print_trees=True, print_sentence=True,\n         trace=1,\n         parser=FeatureChartParser,\n         sent='I saw John with a dog with my cookie'):\n    import sys, time\n    print()\n    grammar = demo_grammar()\n    if print_grammar:\n        print(grammar)\n        print()\n    print(\"*\", parser.__name__)\n    if print_sentence:\n        print(\"Sentence:\", sent)\n    tokens = sent.split()\n    t = time.clock()\n    cp = parser(grammar, trace=trace)\n    chart = cp.chart_parse(tokens)\n    trees = list(chart.parses(grammar.start()))\n    if print_times:\n        print(\"Time: %s\" % (time.clock() - t))\n    if print_trees:\n        for tree in trees: print(tree)\n    else:\n        print(\"Nr trees:\", len(trees))\n\ndef run_profile():\n    import profile\n    profile.run('for i in range(1): demo()', '/tmp/profile.out')\n    import pstats\n    p = pstats.Stats('/tmp/profile.out')\n    p.strip_dirs().sort_stats('time', 'cum').print_stats(60)\n    p.strip_dirs().sort_stats('cum', 'time').print_stats(60)\n\nif __name__ == '__main__':\n    from nltk.data import load\n    demo()\n    print()\n    grammar = load('grammars/book_grammars/feat0.fcfg')\n    cp = FeatureChartParser(grammar, trace=2)\n    sent = 'Kim likes children'\n    tokens = sent.split()\n    trees = cp.parse(tokens)\n    for tree in trees:\n        print(tree)\n"], "nltk\\parse\\generate": [".py", "from __future__ import print_function\n\nimport itertools\nimport sys\nfrom nltk.grammar import Nonterminal\n\n\ndef generate(grammar, start=None, depth=None, n=None):\n    if not start:\n        start = grammar.start()\n    if depth is None:\n        depth = sys.maxsize\n\n    iter = _generate_all(grammar, [start], depth)\n\n    if n:\n        iter = itertools.islice(iter, n)\n\n    return iter\n\n\ndef _generate_all(grammar, items, depth):\n    if items:\n        try:\n            for frag1 in _generate_one(grammar, items[0], depth):\n                for frag2 in _generate_all(grammar, items[1:], depth):\n                    yield frag1 + frag2\n        except RuntimeError as _error:\n            if _error.message == \"maximum recursion depth exceeded\":\n                raise RuntimeError(\"The grammar has rule(s) that yield infinite recursion!!\")\n            else:\n                raise\n    else:\n        yield []\n\n\ndef _generate_one(grammar, item, depth):\n    if depth > 0:\n        if isinstance(item, Nonterminal):\n            for prod in grammar.productions(lhs=item):\n                for frag in _generate_all(grammar, prod.rhs(), depth-1):\n                    yield frag\n        else:\n            yield [item]\n\ndemo_grammar = \"\"\"\n  S -> NP VP\n  NP -> Det N\n  PP -> P NP\n  VP -> 'slept' | 'saw' NP | 'walked' PP\n  Det -> 'the' | 'a'\n  N -> 'man' | 'park' | 'dog'\n  P -> 'in' | 'with'\n\"\"\"\n\n\ndef demo(N=23):\n    from nltk.grammar import CFG\n\n    print('Generating the first %d sentences for demo grammar:' % (N,))\n    print(demo_grammar)\n    grammar = CFG.fromstring(demo_grammar)\n    for n, sent in enumerate(generate(grammar, n=N), 1):\n        print('%3d. %s' % (n, ' '.join(sent)))\n\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\parse\\malt": [".py", "\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom six import text_type\nimport os\nimport sys\nimport tempfile\nimport subprocess\nimport inspect\n\nfrom nltk.data import ZipFilePathPointer\nfrom nltk.internals import find_dir, find_file, find_jars_within_path\n\nfrom nltk.parse.api import ParserI\nfrom nltk.parse.dependencygraph import DependencyGraph\nfrom nltk.parse.util import taggedsents_to_conll\n\n\ndef malt_regex_tagger():\n    from nltk.tag import RegexpTagger\n    _tagger = RegexpTagger(\n    [(r'\\.$','.'), (r'\\,$',','), (r'\\?$','?'),    # fullstop, comma, Qmark\n    (r'\\($','('), (r'\\)$',')'),             # round brackets\n    (r'\\[$','['), (r'\\]$',']'),             # square brackets\n    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),        # cardinal numbers\n    (r'(The|the|A|a|An|an)$', 'DT'),        # articles\n    (r'(He|he|She|she|It|it|I|me|Me|You|you)$', 'PRP'), # pronouns\n    (r'(His|his|Her|her|Its|its)$', 'PRP$'),    # possesive\n    (r'(my|Your|your|Yours|yours)$', 'PRP$'),   # possesive\n    (r'(on|On|in|In|at|At|since|Since)$', 'IN'),# time prepopsitions\n    (r'(for|For|ago|Ago|before|Before)$', 'IN'),# time prepopsitions\n    (r'(till|Till|until|Until)$', 'IN'),        # time prepopsitions\n    (r'(by|By|beside|Beside)$', 'IN'),          # space prepopsitions\n    (r'(under|Under|below|Below)$', 'IN'),      # space prepopsitions\n    (r'(over|Over|above|Above)$', 'IN'),        # space prepopsitions\n    (r'(across|Across|through|Through)$', 'IN'),# space prepopsitions\n    (r'(into|Into|towards|Towards)$', 'IN'),    # space prepopsitions\n    (r'(onto|Onto|from|From)$', 'IN'),          # space prepopsitions\n    (r'.*able$', 'JJ'), # adjectives\n    (r'.*ness$', 'NN'), # nouns formed from adjectives\n    (r'.*ly$', 'RB'),   # adverbs\n    (r'.*s$', 'NNS'),   # plural nouns\n    (r'.*ing$', 'VBG'), # gerunds\n    (r'.*ed$', 'VBD'),  # past tense verbs\n    (r'.*', 'NN'),      # nouns (default)\n    ])\n    return _tagger.tag\n\n\ndef find_maltparser(parser_dirname):\n    if os.path.exists(parser_dirname): # If a full path is given.\n        _malt_dir = parser_dirname\n    else: # Try to find path to maltparser directory in environment variables.\n        _malt_dir = find_dir(parser_dirname, env_vars=('MALT_PARSER',))\n    malt_dependencies = ['','','']\n    _malt_jars = set(find_jars_within_path(_malt_dir))\n    _jars = set(os.path.split(jar)[1] for jar in _malt_jars)\n    malt_dependencies = set(['log4j.jar', 'libsvm.jar', 'liblinear-1.8.jar'])\n\n    assert malt_dependencies.issubset(_jars)\n    assert any(filter(lambda i: i.startswith('maltparser-') and i.endswith('.jar'), _jars))\n    return list(_malt_jars)\n\n\ndef find_malt_model(model_filename):\n    if model_filename == None:\n        return 'malt_temp.mco'\n    elif os.path.exists(model_filename): # If a full path is given.\n        return model_filename\n    else: # Try to find path to malt model in environment variables.\n        return find_file(model_filename, env_vars=('MALT_MODEL',), verbose=False)\n\n\nclass MaltParser(ParserI):\n    def __init__(self, parser_dirname, model_filename=None, tagger=None, additional_java_args=None):\n\n        self.malt_jars = find_maltparser(parser_dirname)\n        self.additional_java_args = additional_java_args if \\\n                        additional_java_args is not None else []\n        self.model = find_malt_model(model_filename)\n        self._trained = self.model != 'malt_temp.mco'\n        self.working_dir = tempfile.gettempdir()\n        self.tagger = tagger if tagger is not None else malt_regex_tagger()\n\n    def parse_tagged_sents(self, sentences, verbose=False, top_relation_label='null'):\n        if not self._trained:\n            raise Exception(\"Parser has not been trained. Call train() first.\")\n\n        with tempfile.NamedTemporaryFile(prefix='malt_input.conll.',\n              dir=self.working_dir, mode='w', delete=False) as input_file:\n              with tempfile.NamedTemporaryFile(prefix='malt_output.conll.',\n                     dir=self.working_dir, mode='w', delete=False) as output_file:\n                for line in taggedsents_to_conll(sentences):\n                    input_file.write(text_type(line))\n                input_file.close()\n\n                cmd =self.generate_malt_command(input_file.name,\n                                output_file.name, mode=\"parse\")\n\n                _current_path = os.getcwd() # Remembers the current path.\n                try: # Change to modelfile path\n                    os.chdir(os.path.split(self.model)[0])\n                except:\n                    pass\n                ret = self._execute(cmd, verbose) # Run command.\n                os.chdir(_current_path) # Change back to current path.\n\n                if ret is not 0:\n                    raise Exception(\"MaltParser parsing (%s) failed with exit \"\n                            \"code %d\" % (' '.join(cmd), ret))\n\n                with open(output_file.name) as infile:\n                    for tree_str in infile.read().split('\\n\\n'):\n                        yield(iter([DependencyGraph(tree_str, top_relation_label=top_relation_label)]))\n\n        os.remove(input_file.name)\n        os.remove(output_file.name)\n\n    def parse_sents(self, sentences, verbose=False, top_relation_label='null'):\n        tagged_sentences = (self.tagger(sentence) for sentence in sentences)\n        return self.parse_tagged_sents(tagged_sentences, verbose, top_relation_label=top_relation_label)\n\n    def generate_malt_command(self, inputfilename, outputfilename=None, mode=None):\n\n        cmd = ['java']\n        cmd+= self.additional_java_args # Adds additional java arguments\n        classpaths_separator = ';' if sys.platform.startswith('win') else ':'\n        cmd+= ['-cp', classpaths_separator.join(self.malt_jars)] # Adds classpaths for jars\n        cmd+= ['org.maltparser.Malt'] # Adds the main function.\n\n        if os.path.exists(self.model): # when parsing\n            cmd+= ['-c', os.path.split(self.model)[-1]]\n        else: # when learning\n            cmd+= ['-c', self.model]\n\n        cmd+= ['-i', inputfilename]\n        if mode == 'parse':\n            cmd+= ['-o', outputfilename]\n        cmd+= ['-m', mode] # mode use to generate parses.\n        return cmd\n\n    @staticmethod\n    def _execute(cmd, verbose=False):\n        output = None if verbose else subprocess.PIPE\n        p = subprocess.Popen(cmd, stdout=output, stderr=output)\n        return p.wait()\n\n    def train(self, depgraphs, verbose=False):\n\n        with tempfile.NamedTemporaryFile(prefix='malt_train.conll.',\n             dir=self.working_dir, mode='w', delete=False) as input_file:\n            input_str = ('\\n'.join(dg.to_conll(10) for dg in depgraphs))\n            input_file.write(text_type(input_str))\n        self.train_from_file(input_file.name, verbose=verbose)\n        os.remove(input_file.name)\n\n    def train_from_file(self, conll_file, verbose=False):\n\n        if isinstance(conll_file, ZipFilePathPointer):\n            with tempfile.NamedTemporaryFile(prefix='malt_train.conll.',\n            dir=self.working_dir, mode='w', delete=False) as input_file:\n                with conll_file.open() as conll_input_file:\n                    conll_str = conll_input_file.read()\n                    input_file.write(text_type(conll_str))\n                return self.train_from_file(input_file.name, verbose=verbose)\n\n        cmd =self.generate_malt_command(conll_file, mode=\"learn\")\n        ret = self._execute(cmd, verbose)\n        if ret != 0:\n            raise Exception(\"MaltParser training (%s) failed with exit \"\n                    \"code %d\" % (' '.join(cmd), ret))\n        self._trained = True\n\n\nif __name__ == '__main__':\n    '''\n    A demostration function to show how NLTK users can use the malt parser API.\n\n    >>> from nltk import pos_tag\n    >>> assert 'MALT_PARSER' in os.environ, str(\n    ... \"Please set MALT_PARSER in your global environment, e.g.:\\n\"\n    ... \"$ export MALT_PARSER='/home/user/maltparser-1.7.2/'\")\n    >>>\n    >>> assert 'MALT_MODEL' in os.environ, str(\n    ... \"Please set MALT_MODEL in your global environment, e.g.:\\n\"\n    ... \"$ export MALT_MODEL='/home/user/engmalt.linear-1.7.mco'\")\n    >>>\n    >>> _dg1_str = str(\"1    John    _    NNP   _    _    2    SUBJ    _    _\\n\"\n    ...             \"2    sees    _    VB    _    _    0    ROOT    _    _\\n\"\n    ...             \"3    a       _    DT    _    _    4    SPEC    _    _\\n\"\n    ...             \"4    dog     _    NN    _    _    2    OBJ     _    _\\n\"\n    ...             \"5    .     _    .    _    _    2    PUNCT     _    _\\n\")\n    >>>\n    >>>\n    >>> _dg2_str  = str(\"1    John    _    NNP   _    _    2    SUBJ    _    _\\n\"\n    ...             \"2    walks   _    VB    _    _    0    ROOT    _    _\\n\"\n    ...             \"3    .     _    .    _    _    2    PUNCT     _    _\\n\")\n    >>> dg1 = DependencyGraph(_dg1_str)\n    >>> dg2 = DependencyGraph(_dg2_str)\n    >>> # Initialize a MaltParser object\n    >>> parser_dirname = 'maltparser-1.7.2'\n    >>> mp = MaltParser(parser_dirname=parser_dirname)\n    >>>\n    >>> # Trains a model.\n    >>> mp.train([dg1,dg2], verbose=False)\n    >>> sent1 = ['John','sees','Mary', '.']\n    >>> sent2 = ['John', 'walks', 'a', 'dog', '.']\n    >>>\n    >>> # Parse a single sentence.\n    >>> parsed_sent1 = mp.parse_one(sent1)\n    >>> parsed_sent2 = mp.parse_one(sent2)\n    >>> print (parsed_sent1.tree())\n    (sees John Mary .)\n    >>> print (parsed_sent2.tree())\n    (walks John (dog a) .)\n    >>>\n    >>> # Parsing multiple sentences.\n    >>> sentences = [sent1,sent2]\n    >>> parsed_sents = mp.parse_sents(sentences)\n    >>> print(next(next(parsed_sents)).tree())\n    (sees John Mary .)\n    >>> print(next(next(parsed_sents)).tree())\n    (walks John (dog a) .)\n    >>>\n    >>> # Initialize a MaltParser object with an English pre-trained model.\n    >>> parser_dirname = 'maltparser-1.7.2'\n    >>> model_name = 'engmalt.linear-1.7.mco'\n    >>> mp = MaltParser(parser_dirname=parser_dirname, model_filename=model_name, tagger=pos_tag)\n    >>> sent1 = 'I shot an elephant in my pajamas .'.split()\n    >>> sent2 = 'Time flies like banana .'.split()\n    >>> # Parse a single sentence.\n    >>> print(mp.parse_one(sent1).tree())\n    (shot I (elephant an) (in (pajamas my)) .)\n    >>> sentences = [sent1,sent2]\n    >>> parsed_sents = mp.parse_sents(sentences)\n    >>> print(next(next(parsed_sents)).tree())\n    (shot I (elephant an) (in (pajamas my)) .)\n    >>> print(next(next(parsed_sents)).tree())\n    (flies Time (like banana) .)\n    '''\n    import doctest\n    doctest.testmod()\n"], "nltk\\parse\\nonprojectivedependencyparser": [".py", "from __future__ import print_function\n\nimport math\nimport logging\n\nfrom six.moves import range\n\nfrom nltk.parse.dependencygraph import DependencyGraph\n\nlogger = logging.getLogger(__name__)\n\n\n\nclass DependencyScorerI(object):\n\n    def __init__(self):\n        if self.__class__ == DependencyScorerI:\n            raise TypeError('DependencyScorerI is an abstract interface')\n\n    def train(self, graphs):\n        raise NotImplementedError()\n\n    def score(self, graph):\n        raise NotImplementedError()\n\n\n\nclass NaiveBayesDependencyScorer(DependencyScorerI):\n\n    def __init__(self):\n        pass  # Do nothing without throwing error\n\n    def train(self, graphs):\n\n        from nltk.classify import NaiveBayesClassifier\n\n        labeled_examples = []\n        for graph in graphs:\n            for head_node in graph.nodes.values():\n                for child_index, child_node in graph.nodes.items():\n                    if child_index in head_node['deps']:\n                        label = \"T\"\n                    else:\n                        label = \"F\"\n                    labeled_examples.append(\n                        (\n                            dict(\n                                a=head_node['word'],\n                                b=head_node['tag'],\n                                c=child_node['word'],\n                                d=child_node['tag'],\n                            ),\n                            label,\n                        )\n                    )\n\n        self.classifier = NaiveBayesClassifier.train(labeled_examples)\n\n    def score(self, graph):\n        edges = []\n        for head_node in graph.nodes.values():\n            for child_node in graph.nodes.values():\n                edges.append(\n                    (\n                        dict(\n                            a=head_node['word'],\n                            b=head_node['tag'],\n                            c=child_node['word'],\n                            d=child_node['tag'],\n                        )\n                    )\n                )\n\n        edge_scores = []\n        row = []\n        count = 0\n        for pdist in self.classifier.prob_classify_many(edges):\n            logger.debug('%.4f %.4f', pdist.prob('T'), pdist.prob('F'))\n            row.append([math.log(pdist.prob(\"T\")+0.00000000001)])\n            count += 1\n            if count == len(graph.nodes):\n                edge_scores.append(row)\n                row = []\n                count = 0\n        return edge_scores\n\n\nclass DemoScorer(DependencyScorerI):\n    def train(self, graphs):\n        print('Training...')\n\n    def score(self, graph):\n        return [[[], [5],  [1],  [1]],\n                [[], [],   [11], [4]],\n                [[], [10], [],   [5]],\n                [[], [8],  [8],  []]]\n\n\n\nclass ProbabilisticNonprojectiveParser(object):\n    def __init__(self):\n        logging.debug('initializing prob. nonprojective...')\n\n    def train(self, graphs, dependency_scorer):\n        self._scorer = dependency_scorer\n        self._scorer.train(graphs)\n\n    def initialize_edge_scores(self, graph):\n        self.scores = self._scorer.score(graph)\n\n    def collapse_nodes(self, new_node, cycle_path, g_graph, b_graph, c_graph):\n        logger.debug('Collapsing nodes...')\n        for cycle_node_index in cycle_path:\n            g_graph.remove_by_address(cycle_node_index)\n        g_graph.add_node(new_node)\n        g_graph.redirect_arcs(cycle_path, new_node['address'])\n\n    def update_edge_scores(self, new_node, cycle_path):\n        logger.debug('cycle %s', cycle_path)\n\n        cycle_path = self.compute_original_indexes(cycle_path)\n\n        logger.debug('old cycle %s', cycle_path)\n        logger.debug('Prior to update: %s', self.scores)\n\n        for i, row in enumerate(self.scores):\n            for j, column in enumerate(self.scores[i]):\n                logger.debug(self.scores[i][j])\n                if (\n                    j in cycle_path\n                    and i not in cycle_path\n                    and self.scores[i][j]\n                ):\n                    subtract_val = self.compute_max_subtract_score(j, cycle_path)\n\n                    logger.debug('%s - %s', self.scores[i][j], subtract_val)\n\n                    new_vals = []\n                    for cur_val in self.scores[i][j]:\n                        new_vals.append(cur_val - subtract_val)\n\n                    self.scores[i][j] = new_vals\n\n        for i, row in enumerate(self.scores):\n            for j, cell in enumerate(self.scores[i]):\n                if i in cycle_path and j in cycle_path:\n                    self.scores[i][j] = []\n\n        logger.debug('After update: %s', self.scores)\n\n    def compute_original_indexes(self, new_indexes):\n        swapped = True\n        while swapped:\n            originals = []\n            swapped = False\n            for new_index in new_indexes:\n                if new_index in self.inner_nodes:\n                    for old_val in self.inner_nodes[new_index]:\n                        if old_val not in originals:\n                            originals.append(old_val)\n                            swapped = True\n                else:\n                    originals.append(new_index)\n            new_indexes = originals\n        return new_indexes\n\n    def compute_max_subtract_score(self, column_index, cycle_indexes):\n        max_score = -100000\n        for row_index in cycle_indexes:\n            for subtract_val in self.scores[row_index][column_index]:\n                if subtract_val > max_score:\n                    max_score = subtract_val\n        return max_score\n\n    def best_incoming_arc(self, node_index):\n        originals = self.compute_original_indexes([node_index])\n        logger.debug('originals: %s', originals)\n\n        max_arc = None\n        max_score = None\n        for row_index in range(len(self.scores)):\n            for col_index in range(len(self.scores[row_index])):\n                if col_index in originals and (max_score is None or self.scores[row_index][col_index] > max_score):\n                    max_score = self.scores[row_index][col_index]\n                    max_arc = row_index\n                    logger.debug('%s, %s', row_index, col_index)\n\n        logger.debug(max_score)\n\n        for key in self.inner_nodes:\n            replaced_nodes = self.inner_nodes[key]\n            if max_arc in replaced_nodes:\n                return key\n\n        return max_arc\n\n    def original_best_arc(self, node_index):\n        originals = self.compute_original_indexes([node_index])\n        max_arc = None\n        max_score = None\n        max_orig = None\n        for row_index in range(len(self.scores)):\n            for col_index in range(len(self.scores[row_index])):\n                if col_index in originals and (max_score is None or self.scores[row_index][col_index] > max_score):\n                    max_score = self.scores[row_index][col_index]\n                    max_arc = row_index\n                    max_orig = col_index\n        return [max_arc, max_orig]\n\n    def parse(self, tokens, tags):\n        self.inner_nodes = {}\n\n        g_graph = DependencyGraph()\n        for index, token in enumerate(tokens):\n            g_graph.nodes[index + 1].update(\n                {\n                    'word': token,\n                    'tag': tags[index],\n                    'rel': 'NTOP',\n                    'address': index + 1,\n                }\n            )\n\n\n        g_graph.connect_graph()\n        original_graph = DependencyGraph()\n        for index, token in enumerate(tokens):\n            original_graph.nodes[index + 1].update(\n                {\n                    'word': token,\n                    'tag': tags[index],\n                    'rel': 'NTOP',\n                    'address': index+1,\n                }\n            )\n\n        b_graph = DependencyGraph()\n        c_graph = DependencyGraph()\n\n        for index, token in enumerate(tokens):\n            c_graph.nodes[index + 1].update(\n                {\n                    'word': token,\n                    'tag': tags[index],\n                    'rel': 'NTOP',\n                    'address': index + 1,\n                }\n            )\n\n        self.initialize_edge_scores(g_graph)\n        logger.debug(self.scores)\n        unvisited_vertices = [\n            vertex['address'] for vertex in c_graph.nodes.values()\n        ]\n        nr_vertices = len(tokens)\n        betas = {}\n        while unvisited_vertices:\n            current_vertex = unvisited_vertices.pop(0)\n            logger.debug('current_vertex: %s', current_vertex)\n            current_node = g_graph.get_by_address(current_vertex)\n            logger.debug('current_node: %s', current_node)\n            best_in_edge = self.best_incoming_arc(current_vertex)\n            betas[current_vertex] = self.original_best_arc(current_vertex)\n            logger.debug('best in arc: %s --> %s', best_in_edge, current_vertex)\n            for new_vertex in [current_vertex, best_in_edge]:\n                b_graph.nodes[new_vertex].update(\n                    {\n                        'word': 'TEMP',\n                        'rel': 'NTOP',\n                        'address': new_vertex,\n                    }\n                )\n            b_graph.add_arc(best_in_edge, current_vertex)\n            cycle_path = b_graph.contains_cycle()\n            if cycle_path:\n                new_node = {\n                    'word': 'NONE',\n                    'rel': 'NTOP',\n                    'address': nr_vertices + 1,\n                }\n                c_graph.add_node(new_node)\n                self.update_edge_scores(new_node, cycle_path)\n                self.collapse_nodes(new_node, cycle_path, g_graph, b_graph, c_graph)\n                for cycle_index in cycle_path:\n                    c_graph.add_arc(new_node['address'], cycle_index)\n\n                self.inner_nodes[new_node['address']] = cycle_path\n\n                unvisited_vertices.insert(0, nr_vertices + 1)\n\n                nr_vertices += 1\n\n                for cycle_node_address in cycle_path:\n                    b_graph.remove_by_address(cycle_node_address)\n\n            logger.debug('g_graph: %s', g_graph)\n            logger.debug('b_graph: %s', b_graph)\n            logger.debug('c_graph: %s', c_graph)\n            logger.debug('Betas: %s', betas)\n            logger.debug('replaced nodes %s', self.inner_nodes)\n\n        logger.debug('Final scores: %s', self.scores)\n\n        logger.debug('Recovering parse...')\n        for i in range(len(tokens) + 1, nr_vertices + 1):\n            betas[betas[i][1]] = betas[i]\n\n        logger.debug('Betas: %s', betas)\n        for node in original_graph.nodes.values():\n            node['deps'] = {}\n        for i in range(1, len(tokens) + 1):\n            original_graph.add_arc(betas[i][0], betas[i][1])\n\n        logger.debug('Done.')\n        yield original_graph\n\n\n\nclass NonprojectiveDependencyParser(object):\n\n    def __init__(self, dependency_grammar):\n        self._grammar = dependency_grammar\n\n    def parse(self, tokens):\n        self._graph = DependencyGraph()\n\n        for index, token in enumerate(tokens):\n            self._graph.nodes[index] = {\n                'word': token,\n                'deps': [],\n                'rel': 'NTOP',\n                'address': index,\n            }\n\n        for head_node in self._graph.nodes.values():\n            deps = []\n            for dep_node in self._graph.nodes.values()  :\n                if (\n                    self._grammar.contains(head_node['word'], dep_node['word'])\n                    and head_node['word'] != dep_node['word']\n                ):\n                    deps.append(dep_node['address'])\n            head_node['deps'] = deps\n\n        roots = []\n        possible_heads = []\n        for i, word in enumerate(tokens):\n            heads = []\n            for j, head in enumerate(tokens):\n                if (i != j) and self._grammar.contains(head, word):\n                    heads.append(j)\n            if len(heads) == 0:\n                roots.append(i)\n            possible_heads.append(heads)\n\n        if len(roots) < 2:\n            if len(roots) == 0:\n                for i in range(len(tokens)):\n                    roots.append(i)\n\n            analyses = []\n            for root in roots:\n                stack = []\n                analysis = [[] for i in range(len(possible_heads))]\n            i = 0\n            forward = True\n            while i >= 0:\n                if forward:\n                    if len(possible_heads[i]) == 1:\n                        analysis[i] = possible_heads[i][0]\n                    elif len(possible_heads[i]) == 0:\n                        analysis[i] = -1\n                    else:\n                        head = possible_heads[i].pop()\n                        analysis[i] = head\n                        stack.append([i, head])\n                if not forward:\n                    index_on_stack = False\n                    for stack_item in stack:\n                        if stack_item[0] == i:\n                            index_on_stack = True\n                    orig_length = len(possible_heads[i])\n\n                    if index_on_stack and orig_length == 0:\n                        for j in range(len(stack) - 1, -1, -1):\n                            stack_item = stack[j]\n                            if stack_item[0] == i:\n                                possible_heads[i].append(stack.pop(j)[1])\n\n                    elif index_on_stack and orig_length > 0:\n                        head = possible_heads[i].pop()\n                        analysis[i] = head\n                        stack.append([i, head])\n                        forward = True\n\n                if i + 1 == len(possible_heads):\n                    analyses.append(analysis[:])\n                    forward = False\n                if forward:\n                    i += 1\n                else:\n                    i -= 1\n\n        for analysis in analyses:\n            if analysis.count(-1) > 1:\n                continue\n\n            graph = DependencyGraph()\n            graph.root = graph.nodes[analysis.index(-1) + 1]\n\n            for address, (token, head_index) in enumerate(zip(tokens, analysis), start=1):\n                head_address = head_index + 1\n\n                node = graph.nodes[address]\n                node.update(\n                    {\n                        'word': token,\n                        'address': address,\n                    }\n                )\n\n                if head_address == 0:\n                    rel = 'ROOT'\n                else:\n                    rel = ''\n                graph.nodes[head_index + 1]['deps'][rel].append(address)\n\n            yield graph\n\n\n\ndef demo():\n    nonprojective_conll_parse_demo()\n    rule_based_demo()\n\n\ndef hall_demo():\n    npp = ProbabilisticNonprojectiveParser()\n    npp.train([], DemoScorer())\n    for parse_graph in npp.parse(['v1', 'v2', 'v3'], [None, None, None]):\n        print(parse_graph)\n\n\ndef nonprojective_conll_parse_demo():\n    from nltk.parse.dependencygraph import conll_data2\n\n    graphs = [\n        DependencyGraph(entry) for entry in conll_data2.split('\\n\\n') if entry\n    ]\n    npp = ProbabilisticNonprojectiveParser()\n    npp.train(graphs, NaiveBayesDependencyScorer())\n    for parse_graph in npp.parse(['Cathy', 'zag', 'hen', 'zwaaien', '.'], ['N', 'V', 'Pron', 'Adj', 'N', 'Punc']):\n        print(parse_graph)\n\n\ndef rule_based_demo():\n    from nltk.grammar import DependencyGrammar\n\n    grammar = DependencyGrammar.fromstring(\"\"\"\n    'taught' -> 'play' | 'man'\n    'man' -> 'the' | 'in'\n    'in' -> 'corner'\n    'corner' -> 'the'\n    'play' -> 'golf' | 'dachshund' | 'to'\n    'dachshund' -> 'his'\n    \"\"\")\n    print(grammar)\n    ndp = NonprojectiveDependencyParser(grammar)\n    graphs = ndp.parse(['the', 'man', 'in', 'the', 'corner', 'taught', 'his', 'dachshund', 'to', 'play', 'golf'])\n    print('Graphs:')\n    for graph in graphs:\n        print(graph)\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\parse\\pchart": [".py", "\nfrom __future__ import print_function, unicode_literals\n\n\n\nfrom functools import reduce\nfrom nltk.tree import Tree, ProbabilisticTree\nfrom nltk.grammar import Nonterminal, PCFG\n\nfrom nltk.parse.api import ParserI\nfrom nltk.parse.chart import Chart, LeafEdge, TreeEdge, AbstractChartRule\nfrom nltk.compat import python_2_unicode_compatible\n\nclass ProbabilisticLeafEdge(LeafEdge):\n    def prob(self): return 1.0\n\nclass ProbabilisticTreeEdge(TreeEdge):\n    def __init__(self, prob, *args, **kwargs):\n        TreeEdge.__init__(self, *args, **kwargs)\n        self._prob = prob\n        self._comparison_key = (self._comparison_key, prob)\n\n    def prob(self): return self._prob\n\n    @staticmethod\n    def from_production(production, index, p):\n        return ProbabilisticTreeEdge(p, (index, index), production.lhs(),\n                                     production.rhs(), 0)\n\nclass ProbabilisticBottomUpInitRule(AbstractChartRule):\n    NUM_EDGES=0\n    def apply(self, chart, grammar):\n        for index in range(chart.num_leaves()):\n            new_edge = ProbabilisticLeafEdge(chart.leaf(index), index)\n            if chart.insert(new_edge, ()):\n                yield new_edge\n\nclass ProbabilisticBottomUpPredictRule(AbstractChartRule):\n    NUM_EDGES=1\n    def apply(self, chart, grammar, edge):\n        if edge.is_incomplete(): return\n        for prod in grammar.productions():\n            if edge.lhs() == prod.rhs()[0]:\n                new_edge = ProbabilisticTreeEdge.from_production(prod, edge.start(), prod.prob())\n                if chart.insert(new_edge, ()):\n                    yield new_edge\n\nclass ProbabilisticFundamentalRule(AbstractChartRule):\n    NUM_EDGES=2\n    def apply(self, chart, grammar, left_edge, right_edge):\n        if not (left_edge.end() == right_edge.start() and\n                left_edge.nextsym() == right_edge.lhs() and\n                left_edge.is_incomplete() and right_edge.is_complete()):\n            return\n\n        p = left_edge.prob() * right_edge.prob()\n        new_edge = ProbabilisticTreeEdge(p,\n                            span=(left_edge.start(), right_edge.end()),\n                            lhs=left_edge.lhs(), rhs=left_edge.rhs(),\n                            dot=left_edge.dot()+1)\n\n        changed_chart = False\n        for cpl1 in chart.child_pointer_lists(left_edge):\n            if chart.insert(new_edge, cpl1+(right_edge,)):\n                changed_chart = True\n\n        if changed_chart: yield new_edge\n\n@python_2_unicode_compatible\nclass SingleEdgeProbabilisticFundamentalRule(AbstractChartRule):\n    NUM_EDGES=1\n\n    _fundamental_rule = ProbabilisticFundamentalRule()\n\n    def apply(self, chart, grammar, edge1):\n        fr = self._fundamental_rule\n        if edge1.is_incomplete():\n            for edge2 in chart.select(start=edge1.end(), is_complete=True,\n                                     lhs=edge1.nextsym()):\n                for new_edge in fr.apply(chart, grammar, edge1, edge2):\n                    yield new_edge\n        else:\n            for edge2 in chart.select(end=edge1.start(), is_complete=False,\n                                      nextsym=edge1.lhs()):\n                for new_edge in fr.apply(chart, grammar, edge2, edge1):\n                    yield new_edge\n\n    def __str__(self):\n        return 'Fundamental Rule'\n\nclass BottomUpProbabilisticChartParser(ParserI):\n    def __init__(self, grammar, beam_size=0, trace=0):\n        if not isinstance(grammar, PCFG):\n            raise ValueError(\"The grammar must be probabilistic PCFG\")\n        self._grammar = grammar\n        self.beam_size = beam_size\n        self._trace = trace\n\n    def grammar(self):\n        return self._grammar\n\n    def trace(self, trace=2):\n        self._trace = trace\n\n    def parse(self, tokens):\n        self._grammar.check_coverage(tokens)\n        chart = Chart(list(tokens))\n        grammar = self._grammar\n\n        bu_init = ProbabilisticBottomUpInitRule()\n        bu = ProbabilisticBottomUpPredictRule()\n        fr = SingleEdgeProbabilisticFundamentalRule()\n\n        queue = []\n\n        for edge in bu_init.apply(chart, grammar):\n            if self._trace > 1:\n                print('  %-50s [%s]' % (chart.pretty_format_edge(edge,width=2),\n                                        edge.prob()))\n            queue.append(edge)\n\n        while len(queue) > 0:\n            self.sort_queue(queue, chart)\n\n            if self.beam_size:\n                self._prune(queue, chart)\n\n            edge = queue.pop()\n            if self._trace > 0:\n                print('  %-50s [%s]' % (chart.pretty_format_edge(edge,width=2),\n                                        edge.prob()))\n\n            queue.extend(bu.apply(chart, grammar, edge))\n            queue.extend(fr.apply(chart, grammar, edge))\n\n        parses = list(chart.parses(grammar.start(), ProbabilisticTree))\n\n        prod_probs = {}\n        for prod in grammar.productions():\n            prod_probs[prod.lhs(), prod.rhs()] = prod.prob()\n        for parse in parses:\n            self._setprob(parse, prod_probs)\n\n        parses.sort(reverse=True, key=lambda tree: tree.prob())\n\n        return iter(parses)\n\n    def _setprob(self, tree, prod_probs):\n        if tree.prob() is not None: return\n\n        lhs = Nonterminal(tree.label())\n        rhs = []\n        for child in tree:\n            if isinstance(child, Tree):\n                rhs.append(Nonterminal(child.label()))\n            else:\n                rhs.append(child)\n        prob = prod_probs[lhs, tuple(rhs)]\n\n        for child in tree:\n            if isinstance(child, Tree):\n                self._setprob(child, prod_probs)\n                prob *= child.prob()\n\n        tree.set_prob(prob)\n\n    def sort_queue(self, queue, chart):\n        raise NotImplementedError()\n\n    def _prune(self, queue, chart):\n        if len(queue) > self.beam_size:\n            split = len(queue)-self.beam_size\n            if self._trace > 2:\n                for edge in queue[:split]:\n                    print('  %-50s [DISCARDED]' % chart.pretty_format_edge(edge,2))\n            del queue[:split]\n\nclass InsideChartParser(BottomUpProbabilisticChartParser):\n    def sort_queue(self, queue, chart):\n        queue.sort(key=lambda edge: edge.prob())\n\n\nimport random\nclass RandomChartParser(BottomUpProbabilisticChartParser):\n    def sort_queue(self, queue, chart):\n        i = random.randint(0, len(queue)-1)\n        (queue[-1], queue[i]) = (queue[i], queue[-1])\n\nclass UnsortedChartParser(BottomUpProbabilisticChartParser):\n    def sort_queue(self, queue, chart): return\n\nclass LongestChartParser(BottomUpProbabilisticChartParser):\n    def sort_queue(self, queue, chart):\n        queue.sort(key=lambda edge: edge.length())\n\n\ndef demo(choice=None, draw_parses=None, print_parses=None):\n    import sys, time\n    from nltk import tokenize\n    from nltk.parse import pchart\n\n    toy_pcfg1 = PCFG.fromstring(\"\"\"\n    S -> NP VP [1.0]\n    NP -> Det N [0.5] | NP PP [0.25] | 'John' [0.1] | 'I' [0.15]\n    Det -> 'the' [0.8] | 'my' [0.2]\n    N -> 'man' [0.5] | 'telescope' [0.5]\n    VP -> VP PP [0.1] | V NP [0.7] | V [0.2]\n    V -> 'ate' [0.35] | 'saw' [0.65]\n    PP -> P NP [1.0]\n    P -> 'with' [0.61] | 'under' [0.39]\n    \"\"\")\n\n    toy_pcfg2 = PCFG.fromstring(\"\"\"\n    S    -> NP VP         [1.0]\n    VP   -> V NP          [.59]\n    VP   -> V             [.40]\n    VP   -> VP PP         [.01]\n    NP   -> Det N         [.41]\n    NP   -> Name          [.28]\n    NP   -> NP PP         [.31]\n    PP   -> P NP          [1.0]\n    V    -> 'saw'         [.21]\n    V    -> 'ate'         [.51]\n    V    -> 'ran'         [.28]\n    N    -> 'boy'         [.11]\n    N    -> 'cookie'      [.12]\n    N    -> 'table'       [.13]\n    N    -> 'telescope'   [.14]\n    N    -> 'hill'        [.5]\n    Name -> 'Jack'        [.52]\n    Name -> 'Bob'         [.48]\n    P    -> 'with'        [.61]\n    P    -> 'under'       [.39]\n    Det  -> 'the'         [.41]\n    Det  -> 'a'           [.31]\n    Det  -> 'my'          [.28]\n    \"\"\")\n\n    demos = [('I saw John with my telescope', toy_pcfg1),\n             ('the boy saw Jack with Bob under the table with a telescope',\n              toy_pcfg2)]\n\n    if choice is None:\n        print()\n        for i in range(len(demos)):\n            print('%3s: %s' % (i+1, demos[i][0]))\n            print('     %r' % demos[i][1])\n            print()\n        print('Which demo (%d-%d)? ' % (1, len(demos)), end=' ')\n        choice = int(sys.stdin.readline().strip())-1\n    try:\n        sent, grammar = demos[choice]\n    except:\n        print('Bad sentence number')\n        return\n\n    tokens = sent.split()\n\n    parsers = [\n        pchart.InsideChartParser(grammar),\n        pchart.RandomChartParser(grammar),\n        pchart.UnsortedChartParser(grammar),\n        pchart.LongestChartParser(grammar),\n        pchart.InsideChartParser(grammar, beam_size = len(tokens)+1)   # was BeamParser\n        ]\n\n    times = []\n    average_p = []\n    num_parses = []\n    all_parses = {}\n    for parser in parsers:\n        print('\\ns: %s\\nparser: %s\\ngrammar: %s' % (sent,parser,grammar))\n        parser.trace(3)\n        t = time.time()\n        parses = list(parser.parse(tokens))\n        times.append(time.time()-t)\n        p = (reduce(lambda a,b:a+b.prob(), parses, 0)/len(parses) if parses else 0)\n        average_p.append(p)\n        num_parses.append(len(parses))\n        for p in parses: all_parses[p.freeze()] = 1\n\n    print()\n    print('       Parser      Beam | Time (secs)   # Parses   Average P(parse)')\n    print('------------------------+------------------------------------------')\n    for i in range(len(parsers)):\n        print('%18s %4d |%11.4f%11d%19.14f' % (parsers[i].__class__.__name__,\n                                             parsers[i].beam_size,\n                                             times[i],num_parses[i],average_p[i]))\n    parses = all_parses.keys()\n    if parses: p = reduce(lambda a,b:a+b.prob(), parses, 0)/len(parses)\n    else: p = 0\n    print('------------------------+------------------------------------------')\n    print('%18s      |%11s%11d%19.14f' % ('(All Parses)', 'n/a', len(parses), p))\n\n    if draw_parses is None:\n        print()\n        print('Draw parses (y/n)? ', end=' ')\n        draw_parses = sys.stdin.readline().strip().lower().startswith('y')\n    if draw_parses:\n        from nltk.draw.tree import draw_trees\n        print('  please wait...')\n        draw_trees(*parses)\n\n    if print_parses is None:\n        print()\n        print('Print parses (y/n)? ', end=' ')\n        print_parses = sys.stdin.readline().strip().lower().startswith('y')\n    if print_parses:\n        for parse in parses:\n            print(parse)\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\parse\\projectivedependencyparser": [".py", "from __future__ import print_function, unicode_literals\n\nfrom collections import defaultdict\nfrom itertools import chain\nfrom functools import total_ordering\n\nfrom nltk.grammar import (DependencyProduction, DependencyGrammar,\n                          ProbabilisticDependencyGrammar)\nfrom nltk.parse.dependencygraph import DependencyGraph\nfrom nltk.internals import raise_unorderable_types\nfrom nltk.compat import python_2_unicode_compatible\n\n\n@total_ordering\n@python_2_unicode_compatible\nclass DependencySpan(object):\n    def __init__(self, start_index, end_index, head_index, arcs, tags):\n        self._start_index = start_index\n        self._end_index = end_index\n        self._head_index = head_index\n        self._arcs = arcs\n        self._tags = tags\n        self._comparison_key = (start_index, end_index, head_index, tuple(arcs))\n        self._hash = hash(self._comparison_key)\n\n    def head_index(self):\n        return self._head_index\n\n    def __repr__(self):\n        return 'Span %d-%d; Head Index: %d' % (self._start_index, self._end_index, self._head_index)\n\n    def __str__(self):\n        str = 'Span %d-%d; Head Index: %d' % (self._start_index, self._end_index, self._head_index)\n        for i in range(len(self._arcs)):\n            str += '\\n%d <- %d, %s' % (i, self._arcs[i], self._tags[i])\n        return str\n\n    def __eq__(self, other):\n        return (type(self) == type(other) and\n                self._comparison_key == other._comparison_key)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __lt__(self, other):\n        if not isinstance(other, DependencySpan):\n            raise_unorderable_types(\"<\", self, other)\n        return self._comparison_key < other._comparison_key\n\n    def __hash__(self):\n        return self._hash\n\n\n@python_2_unicode_compatible\nclass ChartCell(object):\n    def __init__(self, x, y):\n        self._x = x\n        self._y = y\n        self._entries = set([])\n\n    def add(self, span):\n        self._entries.add(span)\n\n    def __str__(self):\n        return 'CC[%d,%d]: %s' % (self._x, self._y, self._entries)\n\n    def __repr__(self):\n        return '%s' % self\n\n\n\n\nclass ProjectiveDependencyParser(object):\n\n    def __init__(self, dependency_grammar):\n        self._grammar = dependency_grammar\n\n    def parse(self, tokens):\n        self._tokens = list(tokens)\n        chart = []\n        for i in range(0, len(self._tokens) + 1):\n            chart.append([])\n            for j in range(0, len(self._tokens) + 1):\n                chart[i].append(ChartCell(i,j))\n                if i==j+1:\n                    chart[i][j].add(DependencySpan(i-1,i,i-1,[-1], ['null']))\n\n        for i in range(1,len(self._tokens)+1):\n            for j in range(i-2,-1,-1):\n                for k in range(i-1,j,-1):\n                    for span1 in chart[k][j]._entries:\n                        for span2 in chart[i][k]._entries:\n                            for newspan in self.concatenate(span1, span2):\n                                chart[i][j].add(newspan)\n\n        for parse in chart[len(self._tokens)][0]._entries:\n            conll_format = \"\"\n            for i in range(len(tokens)):\n                conll_format += '\\t%d\\t%s\\t%s\\t%s\\t%s\\t%s\\t%d\\t%s\\t%s\\t%s\\n' % (i+1, tokens[i], tokens[i], 'null', 'null', 'null', parse._arcs[i] + 1, 'ROOT', '-', '-')\n            dg = DependencyGraph(conll_format)\n            yield dg.tree()\n\n\n    def concatenate(self, span1, span2):\n        spans = []\n        if span1._start_index == span2._start_index:\n            print('Error: Mismatched spans - replace this with thrown error')\n        if span1._start_index > span2._start_index:\n            temp_span = span1\n            span1 = span2\n            span2 = temp_span\n        new_arcs = span1._arcs + span2._arcs\n        new_tags = span1._tags + span2._tags\n        if self._grammar.contains(self._tokens[span1._head_index], self._tokens[span2._head_index]):\n            new_arcs[span2._head_index - span1._start_index] = span1._head_index\n            spans.append(DependencySpan(span1._start_index, span2._end_index, span1._head_index, new_arcs, new_tags))\n        new_arcs = span1._arcs + span2._arcs\n        if self._grammar.contains(self._tokens[span2._head_index], self._tokens[span1._head_index]):\n            new_arcs[span1._head_index - span1._start_index] = span2._head_index\n            spans.append(DependencySpan(span1._start_index, span2._end_index, span2._head_index, new_arcs, new_tags))\n        return spans\n\n\n\n\nclass ProbabilisticProjectiveDependencyParser(object):\n\n    def __init__(self):\n\n    def parse(self, tokens):\n        self._tokens = list(tokens)\n        chart = []\n        for i in range(0, len(self._tokens) + 1):\n            chart.append([])\n            for j in range(0, len(self._tokens) + 1):\n                chart[i].append(ChartCell(i,j))\n                if i==j+1:\n                    if tokens[i-1] in self._grammar._tags:\n                        for tag in self._grammar._tags[tokens[i-1]]:\n                            chart[i][j].add(DependencySpan(i-1,i,i-1,[-1], [tag]))\n                    else:\n                        print('No tag found for input token \\'%s\\', parse is impossible.' % tokens[i-1])\n                        return []\n        for i in range(1,len(self._tokens)+1):\n            for j in range(i-2,-1,-1):\n                for k in range(i-1,j,-1):\n                    for span1 in chart[k][j]._entries:\n                            for span2 in chart[i][k]._entries:\n                                for newspan in self.concatenate(span1, span2):\n                                    chart[i][j].add(newspan)\n        trees = []\n        max_parse = None\n        max_score = 0\n        for parse in chart[len(self._tokens)][0]._entries:\n            conll_format = \"\"\n            malt_format = \"\"\n            for i in range(len(tokens)):\n                malt_format += '%s\\t%s\\t%d\\t%s\\n' % (tokens[i], 'null', parse._arcs[i] + 1, 'null')\n                conll_format += '\\t%d\\t%s\\t%s\\t%s\\t%s\\t%s\\t%d\\t%s\\t%s\\t%s\\n' % (i+1, tokens[i], tokens[i], parse._tags[i], parse._tags[i], 'null', parse._arcs[i] + 1, 'ROOT', '-', '-')\n            dg = DependencyGraph(conll_format)\n            score = self.compute_prob(dg)            \n            trees.append((score, dg.tree()))\n        trees.sort()\n        return (tree for (score, tree) in trees)\n\n\n    def concatenate(self, span1, span2):\n        spans = []\n        if span1._start_index == span2._start_index:\n            print('Error: Mismatched spans - replace this with thrown error')\n        if span1._start_index > span2._start_index:\n            temp_span = span1\n            span1 = span2\n            span2 = temp_span\n        new_arcs = span1._arcs + span2._arcs\n        new_tags = span1._tags + span2._tags\n        if self._grammar.contains(self._tokens[span1._head_index], self._tokens[span2._head_index]):\n            new_arcs[span2._head_index - span1._start_index] = span1._head_index\n            spans.append(DependencySpan(span1._start_index, span2._end_index, span1._head_index, new_arcs, new_tags))\n        new_arcs = span1._arcs + span2._arcs\n        new_tags = span1._tags + span2._tags\n        if self._grammar.contains(self._tokens[span2._head_index], self._tokens[span1._head_index]):\n            new_arcs[span1._head_index - span1._start_index] = span2._head_index\n            spans.append(DependencySpan(span1._start_index, span2._end_index, span2._head_index, new_arcs, new_tags))\n        return spans\n\n    def train(self, graphs):\n        productions = []\n        events = defaultdict(int)\n        tags = {}\n        for dg in graphs:\n            for node_index in range(1, len(dg.nodes)):\n                children = list(chain(*dg.nodes[node_index]['deps'].values()))\n                \n                nr_left_children = dg.left_children(node_index)\n                nr_right_children = dg.right_children(node_index)\n                nr_children = nr_left_children + nr_right_children\n                for child_index in range(0 - (nr_left_children + 1), nr_right_children + 2):\n                    head_word = dg.nodes[node_index]['word']\n                    head_tag = dg.nodes[node_index]['tag']\n                    if head_word in tags:\n                        tags[head_word].add(head_tag)\n                    else:\n                        tags[head_word] = set([head_tag])\n                    child = 'STOP'\n                    child_tag = 'STOP'\n                    prev_word = 'START'\n                    prev_tag = 'START'\n                    if child_index < 0:\n                        array_index = child_index + nr_left_children\n                        if array_index >= 0:\n                            child = dg.nodes[children[array_index]]['word']\n                            child_tag = dg.nodes[children[array_index]]['tag']\n                        if child_index != -1:\n                            prev_word = dg.nodes[children[array_index + 1]]['word']\n                            prev_tag = dg.nodes[children[array_index + 1]]['tag']\n                        if child != 'STOP':\n                            productions.append(DependencyProduction(head_word, [child]))\n                        head_event = '(head (%s %s) (mods (%s, %s, %s) left))' % (child, child_tag, prev_tag, head_word, head_tag)\n                        mod_event = '(mods (%s, %s, %s) left))' % (prev_tag, head_word, head_tag)\n                        events[head_event] += 1\n                        events[mod_event] += 1\n                    elif child_index > 0:\n                        array_index = child_index + nr_left_children - 1\n                        if array_index < nr_children:\n                            child = dg.nodes[children[array_index]]['word']\n                            child_tag = dg.nodes[children[array_index]]['tag']\n                        if child_index != 1:\n                            prev_word = dg.nodes[children[array_index - 1]]['word']\n                            prev_tag =  dg.nodes[children[array_index - 1]]['tag']\n                        if child != 'STOP':\n                            productions.append(DependencyProduction(head_word, [child]))\n                        head_event = '(head (%s %s) (mods (%s, %s, %s) right))' % (child, child_tag, prev_tag, head_word, head_tag)\n                        mod_event = '(mods (%s, %s, %s) right))' % (prev_tag, head_word, head_tag)\n                        events[head_event] += 1\n                        events[mod_event] += 1\n        self._grammar = ProbabilisticDependencyGrammar(productions, events, tags)\n\n    def compute_prob(self, dg):\n        prob = 1.0\n        for node_index in range(1, len(dg.nodes)):\n            children = list(chain(*dg.nodes[node_index]['deps'].values()))\n            \n            nr_left_children = dg.left_children(node_index)\n            nr_right_children = dg.right_children(node_index)\n            nr_children = nr_left_children + nr_right_children\n            for child_index in range(0 - (nr_left_children + 1), nr_right_children + 2):\n                head_word = dg.nodes[node_index]['word']\n                head_tag = dg.nodes[node_index]['tag']\n                child = 'STOP'\n                child_tag = 'STOP'\n                prev_word = 'START'\n                prev_tag = 'START'\n                if child_index < 0:\n                    array_index = child_index + nr_left_children\n                    if array_index >= 0:\n                        child = dg.nodes[children[array_index]]['word']\n                        child_tag = dg.nodes[children[array_index]]['tag']\n                    if child_index != -1:\n                        prev_word = dg.nodes[children[array_index + 1]]['word']\n                        prev_tag = dg.nodes[children[array_index + 1]]['tag']\n                    head_event = '(head (%s %s) (mods (%s, %s, %s) left))' % (child, child_tag, prev_tag, head_word, head_tag)\n                    mod_event = '(mods (%s, %s, %s) left))' % (prev_tag, head_word, head_tag)\n                    h_count = self._grammar._events[head_event]\n                    m_count = self._grammar._events[mod_event]\n                    \n                    if m_count != 0:\n                        prob *= (h_count / m_count)\n                    else:\n                        prob = 0.00000001  # Very small number  \n                    \n                elif child_index > 0:\n                    array_index = child_index + nr_left_children - 1\n                    if array_index < nr_children:\n                        child = dg.nodes[children[array_index]]['word']\n                        child_tag = dg.nodes[children[array_index]]['tag']\n                    if child_index != 1:\n                        prev_word = dg.nodes[children[array_index - 1]]['word']\n                        prev_tag = dg.nodes[children[array_index - 1]]['tag']\n                    head_event = '(head (%s %s) (mods (%s, %s, %s) right))' % (child, child_tag, prev_tag, head_word, head_tag)\n                    mod_event = '(mods (%s, %s, %s) right))' % (prev_tag, head_word, head_tag)\n                    h_count = self._grammar._events[head_event]\n                    m_count = self._grammar._events[mod_event]\n\n                    if m_count != 0:\n                        prob *= (h_count / m_count)\n                    else:\n                        prob = 0.00000001  # Very small number  \n\n        return prob\n\n\n\ndef demo():\n    projective_rule_parse_demo()\n    projective_prob_parse_demo()\n\n\ndef projective_rule_parse_demo():\n    grammar = DependencyGrammar.fromstring(\"\"\"\n    'scratch' -> 'cats' | 'walls'\n    'walls' -> 'the'\n    'cats' -> 'the'\n    \"\"\")\n    print(grammar)\n    pdp = ProjectiveDependencyParser(grammar)\n    trees = pdp.parse(['the', 'cats', 'scratch', 'the', 'walls'])\n    for tree in trees:\n        print(tree)\n\ndef arity_parse_demo():\n    \"\"\"\n    A demonstration showing the creation of a ``DependencyGrammar``\n    in which a specific number of modifiers is listed for a given\n    head.  This can further constrain the number of possible parses\n    created by a ``ProjectiveDependencyParser``.\n    \"\"\"\n    print()\n    print('A grammar with no arity constraints. Each DependencyProduction')\n    print('specifies a relationship between one head word and only one')\n    print('modifier word.')\n    grammar = DependencyGrammar.fromstring(\"\"\"\n    'fell' -> 'price' | 'stock'\n    'price' -> 'of' | 'the'\n    'of' -> 'stock'\n    'stock' -> 'the'\n    \"\"\")\n    print(grammar)\n\n    print()\n    print('For the sentence \\'The price of the stock fell\\', this grammar')\n    print('will produce the following three parses:')\n    pdp = ProjectiveDependencyParser(grammar)\n    trees = pdp.parse(['the', 'price', 'of', 'the', 'stock', 'fell'])\n    for tree in trees:\n        print(tree)\n\n    print()\n    print('By contrast, the following grammar contains a ')\n    print('DependencyProduction that specifies a relationship')\n    print('between a single head word, \\'price\\', and two modifier')\n    print('words, \\'of\\' and \\'the\\'.')\n    grammar = DependencyGrammar.fromstring(\"\"\"\n    'fell' -> 'price' | 'stock'\n    'price' -> 'of' 'the'\n    'of' -> 'stock'\n    'stock' -> 'the'\n    \"\"\")\n    print(grammar)\n\n    print()\n    print('This constrains the number of possible parses to just one:') # unimplemented, soon to replace\n    pdp = ProjectiveDependencyParser(grammar)\n    trees = pdp.parse(['the', 'price', 'of', 'the', 'stock', 'fell'])\n    for tree in trees:\n        print(tree)\n\n\ndef projective_prob_parse_demo():\n    \"\"\"\n    A demo showing the training and use of a projective\n    dependency parser.\n    \"\"\"\n    from nltk.parse.dependencygraph import conll_data2\n\n    graphs = [DependencyGraph(entry)\n              for entry in conll_data2.split('\\n\\n') if entry]\n    ppdp = ProbabilisticProjectiveDependencyParser()\n    print('Training Probabilistic Projective Dependency Parser...')\n    ppdp.train(graphs)\n    \n    sent = ['Cathy', 'zag', 'hen', 'wild', 'zwaaien', '.']\n    print('Parsing \\'', \" \".join(sent), '\\'...')\n    print('Parse:')\n    for tree in ppdp.parse(sent):\n        print(tree)\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\parse\\recursivedescent": [".py", "from __future__ import print_function, unicode_literals\n\nfrom nltk.grammar import Nonterminal\nfrom nltk.tree import Tree, ImmutableTree\nfrom nltk.compat import unicode_repr\n\nfrom nltk.parse.api import ParserI\n\nclass RecursiveDescentParser(ParserI):\n    def __init__(self, grammar, trace=0):\n        self._grammar = grammar\n        self._trace = trace\n\n    def grammar(self):\n        return self._grammar\n\n    def parse(self, tokens):\n\n        tokens = list(tokens)\n        self._grammar.check_coverage(tokens)\n\n        start = self._grammar.start().symbol()\n        initial_tree = Tree(start, [])\n        frontier = [()]\n        if self._trace:\n            self._trace_start(initial_tree, frontier, tokens)\n        return self._parse(tokens, initial_tree, frontier)\n\n    def _parse(self, remaining_text, tree, frontier):\n\n        if len(remaining_text) == 0 and len(frontier) == 0:\n            if self._trace:\n                self._trace_succeed(tree, frontier)\n            yield tree\n\n        elif len(frontier) == 0:\n            if self._trace:\n                self._trace_backtrack(tree, frontier)\n\n        elif isinstance(tree[frontier[0]], Tree):\n            for result in self._expand(remaining_text, tree, frontier):\n                yield result\n\n        else:\n            for result in self._match(remaining_text, tree, frontier):\n                yield result\n\n    def _match(self, rtext, tree, frontier):\n\n        tree_leaf = tree[frontier[0]]\n        if (len(rtext) > 0 and tree_leaf == rtext[0]):\n            newtree = tree.copy(deep=True)\n            newtree[frontier[0]] = rtext[0]\n            if self._trace:\n                self._trace_match(newtree, frontier[1:], rtext[0])\n            for result in self._parse(rtext[1:], newtree, frontier[1:]):\n                yield result\n        else:\n            if self._trace:\n                self._trace_backtrack(tree, frontier, rtext[:1])\n\n    def _expand(self, remaining_text, tree, frontier, production=None):\n\n        if production is None: productions = self._grammar.productions()\n        else: productions = [production]\n\n        for production in productions:\n            lhs = production.lhs().symbol()\n            if lhs == tree[frontier[0]].label():\n                subtree = self._production_to_tree(production)\n                if frontier[0] == ():\n                    newtree = subtree\n                else:\n                    newtree = tree.copy(deep=True)\n                    newtree[frontier[0]] = subtree\n                new_frontier = [frontier[0]+(i,) for i in\n                                range(len(production.rhs()))]\n                if self._trace:\n                    self._trace_expand(newtree, new_frontier, production)\n                for result in self._parse(remaining_text, newtree,\n                                          new_frontier + frontier[1:]):\n                    yield result\n\n    def _production_to_tree(self, production):\n        children = []\n        for elt in production.rhs():\n            if isinstance(elt, Nonterminal):\n                children.append(Tree(elt.symbol(), []))\n            else:\n                children.append(elt)\n        return Tree(production.lhs().symbol(), children)\n\n    def trace(self, trace=2):\n        self._trace = trace\n\n    def _trace_fringe(self, tree, treeloc=None):\n\n        if treeloc == (): print(\"*\", end=' ')\n        if isinstance(tree, Tree):\n            if len(tree) == 0:\n                print(unicode_repr(Nonterminal(tree.label())), end=' ')\n            for i in range(len(tree)):\n                if treeloc is not None and i == treeloc[0]:\n                    self._trace_fringe(tree[i], treeloc[1:])\n                else:\n                    self._trace_fringe(tree[i])\n        else:\n            print(unicode_repr(tree), end=' ')\n\n    def _trace_tree(self, tree, frontier, operation):\n        if self._trace == 2: print('  %c [' % operation, end=' ')\n        else: print('    [', end=' ')\n        if len(frontier) > 0: self._trace_fringe(tree, frontier[0])\n        else: self._trace_fringe(tree)\n        print(']')\n\n    def _trace_start(self, tree, frontier, text):\n        print('Parsing %r' % \" \".join(text))\n        if self._trace > 2: print('Start:')\n        if self._trace > 1: self._trace_tree(tree, frontier, ' ')\n\n    def _trace_expand(self, tree, frontier, production):\n        if self._trace > 2: print('Expand: %s' % production)\n        if self._trace > 1: self._trace_tree(tree, frontier, 'E')\n\n    def _trace_match(self, tree, frontier, tok):\n        if self._trace > 2: print('Match: %r' % tok)\n        if self._trace > 1: self._trace_tree(tree, frontier, 'M')\n\n    def _trace_succeed(self, tree, frontier):\n        if self._trace > 2: print('GOOD PARSE:')\n        if self._trace == 1: print('Found a parse:\\n%s' % tree)\n        if self._trace > 1: self._trace_tree(tree, frontier, '+')\n\n    def _trace_backtrack(self, tree, frontier, toks=None):\n        if self._trace > 2:\n            if toks: print('Backtrack: %r match failed' % toks[0])\n            else: print('Backtrack')\n\nclass SteppingRecursiveDescentParser(RecursiveDescentParser):\n    def __init__(self, grammar, trace=0):\n        super(SteppingRecursiveDescentParser, self).__init__(grammar, trace)\n        self._rtext = None\n        self._tree = None\n        self._frontier = [()]\n        self._tried_e = {}\n        self._tried_m = {}\n        self._history = []\n        self._parses = []\n\n    def _freeze(self, tree):\n        c = tree.copy()\n        return ImmutableTree.convert(c)\n\n    def parse(self, tokens):\n        tokens = list(tokens)\n        self.initialize(tokens)\n        while self.step() is not None:\n            pass\n        return self.parses()\n\n    def initialize(self, tokens):\n\n        self._rtext = tokens\n        start = self._grammar.start().symbol()\n        self._tree = Tree(start, [])\n        self._frontier = [()]\n        self._tried_e = {}\n        self._tried_m = {}\n        self._history = []\n        self._parses = []\n        if self._trace:\n            self._trace_start(self._tree, self._frontier, self._rtext)\n\n    def remaining_text(self):\n        return self._rtext\n\n    def frontier(self):\n        return self._frontier\n\n    def tree(self):\n        return self._tree\n\n    def step(self):\n        if self.untried_match():\n            token = self.match()\n            if token is not None: return token\n\n        production = self.expand()\n        if production is not None: return production\n\n        if self.backtrack():\n            self._trace_backtrack(self._tree, self._frontier)\n            return True\n\n        return None\n\n    def expand(self, production=None):\n\n        if len(self._frontier) == 0:\n            return None\n        if not isinstance(self._tree[self._frontier[0]], Tree):\n            return None\n\n        if production is None:\n            productions = self.untried_expandable_productions()\n        else: productions = [production]\n\n        parses = []\n        for prod in productions:\n            self._tried_e.setdefault(self._freeze(self._tree), []).append(prod)\n\n            for _result in self._expand(self._rtext, self._tree, self._frontier, prod):\n                return prod\n\n        return None\n\n    def match(self):\n\n        tok = self._rtext[0]\n        self._tried_m.setdefault(self._freeze(self._tree), []).append(tok)\n\n        if len(self._frontier) == 0:\n            return None\n        if isinstance(self._tree[self._frontier[0]], Tree):\n            return None\n\n        for _result in self._match(self._rtext, self._tree, self._frontier):\n            return self._history[-1][0][0]\n        return None\n\n    def backtrack(self):\n        if len(self._history) == 0: return False\n        (self._rtext, self._tree, self._frontier) = self._history.pop()\n        return True\n\n    def expandable_productions(self):\n        if len(self._frontier) == 0: return []\n        frontier_child = self._tree[self._frontier[0]]\n        if (len(self._frontier) == 0 or\n            not isinstance(frontier_child, Tree)):\n            return []\n\n        return [p for p in self._grammar.productions()\n                if p.lhs().symbol() == frontier_child.label()]\n\n    def untried_expandable_productions(self):\n\n        tried_expansions = self._tried_e.get(self._freeze(self._tree), [])\n        return [p for p in self.expandable_productions()\n                if p not in tried_expansions]\n\n    def untried_match(self):\n\n        if len(self._rtext) == 0: return False\n        tried_matches = self._tried_m.get(self._freeze(self._tree), [])\n        return (self._rtext[0] not in tried_matches)\n\n    def currently_complete(self):\n        return (len(self._frontier) == 0 and len(self._rtext) == 0)\n\n    def _parse(self, remaining_text, tree, frontier):\n        self._history.append( (self._rtext, self._tree, self._frontier) )\n        self._rtext = remaining_text\n        self._tree = tree\n        self._frontier = frontier\n\n        if (len(frontier) == 0 and len(remaining_text) == 0):\n            self._parses.append(tree)\n            self._trace_succeed(self._tree, self._frontier)\n\n        return [1]\n\n    def parses(self):\n        return iter(self._parses)\n\n    def set_grammar(self, grammar):\n        self._grammar = grammar\n\n\ndef demo():\n\n    from nltk import parse, CFG\n\n    grammar = CFG.fromstring(\"\"\"\n    S -> NP VP\n    NP -> Det N | Det N PP\n    VP -> V NP | V NP PP\n    PP -> P NP\n    NP -> 'I'\n    N -> 'man' | 'park' | 'telescope' | 'dog'\n    Det -> 'the' | 'a'\n    P -> 'in' | 'with'\n    V -> 'saw'\n    \"\"\")\n\n    for prod in grammar.productions():\n        print(prod)\n\n    sent = 'I saw a man in the park'.split()\n    parser = parse.RecursiveDescentParser(grammar, trace=2)\n    for p in parser.parse(sent):\n        print(p)\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\parse\\shiftreduce": [".py", "from __future__ import print_function, unicode_literals\n\nfrom nltk.grammar import Nonterminal\nfrom nltk.tree import Tree\nfrom nltk.compat import unicode_repr\n\nfrom nltk.parse.api import ParserI\n\nclass ShiftReduceParser(ParserI):\n    def __init__(self, grammar, trace=0):\n        self._grammar = grammar\n        self._trace = trace\n        self._check_grammar()\n\n    def grammar(self):\n        return self._grammar\n\n    def parse(self, tokens):\n        tokens = list(tokens)\n        self._grammar.check_coverage(tokens)\n\n        stack = []\n        remaining_text = tokens\n\n        if self._trace:\n            print('Parsing %r' % \" \".join(tokens))\n            self._trace_stack(stack, remaining_text)\n\n        while len(remaining_text) > 0:\n            self._shift(stack, remaining_text)\n            while self._reduce(stack, remaining_text): pass\n\n        if len(stack) == 1: \n            if stack[0].label() == self._grammar.start().symbol():\n                yield stack[0]\n\n    def _shift(self, stack, remaining_text):\n        stack.append(remaining_text[0])\n        remaining_text.remove(remaining_text[0])\n        if self._trace: self._trace_shift(stack, remaining_text)\n\n    def _match_rhs(self, rhs, rightmost_stack):\n\n        if len(rightmost_stack) != len(rhs): return False\n        for i in range(len(rightmost_stack)):\n            if isinstance(rightmost_stack[i], Tree):\n                if not isinstance(rhs[i], Nonterminal): return False\n                if rightmost_stack[i].label() != rhs[i].symbol(): return False\n            else:\n                if isinstance(rhs[i], Nonterminal): return False\n                if rightmost_stack[i] != rhs[i]: return False\n        return True\n\n    def _reduce(self, stack, remaining_text, production=None):\n        if production is None:\n            productions = self._grammar.productions()\n        else:\n            productions = [production]\n\n        for production in productions:\n            rhslen = len(production.rhs())\n\n            if self._match_rhs(production.rhs(), stack[-rhslen:]):\n\n                tree = Tree(production.lhs().symbol(), stack[-rhslen:])\n                stack[-rhslen:] = [tree]\n\n                if self._trace:\n                    self._trace_reduce(stack, production, remaining_text)\n                return production\n\n        return None\n\n    def trace(self, trace=2):\n        self._trace = trace\n\n    def _trace_stack(self, stack, remaining_text, marker=' '):\n        s = '  '+marker+' [ '\n        for elt in stack:\n            if isinstance(elt, Tree):\n                s += unicode_repr(Nonterminal(elt.label())) + ' '\n            else:\n                s += unicode_repr(elt) + ' '\n        s += '* ' + ' '.join(remaining_text) + ']'\n        print(s)\n\n    def _trace_shift(self, stack, remaining_text):\n        if self._trace > 2: print('Shift %r:' % stack[-1])\n        if self._trace == 2: self._trace_stack(stack, remaining_text, 'S')\n        elif self._trace > 0: self._trace_stack(stack, remaining_text)\n\n    def _trace_reduce(self, stack, production, remaining_text):\n        if self._trace > 2:\n            rhs = \" \".join(production.rhs())\n            print('Reduce %r <- %s' % (production.lhs(), rhs))\n        if self._trace == 2: self._trace_stack(stack, remaining_text, 'R')\n        elif self._trace > 1: self._trace_stack(stack, remaining_text)\n\n    def _check_grammar(self):\n        productions = self._grammar.productions()\n\n        for i in range(len(productions)):\n            for j in range(i+1, len(productions)):\n                rhs1 = productions[i].rhs()\n                rhs2 = productions[j].rhs()\n                if rhs1[:len(rhs2)] == rhs2:\n                    print('Warning: %r will never be used' % productions[i])\n\nclass SteppingShiftReduceParser(ShiftReduceParser):\n    def __init__(self, grammar, trace=0):\n        super(SteppingShiftReduceParser, self).__init__(grammar, trace)\n        self._stack = None\n        self._remaining_text = None\n        self._history = []\n\n    def parse(self, tokens):\n        tokens = list(tokens)\n        self.initialize(tokens)\n        while self.step():\n            pass\n        return self.parses()\n\n    def stack(self):\n        return self._stack\n\n    def remaining_text(self):\n        return self._remaining_text\n\n    def initialize(self, tokens):\n        self._stack = []\n        self._remaining_text = tokens\n        self._history = []\n\n    def step(self):\n        return self.reduce() or self.shift()\n\n    def shift(self):\n        if len(self._remaining_text) == 0: return False\n        self._history.append( (self._stack[:], self._remaining_text[:]) )\n        self._shift(self._stack, self._remaining_text)\n        return True\n\n    def reduce(self, production=None):\n        self._history.append( (self._stack[:], self._remaining_text[:]) )\n        return_val = self._reduce(self._stack, self._remaining_text,\n                                  production)\n\n        if not return_val: self._history.pop()\n        return return_val\n\n    def undo(self):\n        if len(self._history) == 0: return False\n        (self._stack, self._remaining_text) = self._history.pop()\n        return True\n\n    def reducible_productions(self):\n        productions = []\n        for production in self._grammar.productions():\n            rhslen = len(production.rhs())\n            if self._match_rhs(production.rhs(), self._stack[-rhslen:]):\n                productions.append(production)\n        return productions\n\n    def parses(self):\n        if (len(self._remaining_text) == 0 and\n            len(self._stack) == 1 and\n            self._stack[0].label() == self._grammar.start().symbol()\n            ):\n            yield self._stack[0]\n\n\n    def set_grammar(self, grammar):\n        self._grammar = grammar\n\n\ndef demo():\n\n    from nltk import parse, CFG\n\n    grammar = CFG.fromstring(\"\"\"\n    S -> NP VP\n    NP -> Det N | Det N PP\n    VP -> V NP | V NP PP\n    PP -> P NP\n    NP -> 'I'\n    N -> 'man' | 'park' | 'telescope' | 'dog'\n    Det -> 'the' | 'a'\n    P -> 'in' | 'with'\n    V -> 'saw'\n    \"\"\")\n\n    sent = 'I saw a man in the park'.split()\n\n    parser = parse.ShiftReduceParser(grammar, trace=2)\n    for p in parser.parse(sent):\n        print(p)\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\parse\\stanford": [".py", "\nfrom __future__ import unicode_literals\n\nimport tempfile\nimport os\nimport re\nimport warnings\nfrom subprocess import PIPE\nfrom io import StringIO\n\nfrom six import text_type\n\nfrom nltk.internals import find_jar, find_jar_iter, config_java, java, _java_options, find_jars_within_path\n\nfrom nltk.parse.api import ParserI\nfrom nltk.parse.dependencygraph import DependencyGraph\nfrom nltk.tree import Tree\n\n_stanford_url = 'https://nlp.stanford.edu/software/lex-parser.shtml'\n\n\nclass GenericStanfordParser(ParserI):\n\n    _MODEL_JAR_PATTERN = r'stanford-parser-(\\d+)(\\.(\\d+))+-models\\.jar'\n    _JAR = r'stanford-parser\\.jar'\n    _MAIN_CLASS = 'edu.stanford.nlp.parser.lexparser.LexicalizedParser'\n\n    _USE_STDIN = False\n    _DOUBLE_SPACED_OUTPUT = False\n\n    def __init__(self, path_to_jar=None, path_to_models_jar=None,\n                 model_path='edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz',\n                 encoding='utf8', verbose=False,\n                 java_options='-mx1000m', corenlp_options=''):\n\n        stanford_jar = max(\n            find_jar_iter(\n                self._JAR, path_to_jar,\n                env_vars=('STANFORD_PARSER', 'STANFORD_CORENLP'),\n                searchpath=(), url=_stanford_url,\n                verbose=verbose, is_regex=True\n            ),\n            key=lambda model_path: os.path.dirname(model_path)\n        )\n\n        model_jar = max(\n            find_jar_iter(\n                self._MODEL_JAR_PATTERN, path_to_models_jar,\n                env_vars=('STANFORD_MODELS', 'STANFORD_CORENLP'),\n                searchpath=(), url=_stanford_url,\n                verbose=verbose, is_regex=True\n            ),\n            key=lambda model_path: os.path.dirname(model_path)\n        )\n\n\n        stanford_dir = os.path.split(stanford_jar)[0]\n        self._classpath = tuple([model_jar] + find_jars_within_path(stanford_dir))\n\n        self.model_path = model_path\n        self._encoding = encoding\n        self.corenlp_options = corenlp_options\n        self.java_options = java_options\n\n    def _parse_trees_output(self, output_):\n        res = []\n        cur_lines = []\n        cur_trees = []\n        blank = False\n        for line in output_.splitlines(False):\n            if line == '':\n                if blank:\n                    res.append(iter(cur_trees))\n                    cur_trees = []\n                    blank = False\n                elif self._DOUBLE_SPACED_OUTPUT:\n                    cur_trees.append(self._make_tree('\\n'.join(cur_lines)))\n                    cur_lines = []\n                    blank = True\n                else:\n                    res.append(iter([self._make_tree('\\n'.join(cur_lines))]))\n                    cur_lines = []\n            else:\n                cur_lines.append(line)\n                blank = False\n        return iter(res)\n\n    def parse_sents(self, sentences, verbose=False):\n        cmd = [\n            self._MAIN_CLASS,\n            '-model', self.model_path,\n            '-sentences', 'newline',\n            '-outputFormat', self._OUTPUT_FORMAT,\n            '-tokenized',\n            '-escaper', 'edu.stanford.nlp.process.PTBEscapingProcessor',\n        ]\n        return self._parse_trees_output(self._execute(\n            cmd, '\\n'.join(' '.join(sentence) for sentence in sentences), verbose))\n\n    def raw_parse(self, sentence, verbose=False):\n        return next(self.raw_parse_sents([sentence], verbose))\n\n    def raw_parse_sents(self, sentences, verbose=False):\n        cmd = [\n            self._MAIN_CLASS,\n            '-model', self.model_path,\n            '-sentences', 'newline',\n            '-outputFormat', self._OUTPUT_FORMAT,\n        ]\n        return self._parse_trees_output(self._execute(cmd, '\\n'.join(sentences), verbose))\n\n    def tagged_parse(self, sentence, verbose=False):\n        return next(self.tagged_parse_sents([sentence], verbose))\n\n    def tagged_parse_sents(self, sentences, verbose=False):\n        tag_separator = '/'\n        cmd = [\n            self._MAIN_CLASS,\n            '-model', self.model_path,\n            '-sentences', 'newline',\n            '-outputFormat', self._OUTPUT_FORMAT,\n            '-tokenized',\n            '-tagSeparator', tag_separator,\n            '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer',\n            '-tokenizerMethod', 'newCoreLabelTokenizerFactory',\n        ]\n        return self._parse_trees_output(self._execute(\n            cmd, '\\n'.join(' '.join(tag_separator.join(tagged) for tagged in sentence) for sentence in sentences), verbose))\n\n    def _execute(self, cmd, input_, verbose=False):\n        encoding = self._encoding\n        cmd.extend(['-encoding', encoding])\n        if self.corenlp_options:\n            cmd.append(self.corenlp_options)\n\n        default_options = ' '.join(_java_options)\n\n        config_java(options=self.java_options, verbose=verbose)\n\n        with tempfile.NamedTemporaryFile(mode='wb', delete=False) as input_file:\n            if isinstance(input_, text_type) and encoding:\n                input_ = input_.encode(encoding)\n            input_file.write(input_)\n            input_file.flush()\n\n            if self._USE_STDIN:\n                input_file.seek(0)\n                stdout, stderr = java(cmd, classpath=self._classpath,\n                                      stdin=input_file, stdout=PIPE, stderr=PIPE)\n            else:\n                cmd.append(input_file.name)\n                stdout, stderr = java(cmd, classpath=self._classpath,\n                                      stdout=PIPE, stderr=PIPE)\n\n            stdout = stdout.replace(b'\\xc2\\xa0', b' ')\n            stdout = stdout.replace(b'\\x00\\xa0', b' ')\n            stdout = stdout.decode(encoding)\n\n        os.unlink(input_file.name)\n\n        config_java(options=default_options, verbose=False)\n\n        return stdout\n\n\nclass StanfordParser(GenericStanfordParser):\n\n    _OUTPUT_FORMAT = 'penn'\n\n    def __init__(self, *args, **kwargs):\n        warnings.warn(\"The StanfordParser will be deprecated\\n\"\n                      \"Please use \\033[91mnltk.parse.corenlp.StanforCoreNLPParser\\033[0m instead.\",\n                      DeprecationWarning, stacklevel=2)\n\n        super(StanfordParser, self).__init__(*args, **kwargs)\n\n    def _make_tree(self, result):\n        return Tree.fromstring(result)\n\n\nclass StanfordDependencyParser(GenericStanfordParser):\n\n\n    _OUTPUT_FORMAT = 'conll2007'\n\n    def __init__(self, *args, **kwargs):\n        warnings.warn(\"The StanfordDependencyParser will be deprecated\\n\"\n                      \"Please use \\033[91mnltk.parse.corenlp.StanforCoreNLPDependencyParser\\033[0m instead.\",\n                      DeprecationWarning, stacklevel=2)\n\n        super(StanfordDependencyParser, self).__init__(*args, **kwargs)\n\n    def _make_tree(self, result):\n        return DependencyGraph(result, top_relation_label='root')\n\n\nclass StanfordNeuralDependencyParser(GenericStanfordParser):\n    '''\n    >>> from nltk.parse.stanford import StanfordNeuralDependencyParser  \n    >>> dep_parser=StanfordNeuralDependencyParser(java_options='-mx4g')\n\n    >>> [parse.tree() for parse in dep_parser.raw_parse(\"The quick brown fox jumps over the lazy dog.\")] # doctest: +NORMALIZE_WHITESPACE\n    [Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over', 'the', 'lazy']), '.'])]\n\n    >>> [list(parse.triples()) for parse in dep_parser.raw_parse(\"The quick brown fox jumps over the lazy dog.\")] # doctest: +NORMALIZE_WHITESPACE\n    [[((u'jumps', u'VBZ'), u'nsubj', (u'fox', u'NN')), ((u'fox', u'NN'), u'det',\n    (u'The', u'DT')), ((u'fox', u'NN'), u'amod', (u'quick', u'JJ')), ((u'fox', u'NN'),\n    u'amod', (u'brown', u'JJ')), ((u'jumps', u'VBZ'), u'nmod', (u'dog', u'NN')),\n    ((u'dog', u'NN'), u'case', (u'over', u'IN')), ((u'dog', u'NN'), u'det',\n    (u'the', u'DT')), ((u'dog', u'NN'), u'amod', (u'lazy', u'JJ')), ((u'jumps', u'VBZ'),\n    u'punct', (u'.', u'.'))]]\n\n    >>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.raw_parse_sents((\n    ...     \"The quick brown fox jumps over the lazy dog.\",\n    ...     \"The quick grey wolf jumps over the lazy fox.\"\n    ... ))], []) # doctest: +NORMALIZE_WHITESPACE\n    [Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over',\n    'the', 'lazy']), '.']), Tree('jumps', [Tree('wolf', ['The', 'quick', 'grey']),\n    Tree('fox', ['over', 'the', 'lazy']), '.'])]\n\n    >>> sum([[parse.tree() for parse in dep_graphs] for dep_graphs in dep_parser.parse_sents((\n    ...     \"I 'm a dog\".split(),\n    ...     \"This is my friends ' cat ( the tabby )\".split(),\n    ... ))], []) # doctest: +NORMALIZE_WHITESPACE\n    [Tree('dog', ['I', \"'m\", 'a']), Tree('cat', ['This', 'is', Tree('friends',\n    ['my', \"'\"]), Tree('tabby', ['-LRB-', 'the', '-RRB-'])])]\n    '''\n\n    _OUTPUT_FORMAT = 'conll'\n    _MAIN_CLASS = 'edu.stanford.nlp.pipeline.StanfordCoreNLP'\n    _JAR = r'stanford-corenlp-(\\d+)(\\.(\\d+))+\\.jar'\n    _MODEL_JAR_PATTERN = r'stanford-corenlp-(\\d+)(\\.(\\d+))+-models\\.jar'\n    _USE_STDIN = True\n    _DOUBLE_SPACED_OUTPUT = True\n\n    def __init__(self, *args, **kwargs):\n        warnings.warn(\"The StanfordNeuralDependencyParser will be deprecated\\n\"\n                      \"Please use \\033[91mnltk.parse.corenlp.StanforCoreNLPNeuralDependencyParser\\033[0m instead.\",\n                      DeprecationWarning, stacklevel=2)\n\n        super(StanfordNeuralDependencyParser, self).__init__(*args, **kwargs)\n        self.corenlp_options += '-annotators tokenize,ssplit,pos,depparse'\n\n    def tagged_parse_sents(self, sentences, verbose=False):\n        '''\n        Currently unimplemented because the neural dependency parser (and\n        the StanfordCoreNLP pipeline class) doesn't support passing in pre-\n        tagged tokens.\n        '''\n        raise NotImplementedError(\n            'tagged_parse[_sents] is not supported by '\n            'StanfordNeuralDependencyParser; use '\n            'parse[_sents] or raw_parse[_sents] instead.'\n        )\n\n    def _make_tree(self, result):\n        return DependencyGraph(result, top_relation_label='ROOT')\n\n\ndef setup_module(module):\n    from nose import SkipTest\n\n    try:\n        StanfordParser(\n            model_path='edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz'\n        )\n        StanfordNeuralDependencyParser()\n    except LookupError:\n        raise SkipTest('doctests from nltk.parse.stanford are skipped because one of the stanford parser or CoreNLP jars doesn\\'t exist')\n"], "nltk\\parse\\transitionparser": [".py", "\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport tempfile\nimport pickle\n\nfrom os import remove\nfrom copy import deepcopy\nfrom operator import itemgetter\ntry:\n    from numpy import array\n    from scipy import sparse\n    from sklearn.datasets import load_svmlight_file\n    from sklearn import svm\nexcept ImportError:\n    pass\n\nfrom nltk.parse import ParserI, DependencyGraph, DependencyEvaluator\n\n\n\nclass Configuration(object):\n\n    def __init__(self, dep_graph):\n        self.stack = [0]  # The root element\n        self.buffer = list(range(1, len(dep_graph.nodes)))  # The rest is in the buffer\n        self.arcs = []  # empty set of arc\n        self._tokens = dep_graph.nodes\n        self._max_address = len(self.buffer)\n\n    def __str__(self):\n        return 'Stack : ' + \\\n            str(self.stack) + '  Buffer : ' + str(self.buffer) + '   Arcs : ' + str(self.arcs)\n\n    def _check_informative(self, feat, flag=False):\n        if feat is None:\n            return False\n        if feat == '':\n            return False\n        if flag is False:\n            if feat == '_':\n                return False\n        return True\n\n    def extract_features(self):\n        result = []\n        if len(self.stack) > 0:\n            stack_idx0 = self.stack[len(self.stack) - 1]\n            token = self._tokens[stack_idx0]\n            if self._check_informative(token['word'], True):\n                result.append('STK_0_FORM_' + token['word'])\n            if 'lemma' in token and self._check_informative(token['lemma']):\n                result.append('STK_0_LEMMA_' + token['lemma'])\n            if self._check_informative(token['tag']):\n                result.append('STK_0_POS_' + token['tag'])\n            if 'feats' in token and self._check_informative(token['feats']):\n                feats = token['feats'].split(\"|\")\n                for feat in feats:\n                    result.append('STK_0_FEATS_' + feat)\n            if len(self.stack) > 1:\n                stack_idx1 = self.stack[len(self.stack) - 2]\n                token = self._tokens[stack_idx1]\n                if self._check_informative(token['tag']):\n                    result.append('STK_1_POS_' + token['tag'])\n\n            left_most = 1000000\n            right_most = -1\n            dep_left_most = ''\n            dep_right_most = ''\n            for (wi, r, wj) in self.arcs:\n                if wi == stack_idx0:\n                    if (wj > wi) and (wj > right_most):\n                        right_most = wj\n                        dep_right_most = r\n                    if (wj < wi) and (wj < left_most):\n                        left_most = wj\n                        dep_left_most = r\n            if self._check_informative(dep_left_most):\n                result.append('STK_0_LDEP_' + dep_left_most)\n            if self._check_informative(dep_right_most):\n                result.append('STK_0_RDEP_' + dep_right_most)\n\n        if len(self.buffer) > 0:\n            buffer_idx0 = self.buffer[0]\n            token = self._tokens[buffer_idx0]\n            if self._check_informative(token['word'], True):\n                result.append('BUF_0_FORM_' + token['word'])\n            if 'lemma' in token and self._check_informative(token['lemma']):\n                result.append('BUF_0_LEMMA_' + token['lemma'])\n            if self._check_informative(token['tag']):\n                result.append('BUF_0_POS_' + token['tag'])\n            if 'feats' in token and self._check_informative(token['feats']):\n                feats = token['feats'].split(\"|\")\n                for feat in feats:\n                    result.append('BUF_0_FEATS_' + feat)\n            if len(self.buffer) > 1:\n                buffer_idx1 = self.buffer[1]\n                token = self._tokens[buffer_idx1]\n                if self._check_informative(token['word'], True):\n                    result.append('BUF_1_FORM_' + token['word'])\n                if self._check_informative(token['tag']):\n                    result.append('BUF_1_POS_' + token['tag'])\n            if len(self.buffer) > 2:\n                buffer_idx2 = self.buffer[2]\n                token = self._tokens[buffer_idx2]\n                if self._check_informative(token['tag']):\n                    result.append('BUF_2_POS_' + token['tag'])\n            if len(self.buffer) > 3:\n                buffer_idx3 = self.buffer[3]\n                token = self._tokens[buffer_idx3]\n                if self._check_informative(token['tag']):\n                    result.append('BUF_3_POS_' + token['tag'])\n            left_most = 1000000\n            right_most = -1\n            dep_left_most = ''\n            dep_right_most = ''\n            for (wi, r, wj) in self.arcs:\n                if wi == buffer_idx0:\n                    if (wj > wi) and (wj > right_most):\n                        right_most = wj\n                        dep_right_most = r\n                    if (wj < wi) and (wj < left_most):\n                        left_most = wj\n                        dep_left_most = r\n            if self._check_informative(dep_left_most):\n                result.append('BUF_0_LDEP_' + dep_left_most)\n            if self._check_informative(dep_right_most):\n                result.append('BUF_0_RDEP_' + dep_right_most)\n\n        return result\n\n\nclass Transition(object):\n    LEFT_ARC = 'LEFTARC'\n    RIGHT_ARC = 'RIGHTARC'\n    SHIFT = 'SHIFT'\n    REDUCE = 'REDUCE'\n\n    def __init__(self, alg_option):\n        self._algo = alg_option\n        if alg_option not in [\n                TransitionParser.ARC_STANDARD,\n                TransitionParser.ARC_EAGER]:\n            raise ValueError(\" Currently we only support %s and %s \" %\n                                        (TransitionParser.ARC_STANDARD, TransitionParser.ARC_EAGER))\n\n    def left_arc(self, conf, relation):\n        if (len(conf.buffer) <= 0) or (len(conf.stack) <= 0):\n            return -1\n        if conf.buffer[0] == 0:\n            return -1\n\n        idx_wi = conf.stack[len(conf.stack) - 1]\n\n        flag = True\n        if self._algo == TransitionParser.ARC_EAGER:\n            for (idx_parent, r, idx_child) in conf.arcs:\n                if idx_child == idx_wi:\n                    flag = False\n\n        if flag:\n            conf.stack.pop()\n            idx_wj = conf.buffer[0]\n            conf.arcs.append((idx_wj, relation, idx_wi))\n        else:\n            return -1\n\n    def right_arc(self, conf, relation):\n        if (len(conf.buffer) <= 0) or (len(conf.stack) <= 0):\n            return -1\n        if self._algo == TransitionParser.ARC_STANDARD:\n            idx_wi = conf.stack.pop()\n            idx_wj = conf.buffer[0]\n            conf.buffer[0] = idx_wi\n            conf.arcs.append((idx_wi, relation, idx_wj))\n        else:  # arc-eager\n            idx_wi = conf.stack[len(conf.stack) - 1]\n            idx_wj = conf.buffer.pop(0)\n            conf.stack.append(idx_wj)\n            conf.arcs.append((idx_wi, relation, idx_wj))\n\n    def reduce(self, conf):\n\n        if self._algo != TransitionParser.ARC_EAGER:\n            return -1\n        if len(conf.stack) <= 0:\n            return -1\n\n        idx_wi = conf.stack[len(conf.stack) - 1]\n        flag = False\n        for (idx_parent, r, idx_child) in conf.arcs:\n            if idx_child == idx_wi:\n                flag = True\n        if flag:\n            conf.stack.pop()  # reduce it\n        else:\n            return -1\n\n    def shift(self, conf):\n        if len(conf.buffer) <= 0:\n            return -1\n        idx_wi = conf.buffer.pop(0)\n        conf.stack.append(idx_wi)\n\n\nclass TransitionParser(ParserI):\n\n    ARC_STANDARD = 'arc-standard'\n    ARC_EAGER = 'arc-eager'\n\n    def __init__(self, algorithm):\n        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n            raise ValueError(\" Currently we only support %s and %s \" %\n                                        (self.ARC_STANDARD, self.ARC_EAGER))\n        self._algorithm = algorithm\n\n        self._dictionary = {}\n        self._transition = {}\n        self._match_transition = {}\n\n    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n        p_node = depgraph.nodes[idx_parent]\n        c_node = depgraph.nodes[idx_child]\n\n        if c_node['word'] is None:\n            return None  # Root word\n\n        if c_node['head'] == p_node['address']:\n            return c_node['rel']\n        else:\n            return None\n\n    def _convert_to_binary_features(self, features):\n        unsorted_result = []\n        for feature in features:\n            self._dictionary.setdefault(feature, len(self._dictionary))\n            unsorted_result.append(self._dictionary[feature])\n\n        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n\n    def _is_projective(self, depgraph):\n        arc_list = []\n        for key in depgraph.nodes:\n            node = depgraph.nodes[key]\n\n            if 'head' in node:\n                childIdx = node['address']\n                parentIdx = node['head']\n                if parentIdx is not None:\n                    arc_list.append((parentIdx, childIdx))\n\n        for (parentIdx, childIdx) in arc_list:\n            if childIdx > parentIdx:\n                temp = childIdx\n                childIdx = parentIdx\n                parentIdx = temp\n            for k in range(childIdx + 1, parentIdx):\n                for m in range(len(depgraph.nodes)):\n                    if (m < childIdx) or (m > parentIdx):\n                        if (k, m) in arc_list:\n                            return False\n                        if (m, k) in arc_list:\n                            return False\n        return True\n\n    def _write_to_file(self, key, binary_features, input_file):\n        self._transition.setdefault(key, len(self._transition) + 1)\n        self._match_transition[self._transition[key]] = key\n\n        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n        input_file.write(input_str.encode('utf-8'))\n\n    def _create_training_examples_arc_std(self, depgraphs, input_file):\n        operation = Transition(self.ARC_STANDARD)\n        count_proj = 0\n        training_seq = []\n\n        for depgraph in depgraphs:\n            if not self._is_projective(depgraph):\n                continue\n\n            count_proj += 1\n            conf = Configuration(depgraph)\n            while len(conf.buffer) > 0:\n                b0 = conf.buffer[0]\n                features = conf.extract_features()\n                binary_features = self._convert_to_binary_features(features)\n\n                if len(conf.stack) > 0:\n                    s0 = conf.stack[len(conf.stack) - 1]\n                    rel = self._get_dep_relation(b0, s0, depgraph)\n                    if rel is not None:\n                        key = Transition.LEFT_ARC + ':' + rel\n                        self._write_to_file(key, binary_features, input_file)\n                        operation.left_arc(conf, rel)\n                        training_seq.append(key)\n                        continue\n\n                    rel = self._get_dep_relation(s0, b0, depgraph)\n                    if rel is not None:\n                        precondition = True\n                        maxID = conf._max_address\n\n                        for w in range(maxID + 1):\n                            if w != b0:\n                                relw = self._get_dep_relation(b0, w, depgraph)\n                                if relw is not None:\n                                    if (b0, relw, w) not in conf.arcs:\n                                        precondition = False\n\n                        if precondition:\n                            key = Transition.RIGHT_ARC + ':' + rel\n                            self._write_to_file(\n                                key,\n                                binary_features,\n                                input_file)\n                            operation.right_arc(conf, rel)\n                            training_seq.append(key)\n                            continue\n\n                key = Transition.SHIFT\n                self._write_to_file(key, binary_features, input_file)\n                operation.shift(conf)\n                training_seq.append(key)\n\n        print(\" Number of training examples : \" + str(len(depgraphs)))\n        print(\" Number of valid (projective) examples : \" + str(count_proj))\n        return training_seq\n\n    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n        operation = Transition(self.ARC_EAGER)\n        countProj = 0\n        training_seq = []\n\n        for depgraph in depgraphs:\n            if not self._is_projective(depgraph):\n                continue\n\n            countProj += 1\n            conf = Configuration(depgraph)\n            while len(conf.buffer) > 0:\n                b0 = conf.buffer[0]\n                features = conf.extract_features()\n                binary_features = self._convert_to_binary_features(features)\n\n                if len(conf.stack) > 0:\n                    s0 = conf.stack[len(conf.stack) - 1]\n                    rel = self._get_dep_relation(b0, s0, depgraph)\n                    if rel is not None:\n                        key = Transition.LEFT_ARC + ':' + rel\n                        self._write_to_file(key, binary_features, input_file)\n                        operation.left_arc(conf, rel)\n                        training_seq.append(key)\n                        continue\n\n                    rel = self._get_dep_relation(s0, b0, depgraph)\n                    if rel is not None:\n                        key = Transition.RIGHT_ARC + ':' + rel\n                        self._write_to_file(key, binary_features, input_file)\n                        operation.right_arc(conf, rel)\n                        training_seq.append(key)\n                        continue\n\n                    flag = False\n                    for k in range(s0):\n                        if self._get_dep_relation(k, b0, depgraph) is not None:\n                            flag = True\n                        if self._get_dep_relation(b0, k, depgraph) is not None:\n                            flag = True\n                    if flag:\n                        key = Transition.REDUCE\n                        self._write_to_file(key, binary_features, input_file)\n                        operation.reduce(conf)\n                        training_seq.append(key)\n                        continue\n\n                key = Transition.SHIFT\n                self._write_to_file(key, binary_features, input_file)\n                operation.shift(conf)\n                training_seq.append(key)\n\n        print(\" Number of training examples : \" + str(len(depgraphs)))\n        print(\" Number of valid (projective) examples : \" + str(countProj))\n        return training_seq\n\n    def train(self, depgraphs, modelfile, verbose=True):\n\n        try:\n            input_file = tempfile.NamedTemporaryFile(\n                prefix='transition_parse.train',\n                dir=tempfile.gettempdir(),\n                delete=False)\n\n            if self._algorithm == self.ARC_STANDARD:\n                self._create_training_examples_arc_std(depgraphs, input_file)\n            else:\n                self._create_training_examples_arc_eager(depgraphs, input_file)\n\n            input_file.close()\n            x_train, y_train = load_svmlight_file(input_file.name)\n            model = svm.SVC(\n                kernel='poly',\n                degree=2,\n                coef0=0,\n                gamma=0.2,\n                C=0.5,\n                verbose=verbose,\n                probability=True)\n\n            model.fit(x_train, y_train)\n            pickle.dump(model, open(modelfile, 'wb'))\n        finally:\n            remove(input_file.name)\n\n    def parse(self, depgraphs, modelFile):\n        result = []\n        model = pickle.load(open(modelFile, 'rb'))\n        operation = Transition(self._algorithm)\n\n        for depgraph in depgraphs:\n            conf = Configuration(depgraph)\n            while len(conf.buffer) > 0:\n                features = conf.extract_features()\n                col = []\n                row = []\n                data = []\n                for feature in features:\n                    if feature in self._dictionary:\n                        col.append(self._dictionary[feature])\n                        row.append(0)\n                        data.append(1.0)\n                np_col = array(sorted(col))  # NB : index must be sorted\n                np_row = array(row)\n                np_data = array(data)\n\n                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n\n\n                prob_dict = {}\n                pred_prob = model.predict_proba(x_test)[0]\n                for i in range(len(pred_prob)):\n                    prob_dict[i] = pred_prob[i]\n                sorted_Prob = sorted(\n                    prob_dict.items(),\n                    key=itemgetter(1),\n                    reverse=True)\n\n                for (y_pred_idx, confidence) in sorted_Prob:\n                    y_pred = model.classes_[y_pred_idx]\n\n                    if y_pred in self._match_transition:\n                        strTransition = self._match_transition[y_pred]\n                        baseTransition = strTransition.split(\":\")[0]\n\n                        if baseTransition == Transition.LEFT_ARC:\n                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n                                break\n                        elif baseTransition == Transition.RIGHT_ARC:\n                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n                                break\n                        elif baseTransition == Transition.REDUCE:\n                            if operation.reduce(conf) != -1:\n                                break\n                        elif baseTransition == Transition.SHIFT:\n                            if operation.shift(conf) != -1:\n                                break\n                    else:\n                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n\n\n            new_depgraph = deepcopy(depgraph)\n            for key in new_depgraph.nodes:\n                node = new_depgraph.nodes[key]\n                node['rel'] = ''\n                node['head'] = 0\n            for (head, rel, child) in conf.arcs:\n                c_node = new_depgraph.nodes[child]\n                c_node['head'] = head\n                c_node['rel'] = rel\n            result.append(new_depgraph)\n\n        return result\n\n\ndef demo():\n\n"], "nltk\\parse\\util": [".py", "\n\nfrom __future__ import print_function\n\nfrom nltk.grammar import CFG, FeatureGrammar, PCFG\nfrom nltk.data import load\n\nfrom nltk.parse.chart import Chart, ChartParser\nfrom nltk.parse.pchart import InsideChartParser\nfrom nltk.parse.featurechart import FeatureChart, FeatureChartParser\n\ndef load_parser(grammar_url, trace=0,\n                parser=None, chart_class=None,\n                beam_size=0, **load_args):\n    grammar = load(grammar_url, **load_args)\n    if not isinstance(grammar, CFG):\n        raise ValueError(\"The grammar must be a CFG, \"\n                         \"or a subclass thereof.\")\n    if isinstance(grammar, PCFG):\n        if parser is None:\n            parser = InsideChartParser\n        return parser(grammar, trace=trace, beam_size=beam_size)\n\n    elif isinstance(grammar, FeatureGrammar):\n        if parser is None:\n            parser = FeatureChartParser\n        if chart_class is None:\n            chart_class = FeatureChart\n        return parser(grammar, trace=trace, chart_class=chart_class)\n\n    else: # Plain CFG.\n        if parser is None:\n            parser = ChartParser\n        if chart_class is None:\n            chart_class = Chart\n        return parser(grammar, trace=trace, chart_class=chart_class)\n\ndef taggedsent_to_conll(sentence):\n\tfor (i, (word, tag)) in enumerate(sentence, start=1):\n\t\tinput_str = [str(i), word, '_', tag, tag, '_', '0', 'a', '_', '_']\n\t\tinput_str = \"\\t\".join(input_str) + \"\\n\"\n\t\tyield input_str\n\n\ndef taggedsents_to_conll(sentences):\n\tfor sentence in sentences:\n\t\tfor input_str in taggedsent_to_conll(sentence):\n\t\t\tyield input_str\n\t\tyield '\\n\\n'\t\t\n\n\nclass TestGrammar(object):\n    def __init__(self, grammar, suite, accept=None, reject=None):\n        self.test_grammar = grammar\n\n        self.cp = load_parser(grammar, trace=0)\n        self.suite = suite\n        self._accept = accept\n        self._reject = reject\n\n\n    def run(self, show_trees=False):\n        for test in self.suite:\n            print(test['doc'] + \":\", end=' ')\n            for key in ['accept', 'reject']:\n                for sent in test[key]:\n                    tokens = sent.split()\n                    trees = list(self.cp.parse(tokens))\n                    if show_trees and trees:\n                        print()\n                        print(sent)\n                        for tree in trees:\n                            print(tree)\n                    if key == 'accept':\n                        if trees == []:\n                            raise ValueError(\"Sentence '%s' failed to parse'\" % sent)\n                        else:\n                            accepted = True\n                    else:\n                        if trees:\n                            raise ValueError(\"Sentence '%s' received a parse'\" % sent)\n                        else:\n                            rejected = True\n            if accepted and rejected:\n                print(\"All tests passed!\")\n\ndef extract_test_sentences(string, comment_chars=\"#%;\", encoding=None):\n    if encoding is not None:\n        string = string.decode(encoding)\n    sentences = []\n    for sentence in string.split('\\n'):\n        if sentence == '' or sentence[0] in comment_chars:\n            continue\n        split_info = sentence.split(':', 1)\n        result = None\n        if len(split_info) == 2:\n            if split_info[0] in ['True','true','False','false']:\n                result = split_info[0] in ['True','true']\n                sentence = split_info[1]\n            else:\n                result = int(split_info[0])\n                sentence = split_info[1]\n        tokens = sentence.split()\n        if tokens == []:\n            continue\n        sentences += [(tokens, result)]\n    return sentences\n\nextract_test_sentences.__test__ = False\n"], "nltk\\parse\\viterbi": [".py", "from __future__ import print_function, unicode_literals\n\nfrom functools import reduce\nfrom nltk.tree import Tree, ProbabilisticTree\nfrom nltk.compat import python_2_unicode_compatible\n\nfrom nltk.parse.api import ParserI\n\n\n@python_2_unicode_compatible\nclass ViterbiParser(ParserI):\n    def __init__(self, grammar, trace=0):\n        self._grammar = grammar\n        self._trace = trace\n\n    def grammar(self):\n        return self._grammar\n\n    def trace(self, trace=2):\n        self._trace = trace\n\n    def parse(self, tokens):\n\n        tokens = list(tokens)\n        self._grammar.check_coverage(tokens)\n\n        constituents = {}\n\n        if self._trace: print(('Inserting tokens into the most likely'+\n                               ' constituents table...'))\n        for index in range(len(tokens)):\n            token = tokens[index]\n            constituents[index,index+1,token] = token\n            if self._trace > 1:\n                self._trace_lexical_insertion(token, index, len(tokens))\n\n        for length in range(1, len(tokens)+1):\n            if self._trace:\n                print(('Finding the most likely constituents'+\n                       ' spanning %d text elements...' % length))\n            for start in range(len(tokens)-length+1):\n                span = (start, start+length)\n                self._add_constituents_spanning(span, constituents,\n                                                tokens)\n\n        tree = constituents.get((0, len(tokens), self._grammar.start()))\n        if tree is not None:\n            yield tree\n\n    def _add_constituents_spanning(self, span, constituents, tokens):\n        changed = True\n        while changed:\n            changed = False\n\n            instantiations = self._find_instantiations(span, constituents)\n\n            for (production, children) in instantiations:\n                subtrees = [c for c in children if isinstance(c, Tree)]\n                p = reduce(lambda pr,t:pr*t.prob(),\n                           subtrees, production.prob())\n                node = production.lhs().symbol()\n                tree = ProbabilisticTree(node, children, prob=p)\n\n                c = constituents.get((span[0], span[1], production.lhs()))\n                if self._trace > 1:\n                    if c is None or c != tree:\n                        if c is None or c.prob() < tree.prob():\n                            print('   Insert:', end=' ')\n                        else:\n                            print('  Discard:', end=' ')\n                        self._trace_production(production, p, span, len(tokens))\n                if c is None or c.prob() < tree.prob():\n                    constituents[span[0], span[1], production.lhs()] = tree\n                    changed = True\n\n    def _find_instantiations(self, span, constituents):\n        rv = []\n        for production in self._grammar.productions():\n            childlists = self._match_rhs(production.rhs(), span, constituents)\n\n            for childlist in childlists:\n                rv.append( (production, childlist) )\n        return rv\n\n    def _match_rhs(self, rhs, span, constituents):\n        (start, end) = span\n\n        if start >= end and rhs == (): return [[]]\n        if start >= end or rhs == (): return []\n\n        childlists = []\n        for split in range(start, end+1):\n            l=constituents.get((start,split,rhs[0]))\n            if l is not None:\n                rights = self._match_rhs(rhs[1:], (split,end), constituents)\n                childlists += [[l]+r for r in rights]\n\n        return childlists\n\n    def _trace_production(self, production, p, span, width):\n\n        str = '|' + '.' * span[0]\n        str += '=' * (span[1] - span[0])\n        str += '.' * (width - span[1]) + '| '\n        str += '%s' % production\n        if self._trace > 2: str = '%-40s %12.10f ' % (str, p)\n\n        print(str)\n\n    def _trace_lexical_insertion(self, token, index, width):\n        str = '   Insert: |' + '.' * index + '=' + '.' * (width-index-1) + '| '\n        str += '%s' % (token,)\n        print(str)\n\n    def __repr__(self):\n        return '<ViterbiParser for %r>' % self._grammar\n\n\n\ndef demo():\n    import sys, time\n    from nltk import tokenize\n    from nltk.parse import ViterbiParser\n    from nltk.grammar import toy_pcfg1, toy_pcfg2\n\n    demos = [('I saw the man with my telescope', toy_pcfg1),\n             ('the boy saw Jack with Bob under the table with a telescope', toy_pcfg2)]\n\n    print()\n    for i in range(len(demos)):\n        print('%3s: %s' % (i+1, demos[i][0]))\n        print('     %r' % demos[i][1])\n        print()\n    print('Which demo (%d-%d)? ' % (1, len(demos)), end=' ')\n    try:\n        snum = int(sys.stdin.readline().strip())-1\n        sent, grammar = demos[snum]\n    except:\n        print('Bad sentence number')\n        return\n\n    tokens = sent.split()\n\n    parser = ViterbiParser(grammar)\n    all_parses = {}\n\n    print('\\nsent: %s\\nparser: %s\\ngrammar: %s' % (sent,parser,grammar))\n    parser.trace(3)\n    t = time.time()\n    parses = parser.parse_all(tokens)\n    time = time.time()-t\n    average = (reduce(lambda a,b:a+b.prob(), parses, 0)/len(parses)\n               if parses else 0)\n    num_parses = len(parses)\n    for p in parses:\n        all_parses[p.freeze()] = 1\n\n    print()\n    print('Time (secs)   # Parses   Average P(parse)')\n    print('-----------------------------------------')\n    print('%11.4f%11d%19.14f' % (time, num_parses, average))\n    parses = all_parses.keys()\n    if parses:\n        p = reduce(lambda a,b:a+b.prob(), parses, 0)/len(parses)\n    else: p = 0\n    print('------------------------------------------')\n    print('%11s%11d%19.14f' % ('n/a', len(parses), p))\n\n    print()\n    print('Draw parses (y/n)? ', end=' ')\n    if sys.stdin.readline().strip().lower().startswith('y'):\n        from nltk.draw.tree import draw_trees\n        print('  please wait...')\n        draw_trees(*parses)\n\n    print()\n    print('Print parses (y/n)? ', end=' ')\n    if sys.stdin.readline().strip().lower().startswith('y'):\n        for parse in parses:\n            print(parse)\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\parse\\__init__": [".py", "\n\nfrom nltk.parse.api import ParserI\nfrom nltk.parse.chart import (ChartParser, SteppingChartParser, TopDownChartParser,\n                              BottomUpChartParser, BottomUpLeftCornerChartParser,\n                              LeftCornerChartParser)\nfrom nltk.parse.featurechart import (FeatureChartParser, FeatureTopDownChartParser,\n                                     FeatureBottomUpChartParser,\n                                     FeatureBottomUpLeftCornerChartParser)\nfrom nltk.parse.earleychart import (IncrementalChartParser, EarleyChartParser,\n                                    IncrementalTopDownChartParser,\n                                    IncrementalBottomUpChartParser,\n                                    IncrementalBottomUpLeftCornerChartParser,\n                                    IncrementalLeftCornerChartParser,\n                                    FeatureIncrementalChartParser,\n                                    FeatureEarleyChartParser,\n                                    FeatureIncrementalTopDownChartParser,\n                                    FeatureIncrementalBottomUpChartParser,\n                                    FeatureIncrementalBottomUpLeftCornerChartParser)\nfrom nltk.parse.pchart import (BottomUpProbabilisticChartParser, InsideChartParser,\n                               RandomChartParser, UnsortedChartParser,\n                               LongestChartParser)\nfrom nltk.parse.recursivedescent import (RecursiveDescentParser,\n                                         SteppingRecursiveDescentParser)\nfrom nltk.parse.shiftreduce import (ShiftReduceParser, SteppingShiftReduceParser)\nfrom nltk.parse.util import load_parser, TestGrammar, extract_test_sentences\nfrom nltk.parse.viterbi import ViterbiParser\nfrom nltk.parse.dependencygraph import DependencyGraph\nfrom nltk.parse.projectivedependencyparser import (ProjectiveDependencyParser,\n                                                   ProbabilisticProjectiveDependencyParser)\nfrom nltk.parse.nonprojectivedependencyparser import (NonprojectiveDependencyParser,\n                                                      NaiveBayesDependencyScorer,\n                                                      ProbabilisticNonprojectiveParser)\nfrom nltk.parse.malt import MaltParser\nfrom nltk.parse.evaluate import DependencyEvaluator\nfrom nltk.parse.transitionparser import TransitionParser\nfrom nltk.parse.bllip import BllipParser\nfrom nltk.parse.corenlp import CoreNLPParser, CoreNLPDependencyParser\n", 1], "nltk\\probability": [".py", "\nfrom __future__ import print_function, unicode_literals, division\n\nimport math\nimport random\nimport warnings\nimport array\nfrom operator import itemgetter\nfrom collections import defaultdict, Counter\nfrom functools import reduce\nfrom abc import ABCMeta, abstractmethod\n\nfrom six import itervalues, text_type, add_metaclass\n\nfrom nltk import compat\nfrom nltk.internals import raise_unorderable_types\n\n_NINF = float('-1e300')\n\n\n@compat.python_2_unicode_compatible\nclass FreqDist(Counter):\n\n    def __init__(self, samples=None):\n        Counter.__init__(self, samples)\n\n        self._N = None\n\n    def N(self):\n        if self._N is None:\n            self._N = sum(self.values())\n        return self._N\n\n    def __setitem__(self, key, val):\n        self._N = None\n        super(FreqDist, self).__setitem__(key, val)\n\n    def __delitem__(self, key):\n        self._N = None\n        super(FreqDist, self).__delitem__(key)\n\n    def update(self, *args, **kwargs):\n        self._N = None\n        super(FreqDist, self).update(*args, **kwargs)\n\n    def setdefault(self, key, val):\n        self._N = None\n        super(FreqDist, self).setdefault(key, val)\n\n    def B(self):\n        return len(self)\n\n    def hapaxes(self):\n        return [item for item in self if self[item] == 1]\n\n\n    def Nr(self, r, bins=None):\n        return self.r_Nr(bins)[r]\n\n    def r_Nr(self, bins=None):\n\n        _r_Nr = defaultdict(int)\n        for count in self.values():\n            _r_Nr[count] += 1\n\n        _r_Nr[0] = bins - self.B() if bins is not None else 0\n\n        return _r_Nr\n\n    def _cumulative_frequencies(self, samples):\n        cf = 0.0\n        for sample in samples:\n            cf += self[sample]\n            yield cf\n\n    def freq(self, sample):\n        n = self.N()\n        if n == 0:\n            return 0\n        return self[sample] / n\n\n    def max(self):\n        if len(self) == 0:\n            raise ValueError('A FreqDist must have at least one sample before max is defined.')\n        return self.most_common(1)[0][0]\n\n    def plot(self, *args, **kwargs):\n        try:\n            from matplotlib import pylab\n        except ImportError:\n            raise ValueError('The plot function requires matplotlib to be installed.'\n                         'See http://matplotlib.org/')\n\n        if len(args) == 0:\n            args = [len(self)]\n        samples = [item for item, _ in self.most_common(*args)]\n\n        cumulative = _get_kwarg(kwargs, 'cumulative', False)\n        if cumulative:\n            freqs = list(self._cumulative_frequencies(samples))\n            ylabel = \"Cumulative Counts\"\n        else:\n            freqs = [self[sample] for sample in samples]\n            ylabel = \"Counts\"\n\n        pylab.grid(True, color=\"silver\")\n        if not \"linewidth\" in kwargs:\n            kwargs[\"linewidth\"] = 2\n        if \"title\" in kwargs:\n            pylab.title(kwargs[\"title\"])\n            del kwargs[\"title\"]\n        pylab.plot(freqs, **kwargs)\n        pylab.xticks(range(len(samples)), [text_type(s) for s in samples], rotation=90)\n        pylab.xlabel(\"Samples\")\n        pylab.ylabel(ylabel)\n        pylab.show()\n\n    def tabulate(self, *args, **kwargs):\n        if len(args) == 0:\n            args = [len(self)]\n        samples = [item for item, _ in self.most_common(*args)]\n\n        cumulative = _get_kwarg(kwargs, 'cumulative', False)\n        if cumulative:\n            freqs = list(self._cumulative_frequencies(samples))\n        else:\n            freqs = [self[sample] for sample in samples]\n\n        width = max(len(\"%s\" % s) for s in samples)\n        width = max(width, max(len(\"%d\" % f) for f in freqs))\n\n        for i in range(len(samples)):\n            print(\"%*s\" % (width, samples[i]), end=' ')\n        print()\n        for i in range(len(samples)):\n            print(\"%*d\" % (width, freqs[i]), end=' ')\n        print()\n\n    def copy(self):\n        return self.__class__(self)\n\n\n    def __add__(self, other):\n        return self.__class__(super(FreqDist, self).__add__(other))\n\n    def __sub__(self, other):\n        return self.__class__(super(FreqDist, self).__sub__(other))\n\n    def __or__(self, other):\n        return self.__class__(super(FreqDist, self).__or__(other))\n\n    def __and__(self, other):\n        return self.__class__(super(FreqDist, self).__and__(other))\n\n    def __le__(self, other):\n        if not isinstance(other, FreqDist):\n            raise_unorderable_types(\"<=\", self, other)\n        return set(self).issubset(other) and all(self[key] <= other[key] for key in self)\n\n    __ge__ = lambda self, other: not self <= other or self == other\n    __lt__ = lambda self, other: self <= other and not self == other\n    __gt__ = lambda self, other: not self <= other\n\n    def __repr__(self):\n        return self.pformat()\n\n    def pprint(self, maxlen=10, stream=None):\n        print(self.pformat(maxlen=maxlen), file=stream)\n\n    def pformat(self, maxlen=10):\n        items = ['{0!r}: {1!r}'.format(*item) for item in self.most_common(maxlen)]\n        if len(self) > maxlen:\n            items.append('...')\n        return 'FreqDist({{{0}}})'.format(', '.join(items))\n\n    def __str__(self):\n        return '<FreqDist with %d samples and %d outcomes>' % (len(self), self.N())\n\n\n\n@add_metaclass(ABCMeta)\nclass ProbDistI(object):\n    SUM_TO_ONE = True\n        Classes inheriting from ProbDistI should implement __init__.\n        \"\"\"\n\n    @abstractmethod\n    def prob(self, sample):\n        \"\"\"\n        Return the probability for a given sample.  Probabilities\n        are always real numbers in the range [0, 1].\n\n        :param sample: The sample whose probability\n               should be returned.\n        :type sample: any\n        :rtype: float\n        \"\"\"\n\n    def logprob(self, sample):\n        \"\"\"\n        Return the base 2 logarithm of the probability for a given sample.\n\n        :param sample: The sample whose probability\n               should be returned.\n        :type sample: any\n        :rtype: float\n        \"\"\"\n        p = self.prob(sample)\n        return (math.log(p, 2) if p != 0 else _NINF)\n\n    @abstractmethod\n    def max(self):\n        \"\"\"\n        Return the sample with the greatest probability.  If two or\n        more samples have the same probability, return one of them;\n        which sample is returned is undefined.\n\n        :rtype: any\n        \"\"\"\n\n    @abstractmethod\n    def samples(self):\n        \"\"\"\n        Return a list of all samples that have nonzero probabilities.\n        Use ``prob`` to find the probability of each sample.\n\n        :rtype: list\n        \"\"\"\n\n    def discount(self):\n        \"\"\"\n        Return the ratio by which counts are discounted on average: c*/c\n\n        :rtype: float\n        \"\"\"\n        return 0.0\n\n    def generate(self):\n        \"\"\"\n        Return a randomly selected sample from this probability distribution.\n        The probability of returning each sample ``samp`` is equal to\n        ``self.prob(samp)``.\n        \"\"\"\n        p = random.random()\n        p_init = p\n        for sample in self.samples():\n            p -= self.prob(sample)\n            if p <= 0: return sample\n        if p < .0001:\n            return sample\n        if self.SUM_TO_ONE:\n            warnings.warn(\"Probability distribution %r sums to %r; generate()\"\n                          \" is returning an arbitrary sample.\" % (self, p_init-p))\n        return random.choice(list(self.samples()))\n\n\n@compat.python_2_unicode_compatible\nclass UniformProbDist(ProbDistI):\n    \"\"\"\n    A probability distribution that assigns equal probability to each\n    sample in a given set; and a zero probability to all other\n    samples.\n    \"\"\"\n    def __init__(self, samples):\n        \"\"\"\n        Construct a new uniform probability distribution, that assigns\n        equal probability to each sample in ``samples``.\n\n        :param samples: The samples that should be given uniform\n            probability.\n        :type samples: list\n        :raise ValueError: If ``samples`` is empty.\n        \"\"\"\n        if len(samples) == 0:\n            raise ValueError('A Uniform probability distribution must '+\n                             'have at least one sample.')\n        self._sampleset = set(samples)\n        self._prob = 1.0/len(self._sampleset)\n        self._samples = list(self._sampleset)\n\n    def prob(self, sample):\n        return (self._prob if sample in self._sampleset else 0)\n\n    def max(self):\n        return self._samples[0]\n\n    def samples(self):\n        return self._samples\n\n    def __repr__(self):\n        return '<UniformProbDist with %d samples>' % len(self._sampleset)\n\n\n@compat.python_2_unicode_compatible\nclass RandomProbDist(ProbDistI):\n    \"\"\"\n    Generates a random probability distribution whereby each sample\n    will be between 0 and 1 with equal probability (uniform random distribution.\n    Also called a continuous uniform distribution).\n    \"\"\"\n    def __init__(self, samples):\n        if len(samples) == 0:\n            raise ValueError('A probability distribution must '+\n                             'have at least one sample.')\n        self._probs = self.unirand(samples)\n        self._samples = list(self._probs.keys())\n\n    @classmethod\n    def unirand(cls, samples):\n        \"\"\"\n        The key function that creates a randomized initial distribution\n        that still sums to 1. Set as a dictionary of prob values so that\n        it can still be passed to MutableProbDist and called with identical\n        syntax to UniformProbDist\n        \"\"\"\n        samples = set(samples)\n        randrow = [random.random() for i in range(len(samples))]\n        total = sum(randrow)\n        for i, x in enumerate(randrow):\n            randrow[i] = x/total\n\n        total = sum(randrow)\n        if total != 1:\n            randrow[-1] -= total - 1\n\n        return dict((s, randrow[i]) for i, s in enumerate(samples))\n\n    def max(self):\n        if not hasattr(self, '_max'):\n            self._max = max((p,v) for (v,p) in self._probs.items())[1]\n        return self._max\n\n    def prob(self, sample):\n        return self._probs.get(sample, 0)\n\n    def samples(self):\n        return self._samples\n\n    def __repr__(self):\n        return '<RandomUniformProbDist with %d samples>' %len(self._probs)\n\n\n@compat.python_2_unicode_compatible\nclass DictionaryProbDist(ProbDistI):\n    \"\"\"\n    A probability distribution whose probabilities are directly\n    specified by a given dictionary.  The given dictionary maps\n    samples to probabilities.\n    \"\"\"\n    def __init__(self, prob_dict=None, log=False, normalize=False):\n        \"\"\"\n        Construct a new probability distribution from the given\n        dictionary, which maps values to probabilities (or to log\n        probabilities, if ``log`` is true).  If ``normalize`` is\n        true, then the probability values are scaled by a constant\n        factor such that they sum to 1.\n\n        If called without arguments, the resulting probability\n        distribution assigns zero probability to all values.\n        \"\"\"\n\n        self._prob_dict = (prob_dict.copy() if prob_dict is not None else {})\n        self._log = log\n\n        if normalize:\n            if len(prob_dict) == 0:\n                raise ValueError('A DictionaryProbDist must have at least one sample ' +\n                             'before it can be normalized.')\n            if log:\n                value_sum = sum_logs(list(self._prob_dict.values()))\n                if value_sum <= _NINF:\n                    logp = math.log(1.0/len(prob_dict), 2)\n                    for x in prob_dict:\n                        self._prob_dict[x] = logp\n                else:\n                    for (x, p) in self._prob_dict.items():\n                        self._prob_dict[x] -= value_sum\n            else:\n                value_sum = sum(self._prob_dict.values())\n                if value_sum == 0:\n                    p = 1.0/len(prob_dict)\n                    for x in prob_dict:\n                        self._prob_dict[x] = p\n                else:\n                    norm_factor = 1.0/value_sum\n                    for (x, p) in self._prob_dict.items():\n                        self._prob_dict[x] *= norm_factor\n\n    def prob(self, sample):\n        if self._log:\n            return (2**(self._prob_dict[sample]) if sample in self._prob_dict else 0)\n        else:\n            return self._prob_dict.get(sample, 0)\n\n    def logprob(self, sample):\n        if self._log:\n            return self._prob_dict.get(sample, _NINF)\n        else:\n            if sample not in self._prob_dict: return _NINF\n            elif self._prob_dict[sample] == 0: return _NINF\n            else: return math.log(self._prob_dict[sample], 2)\n\n    def max(self):\n        if not hasattr(self, '_max'):\n            self._max = max((p,v) for (v,p) in self._prob_dict.items())[1]\n        return self._max\n    def samples(self):\n        return self._prob_dict.keys()\n    def __repr__(self):\n        return '<ProbDist with %d samples>' % len(self._prob_dict)\n\n\n@compat.python_2_unicode_compatible\nclass MLEProbDist(ProbDistI):\n    \"\"\"\n    The maximum likelihood estimate for the probability distribution\n    of the experiment used to generate a frequency distribution.  The\n    \"maximum likelihood estimate\" approximates the probability of\n    each sample as the frequency of that sample in the frequency\n    distribution.\n    \"\"\"\n    def __init__(self, freqdist, bins=None):\n        \"\"\"\n        Use the maximum likelihood estimate to create a probability\n        distribution for the experiment used to generate ``freqdist``.\n\n        :type freqdist: FreqDist\n        :param freqdist: The frequency distribution that the\n            probability estimates should be based on.\n        \"\"\"\n        self._freqdist = freqdist\n\n    def freqdist(self):\n        \"\"\"\n        Return the frequency distribution that this probability\n        distribution is based on.\n\n        :rtype: FreqDist\n        \"\"\"\n        return self._freqdist\n\n    def prob(self, sample):\n        return self._freqdist.freq(sample)\n\n    def max(self):\n        return self._freqdist.max()\n\n    def samples(self):\n        return self._freqdist.keys()\n\n    def __repr__(self):\n        \"\"\"\n        :rtype: str\n        :return: A string representation of this ``ProbDist``.\n        \"\"\"\n        return '<MLEProbDist based on %d samples>' % self._freqdist.N()\n\n\n@compat.python_2_unicode_compatible\nclass LidstoneProbDist(ProbDistI):\n    \"\"\"\n    The Lidstone estimate for the probability distribution of the\n    experiment used to generate a frequency distribution.  The\n    \"Lidstone estimate\" is parameterized by a real number *gamma*,\n    which typically ranges from 0 to 1.  The Lidstone estimate\n    approximates the probability of a sample with count *c* from an\n    experiment with *N* outcomes and *B* bins as\n    ``c+gamma)/(N+B*gamma)``.  This is equivalent to adding\n    *gamma* to the count for each bin, and taking the maximum\n    likelihood estimate of the resulting frequency distribution.\n    \"\"\"\n    SUM_TO_ONE = False\n    def __init__(self, freqdist, gamma, bins=None):\n        \"\"\"\n        Use the Lidstone estimate to create a probability distribution\n        for the experiment used to generate ``freqdist``.\n\n        :type freqdist: FreqDist\n        :param freqdist: The frequency distribution that the\n            probability estimates should be based on.\n        :type gamma: float\n        :param gamma: A real number used to parameterize the\n            estimate.  The Lidstone estimate is equivalent to adding\n            *gamma* to the count for each bin, and taking the\n            maximum likelihood estimate of the resulting frequency\n            distribution.\n        :type bins: int\n        :param bins: The number of sample values that can be generated\n            by the experiment that is described by the probability\n            distribution.  This value must be correctly set for the\n            probabilities of the sample values to sum to one.  If\n            ``bins`` is not specified, it defaults to ``freqdist.B()``.\n        \"\"\"\n        if (bins == 0) or (bins is None and freqdist.N() == 0):\n            name = self.__class__.__name__[:-8]\n            raise ValueError('A %s probability distribution ' % name +\n                             'must have at least one bin.')\n        if (bins is not None) and (bins < freqdist.B()):\n            name = self.__class__.__name__[:-8]\n            raise ValueError('\\nThe number of bins in a %s distribution ' % name +\n                             '(%d) must be greater than or equal to\\n' % bins +\n                             'the number of bins in the FreqDist used ' +\n                             'to create it (%d).' % freqdist.B())\n\n        self._freqdist = freqdist\n        self._gamma = float(gamma)\n        self._N = self._freqdist.N()\n\n        if bins is None:\n            bins = freqdist.B()\n        self._bins = bins\n\n        self._divisor = self._N + bins * gamma\n        if self._divisor == 0.0:\n            self._gamma = 0\n            self._divisor = 1\n\n    def freqdist(self):\n        \"\"\"\n        Return the frequency distribution that this probability\n        distribution is based on.\n\n        :rtype: FreqDist\n        \"\"\"\n        return self._freqdist\n\n    def prob(self, sample):\n        c = self._freqdist[sample]\n        return (c + self._gamma) / self._divisor\n\n    def max(self):\n        return self._freqdist.max()\n\n    def samples(self):\n        return self._freqdist.keys()\n\n    def discount(self):\n        gb = self._gamma * self._bins\n        return gb / (self._N + gb)\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of this ``ProbDist``.\n\n        :rtype: str\n        \"\"\"\n        return '<LidstoneProbDist based on %d samples>' % self._freqdist.N()\n\n\n@compat.python_2_unicode_compatible\nclass LaplaceProbDist(LidstoneProbDist):\n    \"\"\"\n    The Laplace estimate for the probability distribution of the\n    experiment used to generate a frequency distribution.  The\n    \"Laplace estimate\" approximates the probability of a sample with\n    count *c* from an experiment with *N* outcomes and *B* bins as\n    *(c+1)/(N+B)*.  This is equivalent to adding one to the count for\n    each bin, and taking the maximum likelihood estimate of the\n    resulting frequency distribution.\n    \"\"\"\n    def __init__(self, freqdist, bins=None):\n        \"\"\"\n        Use the Laplace estimate to create a probability distribution\n        for the experiment used to generate ``freqdist``.\n\n        :type freqdist: FreqDist\n        :param freqdist: The frequency distribution that the\n            probability estimates should be based on.\n        :type bins: int\n        :param bins: The number of sample values that can be generated\n            by the experiment that is described by the probability\n            distribution.  This value must be correctly set for the\n            probabilities of the sample values to sum to one.  If\n            ``bins`` is not specified, it defaults to ``freqdist.B()``.\n        \"\"\"\n        LidstoneProbDist.__init__(self, freqdist, 1, bins)\n\n    def __repr__(self):\n        \"\"\"\n        :rtype: str\n        :return: A string representation of this ``ProbDist``.\n        \"\"\"\n        return '<LaplaceProbDist based on %d samples>' % self._freqdist.N()\n\n\n@compat.python_2_unicode_compatible\nclass ELEProbDist(LidstoneProbDist):\n    \"\"\"\n    The expected likelihood estimate for the probability distribution\n    of the experiment used to generate a frequency distribution.  The\n    \"expected likelihood estimate\" approximates the probability of a\n    sample with count *c* from an experiment with *N* outcomes and\n    *B* bins as *(c+0.5)/(N+B/2)*.  This is equivalent to adding 0.5\n    to the count for each bin, and taking the maximum likelihood\n    estimate of the resulting frequency distribution.\n    \"\"\"\n    def __init__(self, freqdist, bins=None):\n        \"\"\"\n        Use the expected likelihood estimate to create a probability\n        distribution for the experiment used to generate ``freqdist``.\n\n        :type freqdist: FreqDist\n        :param freqdist: The frequency distribution that the\n            probability estimates should be based on.\n        :type bins: int\n        :param bins: The number of sample values that can be generated\n            by the experiment that is described by the probability\n            distribution.  This value must be correctly set for the\n            probabilities of the sample values to sum to one.  If\n            ``bins`` is not specified, it defaults to ``freqdist.B()``.\n        \"\"\"\n        LidstoneProbDist.__init__(self, freqdist, 0.5, bins)\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of this ``ProbDist``.\n\n        :rtype: str\n        \"\"\"\n        return '<ELEProbDist based on %d samples>' % self._freqdist.N()\n\n\n@compat.python_2_unicode_compatible\nclass HeldoutProbDist(ProbDistI):\n    \"\"\"\n    The heldout estimate for the probability distribution of the\n    experiment used to generate two frequency distributions.  These\n    two frequency distributions are called the \"heldout frequency\n    distribution\" and the \"base frequency distribution.\"  The\n    \"heldout estimate\" uses uses the \"heldout frequency\n    distribution\" to predict the probability of each sample, given its\n    frequency in the \"base frequency distribution\".\n\n    In particular, the heldout estimate approximates the probability\n    for a sample that occurs *r* times in the base distribution as\n    the average frequency in the heldout distribution of all samples\n    that occur *r* times in the base distribution.\n\n    This average frequency is *Tr[r]/(Nr[r].N)*, where:\n\n    - *Tr[r]* is the total count in the heldout distribution for\n      all samples that occur *r* times in the base distribution.\n    - *Nr[r]* is the number of samples that occur *r* times in\n      the base distribution.\n    - *N* is the number of outcomes recorded by the heldout\n      frequency distribution.\n\n    In order to increase the efficiency of the ``prob`` member\n    function, *Tr[r]/(Nr[r].N)* is precomputed for each value of *r*\n    when the ``HeldoutProbDist`` is created.\n\n    :type _estimate: list(float)\n    :ivar _estimate: A list mapping from *r*, the number of\n        times that a sample occurs in the base distribution, to the\n        probability estimate for that sample.  ``_estimate[r]`` is\n        calculated by finding the average frequency in the heldout\n        distribution of all samples that occur *r* times in the base\n        distribution.  In particular, ``_estimate[r]`` =\n        *Tr[r]/(Nr[r].N)*.\n    :type _max_r: int\n    :ivar _max_r: The maximum number of times that any sample occurs\n        in the base distribution.  ``_max_r`` is used to decide how\n        large ``_estimate`` must be.\n    \"\"\"\n    SUM_TO_ONE = False\n    def __init__(self, base_fdist, heldout_fdist, bins=None):\n        \"\"\"\n        Use the heldout estimate to create a probability distribution\n        for the experiment used to generate ``base_fdist`` and\n        ``heldout_fdist``.\n\n        :type base_fdist: FreqDist\n        :param base_fdist: The base frequency distribution.\n        :type heldout_fdist: FreqDist\n        :param heldout_fdist: The heldout frequency distribution.\n        :type bins: int\n        :param bins: The number of sample values that can be generated\n            by the experiment that is described by the probability\n            distribution.  This value must be correctly set for the\n            probabilities of the sample values to sum to one.  If\n            ``bins`` is not specified, it defaults to ``freqdist.B()``.\n        \"\"\"\n\n        self._base_fdist = base_fdist\n        self._heldout_fdist = heldout_fdist\n\n        self._max_r = base_fdist[base_fdist.max()]\n\n        Tr = self._calculate_Tr()\n        r_Nr = base_fdist.r_Nr(bins)\n        Nr = [r_Nr[r] for r in range(self._max_r+1)]\n        N = heldout_fdist.N()\n\n        self._estimate = self._calculate_estimate(Tr, Nr, N)\n\n    def _calculate_Tr(self):\n        \"\"\"\n        Return the list *Tr*, where *Tr[r]* is the total count in\n        ``heldout_fdist`` for all samples that occur *r*\n        times in ``base_fdist``.\n\n        :rtype: list(float)\n        \"\"\"\n        Tr = [0.0] * (self._max_r+1)\n        for sample in self._heldout_fdist:\n            r = self._base_fdist[sample]\n            Tr[r] += self._heldout_fdist[sample]\n        return Tr\n\n    def _calculate_estimate(self, Tr, Nr, N):\n        \"\"\"\n        Return the list *estimate*, where *estimate[r]* is the probability\n        estimate for any sample that occurs *r* times in the base frequency\n        distribution.  In particular, *estimate[r]* is *Tr[r]/(N[r].N)*.\n        In the special case that *N[r]=0*, *estimate[r]* will never be used;\n        so we define *estimate[r]=None* for those cases.\n\n        :rtype: list(float)\n        :type Tr: list(float)\n        :param Tr: the list *Tr*, where *Tr[r]* is the total count in\n            the heldout distribution for all samples that occur *r*\n            times in base distribution.\n        :type Nr: list(float)\n        :param Nr: The list *Nr*, where *Nr[r]* is the number of\n            samples that occur *r* times in the base distribution.\n        :type N: int\n        :param N: The total number of outcomes recorded by the heldout\n            frequency distribution.\n        \"\"\"\n        estimate = []\n        for r in range(self._max_r+1):\n            if Nr[r] == 0: estimate.append(None)\n            else: estimate.append(Tr[r]/(Nr[r]*N))\n        return estimate\n\n    def base_fdist(self):\n        \"\"\"\n        Return the base frequency distribution that this probability\n        distribution is based on.\n\n        :rtype: FreqDist\n        \"\"\"\n        return self._base_fdist\n\n    def heldout_fdist(self):\n        \"\"\"\n        Return the heldout frequency distribution that this\n        probability distribution is based on.\n\n        :rtype: FreqDist\n        \"\"\"\n        return self._heldout_fdist\n\n    def samples(self):\n        return self._base_fdist.keys()\n\n    def prob(self, sample):\n        r = self._base_fdist[sample]\n        return self._estimate[r]\n\n    def max(self):\n        return self._base_fdist.max()\n\n    def discount(self):\n        raise NotImplementedError()\n\n    def __repr__(self):\n        \"\"\"\n        :rtype: str\n        :return: A string representation of this ``ProbDist``.\n        \"\"\"\n        s = '<HeldoutProbDist: %d base samples; %d heldout samples>'\n        return s % (self._base_fdist.N(), self._heldout_fdist.N())\n\n\n@compat.python_2_unicode_compatible\nclass CrossValidationProbDist(ProbDistI):\n    \"\"\"\n    The cross-validation estimate for the probability distribution of\n    the experiment used to generate a set of frequency distribution.\n    The \"cross-validation estimate\" for the probability of a sample\n    is found by averaging the held-out estimates for the sample in\n    each pair of frequency distributions.\n    \"\"\"\n    SUM_TO_ONE = False\n    def __init__(self, freqdists, bins):\n        \"\"\"\n        Use the cross-validation estimate to create a probability\n        distribution for the experiment used to generate\n        ``freqdists``.\n\n        :type freqdists: list(FreqDist)\n        :param freqdists: A list of the frequency distributions\n            generated by the experiment.\n        :type bins: int\n        :param bins: The number of sample values that can be generated\n            by the experiment that is described by the probability\n            distribution.  This value must be correctly set for the\n            probabilities of the sample values to sum to one.  If\n            ``bins`` is not specified, it defaults to ``freqdist.B()``.\n        \"\"\"\n        self._freqdists = freqdists\n\n        self._heldout_probdists = []\n        for fdist1 in freqdists:\n            for fdist2 in freqdists:\n                if fdist1 is not fdist2:\n                    probdist = HeldoutProbDist(fdist1, fdist2, bins)\n                    self._heldout_probdists.append(probdist)\n\n    def freqdists(self):\n        \"\"\"\n        Return the list of frequency distributions that this ``ProbDist`` is based on.\n\n        :rtype: list(FreqDist)\n        \"\"\"\n        return self._freqdists\n\n    def samples(self):\n        return set(sum([list(fd) for fd in self._freqdists], []))\n\n    def prob(self, sample):\n        prob = 0.0\n        for heldout_probdist in self._heldout_probdists:\n            prob += heldout_probdist.prob(sample)\n        return prob/len(self._heldout_probdists)\n\n    def discount(self):\n        raise NotImplementedError()\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of this ``ProbDist``.\n\n        :rtype: str\n        \"\"\"\n        return '<CrossValidationProbDist: %d-way>' % len(self._freqdists)\n\n\n@compat.python_2_unicode_compatible\nclass WittenBellProbDist(ProbDistI):\n    \"\"\"\n    The Witten-Bell estimate of a probability distribution. This distribution\n    allocates uniform probability mass to as yet unseen events by using the\n    number of events that have only been seen once. The probability mass\n    reserved for unseen events is equal to *T / (N + T)*\n    where *T* is the number of observed event types and *N* is the total\n    number of observed events. This equates to the maximum likelihood estimate\n    of a new type event occurring. The remaining probability mass is discounted\n    such that all probability estimates sum to one, yielding:\n\n        - *p = T / Z (N + T)*, if count = 0\n        - *p = c / (N + T)*, otherwise\n    \"\"\"\n\n    def __init__(self, freqdist, bins=None):\n        \"\"\"\n        Creates a distribution of Witten-Bell probability estimates.  This\n        distribution allocates uniform probability mass to as yet unseen\n        events by using the number of events that have only been seen once. The\n        probability mass reserved for unseen events is equal to *T / (N + T)*\n        where *T* is the number of observed event types and *N* is the total\n        number of observed events. This equates to the maximum likelihood\n        estimate of a new type event occurring. The remaining probability mass\n        is discounted such that all probability estimates sum to one,\n        yielding:\n\n            - *p = T / Z (N + T)*, if count = 0\n            - *p = c / (N + T)*, otherwise\n\n        The parameters *T* and *N* are taken from the ``freqdist`` parameter\n        (the ``B()`` and ``N()`` values). The normalizing factor *Z* is\n        calculated using these values along with the ``bins`` parameter.\n\n        :param freqdist: The frequency counts upon which to base the\n            estimation.\n        :type freqdist: FreqDist\n        :param bins: The number of possible event types. This must be at least\n            as large as the number of bins in the ``freqdist``. If None, then\n            it's assumed to be equal to that of the ``freqdist``\n        :type bins: int\n        \"\"\"\n        assert bins is None or bins >= freqdist.B(),\\\n               'bins parameter must not be less than %d=freqdist.B()' % freqdist.B()\n        if bins is None:\n            bins = freqdist.B()\n        self._freqdist = freqdist\n        self._T = self._freqdist.B()\n        self._Z = bins - self._freqdist.B()\n        self._N = self._freqdist.N()\n        if self._N==0:\n            self._P0 = 1.0 / self._Z\n        else:\n            self._P0 = self._T / (self._Z * (self._N + self._T))\n\n    def prob(self, sample):\n        c = self._freqdist[sample]\n        return (c / (self._N + self._T) if c != 0 else self._P0)\n\n    def max(self):\n        return self._freqdist.max()\n\n    def samples(self):\n        return self._freqdist.keys()\n\n    def freqdist(self):\n        return self._freqdist\n\n    def discount(self):\n        raise NotImplementedError()\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of this ``ProbDist``.\n\n        :rtype: str\n        \"\"\"\n        return '<WittenBellProbDist based on %d samples>' % self._freqdist.N()\n\n\n\n\n\n@compat.python_2_unicode_compatible\nclass SimpleGoodTuringProbDist(ProbDistI):\n    \"\"\"\n    SimpleGoodTuring ProbDist approximates from frequency to frequency of\n    frequency into a linear line under log space by linear regression.\n    Details of Simple Good-Turing algorithm can be found in:\n\n    - Good Turing smoothing without tears\" (Gale & Sampson 1995),\n      Journal of Quantitative Linguistics, vol. 2 pp. 217-237.\n    - \"Speech and Language Processing (Jurafsky & Martin),\n      2nd Edition, Chapter 4.5 p103 (log(Nc) =  a + b*log(c))\n    - http://www.grsampson.net/RGoodTur.html\n\n    Given a set of pair (xi, yi),  where the xi denotes the frequency and\n    yi denotes the frequency of frequency, we want to minimize their\n    square variation. E(x) and E(y) represent the mean of xi and yi.\n\n    - slope: b = sigma ((xi-E(x)(yi-E(y))) / sigma ((xi-E(x))(xi-E(x)))\n    - intercept: a = E(y) - b.E(x)\n    \"\"\"\n    SUM_TO_ONE = False\n    def __init__(self, freqdist, bins=None):\n        \"\"\"\n        :param freqdist: The frequency counts upon which to base the\n            estimation.\n        :type freqdist: FreqDist\n        :param bins: The number of possible event types. This must be\n            larger than the number of bins in the ``freqdist``. If None,\n            then it's assumed to be equal to ``freqdist``.B() + 1\n        :type bins: int\n        \"\"\"\n        assert bins is None or bins > freqdist.B(),\\\n               'bins parameter must not be less than %d=freqdist.B()+1' % (freqdist.B()+1)\n        if bins is None:\n            bins = freqdist.B() + 1\n        self._freqdist = freqdist\n        self._bins = bins\n        r, nr = self._r_Nr()\n        self.find_best_fit(r, nr)\n        self._switch(r, nr)\n        self._renormalize(r, nr)\n\n    def _r_Nr_non_zero(self):\n        r_Nr = self._freqdist.r_Nr()\n        del r_Nr[0]\n        return r_Nr\n\n    def _r_Nr(self):\n        \"\"\"\n        Split the frequency distribution in two list (r, Nr), where Nr(r) > 0\n        \"\"\"\n        nonzero = self._r_Nr_non_zero()\n\n        if not nonzero:\n            return [], []\n        return zip(*sorted(nonzero.items()))\n\n    def find_best_fit(self, r, nr):\n        \"\"\"\n        Use simple linear regression to tune parameters self._slope and\n        self._intercept in the log-log space based on count and Nr(count)\n        (Work in log space to avoid floating point underflow.)\n        \"\"\"\n\n        if not r or not nr:\n            return\n\n        zr = []\n        for j in range(len(r)):\n            i = (r[j-1] if j > 0 else 0)\n            k = (2 * r[j] - i if j == len(r) - 1 else r[j+1])\n            zr_ = 2.0 * nr[j] / (k - i)\n            zr.append(zr_)\n\n        log_r = [math.log(i) for i in r]\n        log_zr = [math.log(i) for i in zr]\n\n        xy_cov = x_var = 0.0\n        x_mean = sum(log_r) / len(log_r)\n        y_mean = sum(log_zr) / len(log_zr)\n        for (x, y) in zip(log_r, log_zr):\n            xy_cov += (x - x_mean) * (y - y_mean)\n            x_var += (x - x_mean)**2\n        self._slope = (xy_cov / x_var if x_var != 0 else 0.0)\n        if self._slope >= -1:\n            warnings.warn('SimpleGoodTuring did not find a proper best fit '\n                          'line for smoothing probabilities of occurrences. '\n                          'The probability estimates are likely to be '\n                          'unreliable.')\n        self._intercept = y_mean - self._slope * x_mean\n\n    def _switch(self, r, nr):\n        \"\"\"\n        Calculate the r frontier where we must switch from Nr to Sr\n        when estimating E[Nr].\n        \"\"\"\n        for i, r_ in enumerate(r):\n            if len(r) == i + 1 or r[i+1] != r_ + 1:\n                self._switch_at = r_\n                break\n\n            Sr = self.smoothedNr\n            smooth_r_star = (r_ + 1) * Sr(r_+1) / Sr(r_)\n            unsmooth_r_star = (r_ + 1) * nr[i+1] / nr[i]\n\n            std = math.sqrt(self._variance(r_, nr[i], nr[i+1]))\n            if abs(unsmooth_r_star-smooth_r_star) <= 1.96 * std:\n                self._switch_at = r_\n                break\n\n    def _variance(self, r, nr, nr_1):\n        r = float(r)\n        nr = float(nr)\n        nr_1 = float(nr_1)\n        return (r + 1.0)**2 * (nr_1 / nr**2) * (1.0 + nr_1 / nr)\n\n    def _renormalize(self, r, nr):\n        \"\"\"\n        It is necessary to renormalize all the probability estimates to\n        ensure a proper probability distribution results. This can be done\n        by keeping the estimate of the probability mass for unseen items as\n        N(1)/N and renormalizing all the estimates for previously seen items\n        (as Gale and Sampson (1995) propose). (See M&S P.213, 1999)\n        \"\"\"\n        prob_cov = 0.0\n        for r_, nr_ in zip(r, nr):\n            prob_cov  += nr_ * self._prob_measure(r_)\n        if prob_cov:\n            self._renormal = (1 - self._prob_measure(0)) / prob_cov\n\n    def smoothedNr(self, r):\n        \"\"\"\n        Return the number of samples with count r.\n\n        :param r: The amount of frequency.\n        :type r: int\n        :rtype: float\n        \"\"\"\n\n\n        return math.exp(self._intercept + self._slope * math.log(r))\n\n    def prob(self, sample):\n        \"\"\"\n        Return the sample's probability.\n\n        :param sample: sample of the event\n        :type sample: str\n        :rtype: float\n        \"\"\"\n        count = self._freqdist[sample]\n        p = self._prob_measure(count)\n        if count == 0:\n            if self._bins == self._freqdist.B():\n                p = 0.0\n            else:\n                p = p / (self._bins - self._freqdist.B())\n        else:\n            p = p * self._renormal\n        return p\n\n    def _prob_measure(self, count):\n        if count == 0 and self._freqdist.N() == 0 :\n            return 1.0\n        elif count == 0 and self._freqdist.N() != 0:\n            return self._freqdist.Nr(1) / self._freqdist.N()\n\n        if self._switch_at > count:\n            Er_1 = self._freqdist.Nr(count+1)\n            Er = self._freqdist.Nr(count)\n        else:\n            Er_1 = self.smoothedNr(count+1)\n            Er = self.smoothedNr(count)\n\n        r_star = (count + 1) * Er_1 / Er\n        return r_star / self._freqdist.N()\n\n    def check(self):\n        prob_sum = 0.0\n        for i in  range(0, len(self._Nr)):\n            prob_sum += self._Nr[i] * self._prob_measure(i) / self._renormal\n        print(\"Probability Sum:\", prob_sum)\n\n    def discount(self):\n        \"\"\"\n        This function returns the total mass of probability transfers from the\n        seen samples to the unseen samples.\n        \"\"\"\n        return  self.smoothedNr(1) / self._freqdist.N()\n\n    def max(self):\n        return self._freqdist.max()\n\n    def samples(self):\n        return self._freqdist.keys()\n\n    def freqdist(self):\n        return self._freqdist\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of this ``ProbDist``.\n\n        :rtype: str\n        \"\"\"\n        return '<SimpleGoodTuringProbDist based on %d samples>'\\\n                % self._freqdist.N()\n\n\nclass MutableProbDist(ProbDistI):\n    \"\"\"\n    An mutable probdist where the probabilities may be easily modified. This\n    simply copies an existing probdist, storing the probability values in a\n    mutable dictionary and providing an update method.\n    \"\"\"\n\n    def __init__(self, prob_dist, samples, store_logs=True):\n        \"\"\"\n        Creates the mutable probdist based on the given prob_dist and using\n        the list of samples given. These values are stored as log\n        probabilities if the store_logs flag is set.\n\n        :param prob_dist: the distribution from which to garner the\n            probabilities\n        :type prob_dist: ProbDist\n        :param samples: the complete set of samples\n        :type samples: sequence of any\n        :param store_logs: whether to store the probabilities as logarithms\n        :type store_logs: bool\n        \"\"\"\n        self._samples = samples\n        self._sample_dict = dict((samples[i], i) for i in range(len(samples)))\n        self._data = array.array(str(\"d\"), [0.0]) * len(samples)\n        for i in range(len(samples)):\n            if store_logs:\n                self._data[i] = prob_dist.logprob(samples[i])\n            else:\n                self._data[i] = prob_dist.prob(samples[i])\n        self._logs = store_logs\n\n    def max(self):\n        return max((p,v) for (v,p) in self._sample_dict.items())[1]\n\n    def samples(self):\n        return self._samples\n\n    def prob(self, sample):\n        i = self._sample_dict.get(sample)\n        if i is None:\n            return 0.0\n        return (2**(self._data[i]) if self._logs else self._data[i])\n\n    def logprob(self, sample):\n        i = self._sample_dict.get(sample)\n        if i is None:\n            return float('-inf')\n        return (self._data[i] if self._logs else math.log(self._data[i], 2))\n\n    def update(self, sample, prob, log=True):\n        \"\"\"\n        Update the probability for the given sample. This may cause the object\n        to stop being the valid probability distribution - the user must\n        ensure that they update the sample probabilities such that all samples\n        have probabilities between 0 and 1 and that all probabilities sum to\n        one.\n\n        :param sample: the sample for which to update the probability\n        :type sample: any\n        :param prob: the new probability\n        :type prob: float\n        :param log: is the probability already logged\n        :type log: bool\n        \"\"\"\n        i = self._sample_dict.get(sample)\n        assert i is not None\n        if self._logs:\n            self._data[i] = (prob if log else math.log(prob, 2))\n        else:\n            self._data[i] = (2**(prob) if log else prob)\n\n\n\n@compat.python_2_unicode_compatible\nclass KneserNeyProbDist(ProbDistI):\n    \"\"\"\n    Kneser-Ney estimate of a probability distribution. This is a version of\n    back-off that counts how likely an n-gram is provided the n-1-gram had\n    been seen in training. Extends the ProbDistI interface, requires a trigram\n    FreqDist instance to train on. Optionally, a different from default discount\n    value can be specified. The default discount is set to 0.75.\n\n    \"\"\"\n    def __init__(self, freqdist, bins=None, discount=0.75):\n        \"\"\"\n        :param freqdist: The trigram frequency distribution upon which to base\n            the estimation\n        :type freqdist: FreqDist\n        :param bins: Included for compatibility with nltk.tag.hmm\n        :type bins: int or float\n        :param discount: The discount applied when retrieving counts of\n            trigrams\n        :type discount: float (preferred, but can be set to int)\n        \"\"\"\n\n        if not bins:\n            self._bins = freqdist.B()\n        else:\n            self._bins = bins\n        self._D = discount\n\n        self._cache = {}\n\n        self._bigrams = defaultdict(int)\n        self._trigrams = freqdist\n\n        self._wordtypes_after = defaultdict(float)\n        self._trigrams_contain = defaultdict(float)\n        self._wordtypes_before = defaultdict(float)\n        for w0, w1, w2 in freqdist:\n            self._bigrams[(w0,w1)] += freqdist[(w0, w1, w2)]\n            self._wordtypes_after[(w0,w1)] += 1\n            self._trigrams_contain[w1] += 1\n            self._wordtypes_before[(w1,w2)] += 1\n\n    def prob(self, trigram):\n        if len(trigram) != 3:\n            raise ValueError('Expected an iterable with 3 members.')\n        trigram = tuple(trigram)\n        w0, w1, w2 = trigram\n\n        if trigram in self._cache:\n            return self._cache[trigram]\n        else:\n            if trigram in self._trigrams:\n                prob = (self._trigrams[trigram]\n                        - self.discount())/self._bigrams[(w0, w1)]\n\n            elif (w0,w1) in self._bigrams and (w1,w2) in self._wordtypes_before:\n                aftr = self._wordtypes_after[(w0, w1)]\n                bfr = self._wordtypes_before[(w1, w2)]\n\n                leftover_prob = ((aftr * self.discount())\n                                 / self._bigrams[(w0, w1)])\n\n                beta = bfr /(self._trigrams_contain[w1] - aftr)\n\n                prob = leftover_prob * beta\n\n            else:\n                prob = 0.0\n\n            self._cache[trigram] = prob\n            return prob\n\n    def discount(self):\n        \"\"\"\n        Return the value by which counts are discounted. By default set to 0.75.\n\n        :rtype: float\n        \"\"\"\n        return self._D\n\n    def set_discount(self, discount):\n        \"\"\"\n        Set the value by which counts are discounted to the value of discount.\n\n        :param discount: the new value to discount counts by\n        :type discount: float (preferred, but int possible)\n        :rtype: None\n        \"\"\"\n        self._D = discount\n\n    def samples(self):\n        return self._trigrams.keys()\n\n    def max(self):\n        return self._trigrams.max()\n\n    def __repr__(self):\n        '''\n        Return a string representation of this ProbDist\n\n        :rtype: str\n        '''\n        return '<KneserNeyProbDist based on {0} trigrams'.format(self._trigrams.N())\n\n\ndef log_likelihood(test_pdist, actual_pdist):\n    if (not isinstance(test_pdist, ProbDistI) or\n        not isinstance(actual_pdist, ProbDistI)):\n        raise ValueError('expected a ProbDist.')\n    return sum(actual_pdist.prob(s) * math.log(test_pdist.prob(s), 2)\n               for s in actual_pdist)\n\ndef entropy(pdist):\n    probs = (pdist.prob(s) for s in pdist.samples())\n    return -sum(p * math.log(p,2) for p in probs)\n\n\n@compat.python_2_unicode_compatible\nclass ConditionalFreqDist(defaultdict):\n    \"\"\"\n    A collection of frequency distributions for a single experiment\n    run under different conditions.  Conditional frequency\n    distributions are used to record the number of times each sample\n    occurred, given the condition under which the experiment was run.\n    For example, a conditional frequency distribution could be used to\n    record the frequency of each word (type) in a document, given its\n    length.  Formally, a conditional frequency distribution can be\n    defined as a function that maps from each condition to the\n    FreqDist for the experiment under that condition.\n\n    Conditional frequency distributions are typically constructed by\n    repeatedly running an experiment under a variety of conditions,\n    and incrementing the sample outcome counts for the appropriate\n    conditions.  For example, the following code will produce a\n    conditional frequency distribution that encodes how often each\n    word type occurs, given the length of that word type:\n\n        >>> from nltk.probability import ConditionalFreqDist\n        >>> from nltk.tokenize import word_tokenize\n        >>> sent = \"the the the dog dog some other words that we do not care about\"\n        >>> cfdist = ConditionalFreqDist()\n        >>> for word in word_tokenize(sent):\n        ...     condition = len(word)\n        ...     cfdist[condition][word] += 1\n\n    An equivalent way to do this is with the initializer:\n\n        >>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))\n\n    The frequency distribution for each condition is accessed using\n    the indexing operator:\n\n        >>> cfdist[3]\n        FreqDist({'the': 3, 'dog': 2, 'not': 1})\n        >>> cfdist[3].freq('the')\n        0.5\n        >>> cfdist[3]['dog']\n        2\n\n    When the indexing operator is used to access the frequency\n    distribution for a condition that has not been accessed before,\n    ``ConditionalFreqDist`` creates a new empty FreqDist for that\n    condition.\n\n    \"\"\"\n    def __init__(self, cond_samples=None):\n        \"\"\"\n        Construct a new empty conditional frequency distribution.  In\n        particular, the count for every sample, under every condition,\n        is zero.\n\n        :param cond_samples: The samples to initialize the conditional\n            frequency distribution with\n        :type cond_samples: Sequence of (condition, sample) tuples\n        \"\"\"\n        defaultdict.__init__(self, FreqDist)\n\n        if cond_samples:\n            for (cond, sample) in cond_samples:\n                self[cond][sample] += 1\n\n    def __reduce__(self):\n        kv_pairs = ((cond, self[cond]) for cond in self.conditions())\n        return (self.__class__, (), None, None, kv_pairs)\n\n    def conditions(self):\n        \"\"\"\n        Return a list of the conditions that have been accessed for\n        this ``ConditionalFreqDist``.  Use the indexing operator to\n        access the frequency distribution for a given condition.\n        Note that the frequency distributions for some conditions\n        may contain zero sample outcomes.\n\n        :rtype: list\n        \"\"\"\n        return list(self.keys())\n\n    def N(self):\n        \"\"\"\n        Return the total number of sample outcomes that have been\n        recorded by this ``ConditionalFreqDist``.\n\n        :rtype: int\n        \"\"\"\n        return sum(fdist.N() for fdist in itervalues(self))\n\n    def plot(self, *args, **kwargs):\n        \"\"\"\n        Plot the given samples from the conditional frequency distribution.\n        For a cumulative plot, specify cumulative=True.\n        (Requires Matplotlib to be installed.)\n\n        :param samples: The samples to plot\n        :type samples: list\n        :param title: The title for the graph\n        :type title: str\n        :param conditions: The conditions to plot (default is all)\n        :type conditions: list\n        \"\"\"\n        try:\n            from matplotlib import pylab\n        except ImportError:\n            raise ValueError('The plot function requires matplotlib to be installed.'\n                         'See http://matplotlib.org/')\n\n        cumulative = _get_kwarg(kwargs, 'cumulative', False)\n        conditions = _get_kwarg(kwargs, 'conditions', sorted(self.conditions()))\n        title = _get_kwarg(kwargs, 'title', '')\n        samples = _get_kwarg(kwargs, 'samples',\n                             sorted(set(v for c in conditions for v in self[c])))  # this computation could be wasted\n        if not \"linewidth\" in kwargs:\n            kwargs[\"linewidth\"] = 2\n\n        for condition in conditions:\n            if cumulative:\n                freqs = list(self[condition]._cumulative_frequencies(samples))\n                ylabel = \"Cumulative Counts\"\n                legend_loc = 'lower right'\n            else:\n                freqs = [self[condition][sample] for sample in samples]\n                ylabel = \"Counts\"\n                legend_loc = 'upper right'\n            kwargs['label'] = \"%s\" % condition\n            pylab.plot(freqs, *args, **kwargs)\n\n        pylab.legend(loc=legend_loc)\n        pylab.grid(True, color=\"silver\")\n        pylab.xticks(range(len(samples)), [text_type(s) for s in samples], rotation=90)\n        if title:\n            pylab.title(title)\n        pylab.xlabel(\"Samples\")\n        pylab.ylabel(ylabel)\n        pylab.show()\n\n    def tabulate(self, *args, **kwargs):\n        \"\"\"\n        Tabulate the given samples from the conditional frequency distribution.\n\n        :param samples: The samples to plot\n        :type samples: list\n        :param conditions: The conditions to plot (default is all)\n        :type conditions: list\n        :param cumulative: A flag to specify whether the freqs are cumulative (default = False)\n        :type title: bool\n        \"\"\"\n\n        cumulative = _get_kwarg(kwargs, 'cumulative', False)\n        conditions = _get_kwarg(kwargs, 'conditions', sorted(self.conditions()))\n        samples = _get_kwarg(kwargs, 'samples',\n                             sorted(set(v for c in conditions for v in self[c])))  # this computation could be wasted\n\n        width = max(len(\"%s\" % s) for s in samples)\n        freqs = dict()\n        for c in conditions:\n            if cumulative:\n                freqs[c] = list(self[c]._cumulative_frequencies(samples))\n            else:\n                freqs[c] = [self[c][sample] for sample in samples]\n            width = max(width, max(len(\"%d\" % f) for f in freqs[c]))\n\n        condition_size = max(len(\"%s\" % c) for c in conditions)\n        print(' ' * condition_size, end=' ')\n        for s in samples:\n            print(\"%*s\" % (width, s), end=' ')\n        print()\n        for c in conditions:\n            print(\"%*s\" % (condition_size, c), end=' ')\n            for f in freqs[c]:\n                print(\"%*d\" % (width, f), end=' ')\n            print()\n\n\n    def __add__(self, other):\n        \"\"\"\n        Add counts from two ConditionalFreqDists.\n        \"\"\"\n        if not isinstance(other, ConditionalFreqDist):\n            return NotImplemented\n        result = ConditionalFreqDist()\n        for cond in self.conditions():\n            newfreqdist = self[cond] + other[cond]\n            if newfreqdist:\n                result[cond] = newfreqdist\n        for cond in other.conditions():\n            if cond not in self.conditions():\n                for elem, count in other[cond].items():\n                    if count > 0:\n                        result[cond][elem] = count\n        return result\n\n    def __sub__(self, other):\n        \"\"\"\n        Subtract count, but keep only results with positive counts.\n        \"\"\"\n        if not isinstance(other, ConditionalFreqDist):\n            return NotImplemented\n        result = ConditionalFreqDist()\n        for cond in self.conditions():\n            newfreqdist = self[cond] - other[cond]\n            if newfreqdist:\n                result[cond] = newfreqdist\n        for cond in other.conditions():\n            if cond not in self.conditions():\n                for elem, count in other[cond].items():\n                    if count < 0:\n                        result[cond][elem] = 0 - count\n        return result\n\n    def __or__(self, other):\n        \"\"\"\n        Union is the maximum of value in either of the input counters.\n        \"\"\"\n        if not isinstance(other, ConditionalFreqDist):\n            return NotImplemented\n        result = ConditionalFreqDist()\n        for cond in self.conditions():\n            newfreqdist = self[cond] | other[cond]\n            if newfreqdist:\n                result[cond] = newfreqdist\n        for cond in other.conditions():\n            if cond not in self.conditions():\n                for elem, count in other[cond].items():\n                    if count > 0:\n                        result[cond][elem] = count\n        return result\n\n    def __and__(self, other):\n        \"\"\"\n        Intersection is the minimum of corresponding counts.\n        \"\"\"\n        if not isinstance(other, ConditionalFreqDist):\n            return NotImplemented\n        result = ConditionalFreqDist()\n        for cond in self.conditions():\n            newfreqdist = self[cond] & other[cond]\n            if newfreqdist:\n                result[cond] = newfreqdist\n        return result\n\n    def __le__(self, other):\n        if not isinstance(other, ConditionalFreqDist):\n            raise_unorderable_types(\"<=\", self, other)\n        return set(self.conditions()).issubset(other.conditions()) \\\n               and all(self[c] <= other[c] for c in self.conditions())\n    def __lt__(self, other):\n        if not isinstance(other, ConditionalFreqDist):\n            raise_unorderable_types(\"<\", self, other)\n        return self <= other and self != other\n    def __ge__(self, other):\n        if not isinstance(other, ConditionalFreqDist):\n            raise_unorderable_types(\">=\", self, other)\n        return other <= self\n    def __gt__(self, other):\n        if not isinstance(other, ConditionalFreqDist):\n            raise_unorderable_types(\">\", self, other)\n        return other < self\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of this ``ConditionalFreqDist``.\n\n        :rtype: str\n        \"\"\"\n        return '<ConditionalFreqDist with %d conditions>' % len(self)\n\n\n@compat.python_2_unicode_compatible\n@add_metaclass(ABCMeta)\nclass ConditionalProbDistI(dict):\n    \"\"\"\n    A collection of probability distributions for a single experiment\n    run under different conditions.  Conditional probability\n    distributions are used to estimate the likelihood of each sample,\n    given the condition under which the experiment was run.  For\n    example, a conditional probability distribution could be used to\n    estimate the probability of each word type in a document, given\n    the length of the word type.  Formally, a conditional probability\n    distribution can be defined as a function that maps from each\n    condition to the ``ProbDist`` for the experiment under that\n    condition.\n    \"\"\"\n    @abstractmethod\n    def __init__(self):\n        \"\"\"\n        Classes inheriting from ConditionalProbDistI should implement __init__.\n        \"\"\"\n\n    def conditions(self):\n        \"\"\"\n        Return a list of the conditions that are represented by\n        this ``ConditionalProbDist``.  Use the indexing operator to\n        access the probability distribution for a given condition.\n\n        :rtype: list\n        \"\"\"\n        return list(self.keys())\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of this ``ConditionalProbDist``.\n\n        :rtype: str\n        \"\"\"\n        return '<%s with %d conditions>' % (type(self).__name__, len(self))\n\n\nclass ConditionalProbDist(ConditionalProbDistI):\n    \"\"\"\n    A conditional probability distribution modeling the experiments\n    that were used to generate a conditional frequency distribution.\n    A ConditionalProbDist is constructed from a\n    ``ConditionalFreqDist`` and a ``ProbDist`` factory:\n\n    - The ``ConditionalFreqDist`` specifies the frequency\n      distribution for each condition.\n    - The ``ProbDist`` factory is a function that takes a\n      condition's frequency distribution, and returns its\n      probability distribution.  A ``ProbDist`` class's name (such as\n      ``MLEProbDist`` or ``HeldoutProbDist``) can be used to specify\n      that class's constructor.\n\n    The first argument to the ``ProbDist`` factory is the frequency\n    distribution that it should model; and the remaining arguments are\n    specified by the ``factory_args`` parameter to the\n    ``ConditionalProbDist`` constructor.  For example, the following\n    code constructs a ``ConditionalProbDist``, where the probability\n    distribution for each condition is an ``ELEProbDist`` with 10 bins:\n\n        >>> from nltk.corpus import brown\n        >>> from nltk.probability import ConditionalFreqDist\n        >>> from nltk.probability import ConditionalProbDist, ELEProbDist\n        >>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])\n        >>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)\n        >>> cpdist['passed'].max()\n        'VBD'\n        >>> cpdist['passed'].prob('VBD')\n        0.423...\n\n    \"\"\"\n    def __init__(self, cfdist, probdist_factory,\n                 *factory_args, **factory_kw_args):\n        \"\"\"\n        Construct a new conditional probability distribution, based on\n        the given conditional frequency distribution and ``ProbDist``\n        factory.\n\n        :type cfdist: ConditionalFreqDist\n        :param cfdist: The ``ConditionalFreqDist`` specifying the\n            frequency distribution for each condition.\n        :type probdist_factory: class or function\n        :param probdist_factory: The function or class that maps\n            a condition's frequency distribution to its probability\n            distribution.  The function is called with the frequency\n            distribution as its first argument,\n            ``factory_args`` as its remaining arguments, and\n            ``factory_kw_args`` as keyword arguments.\n        :type factory_args: (any)\n        :param factory_args: Extra arguments for ``probdist_factory``.\n            These arguments are usually used to specify extra\n            properties for the probability distributions of individual\n            conditions, such as the number of bins they contain.\n        :type factory_kw_args: (any)\n        :param factory_kw_args: Extra keyword arguments for ``probdist_factory``.\n        \"\"\"\n        self._probdist_factory = probdist_factory\n        self._factory_args = factory_args\n        self._factory_kw_args = factory_kw_args\n\n        for condition in cfdist:\n            self[condition] = probdist_factory(cfdist[condition],\n                                               *factory_args, **factory_kw_args)\n\n    def __missing__(self, key):\n        self[key] = self._probdist_factory(FreqDist(),\n                                           *self._factory_args,\n                                           **self._factory_kw_args)\n        return self[key]\n\nclass DictionaryConditionalProbDist(ConditionalProbDistI):\n    \"\"\"\n    An alternative ConditionalProbDist that simply wraps a dictionary of\n    ProbDists rather than creating these from FreqDists.\n    \"\"\"\n\n    def __init__(self, probdist_dict):\n        \"\"\"\n        :param probdist_dict: a dictionary containing the probdists indexed\n            by the conditions\n        :type probdist_dict: dict any -> probdist\n        \"\"\"\n        self.update(probdist_dict)\n\n    def __missing__(self, key):\n        self[key] = DictionaryProbDist()\n        return self[key]\n\n\n_ADD_LOGS_MAX_DIFF = math.log(1e-30, 2)\n\ndef add_logs(logx, logy):\n    \"\"\"\n    Given two numbers ``logx`` = *log(x)* and ``logy`` = *log(y)*, return\n    *log(x+y)*.  Conceptually, this is the same as returning\n    ``log(2**(logx)+2**(logy))``, but the actual implementation\n    avoids overflow errors that could result from direct computation.\n    \"\"\"\n    if (logx < logy + _ADD_LOGS_MAX_DIFF):\n        return logy\n    if (logy < logx + _ADD_LOGS_MAX_DIFF):\n        return logx\n    base = min(logx, logy)\n    return base + math.log(2**(logx-base) + 2**(logy-base), 2)\n\ndef sum_logs(logs):\n    return (reduce(add_logs, logs[1:], logs[0]) if len(logs) != 0 else _NINF)\n\n\nclass ProbabilisticMixIn(object):\n    \"\"\"\n    A mix-in class to associate probabilities with other classes\n    (trees, rules, etc.).  To use the ``ProbabilisticMixIn`` class,\n    define a new class that derives from an existing class and from\n    ProbabilisticMixIn.  You will need to define a new constructor for\n    the new class, which explicitly calls the constructors of both its\n    parent classes.  For example:\n\n        >>> from nltk.probability import ProbabilisticMixIn\n        >>> class A:\n        ...     def __init__(self, x, y): self.data = (x,y)\n        ...\n        >>> class ProbabilisticA(A, ProbabilisticMixIn):\n        ...     def __init__(self, x, y, **prob_kwarg):\n        ...         A.__init__(self, x, y)\n        ...         ProbabilisticMixIn.__init__(self, **prob_kwarg)\n\n    See the documentation for the ProbabilisticMixIn\n    ``constructor<__init__>`` for information about the arguments it\n    expects.\n\n    You should generally also redefine the string representation\n    methods, the comparison methods, and the hashing method.\n    \"\"\"\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize this object's probability.  This initializer should\n        be called by subclass constructors.  ``prob`` should generally be\n        the first argument for those constructors.\n\n        :param prob: The probability associated with the object.\n        :type prob: float\n        :param logprob: The log of the probability associated with\n            the object.\n        :type logprob: float\n        \"\"\"\n        if 'prob' in kwargs:\n            if 'logprob' in kwargs:\n                raise TypeError('Must specify either prob or logprob '\n                                '(not both)')\n            else:\n                ProbabilisticMixIn.set_prob(self, kwargs['prob'])\n        elif 'logprob' in kwargs:\n            ProbabilisticMixIn.set_logprob(self, kwargs['logprob'])\n        else:\n            self.__prob = self.__logprob = None\n\n    def set_prob(self, prob):\n        \"\"\"\n        Set the probability associated with this object to ``prob``.\n\n        :param prob: The new probability\n        :type prob: float\n        \"\"\"\n        self.__prob = prob\n        self.__logprob = None\n\n    def set_logprob(self, logprob):\n        \"\"\"\n        Set the log probability associated with this object to\n        ``logprob``.  I.e., set the probability associated with this\n        object to ``2**(logprob)``.\n\n        :param logprob: The new log probability\n        :type logprob: float\n        \"\"\"\n        self.__logprob = logprob\n        self.__prob = None\n\n    def prob(self):\n        \"\"\"\n        Return the probability associated with this object.\n\n        :rtype: float\n        \"\"\"\n        if self.__prob is None:\n            if self.__logprob is None: return None\n            self.__prob = 2**(self.__logprob)\n        return self.__prob\n\n    def logprob(self):\n        \"\"\"\n        Return ``log(p)``, where ``p`` is the probability associated\n        with this object.\n\n        :rtype: float\n        \"\"\"\n        if self.__logprob is None:\n            if self.__prob is None: return None\n            self.__logprob = math.log(self.__prob, 2)\n        return self.__logprob\n\nclass ImmutableProbabilisticMixIn(ProbabilisticMixIn):\n    def set_prob(self, prob):\n        raise ValueError('%s is immutable' % self.__class__.__name__)\n    def set_logprob(self, prob):\n        raise ValueError('%s is immutable' % self.__class__.__name__)\n\n\ndef _get_kwarg(kwargs, key, default):\n    if key in kwargs:\n        arg = kwargs[key]\n        del kwargs[key]\n    else:\n        arg = default\n    return arg\n\n\ndef _create_rand_fdist(numsamples, numoutcomes):\n    \"\"\"\n    Create a new frequency distribution, with random samples.  The\n    samples are numbers from 1 to ``numsamples``, and are generated by\n    summing two numbers, each of which has a uniform distribution.\n    \"\"\"\n    import random\n    fdist = FreqDist()\n    for x in range(numoutcomes):\n        y = (random.randint(1, (1 + numsamples) // 2) +\n             random.randint(0, numsamples // 2))\n        fdist[y] += 1\n    return fdist\n\ndef _create_sum_pdist(numsamples):\n    \"\"\"\n    Return the true probability distribution for the experiment\n    ``_create_rand_fdist(numsamples, x)``.\n    \"\"\"\n    fdist = FreqDist()\n    for x in range(1, (1 + numsamples) // 2 + 1):\n        for y in range(0, numsamples // 2 + 1):\n            fdist[x+y] += 1\n    return MLEProbDist(fdist)\n\ndef demo(numsamples=6, numoutcomes=500):\n    \"\"\"\n    A demonstration of frequency distributions and probability\n    distributions.  This demonstration creates three frequency\n    distributions with, and uses them to sample a random process with\n    ``numsamples`` samples.  Each frequency distribution is sampled\n    ``numoutcomes`` times.  These three frequency distributions are\n    then used to build six probability distributions.  Finally, the\n    probability estimates of these distributions are compared to the\n    actual probability of each sample.\n\n    :type numsamples: int\n    :param numsamples: The number of samples to use in each demo\n        frequency distributions.\n    :type numoutcomes: int\n    :param numoutcomes: The total number of outcomes for each\n        demo frequency distribution.  These outcomes are divided into\n        ``numsamples`` bins.\n    :rtype: None\n    \"\"\"\n\n    fdist1 = _create_rand_fdist(numsamples, numoutcomes)\n    fdist2 = _create_rand_fdist(numsamples, numoutcomes)\n    fdist3 = _create_rand_fdist(numsamples, numoutcomes)\n\n    pdists = [\n        MLEProbDist(fdist1),\n        LidstoneProbDist(fdist1, 0.5, numsamples),\n        HeldoutProbDist(fdist1, fdist2, numsamples),\n        HeldoutProbDist(fdist2, fdist1, numsamples),\n        CrossValidationProbDist([fdist1, fdist2, fdist3], numsamples),\n        SimpleGoodTuringProbDist(fdist1),\n        SimpleGoodTuringProbDist(fdist1, 7),\n        _create_sum_pdist(numsamples),\n    ]\n\n    vals = []\n    for n in range(1,numsamples+1):\n        vals.append(tuple([n, fdist1.freq(n)] +\n                          [pdist.prob(n) for pdist in pdists]))\n\n    print(('%d samples (1-%d); %d outcomes were sampled for each FreqDist' %\n           (numsamples, numsamples, numoutcomes)))\n    print('='*9*(len(pdists)+2))\n    FORMATSTR = '      FreqDist '+ '%8s '*(len(pdists)-1) + '|  Actual'\n    print(FORMATSTR % tuple(repr(pdist)[1:9] for pdist in pdists[:-1]))\n    print('-'*9*(len(pdists)+2))\n    FORMATSTR = '%3d   %8.6f ' + '%8.6f '*(len(pdists)-1) + '| %8.6f'\n    for val in vals:\n        print(FORMATSTR % val)\n\n    zvals = list(zip(*vals))\n    sums = [sum(val) for val in zvals[1:]]\n    print('-'*9*(len(pdists)+2))\n    FORMATSTR = 'Total ' + '%8.6f '*(len(pdists)) + '| %8.6f'\n    print(FORMATSTR % tuple(sums))\n    print('='*9*(len(pdists)+2))\n\n    if len(\"%s\" % fdist1) < 70:\n        print('  fdist1: %s' % fdist1)\n        print('  fdist2: %s' % fdist2)\n        print('  fdist3: %s' % fdist3)\n    print()\n\n    print('Generating:')\n    for pdist in pdists:\n        fdist = FreqDist(pdist.generate() for i in range(5000))\n        print('%20s %s' % (pdist.__class__.__name__[:20], (\"%s\" % fdist)[:55]))\n    print()\n\ndef gt_demo():\n    from nltk import corpus\n    emma_words = corpus.gutenberg.words('austen-emma.txt')\n    fd = FreqDist(emma_words)\n    sgt = SimpleGoodTuringProbDist(fd)\n    print('%18s %8s  %14s' \\\n        % (\"word\", \"freqency\", \"SimpleGoodTuring\"))\n    fd_keys_sorted=(key for key, value in sorted(fd.items(), key=lambda item: item[1], reverse=True))\n    for key in fd_keys_sorted:\n        print('%18s %8d  %14e' \\\n            % (key, fd[key], sgt.prob(key)))\n\nif __name__ == '__main__':\n    demo(6, 10)\n    demo(5, 5000)\n    gt_demo()\n\n__all__ = ['ConditionalFreqDist', 'ConditionalProbDist',\n           'ConditionalProbDistI', 'CrossValidationProbDist',\n           'DictionaryConditionalProbDist', 'DictionaryProbDist', 'ELEProbDist',\n           'FreqDist', 'SimpleGoodTuringProbDist', 'HeldoutProbDist',\n           'ImmutableProbabilisticMixIn', 'LaplaceProbDist', 'LidstoneProbDist',\n           'MLEProbDist', 'MutableProbDist', 'KneserNeyProbDist', 'ProbDistI', 'ProbabilisticMixIn',\n           'UniformProbDist', 'WittenBellProbDist', 'add_logs',\n           'log_likelihood', 'sum_logs', 'entropy']\n"], "nltk\\sem\\boxer": [".py", "\nfrom __future__ import print_function, unicode_literals\n\nimport os\nimport re\nimport operator\nimport subprocess\nfrom optparse import OptionParser\nimport tempfile\nfrom functools import reduce\n\nfrom nltk.internals import find_binary\n\nfrom nltk.sem.logic import (ExpectedMoreTokensException, LogicalExpressionException,\n                            UnexpectedTokenException, Variable)\n\nfrom nltk.sem.drt import (DRS, DrtApplicationExpression, DrtEqualityExpression,\n                          DrtNegatedExpression, DrtOrExpression, DrtParser,\n                          DrtProposition, DrtTokens, DrtVariableExpression)\n\nfrom nltk.compat import python_2_unicode_compatible\n\nclass Boxer(object):\n\n    def __init__(self, boxer_drs_interpreter=None, elimeq=False, bin_dir=None, verbose=False, resolve=True):\n        if boxer_drs_interpreter is None:\n            boxer_drs_interpreter = NltkDrtBoxerDrsInterpreter()\n        self._boxer_drs_interpreter = boxer_drs_interpreter\n\n        self._resolve = resolve\n        self._elimeq = elimeq\n\n        self.set_bin_dir(bin_dir, verbose)\n\n    def set_bin_dir(self, bin_dir, verbose=False):\n        self._candc_bin = self._find_binary('candc', bin_dir, verbose)\n        self._candc_models_path = os.path.normpath(os.path.join(self._candc_bin[:-5], '../models'))\n        self._boxer_bin = self._find_binary('boxer', bin_dir, verbose)\n\n    def interpret(self, input, discourse_id=None, question=False, verbose=False):\n        discourse_ids = ([discourse_id] if discourse_id is not None else None)\n        d, = self.interpret_multi_sents([[input]], discourse_ids, question, verbose)\n        if not d:\n            raise Exception('Unable to interpret: \"{0}\"'.format(input))\n        return d\n\n    def interpret_multi(self, input, discourse_id=None, question=False, verbose=False):\n        discourse_ids = ([discourse_id] if discourse_id is not None else None)\n        d, = self.interpret_multi_sents([input], discourse_ids, question, verbose)\n        if not d:\n            raise Exception('Unable to interpret: \"{0}\"'.format(input))\n        return d\n\n    def interpret_sents(self, inputs, discourse_ids=None, question=False, verbose=False):\n        return self.interpret_multi_sents([[input] for input in inputs], discourse_ids, question, verbose)\n\n    def interpret_multi_sents(self, inputs, discourse_ids=None, question=False, verbose=False):\n        if discourse_ids is not None:\n            assert len(inputs) == len(discourse_ids)\n            assert reduce(operator.and_, (id is not None for id in discourse_ids))\n            use_disc_id = True\n        else:\n            discourse_ids = list(map(str, range(len(inputs))))\n            use_disc_id = False\n\n        candc_out = self._call_candc(inputs, discourse_ids, question, verbose=verbose)\n        boxer_out = self._call_boxer(candc_out, verbose=verbose)\n\n\n        drs_dict = self._parse_to_drs_dict(boxer_out, use_disc_id)\n        return [drs_dict.get(id, None) for id in discourse_ids]\n\n    def _call_candc(self, inputs, discourse_ids, question, verbose=False):\n        args = ['--models', os.path.join(self._candc_models_path, ['boxer','questions'][question]),\n                '--candc-printer', 'boxer']\n        return self._call('\\n'.join(sum(([\"<META>'{0}'\".format(id)] + d for d,id in zip(inputs,discourse_ids)), [])), self._candc_bin, args, verbose)\n\n    def _call_boxer(self, candc_out, verbose=False):\n        f = None\n        try:\n            fd, temp_filename = tempfile.mkstemp(prefix='boxer-', suffix='.in', text=True)\n            f = os.fdopen(fd, 'w')\n            f.write(candc_out)\n        finally:\n            if f: f.close()\n\n        args = ['--box', 'false',\n                '--semantics', 'drs',\n                '--resolve', ['false','true'][self._resolve],\n                '--elimeq', ['false','true'][self._elimeq],\n                '--format', 'prolog',\n                '--instantiate', 'true',\n                '--input', temp_filename]\n        stdout = self._call(None, self._boxer_bin, args, verbose)\n        os.remove(temp_filename)\n        return stdout\n\n    def _find_binary(self, name, bin_dir, verbose=False):\n        return find_binary(name,\n            path_to_bin=bin_dir,\n            env_vars=['CANDC'],\n            url='http://svn.ask.it.usyd.edu.au/trac/candc/',\n            binary_names=[name, name + '.exe'],\n            verbose=verbose)\n\n    def _call(self, input_str, binary, args=[], verbose=False):\n        if verbose:\n            print('Calling:', binary)\n            print('Args:', args)\n            print('Input:', input_str)\n            print('Command:', binary + ' ' + ' '.join(args))\n\n        if input_str is None:\n            cmd = [binary] + args\n            p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        else:\n            cmd = 'echo \"{0}\" | {1} {2}'.format(input_str, binary, ' '.join(args))\n            p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n        stdout, stderr = p.communicate()\n\n        if verbose:\n            print('Return code:', p.returncode)\n            if stdout: print('stdout:\\n', stdout, '\\n')\n            if stderr: print('stderr:\\n', stderr, '\\n')\n        if p.returncode != 0:\n            raise Exception('ERROR CALLING: {0} {1}\\nReturncode: {2}\\n{3}'.format(binary, ' '.join(args), p.returncode, stderr))\n\n        return stdout\n\n    def _parse_to_drs_dict(self, boxer_out, use_disc_id):\n        lines = boxer_out.split('\\n')\n        drs_dict = {}\n        i = 0\n        while i < len(lines):\n            line = lines[i]\n            if line.startswith('id('):\n                comma_idx = line.index(',')\n                discourse_id = line[3:comma_idx]\n                if discourse_id[0] == \"'\" and discourse_id[-1] == \"'\":\n                    discourse_id = discourse_id[1:-1]\n                drs_id = line[comma_idx+1:line.index(')')]\n                i += 1\n                line = lines[i]\n                assert line.startswith('sem({0},'.format(drs_id))\n                if line[-4:] == \"').'\":\n                    line = line[:-4] + \").\"\n                assert line.endswith(').'), \"can't parse line: {0}\".format(line)\n\n                search_start = len('sem({0},['.format(drs_id))\n                brace_count = 1\n                drs_start = -1\n                for j,c in enumerate(line[search_start:]):\n                    if(c == '['):\n                        brace_count += 1\n                    if(c == ']'):\n                        brace_count -= 1\n                        if(brace_count == 0):\n                            drs_start = search_start + j + 1\n                            if line[drs_start:drs_start+3] == \"','\":\n                                drs_start = drs_start + 3\n                            else:\n                                drs_start = drs_start + 1\n                            break\n                assert drs_start > -1\n\n                drs_input = line[drs_start:-2].strip()\n                parsed = self._parse_drs(drs_input, discourse_id, use_disc_id)\n                drs_dict[discourse_id] = self._boxer_drs_interpreter.interpret(parsed)\n            i += 1\n        return drs_dict\n\n    def _parse_drs(self, drs_string, discourse_id, use_disc_id):\n        return BoxerOutputDrsParser([None,discourse_id][use_disc_id]).parse(drs_string)\n\n\nclass BoxerOutputDrsParser(DrtParser):\n    def __init__(self, discourse_id=None):\n        DrtParser.__init__(self)\n        self.discourse_id = discourse_id\n        self.sentence_id_offset = None\n        self.quote_chars = [(\"'\", \"'\", \"\\\\\", False)]\n\n    def parse(self, data, signature=None):\n        return DrtParser.parse(self, data, signature)\n\n    def get_all_symbols(self):\n        return ['(', ')', ',', '[', ']',':']\n\n    def handle(self, tok, context):\n        return self.handle_drs(tok)\n\n    def attempt_adjuncts(self, expression, context):\n        return expression\n\n    def parse_condition(self, indices):\n        tok = self.token()\n        accum = self.handle_condition(tok, indices)\n        if accum is None:\n            raise UnexpectedTokenException(tok)\n        return accum\n\n    def handle_drs(self, tok):\n        if tok == 'drs':\n            return self.parse_drs()\n        elif tok in ['merge', 'smerge']:\n            return self._handle_binary_expression(self._make_merge_expression)(None, [])\n        elif tok in ['alfa']:\n            return self._handle_alfa(self._make_merge_expression)(None, [])\n\n    def handle_condition(self, tok, indices):\n        if tok == 'not':\n            return [self._handle_not()]\n\n        if tok == 'or':\n            conds = [self._handle_binary_expression(self._make_or_expression)]\n        elif tok == 'imp':\n            conds = [self._handle_binary_expression(self._make_imp_expression)]\n        elif tok == 'eq':\n            conds = [self._handle_eq()]\n        elif tok == 'prop':\n            conds = [self._handle_prop()]\n\n        elif tok == 'pred':\n            conds = [self._handle_pred()]\n        elif tok == 'named':\n            conds = [self._handle_named()]\n        elif tok == 'rel':\n            conds = [self._handle_rel()]\n        elif tok == 'timex':\n            conds = self._handle_timex()\n        elif tok == 'card':\n            conds = [self._handle_card()]\n\n        elif tok == 'whq':\n            conds = [self._handle_whq()]\n        elif tok == 'duplex':\n                conds = [self._handle_duplex()]\n\n        else:\n            conds = []\n\n        return sum([[cond(sent_index, word_indices) for cond in conds] for sent_index, word_indices in self._sent_and_word_indices(indices)], [])\n\n    def _handle_not(self):\n        self.assertToken(self.token(), '(')\n        drs = self.process_next_expression(None)\n        self.assertToken(self.token(), ')')\n        return BoxerNot(drs)\n\n    def _handle_pred(self):\n        self.assertToken(self.token(), '(')\n        variable = self.parse_variable()\n        self.assertToken(self.token(), ',')\n        name = self.token()\n        self.assertToken(self.token(), ',')\n        pos = self.token()\n        self.assertToken(self.token(), ',')\n        sense = int(self.token())\n        self.assertToken(self.token(), ')')\n\n        def _handle_pred_f(sent_index, word_indices):\n            return BoxerPred(self.discourse_id, sent_index, word_indices, variable, name, pos, sense)\n        return _handle_pred_f\n\n    def _handle_duplex(self):\n        self.assertToken(self.token(), '(')\n        ans_types = []\n      \n        self.assertToken(self.token(), 'whq')\n        self.assertToken(self.token(), ',')\n        d1 = self.process_next_expression(None)\n        self.assertToken(self.token(), ',')\n        ref = self.parse_variable()\n        self.assertToken(self.token(), ',')\n        d2 = self.process_next_expression(None)\n        self.assertToken(self.token(), ')')\n        return lambda sent_index, word_indices: BoxerWhq(self.discourse_id, sent_index, word_indices, ans_types, d1, ref, d2)\n\n\n    def _handle_named(self):\n        self.assertToken(self.token(), '(')\n        variable = self.parse_variable()\n        self.assertToken(self.token(), ',')\n        name = self.token()\n        self.assertToken(self.token(), ',')\n        type = self.token()\n        self.assertToken(self.token(), ',')\n        sense = self.token() # as per boxer rev 2554\n        self.assertToken(self.token(), ')')\n        return lambda sent_index, word_indices: BoxerNamed(self.discourse_id, sent_index, word_indices, variable, name, type, sense)\n\n    def _handle_rel(self):\n        self.assertToken(self.token(), '(')\n        var1 = self.parse_variable()\n        self.assertToken(self.token(), ',')\n        var2 = self.parse_variable()\n        self.assertToken(self.token(), ',')\n        rel = self.token()\n        self.assertToken(self.token(), ',')\n        sense = int(self.token())\n        self.assertToken(self.token(), ')')\n        return lambda sent_index, word_indices: BoxerRel(self.discourse_id, sent_index, word_indices, var1, var2, rel, sense)\n\n    def _handle_timex(self):\n        self.assertToken(self.token(), '(')\n        arg = self.parse_variable()\n        self.assertToken(self.token(), ',')\n        new_conds = self._handle_time_expression(arg)\n        self.assertToken(self.token(), ')')\n        return new_conds\n\n    def _handle_time_expression(self, arg):\n        tok = self.token()\n        self.assertToken(self.token(), '(')\n        if tok == 'date':\n            conds = self._handle_date(arg)\n        elif tok == 'time':\n            conds = self._handle_time(arg)\n        else:\n            return None\n        self.assertToken(self.token(), ')')\n        return [lambda sent_index, word_indices: BoxerPred(self.discourse_id, sent_index, word_indices, arg, tok, 'n', 0)] + \\\n               [lambda sent_index, word_indices: cond for cond in conds]\n\n    def _handle_date(self, arg):\n        conds = []\n        (sent_index, word_indices), = self._sent_and_word_indices(self._parse_index_list())\n        self.assertToken(self.token(), '(')\n        pol = self.token()\n        self.assertToken(self.token(), ')')\n        conds.append(BoxerPred(self.discourse_id, sent_index, word_indices, arg, 'date_pol_{0}'.format(pol), 'a', 0))\n        self.assertToken(self.token(), ',')\n\n        (sent_index, word_indices), = self._sent_and_word_indices(self._parse_index_list())\n        year = self.token()\n        if year != 'XXXX':\n            year = year.replace(':', '_')\n            conds.append(BoxerPred(self.discourse_id, sent_index, word_indices, arg, 'date_year_{0}'.format(year), 'a', 0))\n        self.assertToken(self.token(), ',')\n\n        (sent_index, word_indices), = self._sent_and_word_indices(self._parse_index_list())\n        month = self.token()\n        if month != 'XX':\n            conds.append(BoxerPred(self.discourse_id, sent_index, word_indices, arg, 'date_month_{0}'.format(month), 'a', 0))\n        self.assertToken(self.token(), ',')\n\n        (sent_index, word_indices), = self._sent_and_word_indices(self._parse_index_list())\n        day = self.token()\n        if day != 'XX':\n            conds.append(BoxerPred(self.discourse_id, sent_index, word_indices, arg, 'date_day_{0}'.format(day), 'a', 0))\n\n        return conds\n\n    def _handle_time(self, arg):\n        conds = []\n        self._parse_index_list()\n        hour = self.token()\n        if hour != 'XX':\n            conds.append(self._make_atom('r_hour_2',arg,hour))\n        self.assertToken(self.token(), ',')\n\n        self._parse_index_list()\n        min = self.token()\n        if min != 'XX':\n            conds.append(self._make_atom('r_min_2',arg,min))\n        self.assertToken(self.token(), ',')\n\n        self._parse_index_list()\n        sec = self.token()\n        if sec != 'XX':\n            conds.append(self._make_atom('r_sec_2',arg,sec))\n\n        return conds\n\n    def _handle_card(self):\n        self.assertToken(self.token(), '(')\n        variable = self.parse_variable()\n        self.assertToken(self.token(), ',')\n        value = self.token()\n        self.assertToken(self.token(), ',')\n        type = self.token()\n        self.assertToken(self.token(), ')')\n        return lambda sent_index, word_indices: BoxerCard(self.discourse_id, sent_index, word_indices, variable, value, type)\n\n    def _handle_prop(self):\n        self.assertToken(self.token(), '(')\n        variable = self.parse_variable()\n        self.assertToken(self.token(), ',')\n        drs = self.process_next_expression(None)\n        self.assertToken(self.token(), ')')\n        return lambda sent_index, word_indices: BoxerProp(self.discourse_id, sent_index, word_indices, variable, drs)\n\n    def _parse_index_list(self):\n        indices = []\n        self.assertToken(self.token(), '[')\n        while self.token(0) != ']':\n            indices.append(self.parse_index())\n            if self.token(0) == ',':\n                self.token() #swallow ','\n        self.token() #swallow ']'\n        self.assertToken(self.token(), ':')\n        return indices\n\n    def parse_drs(self):\n        self.assertToken(self.token(), '(')\n        self.assertToken(self.token(), '[')\n        refs = set()\n        while self.token(0) != ']':\n            indices = self._parse_index_list()\n            refs.add(self.parse_variable())\n            if self.token(0) == ',':\n                self.token() #swallow ','\n        self.token() #swallow ']'\n        self.assertToken(self.token(), ',')\n        self.assertToken(self.token(), '[')\n        conds = []\n        while self.token(0) != ']':\n            indices = self._parse_index_list()\n            conds.extend(self.parse_condition(indices))\n            if self.token(0) == ',':\n                self.token() #swallow ','\n        self.token() #swallow ']'\n        self.assertToken(self.token(), ')')\n        return BoxerDrs(list(refs), conds)\n\n    def _handle_binary_expression(self, make_callback):\n        self.assertToken(self.token(), '(')\n        drs1 = self.process_next_expression(None)\n        self.assertToken(self.token(), ',')\n        drs2 = self.process_next_expression(None)\n        self.assertToken(self.token(), ')')\n        return lambda sent_index, word_indices: make_callback(sent_index, word_indices, drs1, drs2)\n\n    def _handle_alfa(self, make_callback):\n        self.assertToken(self.token(), '(')\n        type = self.token()\n        self.assertToken(self.token(), ',')\n        drs1 = self.process_next_expression(None)\n        self.assertToken(self.token(), ',')\n        drs2 = self.process_next_expression(None)\n        self.assertToken(self.token(), ')')\n        return lambda sent_index, word_indices: make_callback(sent_index, word_indices, drs1, drs2)\n\n    def _handle_eq(self):\n        self.assertToken(self.token(), '(')\n        var1 = self.parse_variable()\n        self.assertToken(self.token(), ',')\n        var2 = self.parse_variable()\n        self.assertToken(self.token(), ')')\n        return lambda sent_index, word_indices: BoxerEq(self.discourse_id, sent_index, word_indices, var1, var2)\n\n\n    def _handle_whq(self):\n        self.assertToken(self.token(), '(')\n        self.assertToken(self.token(), '[')\n        ans_types = []\n        while self.token(0) != ']':\n            cat = self.token()\n            self.assertToken(self.token(), ':')\n            if cat == 'des':\n                ans_types.append(self.token())\n            elif cat == 'num':\n                ans_types.append('number')\n                typ = self.token()\n                if typ == 'cou':\n                    ans_types.append('count')\n                else:\n                    ans_types.append(typ)\n            else:\n                ans_types.append(self.token())\n        self.token() #swallow the ']'\n\n        self.assertToken(self.token(), ',')\n        d1 = self.process_next_expression(None)\n        self.assertToken(self.token(), ',')\n        ref = self.parse_variable()\n        self.assertToken(self.token(), ',')\n        d2 = self.process_next_expression(None)\n        self.assertToken(self.token(), ')')\n        return lambda sent_index, word_indices: BoxerWhq(self.discourse_id, sent_index, word_indices, ans_types, d1, ref, d2)\n\n    def _make_merge_expression(self, sent_index, word_indices, drs1, drs2):\n        return BoxerDrs(drs1.refs + drs2.refs, drs1.conds + drs2.conds)\n\n    def _make_or_expression(self, sent_index, word_indices, drs1, drs2):\n        return BoxerOr(self.discourse_id, sent_index, word_indices, drs1, drs2)\n\n    def _make_imp_expression(self, sent_index, word_indices, drs1, drs2):\n        return BoxerDrs(drs1.refs, drs1.conds, drs2)\n\n    def parse_variable(self):\n        var = self.token()\n        assert re.match('^[exps]\\d+$', var), var\n        return var\n\n    def parse_index(self):\n        return int(self.token())\n\n    def _sent_and_word_indices(self, indices):\n        sent_indices = set((i / 1000)-1 for i in indices if i>=0)\n        if sent_indices:\n            pairs = []\n            for sent_index in sent_indices:\n                word_indices = [(i % 1000)-1 for i in indices if sent_index == (i / 1000)-1]\n                pairs.append((sent_index, word_indices))\n            return pairs\n        else:\n            word_indices = [(i % 1000)-1 for i in indices]\n            return [(None, word_indices)]\n\n\nclass BoxerDrsParser(DrtParser):\n    def __init__(self, discourse_id=None):\n        DrtParser.__init__(self)\n        self.discourse_id = discourse_id\n\n    def get_all_symbols(self):\n        return [DrtTokens.OPEN, DrtTokens.CLOSE, DrtTokens.COMMA, DrtTokens.OPEN_BRACKET, DrtTokens.CLOSE_BRACKET]\n\n    def attempt_adjuncts(self, expression, context):\n        return expression\n\n    def handle(self, tok, context):\n        try:\n            if tok == 'pred':\n                self.assertNextToken(DrtTokens.OPEN)\n                disc_id = (self.token(), self.discourse_id)[self.discourse_id is not None]\n                self.assertNextToken(DrtTokens.COMMA)\n                sent_id = self.nullableIntToken()\n                self.assertNextToken(DrtTokens.COMMA)\n                word_ids = list(map(int, self.handle_refs()))\n                self.assertNextToken(DrtTokens.COMMA)\n                variable = int(self.token())\n                self.assertNextToken(DrtTokens.COMMA)\n                name = self.token()\n                self.assertNextToken(DrtTokens.COMMA)\n                pos = self.token()\n                self.assertNextToken(DrtTokens.COMMA)\n                sense = int(self.token())\n                self.assertNextToken(DrtTokens.CLOSE)\n                return BoxerPred(disc_id, sent_id, word_ids, variable, name, pos, sense)\n            elif tok == 'named':\n                self.assertNextToken(DrtTokens.OPEN)\n                disc_id = (self.token(), self.discourse_id)[self.discourse_id is not None]\n                self.assertNextToken(DrtTokens.COMMA)\n                sent_id = int(self.token())\n                self.assertNextToken(DrtTokens.COMMA)\n                word_ids = map(int, self.handle_refs())\n                self.assertNextToken(DrtTokens.COMMA)\n                variable = int(self.token())\n                self.assertNextToken(DrtTokens.COMMA)\n                name = self.token()\n                self.assertNextToken(DrtTokens.COMMA)\n                type = self.token()\n                self.assertNextToken(DrtTokens.COMMA)\n                sense = int(self.token())\n                self.assertNextToken(DrtTokens.CLOSE)\n                return BoxerNamed(disc_id, sent_id, word_ids, variable, name, type, sense)\n            elif tok == 'rel':\n                self.assertNextToken(DrtTokens.OPEN)\n                disc_id = (self.token(), self.discourse_id)[self.discourse_id is not None]\n                self.assertNextToken(DrtTokens.COMMA)\n                sent_id = self.nullableIntToken()\n                self.assertNextToken(DrtTokens.COMMA)\n                word_ids = list(map(int, self.handle_refs()))\n                self.assertNextToken(DrtTokens.COMMA)\n                var1 = int(self.token())\n                self.assertNextToken(DrtTokens.COMMA)\n                var2 = int(self.token())\n                self.assertNextToken(DrtTokens.COMMA)\n                rel = self.token()\n                self.assertNextToken(DrtTokens.COMMA)\n                sense = int(self.token())\n                self.assertNextToken(DrtTokens.CLOSE)\n                return BoxerRel(disc_id, sent_id, word_ids, var1, var2, rel, sense)\n            elif tok == 'prop':\n                self.assertNextToken(DrtTokens.OPEN)\n                disc_id = (self.token(), self.discourse_id)[self.discourse_id is not None]\n                self.assertNextToken(DrtTokens.COMMA)\n                sent_id = int(self.token())\n                self.assertNextToken(DrtTokens.COMMA)\n                word_ids = list(map(int, self.handle_refs()))\n                self.assertNextToken(DrtTokens.COMMA)\n                variable = int(self.token())\n                self.assertNextToken(DrtTokens.COMMA)\n                drs = self.process_next_expression(None)\n                self.assertNextToken(DrtTokens.CLOSE)\n                return BoxerProp(disc_id, sent_id, word_ids, variable, drs)\n            elif tok == 'not':\n                self.assertNextToken(DrtTokens.OPEN)\n                drs = self.process_next_expression(None)\n                self.assertNextToken(DrtTokens.CLOSE)\n                return BoxerNot(drs)\n            elif tok == 'imp':\n                self.assertNextToken(DrtTokens.OPEN)\n                drs1 = self.process_next_expression(None)\n                self.assertNextToken(DrtTokens.COMMA)\n                drs2 = self.process_next_expression(None)\n                self.assertNextToken(DrtTokens.CLOSE)\n                return BoxerDrs(drs1.refs, drs1.conds, drs2)\n            elif tok == 'or':\n                self.assertNextToken(DrtTokens.OPEN)\n                disc_id = (self.token(), self.discourse_id)[self.discourse_id is not None]\n                self.assertNextToken(DrtTokens.COMMA)\n                sent_id = self.nullableIntToken()\n                self.assertNextToken(DrtTokens.COMMA)\n                word_ids = map(int, self.handle_refs())\n                self.assertNextToken(DrtTokens.COMMA)\n                drs1 = self.process_next_expression(None)\n                self.assertNextToken(DrtTokens.COMMA)\n                drs2 = self.process_next_expression(None)\n                self.assertNextToken(DrtTokens.CLOSE)\n                return BoxerOr(disc_id, sent_id, word_ids, drs1, drs2)\n            elif tok == 'eq':\n                self.assertNextToken(DrtTokens.OPEN)\n                disc_id = (self.token(), self.discourse_id)[self.discourse_id is not None]\n                self.assertNextToken(DrtTokens.COMMA)\n                sent_id = self.nullableIntToken()\n                self.assertNextToken(DrtTokens.COMMA)\n                word_ids = list(map(int, self.handle_refs()))\n                self.assertNextToken(DrtTokens.COMMA)\n                var1 = int(self.token())\n                self.assertNextToken(DrtTokens.COMMA)\n                var2 = int(self.token())\n                self.assertNextToken(DrtTokens.CLOSE)\n                return BoxerEq(disc_id, sent_id, word_ids, var1, var2)\n            elif tok == 'card':\n                self.assertNextToken(DrtTokens.OPEN)\n                disc_id = (self.token(), self.discourse_id)[self.discourse_id is not None]\n                self.assertNextToken(DrtTokens.COMMA)\n                sent_id = self.nullableIntToken()\n                self.assertNextToken(DrtTokens.COMMA)\n                word_ids = map(int, self.handle_refs())\n                self.assertNextToken(DrtTokens.COMMA)\n                var = int(self.token())\n                self.assertNextToken(DrtTokens.COMMA)\n                value = self.token()\n                self.assertNextToken(DrtTokens.COMMA)\n                type = self.token()\n                self.assertNextToken(DrtTokens.CLOSE)\n                return BoxerCard(disc_id, sent_id, word_ids, var, value, type)\n            elif tok == 'whq':\n                self.assertNextToken(DrtTokens.OPEN)\n                disc_id = (self.token(), self.discourse_id)[self.discourse_id is not None]\n                self.assertNextToken(DrtTokens.COMMA)\n                sent_id = self.nullableIntToken()\n                self.assertNextToken(DrtTokens.COMMA)\n                word_ids = list(map(int, self.handle_refs()))\n                self.assertNextToken(DrtTokens.COMMA)\n                ans_types = self.handle_refs()\n                self.assertNextToken(DrtTokens.COMMA)\n                drs1 = self.process_next_expression(None)\n                self.assertNextToken(DrtTokens.COMMA)\n                var = int(self.token())\n                self.assertNextToken(DrtTokens.COMMA)\n                drs2 = self.process_next_expression(None)\n                self.assertNextToken(DrtTokens.CLOSE)\n                return BoxerWhq(disc_id, sent_id, word_ids, ans_types, drs1, var, drs2)\n        except Exception as e:\n            raise LogicalExpressionException(self._currentIndex, str(e))\n        assert False, repr(tok)\n\n    def nullableIntToken(self):\n        t = self.token()\n        return [None,int(t)][t != 'None']\n\n    def get_next_token_variable(self, description):\n        try:\n            return self.token()\n        except ExpectedMoreTokensException as e:\n            raise ExpectedMoreTokensException(e.index, 'Variable expected.')\n\n\n\nclass AbstractBoxerDrs(object):\n    def variables(self):\n        variables, events, propositions = self._variables()\n        return (variables - (events | propositions), events, propositions - events)\n\n    def variable_types(self):\n        vartypes = {}\n        for t,vars in zip(('z','e','p'), self.variables()):\n            for v in vars:\n                vartypes[v] = t\n        return vartypes\n\n    def _variables(self):\n        return (set(), set(), set())\n\n    def atoms(self):\n        return set()\n\n    def clean(self):\n        return self\n\n    def _clean_name(self, name):\n        return name.replace('-','_').replace(\"'\", \"_\")\n\n    def renumber_sentences(self, f):\n        return self\n\n    def __hash__(self):\n        return hash(\"{0}\".format(self))\n\n\n@python_2_unicode_compatible\nclass BoxerDrs(AbstractBoxerDrs):\n    def __init__(self, refs, conds, consequent=None):\n        AbstractBoxerDrs.__init__(self)\n        self.refs = refs\n        self.conds = conds\n        self.consequent = consequent\n\n    def _variables(self):\n        variables = (set(), set(), set())\n        for cond in self.conds:\n            for s,v in zip(variables, cond._variables()):\n                s.update(v)\n        if self.consequent is not None:\n            for s,v in zip(variables, self.consequent._variables()):\n                s.update(v)\n        return variables\n\n    def atoms(self):\n        atoms = reduce(operator.or_, (cond.atoms() for cond in self.conds), set())\n        if self.consequent is not None:\n            atoms.update(self.consequent.atoms())\n        return atoms\n\n    def clean(self):\n        consequent = (self.consequent.clean() if self.consequent else None)\n        return BoxerDrs(self.refs, [c.clean() for c in self.conds], consequent)\n\n    def renumber_sentences(self, f):\n        consequent = (self.consequent.renumber_sentences(f) if self.consequent else None)\n        return BoxerDrs(self.refs, [c.renumber_sentences(f) for c in self.conds], consequent)\n\n    def __repr__(self):\n        s = 'drs([%s], [%s])' % (', '.join(\"%s\" % r for r in self.refs),\n                                 ', '.join(\"%s\" % c for c in self.conds))\n        if self.consequent is not None:\n            s = 'imp(%s, %s)' % (s, self.consequent)\n        return s\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__ and \\\n               self.refs == other.refs and \\\n               len(self.conds) == len(other.conds) and \\\n               reduce(operator.and_, (c1==c2 for c1,c2 in zip(self.conds, other.conds))) and \\\n               self.consequent == other.consequent\n\n    def __ne__(self, other):\n        return not self == other\n\n    __hash__ = AbstractBoxerDrs.__hash__\n\n\n@python_2_unicode_compatible\nclass BoxerNot(AbstractBoxerDrs):\n    def __init__(self, drs):\n        AbstractBoxerDrs.__init__(self)\n        self.drs = drs\n\n    def _variables(self):\n        return self.drs._variables()\n\n    def atoms(self):\n        return self.drs.atoms()\n\n    def clean(self):\n        return BoxerNot(self.drs.clean())\n\n    def renumber_sentences(self, f):\n        return BoxerNot(self.drs.renumber_sentences(f))\n\n    def __repr__(self):\n        return 'not(%s)' % (self.drs)\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__ and self.drs == other.drs\n\n    def __ne__(self, other):\n        return not self == other\n\n    __hash__ = AbstractBoxerDrs.__hash__\n\n@python_2_unicode_compatible\nclass BoxerIndexed(AbstractBoxerDrs):\n    def __init__(self, discourse_id, sent_index, word_indices):\n        AbstractBoxerDrs.__init__(self)\n        self.discourse_id = discourse_id\n        self.sent_index = sent_index\n        self.word_indices = word_indices\n\n    def atoms(self):\n        return set([self])\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__ and \\\n               self.discourse_id == other.discourse_id and \\\n               self.sent_index == other.sent_index and \\\n               self.word_indices == other.word_indices and \\\n               reduce(operator.and_, (s==o for s,o in zip(self, other)))\n\n    def __ne__(self, other):\n        return not self == other\n\n    __hash__ = AbstractBoxerDrs.__hash__\n\n    def __repr__(self):\n        s = '%s(%s, %s, [%s]' % (self._pred(), self.discourse_id,\n                                 self.sent_index, ', '.join(\"%s\" % wi for wi in self.word_indices))\n        for v in self:\n            s += ', %s' % v\n        return s + ')'\n\nclass BoxerPred(BoxerIndexed):\n    def __init__(self, discourse_id, sent_index, word_indices, var, name, pos, sense):\n        BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)\n        self.var = var\n        self.name = name\n        self.pos = pos\n        self.sense = sense\n\n    def _variables(self):\n        return (set([self.var]), set(), set())\n\n    def change_var(self, var):\n        return BoxerPred(self.discourse_id, self.sent_index, self.word_indices, var, self.name, self.pos, self.sense)\n\n    def clean(self):\n        return BoxerPred(self.discourse_id, self.sent_index, self.word_indices, self.var, self._clean_name(self.name), self.pos, self.sense)\n\n    def renumber_sentences(self, f):\n        new_sent_index = f(self.sent_index)\n        return BoxerPred(self.discourse_id, new_sent_index, self.word_indices, self.var, self.name, self.pos, self.sense)\n\n    def __iter__(self):\n        return iter((self.var, self.name, self.pos, self.sense))\n\n    def _pred(self):\n        return 'pred'\n\nclass BoxerNamed(BoxerIndexed):\n    def __init__(self, discourse_id, sent_index, word_indices, var, name, type, sense):\n        BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)\n        self.var = var\n        self.name = name\n        self.type = type\n        self.sense = sense\n\n    def _variables(self):\n        return (set([self.var]), set(), set())\n\n    def change_var(self, var):\n        return BoxerNamed(self.discourse_id, self.sent_index, self.word_indices, var, self.name, self.type, self.sense)\n\n    def clean(self):\n        return BoxerNamed(self.discourse_id, self.sent_index, self.word_indices, self.var, self._clean_name(self.name), self.type, self.sense)\n\n    def renumber_sentences(self, f):\n        return BoxerNamed(self.discourse_id, f(self.sent_index), self.word_indices, self.var, self.name, self.type, self.sense)\n\n    def __iter__(self):\n        return iter((self.var, self.name, self.type, self.sense))\n\n    def _pred(self):\n        return 'named'\n\nclass BoxerRel(BoxerIndexed):\n    def __init__(self, discourse_id, sent_index, word_indices, var1, var2, rel, sense):\n        BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)\n        self.var1 = var1\n        self.var2 = var2\n        self.rel = rel\n        self.sense = sense\n\n    def _variables(self):\n        return (set([self.var1, self.var2]), set(), set())\n\n    def clean(self):\n        return BoxerRel(self.discourse_id, self.sent_index, self.word_indices, self.var1, self.var2, self._clean_name(self.rel), self.sense)\n\n    def renumber_sentences(self, f):\n        return BoxerRel(self.discourse_id, f(self.sent_index), self.word_indices, self.var1, self.var2, self.rel, self.sense)\n\n    def __iter__(self):\n        return iter((self.var1, self.var2, self.rel, self.sense))\n\n    def _pred(self):\n        return 'rel'\n\nclass BoxerProp(BoxerIndexed):\n    def __init__(self, discourse_id, sent_index, word_indices, var, drs):\n        BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)\n        self.var = var\n        self.drs = drs\n\n    def _variables(self):\n        return tuple(map(operator.or_, (set(), set(), set([self.var])), self.drs._variables()))\n\n    def referenced_labels(self):\n        return set([self.drs])\n\n    def atoms(self):\n        return self.drs.atoms()\n\n    def clean(self):\n        return BoxerProp(self.discourse_id, self.sent_index, self.word_indices, self.var, self.drs.clean())\n\n    def renumber_sentences(self, f):\n        return BoxerProp(self.discourse_id, f(self.sent_index), self.word_indices, self.var, self.drs.renumber_sentences(f))\n\n    def __iter__(self):\n        return iter((self.var, self.drs))\n\n    def _pred(self):\n        return 'prop'\n\nclass BoxerEq(BoxerIndexed):\n    def __init__(self, discourse_id, sent_index, word_indices, var1, var2):\n        BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)\n        self.var1 = var1\n        self.var2 = var2\n\n    def _variables(self):\n        return (set([self.var1, self.var2]), set(), set())\n\n    def atoms(self):\n        return set()\n\n    def renumber_sentences(self, f):\n        return BoxerEq(self.discourse_id, f(self.sent_index), self.word_indices, self.var1, self.var2)\n\n    def __iter__(self):\n        return iter((self.var1, self.var2))\n\n    def _pred(self):\n        return 'eq'\n\nclass BoxerCard(BoxerIndexed):\n    def __init__(self, discourse_id, sent_index, word_indices, var, value, type):\n        BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)\n        self.var = var\n        self.value = value\n        self.type = type\n\n    def _variables(self):\n        return (set([self.var]), set(), set())\n\n    def renumber_sentences(self, f):\n        return BoxerCard(self.discourse_id, f(self.sent_index), self.word_indices, self.var, self.value, self.type)\n\n    def __iter__(self):\n        return iter((self.var, self.value, self.type))\n\n    def _pred(self):\n        return 'card'\n\nclass BoxerOr(BoxerIndexed):\n    def __init__(self, discourse_id, sent_index, word_indices, drs1, drs2):\n        BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)\n        self.drs1 = drs1\n        self.drs2 = drs2\n\n    def _variables(self):\n        return tuple(map(operator.or_, self.drs1._variables(), self.drs2._variables()))\n\n    def atoms(self):\n        return self.drs1.atoms() | self.drs2.atoms()\n\n    def clean(self):\n        return BoxerOr(self.discourse_id, self.sent_index, self.word_indices, self.drs1.clean(), self.drs2.clean())\n\n    def renumber_sentences(self, f):\n        return BoxerOr(self.discourse_id, f(self.sent_index), self.word_indices, self.drs1, self.drs2)\n\n    def __iter__(self):\n        return iter((self.drs1, self.drs2))\n\n    def _pred(self):\n        return 'or'\n\nclass BoxerWhq(BoxerIndexed):\n    def __init__(self, discourse_id, sent_index, word_indices, ans_types, drs1, variable, drs2):\n        BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)\n        self.ans_types = ans_types\n        self.drs1 = drs1\n        self.variable = variable\n        self.drs2 = drs2\n\n    def _variables(self):\n        return tuple(map(operator.or_, (set([self.variable]), set(), set()), self.drs1._variables(), self.drs2._variables()))\n\n    def atoms(self):\n        return self.drs1.atoms() | self.drs2.atoms()\n\n    def clean(self):\n        return BoxerWhq(self.discourse_id, self.sent_index, self.word_indices, self.ans_types, self.drs1.clean(), self.variable, self.drs2.clean())\n\n    def renumber_sentences(self, f):\n        return BoxerWhq(self.discourse_id, f(self.sent_index), self.word_indices, self.ans_types, self.drs1, self.variable, self.drs2)\n\n    def __iter__(self):\n        return iter(('['+','.join(self.ans_types)+']', self.drs1, self.variable, self.drs2))\n\n    def _pred(self):\n        return 'whq'\n\n\n\nclass PassthroughBoxerDrsInterpreter(object):\n    def interpret(self, ex):\n        return ex\n\n\nclass NltkDrtBoxerDrsInterpreter(object):\n    def __init__(self, occur_index=False):\n        self._occur_index = occur_index\n\n    def interpret(self, ex):\n        if isinstance(ex, BoxerDrs):\n            drs = DRS([Variable(r) for r in ex.refs], list(map(self.interpret, ex.conds)))\n            if ex.consequent is not None:\n                drs.consequent = self.interpret(ex.consequent)\n            return drs\n        elif isinstance(ex, BoxerNot):\n            return DrtNegatedExpression(self.interpret(ex.drs))\n        elif isinstance(ex, BoxerPred):\n            pred = self._add_occur_indexing('%s_%s' % (ex.pos, ex.name), ex)\n            return self._make_atom(pred, ex.var)\n        elif isinstance(ex, BoxerNamed):\n            pred = self._add_occur_indexing('ne_%s_%s' % (ex.type, ex.name), ex)\n            return self._make_atom(pred, ex.var)\n        elif isinstance(ex, BoxerRel):\n            pred = self._add_occur_indexing('%s' % (ex.rel), ex)\n            return self._make_atom(pred, ex.var1, ex.var2)\n        elif isinstance(ex, BoxerProp):\n            return DrtProposition(Variable(ex.var), self.interpret(ex.drs))\n        elif isinstance(ex, BoxerEq):\n            return DrtEqualityExpression(DrtVariableExpression(Variable(ex.var1)),\n                                         DrtVariableExpression(Variable(ex.var2)))\n        elif isinstance(ex, BoxerCard):\n            pred = self._add_occur_indexing('card_%s_%s' % (ex.type, ex.value), ex)\n            return self._make_atom(pred, ex.var)\n        elif isinstance(ex, BoxerOr):\n            return DrtOrExpression(self.interpret(ex.drs1), self.interpret(ex.drs2))\n        elif isinstance(ex, BoxerWhq):\n            drs1 = self.interpret(ex.drs1)\n            drs2 = self.interpret(ex.drs2)\n            return DRS(drs1.refs + drs2.refs, drs1.conds + drs2.conds)\n        assert False, '%s: %s' % (ex.__class__.__name__, ex)\n\n    def _make_atom(self, pred, *args):\n        accum = DrtVariableExpression(Variable(pred))\n        for arg in args:\n            accum = DrtApplicationExpression(accum, DrtVariableExpression(Variable(arg)))\n        return accum\n\n    def _add_occur_indexing(self, base, ex):\n        if self._occur_index and ex.sent_index is not None:\n            if ex.discourse_id:\n                base += '_%s'  % ex.discourse_id\n            base += '_s%s' % ex.sent_index\n            base += '_w%s' % sorted(ex.word_indices)[0]\n        return base\n\n\nclass UnparseableInputException(Exception):\n    pass\n\n\nif __name__ == '__main__':\n    opts = OptionParser(\"usage: %prog TEXT [options]\")\n    opts.add_option(\"--verbose\", \"-v\", help=\"display verbose logs\", action=\"store_true\", default=False, dest=\"verbose\")\n    opts.add_option(\"--fol\", \"-f\", help=\"output FOL\", action=\"store_true\", default=False, dest=\"fol\")\n    opts.add_option(\"--question\", \"-q\", help=\"input is a question\", action=\"store_true\", default=False, dest=\"question\")\n    opts.add_option(\"--occur\", \"-o\", help=\"occurrence index\", action=\"store_true\", default=False, dest=\"occur_index\")\n    (options, args) = opts.parse_args()\n\n    if len(args) != 1:\n        opts.error(\"incorrect number of arguments\")\n\n    interpreter = NltkDrtBoxerDrsInterpreter(occur_index=options.occur_index)\n    drs = Boxer(interpreter).interpret_multi(args[0].split(r'\\n'), question=options.question, verbose=options.verbose)\n    if drs is None:\n        print(None)\n    else:\n        drs = drs.simplify().eliminate_equality()\n        if options.fol:\n            print(drs.fol().normalize())\n        else:\n            drs.pretty_print()\n"], "nltk\\sem\\chat80": [".py", "\nfrom __future__ import print_function, unicode_literals\n\nimport re\nimport shelve\nimport os\nimport sys\n\nfrom six import string_types\n\nimport nltk.data\nfrom nltk.compat import python_2_unicode_compatible\n\n\nborders = {'rel_name': 'borders',\n           'closures': ['symmetric'],\n           'schema': ['region', 'border'],\n           'filename': 'borders.pl'}\n\ncontains = {'rel_name': 'contains0',\n            'closures': ['transitive'],\n            'schema': ['region', 'contain'],\n            'filename': 'contain.pl'}\n\ncity = {'rel_name': 'city',\n        'closures': [],\n        'schema': ['city', 'country', 'population'],\n        'filename': 'cities.pl'}\n\ncountry = {'rel_name': 'country',\n           'closures': [],\n           'schema': ['country', 'region', 'latitude', 'longitude',\n                      'area', 'population', 'capital', 'currency'],\n           'filename': 'countries.pl'}\n\ncircle_of_lat = {'rel_name': 'circle_of_latitude',\n                 'closures': [],\n                 'schema': ['circle_of_latitude', 'degrees'],\n                 'filename': 'world1.pl'}\n\ncircle_of_long = {'rel_name': 'circle_of_longitude',\n                 'closures': [],\n                 'schema': ['circle_of_longitude', 'degrees'],\n                 'filename': 'world1.pl'}\n\ncontinent = {'rel_name': 'continent',\n             'closures': [],\n             'schema': ['continent'],\n             'filename': 'world1.pl'}\n\nregion = {'rel_name': 'in_continent',\n          'closures': [],\n          'schema': ['region', 'continent'],\n          'filename': 'world1.pl'}\n\nocean = {'rel_name': 'ocean',\n         'closures': [],\n         'schema': ['ocean'],\n         'filename': 'world1.pl'}\n\nsea = {'rel_name': 'sea',\n       'closures': [],\n       'schema': ['sea'],\n       'filename': 'world1.pl'}\n\n\n\nitems = ['borders', 'contains', 'city', 'country', 'circle_of_lat',\n         'circle_of_long', 'continent', 'region', 'ocean', 'sea']\nitems = tuple(sorted(items))\n\nitem_metadata = {\n    'borders': borders,\n    'contains': contains,\n    'city': city,\n    'country': country,\n    'circle_of_lat': circle_of_lat,\n    'circle_of_long': circle_of_long,\n    'continent': continent,\n    'region': region,\n    'ocean': ocean,\n    'sea': sea\n    }\n\nrels = item_metadata.values()\n\nnot_unary = ['borders.pl', 'contain.pl']\n\n\n@python_2_unicode_compatible\nclass Concept(object):\n    def __init__(self, prefLabel, arity, altLabels=[], closures=[], extension=set()):\n        self.prefLabel = prefLabel\n        self.arity = arity\n        self.altLabels = altLabels\n        self.closures = closures\n        self._extension = extension\n        self.extension = sorted(list(extension))\n\n    def __str__(self):\n\n        return \"Label = '%s'\\nArity = %s\\nExtension = %s\" % \\\n               (self.prefLabel, self.arity, self.extension)\n\n    def __repr__(self):\n        return \"Concept('%s')\" % self.prefLabel\n\n    def augment(self, data):\n        self._extension.add(data)\n        self.extension = sorted(list(self._extension))\n        return self._extension\n\n\n    def _make_graph(self, s):\n        g = {}\n        for (x, y) in s:\n            if x in g:\n                g[x].append(y)\n            else:\n                g[x] = [y]\n        return g\n\n    def _transclose(self, g):\n        for x in g:\n            for adjacent in g[x]:\n                if adjacent in g:\n                    for y in g[adjacent]:\n                        if y not in g[x]:\n                            g[x].append(y)\n        return g\n\n    def _make_pairs(self, g):\n        pairs = []\n        for node in g:\n            for adjacent in g[node]:\n                pairs.append((node, adjacent))\n        return set(pairs)\n\n\n    def close(self):\n        from nltk.sem import is_rel\n        assert is_rel(self._extension)\n        if 'symmetric' in self.closures:\n            pairs = []\n            for (x, y) in self._extension:\n                pairs.append((y, x))\n            sym = set(pairs)\n            self._extension = self._extension.union(sym)\n        if 'transitive' in self.closures:\n            all =  self._make_graph(self._extension)\n            closed =  self._transclose(all)\n            trans = self._make_pairs(closed)\n            self._extension = self._extension.union(trans)\n        self.extension = sorted(list(self._extension))\n\n\ndef clause2concepts(filename, rel_name, schema, closures=[]):\n    concepts = []\n    subj = 0\n    pkey = schema[0]\n    fields = schema[1:]\n\n    records = _str2records(filename, rel_name)\n\n    if not filename in not_unary:\n        concepts.append(unary_concept(pkey, subj, records))\n\n    for field in fields:\n        obj = schema.index(field)\n        concepts.append(binary_concept(field, closures, subj, obj, records))\n\n    return concepts\n\ndef cities2table(filename, rel_name, dbname, verbose=False, setup=False):\n    import sqlite3\n    records = _str2records(filename, rel_name)\n    connection =  sqlite3.connect(dbname)\n    cur = connection.cursor()\n    if setup:\n        cur.execute('''CREATE TABLE city_table\n        (City text, Country text, Population int)''')\n\n    table_name = \"city_table\"\n    for t in records:\n        cur.execute('insert into %s values (?,?,?)' % table_name, t)\n        if verbose:\n            print(\"inserting values into %s: \" % table_name, t)\n    connection.commit()\n    if verbose:\n        print(\"Committing update to %s\" % dbname)\n    cur.close()\n\ndef sql_query(dbname, query):\n    import sqlite3\n    try:\n        path = nltk.data.find(dbname)\n        connection =  sqlite3.connect(str(path))\n        cur = connection.cursor()\n        return cur.execute(query)\n    except (ValueError, sqlite3.OperationalError):\n        import warnings\n        warnings.warn(\"Make sure the database file %s is installed and uncompressed.\" % dbname)\n        raise\n\ndef _str2records(filename, rel):\n    recs = []\n    contents = nltk.data.load(\"corpora/chat80/%s\" % filename, format=\"text\")\n    for line in contents.splitlines():\n        if line.startswith(rel):\n            line = re.sub(rel+r'\\(', '', line)\n            line = re.sub(r'\\)\\.$', '', line)\n            record = line.split(',')\n            recs.append(record)\n    return recs\n\ndef unary_concept(label, subj, records):\n    c = Concept(label, arity=1, extension=set())\n    for record in records:\n        c.augment(record[subj])\n    return c\n\ndef binary_concept(label, closures, subj, obj, records):\n    if not label == 'border' and not label == 'contain':\n        label = label + '_of'\n    c = Concept(label, arity=2, closures=closures, extension=set())\n    for record in records:\n        c.augment((record[subj], record[obj]))\n    c.close()\n    return c\n\n\ndef process_bundle(rels):\n    concepts = {}\n    for rel in rels:\n        rel_name = rel['rel_name']\n        closures = rel['closures']\n        schema = rel['schema']\n        filename = rel['filename']\n\n        concept_list = clause2concepts(filename, rel_name, schema, closures)\n        for c in concept_list:\n            label = c.prefLabel\n            if (label in concepts):\n                for data in c.extension:\n                    concepts[label].augment(data)\n                concepts[label].close()\n            else:\n                concepts[label] = c\n    return concepts\n\n\ndef make_valuation(concepts, read=False, lexicon=False):\n    vals = []\n\n    for c in concepts:\n        vals.append((c.prefLabel, c.extension))\n    if lexicon: read = True\n    if read:\n        from nltk.sem import Valuation\n        val = Valuation({})\n        val.update(vals)\n        val = label_indivs(val, lexicon=lexicon)\n        return val\n    else:\n        return vals\n\n\ndef val_dump(rels, db):\n    concepts = process_bundle(rels).values()\n    valuation = make_valuation(concepts, read=True)\n    db_out = shelve.open(db, 'n')\n\n    db_out.update(valuation)\n\n    db_out.close()\n\n\ndef val_load(db):\n    dbname = db+\".db\"\n\n    if not os.access(dbname, os.R_OK):\n        sys.exit(\"Cannot read file: %s\" % dbname)\n    else:\n        db_in = shelve.open(db)\n        from nltk.sem import Valuation\n        val = Valuation(db_in)\n        return val\n\n\n\n\n\ndef label_indivs(valuation, lexicon=False):\n    domain = valuation.domain\n    pairs = [(e, e) for e in domain]\n    if lexicon:\n        lex = make_lex(domain)\n        with open(\"chat_pnames.cfg\", 'w') as outfile:\n            outfile.writelines(lex)\n    valuation.update(pairs)\n    return valuation\n\ndef make_lex(symbols):\n    lex = []\n    header = \"\"\"\n\n\"\"\"\n    lex.append(header)\n    template = \"PropN[num=sg, sem=<\\P.(P %s)>] -> '%s'\\n\"\n\n    for s in symbols:\n        parts = s.split('_')\n        caps = [p.capitalize() for p in parts]\n        pname = '_'.join(caps)\n        rule = template % (s, pname)\n        lex.append(rule)\n    return lex\n\n\n\ndef concepts(items = items):\n    \"\"\"\n    Build a list of concepts corresponding to the relation names in ``items``.\n\n    :param items: names of the Chat-80 relations to extract\n    :type items: list(str)\n    :return: the ``Concept`` objects which are extracted from the relations\n    :rtype: list(Concept)\n    \"\"\"\n    if isinstance(items, string_types): items = (items,)\n\n    rels = [item_metadata[r] for r in items]\n\n    concept_map = process_bundle(rels)\n    return concept_map.values()\n\n\n\n\n\n\ndef main():\n    import sys\n    from optparse import OptionParser\n    description = \\\n    \"\"\"\nExtract data from the Chat-80 Prolog files and convert them into a\nValuation object for use in the NLTK semantics package.\n    \"\"\"\n\n    opts = OptionParser(description=description)\n    opts.set_defaults(verbose=True, lex=False, vocab=False)\n    opts.add_option(\"-s\", \"--store\", dest=\"outdb\",\n                    help=\"store a valuation in DB\", metavar=\"DB\")\n    opts.add_option(\"-l\", \"--load\", dest=\"indb\",\n                    help=\"load a stored valuation from DB\", metavar=\"DB\")\n    opts.add_option(\"-c\", \"--concepts\", action=\"store_true\",\n                    help=\"print concepts instead of a valuation\")\n    opts.add_option(\"-r\", \"--relation\", dest=\"label\",\n                    help=\"print concept with label REL (check possible labels with '-v' option)\", metavar=\"REL\")\n    opts.add_option(\"-q\", \"--quiet\", action=\"store_false\", dest=\"verbose\",\n                    help=\"don't print out progress info\")\n    opts.add_option(\"-x\", \"--lex\", action=\"store_true\", dest=\"lex\",\n                    help=\"write a file of lexical entries for country names, then exit\")\n    opts.add_option(\"-v\", \"--vocab\", action=\"store_true\", dest=\"vocab\",\n                        help=\"print out the vocabulary of concept labels and their arity, then exit\")\n\n    (options, args) = opts.parse_args()\n    if options.outdb and options.indb:\n        opts.error(\"Options --store and --load are mutually exclusive\")\n\n\n    if options.outdb:\n        if options.verbose:\n            outdb = options.outdb+\".db\"\n            print(\"Dumping a valuation to %s\" % outdb)\n        val_dump(rels, options.outdb)\n        sys.exit(0)\n    else:\n        if options.indb is not None:\n            dbname = options.indb+\".db\"\n            if not os.access(dbname, os.R_OK):\n                sys.exit(\"Cannot read file: %s\" % dbname)\n            else:\n                valuation = val_load(options.indb)\n        else:\n            concept_map = process_bundle(rels)\n            concepts = concept_map.values()\n            if options.vocab:\n                items = sorted([(c.arity, c.prefLabel) for c in concepts])\n                for (arity, label) in items:\n                    print(label, arity)\n                sys.exit(0)\n            if options.concepts:\n                for c in concepts:\n                    print(c)\n                    print()\n            if options.label:\n                print(concept_map[options.label])\n                sys.exit(0)\n            else:\n                if options.lex:\n                    if options.verbose:\n                        print(\"Writing out lexical rules\")\n                    make_valuation(concepts, lexicon=True)\n                else:\n                    valuation = make_valuation(concepts, read=True)\n                    print(valuation)\n\n\ndef sql_demo():\n    \"\"\"\n    Print out every row from the 'city.db' database.\n    \"\"\"\n    print()\n    print(\"Using SQL to extract rows from 'city.db' RDB.\")\n    for row in sql_query('corpora/city_database/city.db', \"SELECT * FROM city_table\"):\n        print(row)\n\n\nif __name__ == '__main__':\n    main()\n    sql_demo()\n"], "nltk\\sem\\cooper_storage": [".py", "from __future__ import print_function\n\nfrom nltk.sem.logic import LambdaExpression, ApplicationExpression, Variable\nfrom nltk.parse import load_parser\nfrom nltk.parse.featurechart import InstantiateVarsChart\n\nclass CooperStore(object):\n    def __init__(self, featstruct):\n        self.featstruct = featstruct\n        self.readings = []\n        try:\n            self.core = featstruct['CORE']\n            self.store = featstruct['STORE']\n        except KeyError:\n            print(\"%s is not a Cooper storage structure\" % featstruct)\n\n    def _permute(self, lst):\n        remove = lambda lst0, index: lst0[:index] + lst0[index+1:]\n        if lst:\n            for index, x in enumerate(lst):\n                for y in self._permute(remove(lst, index)):\n                    yield (x,)+y\n        else: yield ()\n\n    def s_retrieve(self, trace=False):\n        for perm, store_perm in enumerate(self._permute(self.store)):\n            if trace:\n                print(\"Permutation %s\" % (perm+1))\n            term = self.core\n            for bindop in store_perm:\n                quant, varex = tuple(bindop.args)\n                term = ApplicationExpression(quant, LambdaExpression(varex.variable, term))\n                if trace:\n                    print(\"  \", term)\n                term = term.simplify()\n            self.readings.append(term)\n\n\ndef parse_with_bindops(sentence, grammar=None, trace=0):\n    if not grammar:\n        grammar = 'grammars/book_grammars/storage.fcfg'\n    parser = load_parser(grammar, trace=trace, chart_class=InstantiateVarsChart)\n    tokens = sentence.split()\n    return list(parser.parse(tokens))\n\n\ndef demo():\n    from nltk.sem import cooper_storage as cs\n    sentence = \"every girl chases a dog\"\n    print()\n    print(\"Analyis of sentence '%s'\" % sentence)\n    print(\"=\" * 50)\n    trees = cs.parse_with_bindops(sentence, trace=0)\n    for tree in trees:\n        semrep = cs.CooperStore(tree.label()['SEM'])\n        print()\n        print(\"Binding operators:\")\n        print(\"-\" * 15)\n        for s in semrep.store:\n            print(s)\n        print()\n        print(\"Core:\")\n        print(\"-\" * 15)\n        print(semrep.core)\n        print()\n        print(\"S-Retrieval:\")\n        print(\"-\" * 15)\n        semrep.s_retrieve(trace=True)\n        print(\"Readings:\")\n        print(\"-\" * 15)\n\n        for i, reading in enumerate(semrep.readings):\n            print(\"%s: %s\" % (i+1, reading))\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\sem\\drt": [".py", "from __future__ import print_function, unicode_literals\n\nimport operator\nfrom functools import reduce\nfrom itertools import chain\n\nfrom six import string_types\n\nfrom nltk.compat import python_2_unicode_compatible\nfrom nltk.sem.logic import (APP, AbstractVariableExpression, AllExpression,\n                            AndExpression, ApplicationExpression, BinaryExpression,\n                            BooleanExpression, ConstantExpression, EqualityExpression,\n                            EventVariableExpression, ExistsExpression, Expression,\n                            FunctionVariableExpression, ImpExpression,\n                            IndividualVariableExpression, LambdaExpression, Tokens,\n                            LogicParser, NegatedExpression, OrExpression, Variable,\n                            is_eventvar, is_funcvar, is_indvar, unique_variable)\n\ntry:\n    from six.moves.tkinter import Canvas, Tk\n    from six.moves.tkinter_font import Font\n    from nltk.util import in_idle\n\nexcept ImportError:\n    pass\n\nclass DrtTokens(Tokens):\n    DRS = 'DRS'\n    DRS_CONC = '+'\n    PRONOUN = 'PRO'\n    OPEN_BRACKET = '['\n    CLOSE_BRACKET = ']'\n    COLON = ':'\n\n    PUNCT = [DRS_CONC, OPEN_BRACKET, CLOSE_BRACKET, COLON]\n\n    SYMBOLS = Tokens.SYMBOLS + PUNCT\n\n    TOKENS = Tokens.TOKENS + [DRS] + PUNCT\n\n\nclass DrtParser(LogicParser):\n    def __init__(self):\n        LogicParser.__init__(self)\n\n        self.operator_precedence = dict(\n                               [(x,1) for x in DrtTokens.LAMBDA_LIST]             + \\\n                               [(x,2) for x in DrtTokens.NOT_LIST]                + \\\n                               [(APP,3)]                                          + \\\n                               [(x,4) for x in DrtTokens.EQ_LIST+Tokens.NEQ_LIST] + \\\n                               [(DrtTokens.COLON,5)]                              + \\\n                               [(DrtTokens.DRS_CONC,6)]                           + \\\n                               [(x,7) for x in DrtTokens.OR_LIST]                 + \\\n                               [(x,8) for x in DrtTokens.IMP_LIST]                + \\\n                               [(None,9)])\n\n    def get_all_symbols(self):\n        return DrtTokens.SYMBOLS\n\n    def isvariable(self, tok):\n        return tok not in DrtTokens.TOKENS\n\n    def handle(self, tok, context):\n    This is the base abstract DRT Expression from which every DRT\n    Expression extends.\n    \"\"\"\n\n    _drt_parser = DrtParser()\n\n    @classmethod\n    def fromstring(cls, s):\n        return cls._drt_parser.parse(s)\n\n    def applyto(self, other):\n        return DrtApplicationExpression(self, other)\n\n    def __neg__(self):\n        return DrtNegatedExpression(self)\n\n    def __and__(self, other):\n        raise NotImplementedError()\n\n    def __or__(self, other):\n        assert isinstance(other, DrtExpression)\n        return DrtOrExpression(self, other)\n\n    def __gt__(self, other):\n        assert isinstance(other, DrtExpression)\n        if isinstance(self, DRS):\n            return DRS(self.refs, self.conds, other)\n        if isinstance(self, DrtConcatenation):\n            return DrtConcatenation(self.first, self.second, other)\n        raise Exception('Antecedent of implication must be a DRS')\n\n    def equiv(self, other, prover=None):\n        \"\"\"\n        Check for logical equivalence.\n        Pass the expression (self <-> other) to the theorem prover.\n        If the prover says it is valid, then the self and other are equal.\n\n        :param other: an ``DrtExpression`` to check equality against\n        :param prover: a ``nltk.inference.api.Prover``\n        \"\"\"\n        assert isinstance(other, DrtExpression)\n\n        f1 = self.simplify().fol();\n        f2 = other.simplify().fol();\n        return f1.equiv(f2, prover)\n\n    @property\n    def type(self):\n        raise AttributeError(\"'%s' object has no attribute 'type'\" %\n                             self.__class__.__name__)\n\n    def typecheck(self, signature=None):\n        raise NotImplementedError()\n\n    def __add__(self, other):\n        return DrtConcatenation(self, other, None)\n\n    def get_refs(self, recursive=False):\n        \"\"\"\n        Return the set of discourse referents in this DRS.\n        :param recursive: bool Also find discourse referents in subterms?\n        :return: list of ``Variable`` objects\n        \"\"\"\n        raise NotImplementedError()\n\n    def is_pronoun_function(self):\n        return isinstance(self, DrtApplicationExpression) and \\\n               isinstance(self.function, DrtAbstractVariableExpression) and \\\n               self.function.variable.name == DrtTokens.PRONOUN and \\\n               isinstance(self.argument, DrtIndividualVariableExpression)\n\n    def make_EqualityExpression(self, first, second):\n        return DrtEqualityExpression(first, second)\n\n    def make_VariableExpression(self, variable):\n        return DrtVariableExpression(variable)\n\n    def resolve_anaphora(self):\n        return resolve_anaphora(self)\n\n    def eliminate_equality(self):\n        return self.visit_structured(lambda e: e.eliminate_equality(),\n                                     self.__class__)\n\n    def pretty_format(self):\n        \"\"\"\n        Draw the DRS\n        :return: the pretty print string\n        \"\"\"\n        return '\\n'.join(self._pretty())\n\n    def pretty_print(self):\n        print(self.pretty_format())\n\n    def draw(self):\n        DrsDrawer(self).draw()\n\n\n@python_2_unicode_compatible\nclass DRS(DrtExpression, Expression):\n    def __init__(self, refs, conds, consequent=None):\n        \"\"\"\n        :param refs: list of ``DrtIndividualVariableExpression`` for the\n        discourse referents\n        :param conds: list of ``Expression`` for the conditions\n        \"\"\"\n        self.refs = refs\n        self.conds = conds\n        self.consequent = consequent\n\n    def replace(self, variable, expression, replace_bound=False, alpha_convert=True):\n        \"\"\"Replace all instances of variable v with expression E in self,\n        where v is free in self.\"\"\"\n        if variable in self.refs:\n            if not replace_bound:\n                return self\n            else:\n                i = self.refs.index(variable)\n                if self.consequent:\n                    consequent = self.consequent.replace(variable, expression, True, alpha_convert)\n                else:\n                    consequent = None\n                return DRS(self.refs[:i]+[expression.variable]+self.refs[i+1:],\n                           [cond.replace(variable, expression, True, alpha_convert)\n                            for cond in self.conds],\n                           consequent)\n        else:\n            if alpha_convert:\n                for ref in (set(self.refs) & expression.free()):\n                    newvar = unique_variable(ref)\n                    newvarex = DrtVariableExpression(newvar)\n                    i = self.refs.index(ref)\n                    if self.consequent:\n                        consequent = self.consequent.replace(ref, newvarex, True, alpha_convert)\n                    else:\n                        consequent = None\n                    self = DRS(self.refs[:i]+[newvar]+self.refs[i+1:],\n                               [cond.replace(ref, newvarex, True, alpha_convert)\n                                for cond in self.conds],\n                               consequent)\n\n            if self.consequent:\n                consequent = self.consequent.replace(variable, expression, replace_bound, alpha_convert)\n            else:\n                consequent = None\n            return DRS(self.refs,\n                       [cond.replace(variable, expression, replace_bound, alpha_convert)\n                        for cond in self.conds],\n                       consequent)\n\n    def free(self):\n        conds_free = reduce(operator.or_, [c.free() for c in self.conds], set())\n        if self.consequent:\n            conds_free.update(self.consequent.free())\n        return conds_free - set(self.refs)\n\n    def get_refs(self, recursive=False):\n        if recursive:\n            conds_refs = self.refs + list(chain(*(c.get_refs(True) for c in self.conds)))\n            if self.consequent:\n                conds_refs.extend(self.consequent.get_refs(True))\n            return conds_refs\n        else:\n            return self.refs\n\n    def visit(self, function, combinator):\n        parts = list(map(function, self.conds))\n        if self.consequent:\n            parts.append(function(self.consequent))\n        return combinator(parts)\n\n    def visit_structured(self, function, combinator):\n        consequent = (function(self.consequent) if self.consequent else None)\n        return combinator(self.refs, list(map(function, self.conds)), consequent)\n\n    def eliminate_equality(self):\n        drs = self\n        i = 0\n        while i < len(drs.conds):\n            cond = drs.conds[i]\n            if isinstance(cond, EqualityExpression) and \\\n               isinstance(cond.first, AbstractVariableExpression) and \\\n               isinstance(cond.second, AbstractVariableExpression):\n                drs = DRS(list(set(drs.refs)-set([cond.second.variable])),\n                          drs.conds[:i]+drs.conds[i+1:],\n                          drs.consequent)\n                if cond.second.variable != cond.first.variable:\n                    drs = drs.replace(cond.second.variable, cond.first, False, False)\n                    i = 0\n                i -= 1\n            i += 1\n\n        conds = []\n        for cond in drs.conds:\n            new_cond = cond.eliminate_equality()\n            new_cond_simp = new_cond.simplify()\n            if not isinstance(new_cond_simp, DRS) or \\\n               new_cond_simp.refs or new_cond_simp.conds or \\\n               new_cond_simp.consequent:\n                conds.append(new_cond)\n\n        consequent = (drs.consequent.eliminate_equality() if drs.consequent else None)\n        return DRS(drs.refs, conds, consequent)\n\n    def fol(self):\n        if self.consequent:\n            accum = None\n            if self.conds:\n                accum = reduce(AndExpression, [c.fol() for c in self.conds])\n\n            if accum:\n                accum = ImpExpression(accum, self.consequent.fol())\n            else:\n                accum = self.consequent.fol()\n\n            for ref in self.refs[::-1]:\n                accum = AllExpression(ref, accum)\n\n            return accum\n\n        else:\n            if not self.conds:\n                raise Exception(\"Cannot convert DRS with no conditions to FOL.\")\n            accum = reduce(AndExpression, [c.fol() for c in self.conds])\n            for ref in map(Variable, self._order_ref_strings(self.refs)[::-1]):\n                accum = ExistsExpression(ref, accum)\n            return accum\n\n    def _pretty(self):\n        refs_line = ' '.join(self._order_ref_strings(self.refs))\n\n        cond_lines = [cond for cond_line in [filter(lambda s: s.strip(), cond._pretty())\n                                             for cond in self.conds]\n                      for cond in cond_line]\n        length = max([len(refs_line)] + list(map(len, cond_lines)))\n        drs = ([' _' + '_' * length            + '_ ',\n                '| ' + refs_line.ljust(length) + ' |',\n                '|-' + '-' * length            + '-|'] +\n               ['| ' + line.ljust(length)      + ' |' for line in cond_lines] +\n               ['|_' + '_' * length            + '_|'])\n        if self.consequent:\n            return DrtBinaryExpression._assemble_pretty(drs, DrtTokens.IMP,\n                                                        self.consequent._pretty())\n        return drs\n\n    def _order_ref_strings(self, refs):\n        strings = [\"%s\" % ref for ref in refs]\n        ind_vars = []\n        func_vars = []\n        event_vars = []\n        other_vars = []\n        for s in strings:\n            if is_indvar(s):\n                ind_vars.append(s)\n            elif is_funcvar(s):\n                func_vars.append(s)\n            elif is_eventvar(s):\n                event_vars.append(s)\n            else:\n                other_vars.append(s)\n        return sorted(other_vars) + \\\n               sorted(event_vars, key=lambda v: int([v[2:],-1][len(v[2:]) == 0])) + \\\n               sorted(func_vars, key=lambda v: (v[0], int([v[1:],-1][len(v[1:])==0]))) + \\\n               sorted(ind_vars, key=lambda v: (v[0], int([v[1:],-1][len(v[1:])==0])))\n\n    def __eq__(self, other):\n        r\"\"\"Defines equality modulo alphabetic variance.\n        If we are comparing \\x.M  and \\y.N, then check equality of M and N[x/y].\"\"\"\n        if isinstance(other, DRS):\n            if len(self.refs) == len(other.refs):\n                converted_other = other\n                for (r1, r2) in zip(self.refs, converted_other.refs):\n                    varex = self.make_VariableExpression(r1)\n                    converted_other = converted_other.replace(r2, varex, True)\n                if self.consequent == converted_other.consequent and \\\n                   len(self.conds) == len(converted_other.conds):\n                    for c1, c2 in zip(self.conds, converted_other.conds):\n                        if not (c1 == c2):\n                            return False\n                    return True\n        return False\n\n    def __ne__(self, other):\n        return not self == other\n\n    __hash__ = Expression.__hash__\n\n    def __str__(self):\n        drs = '([%s],[%s])' % (','.join(self._order_ref_strings(self.refs)),\n                               ', '.join(\"%s\" % cond for cond in self.conds)) # map(str, self.conds)))\n        if self.consequent:\n            return DrtTokens.OPEN + drs + ' ' + DrtTokens.IMP + ' ' + \\\n                   \"%s\" % self.consequent + DrtTokens.CLOSE\n        return drs\n\n\ndef DrtVariableExpression(variable):\n    \"\"\"\n    This is a factory method that instantiates and returns a subtype of\n    ``DrtAbstractVariableExpression`` appropriate for the given variable.\n    \"\"\"\n    if is_indvar(variable.name):\n        return DrtIndividualVariableExpression(variable)\n    elif is_funcvar(variable.name):\n        return DrtFunctionVariableExpression(variable)\n    elif is_eventvar(variable.name):\n        return DrtEventVariableExpression(variable)\n    else:\n        return DrtConstantExpression(variable)\n\n\nclass DrtAbstractVariableExpression(DrtExpression, AbstractVariableExpression):\n    def fol(self):\n        return self\n\n    def get_refs(self, recursive=False):\n        return []\n\n    def _pretty(self):\n        s = \"%s\" % self\n        blank = ' '*len(s)\n        return [blank, blank, s, blank]\n\n    def eliminate_equality(self):\n        return self\n\nclass DrtIndividualVariableExpression(DrtAbstractVariableExpression, IndividualVariableExpression):\n    pass\n\nclass DrtFunctionVariableExpression(DrtAbstractVariableExpression, FunctionVariableExpression):\n    pass\n\nclass DrtEventVariableExpression(DrtIndividualVariableExpression, EventVariableExpression):\n    pass\n\nclass DrtConstantExpression(DrtAbstractVariableExpression, ConstantExpression):\n    pass\n\n\n@python_2_unicode_compatible\nclass DrtProposition(DrtExpression, Expression):\n    def __init__(self, variable, drs):\n        self.variable = variable\n        self.drs = drs\n\n    def replace(self, variable, expression, replace_bound=False, alpha_convert=True):\n        if self.variable == variable:\n            assert isinstance(expression, DrtAbstractVariableExpression), \"Can only replace a proposition label with a variable\"\n            return DrtProposition(expression.variable, self.drs.replace(variable, expression, replace_bound, alpha_convert))\n        else:\n            return DrtProposition(self.variable, self.drs.replace(variable, expression, replace_bound, alpha_convert))\n\n    def eliminate_equality(self):\n        return DrtProposition(self.variable, self.drs.eliminate_equality())\n\n    def get_refs(self, recursive=False):\n        return (self.drs.get_refs(True) if recursive else [])\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__ and \\\n               self.variable == other.variable and \\\n               self.drs == other.drs\n\n    def __ne__(self, other):\n        return not self == other\n\n    __hash__ = Expression.__hash__\n\n    def fol(self):\n        return self.drs.fol()\n\n    def _pretty(self):\n        drs_s = self.drs._pretty()\n        blank = ' ' * len(\"%s\" % self.variable)\n        return ([blank                + ' ' + line for line in drs_s[:1]] +\n                [\"%s\" % self.variable + ':' + line for line in drs_s[1:2]] +\n                [blank                + ' ' + line for line in drs_s[2:]])\n\n    def visit(self, function, combinator):\n        return combinator([function(self.drs)])\n\n    def visit_structured(self, function, combinator):\n        return combinator(self.variable, function(self.drs))\n\n    def __str__(self):\n        return 'prop(%s, %s)' % (self.variable, self.drs)\n\n\nclass DrtNegatedExpression(DrtExpression, NegatedExpression):\n    def fol(self):\n        return NegatedExpression(self.term.fol())\n\n    def get_refs(self, recursive=False):\n        return self.term.get_refs(recursive)\n\n    def _pretty(self):\n        term_lines = self.term._pretty()\n        return (['    ' + line for line in term_lines[:2]] +\n                ['__  ' + line for line in term_lines[2:3]] +\n                ['  | ' + line for line in term_lines[3:4]] +\n                ['    ' + line for line in term_lines[4:]])\n\nclass DrtLambdaExpression(DrtExpression, LambdaExpression):\n    def alpha_convert(self, newvar):\n        \"\"\"Rename all occurrences of the variable introduced by this variable\n        binder in the expression to ``newvar``.\n        :param newvar: ``Variable``, for the new variable\n        \"\"\"\n        return self.__class__(newvar, self.term.replace(self.variable,\n                          DrtVariableExpression(newvar), True))\n\n    def fol(self):\n        return LambdaExpression(self.variable, self.term.fol())\n\n    def _pretty(self):\n        variables = [self.variable]\n        term = self.term\n        while term.__class__ == self.__class__:\n            variables.append(term.variable)\n            term = term.term\n        var_string = ' '.join(\"%s\" % v for v in variables) + DrtTokens.DOT\n        term_lines = term._pretty()\n        blank = ' ' * len(var_string)\n        return (['    ' + blank      + line for line in term_lines[:1]] +\n                [' \\  ' + blank      + line for line in term_lines[1:2]] +\n                [' /\\ ' + var_string + line for line in term_lines[2:3]] +\n                ['    ' + blank      + line for line in term_lines[3:]])\n\nclass DrtBinaryExpression(DrtExpression, BinaryExpression):\n    def get_refs(self, recursive=False):\n        return self.first.get_refs(True) + self.second.get_refs(True) if recursive else []\n\n    def _pretty(self):\n        return DrtBinaryExpression._assemble_pretty(self._pretty_subex(self.first), self.getOp(), self._pretty_subex(self.second))\n\n    @staticmethod\n    def _assemble_pretty(first_lines, op, second_lines):\n        max_lines = max(len(first_lines), len(second_lines))\n        first_lines = _pad_vertically(first_lines, max_lines)\n        second_lines = _pad_vertically(second_lines, max_lines)\n        blank = ' ' * len(op)\n        first_second_lines = list(zip(first_lines, second_lines))\n        return ([' ' + first_line + ' ' + blank + ' ' + second_line + ' ' for first_line, second_line in first_second_lines[:2]] +\n                ['(' + first_line + ' ' + op    + ' ' + second_line + ')' for first_line, second_line in first_second_lines[2:3]] +\n                [' ' + first_line + ' ' + blank + ' ' + second_line + ' ' for first_line, second_line in first_second_lines[3:]])\n\n    def _pretty_subex(self, subex):\n        return subex._pretty()\n\nclass DrtBooleanExpression(DrtBinaryExpression, BooleanExpression):\n    pass\n\nclass DrtOrExpression(DrtBooleanExpression, OrExpression):\n    def fol(self):\n        return OrExpression(self.first.fol(), self.second.fol())\n\n    def _pretty_subex(self, subex):\n        if isinstance(subex, DrtOrExpression):\n            return [line[1:-1] for line in subex._pretty()]\n        return DrtBooleanExpression._pretty_subex(self, subex)\n\nclass DrtEqualityExpression(DrtBinaryExpression, EqualityExpression):\n    def fol(self):\n        return EqualityExpression(self.first.fol(), self.second.fol())\n\n@python_2_unicode_compatible\nclass DrtConcatenation(DrtBooleanExpression):\n    def __init__(self, first, second, consequent=None):\n        DrtBooleanExpression.__init__(self, first, second)\n        self.consequent = consequent\n\n    def replace(self, variable, expression, replace_bound=False, alpha_convert=True):\n        \"\"\"Replace all instances of variable v with expression E in self,\n        where v is free in self.\"\"\"\n        first = self.first\n        second = self.second\n        consequent = self.consequent\n\n        if variable in self.get_refs():\n            if replace_bound:\n                first  = first.replace(variable, expression, replace_bound, alpha_convert)\n                second = second.replace(variable, expression, replace_bound, alpha_convert)\n                if consequent:\n                    consequent = consequent.replace(variable, expression, replace_bound, alpha_convert)\n        else:\n            if alpha_convert:\n                for ref in (set(self.get_refs(True)) & expression.free()):\n                    v = DrtVariableExpression(unique_variable(ref))\n                    first  = first.replace(ref, v, True, alpha_convert)\n                    second = second.replace(ref, v, True, alpha_convert)\n                    if consequent:\n                        consequent = consequent.replace(ref, v, True, alpha_convert)\n\n            first  = first.replace(variable, expression, replace_bound, alpha_convert)\n            second = second.replace(variable, expression, replace_bound, alpha_convert)\n            if consequent:\n                consequent = consequent.replace(variable, expression, replace_bound, alpha_convert)\n\n        return self.__class__(first, second, consequent)\n\n    def eliminate_equality(self):\n        drs = self.simplify()\n        assert not isinstance(drs, DrtConcatenation)\n        return drs.eliminate_equality()\n\n    def simplify(self):\n        first = self.first.simplify()\n        second = self.second.simplify()\n        consequent = (self.consequent.simplify() if self.consequent else None)\n\n        if isinstance(first, DRS) and isinstance(second, DRS):\n            for ref in (set(first.get_refs(True)) & set(second.get_refs(True))):\n                newvar = DrtVariableExpression(unique_variable(ref))\n                second = second.replace(ref, newvar, True)\n\n            return DRS(first.refs + second.refs, first.conds + second.conds, consequent)\n        else:\n            return self.__class__(first, second, consequent)\n\n    def get_refs(self, recursive=False):\n        refs = self.first.get_refs(recursive) + self.second.get_refs(recursive)\n        if self.consequent and recursive:\n            refs.extend(self.consequent.get_refs(True))\n        return refs\n\n    def getOp(self):\n        return DrtTokens.DRS_CONC\n\n    def __eq__(self, other):\n        r\"\"\"Defines equality modulo alphabetic variance.\n        If we are comparing \\x.M  and \\y.N, then check equality of M and N[x/y].\"\"\"\n        if isinstance(other, DrtConcatenation):\n            self_refs = self.get_refs()\n            other_refs = other.get_refs()\n            if len(self_refs) == len(other_refs):\n                converted_other = other\n                for (r1,r2) in zip(self_refs, other_refs):\n                    varex = self.make_VariableExpression(r1)\n                    converted_other = converted_other.replace(r2, varex, True)\n                return self.first == converted_other.first and \\\n                        self.second == converted_other.second and \\\n                        self.consequent == converted_other.consequent\n        return False\n\n    def __ne__(self, other):\n        return not self == other\n\n    __hash__ = DrtBooleanExpression.__hash__\n\n    def fol(self):\n        e = AndExpression(self.first.fol(), self.second.fol())\n        if self.consequent:\n            e = ImpExpression(e, self.consequent.fol())\n        return e\n\n    def _pretty(self):\n        drs = DrtBinaryExpression._assemble_pretty(self._pretty_subex(self.first),\n                                                   self.getOp(),\n                                                   self._pretty_subex(self.second))\n        if self.consequent:\n            drs = DrtBinaryExpression._assemble_pretty(drs, DrtTokens.IMP,\n                                                       self._pretty(self.consequent))\n        return drs\n\n    def _pretty_subex(self, subex):\n        if isinstance(subex, DrtConcatenation):\n            return [line[1:-1] for line in subex._pretty()]\n        return DrtBooleanExpression._pretty_subex(self, subex)\n\n\n    def visit(self, function, combinator):\n        if self.consequent:\n            return combinator([function(self.first), function(self.second), function(self.consequent)])\n        else:\n            return combinator([function(self.first), function(self.second)])\n\n    def __str__(self):\n        first = self._str_subex(self.first)\n        second = self._str_subex(self.second)\n        drs = Tokens.OPEN + first + ' ' + self.getOp() \\\n                + ' ' + second + Tokens.CLOSE\n        if self.consequent:\n            return DrtTokens.OPEN + drs + ' ' + DrtTokens.IMP + ' ' + \\\n                   \"%s\" % self.consequent + DrtTokens.CLOSE\n        return drs\n\n    def _str_subex(self, subex):\n        s = \"%s\" % subex\n        if isinstance(subex, DrtConcatenation) and subex.consequent is None:\n            return s[1:-1]\n        return s\n\n\nclass DrtApplicationExpression(DrtExpression, ApplicationExpression):\n    def fol(self):\n        return ApplicationExpression(self.function.fol(), self.argument.fol())\n\n    def get_refs(self, recursive=False):\n        return (self.function.get_refs(True) + self.argument.get_refs(True)\n                if recursive else [])\n\n    def _pretty(self):\n        function, args = self.uncurry()\n        function_lines = function._pretty()\n        args_lines = [arg._pretty() for arg in args]\n        max_lines = max(map(len, [function_lines] + args_lines))\n        function_lines = _pad_vertically(function_lines, max_lines)\n        args_lines = [_pad_vertically(arg_lines, max_lines) for arg_lines in args_lines]\n        func_args_lines = list(zip(function_lines, list(zip(*args_lines))))\n        return ([func_line + ' ' + ' '.join(args_line) + ' ' for func_line, args_line in func_args_lines[:2]] +\n                [func_line + '(' + ','.join(args_line) + ')' for func_line, args_line in func_args_lines[2:3]] +\n                [func_line + ' ' + ' '.join(args_line) + ' ' for func_line, args_line in func_args_lines[3:]])\n\n\ndef _pad_vertically(lines, max_lines):\n    pad_line = [' ' * len(lines[0])]\n    return lines + pad_line * (max_lines - len(lines))\n\n\n@python_2_unicode_compatible\nclass PossibleAntecedents(list, DrtExpression, Expression):\n    def free(self):\n        return set(self)\n\n    def replace(self, variable, expression, replace_bound=False, alpha_convert=True):\n        \"\"\"Replace all instances of variable v with expression E in self,\n        where v is free in self.\"\"\"\n        result = PossibleAntecedents()\n        for item in self:\n            if item == variable:\n                self.append(expression)\n            else:\n                self.append(item)\n        return result\n\n    def _pretty(self):\n        s = \"%s\" % self\n        blank = ' ' * len(s)\n        return [blank, blank, s]\n\n    def __str__(self):\n        return '[' + ','.join(\"%s\" % it for it in self) + ']'\n\n\nclass AnaphoraResolutionException(Exception):\n    pass\n\n\ndef resolve_anaphora(expression, trail=[]):\n    if isinstance(expression, ApplicationExpression):\n        if expression.is_pronoun_function():\n            possible_antecedents = PossibleAntecedents()\n            for ancestor in trail:\n                for ref in ancestor.get_refs():\n                    refex = expression.make_VariableExpression(ref)\n\n                    if refex.__class__ == expression.argument.__class__ and \\\n                       not (refex == expression.argument):\n                        possible_antecedents.append(refex)\n\n            if len(possible_antecedents) == 1:\n                resolution = possible_antecedents[0]\n            else:\n                resolution = possible_antecedents\n            return expression.make_EqualityExpression(expression.argument, resolution)\n        else:\n            r_function = resolve_anaphora(expression.function, trail + [expression])\n            r_argument = resolve_anaphora(expression.argument, trail + [expression])\n            return expression.__class__(r_function, r_argument)\n\n    elif isinstance(expression, DRS):\n        r_conds = []\n        for cond in expression.conds:\n            r_cond = resolve_anaphora(cond, trail + [expression])\n\n            if isinstance(r_cond, EqualityExpression):\n                if isinstance(r_cond.first, PossibleAntecedents):\n                    temp = r_cond.first\n                    r_cond.first = r_cond.second\n                    r_cond.second = temp\n                if isinstance(r_cond.second, PossibleAntecedents):\n                    if not r_cond.second:\n                        raise AnaphoraResolutionException(\"Variable '%s' does not \"\n                                \"resolve to anything.\" % r_cond.first)\n\n            r_conds.append(r_cond)\n        if expression.consequent:\n            consequent = resolve_anaphora(expression.consequent, trail + [expression])\n        else:\n            consequent = None\n        return expression.__class__(expression.refs, r_conds, consequent)\n\n    elif isinstance(expression, AbstractVariableExpression):\n        return expression\n\n    elif isinstance(expression, NegatedExpression):\n        return expression.__class__(resolve_anaphora(expression.term, trail + [expression]))\n\n    elif isinstance(expression, DrtConcatenation):\n        if expression.consequent:\n            consequent = resolve_anaphora(expression.consequent, trail + [expression])\n        else:\n            consequent = None\n        return expression.__class__(resolve_anaphora(expression.first, trail + [expression]),\n                                    resolve_anaphora(expression.second, trail + [expression]),\n                                    consequent)\n\n    elif isinstance(expression, BinaryExpression):\n        return expression.__class__(resolve_anaphora(expression.first, trail + [expression]),\n                                    resolve_anaphora(expression.second, trail + [expression]))\n\n    elif isinstance(expression, LambdaExpression):\n        return expression.__class__(expression.variable, resolve_anaphora(expression.term, trail + [expression]))\n\n\nclass DrsDrawer(object):\n    BUFFER = 3     #Space between elements\n    TOPSPACE = 10  #Space above whole DRS\n    OUTERSPACE = 6 #Space to the left, right, and bottom of the whle DRS\n\n    def __init__(self, drs, size_canvas=True, canvas=None):\n        \"\"\"\n        :param drs: ``DrtExpression``, The DRS to be drawn\n        :param size_canvas: bool, True if the canvas size should be the exact size of the DRS\n        :param canvas: ``Canvas`` The canvas on which to draw the DRS.  If none is given, create a new canvas.\n        \"\"\"\n        master = None\n        if not canvas:\n            master = Tk()\n            master.title(\"DRT\")\n\n            font = Font(family='helvetica', size=12)\n\n            if size_canvas:\n                canvas = Canvas(master, width=0, height=0)\n                canvas.font = font\n                self.canvas = canvas\n                (right, bottom) = self._visit(drs, self.OUTERSPACE, self.TOPSPACE)\n\n                width = max(right+self.OUTERSPACE, 100)\n                height = bottom+self.OUTERSPACE\n                canvas = Canvas(master, width=width, height=height)#, bg='white')\n            else:\n                canvas = Canvas(master, width=300, height=300)\n\n            canvas.pack()\n            canvas.font = font\n\n        self.canvas = canvas\n        self.drs = drs\n        self.master = master\n\n    def _get_text_height(self):\n        return self.canvas.font.metrics(\"linespace\")\n\n    def draw(self, x=OUTERSPACE, y=TOPSPACE):\n        self._handle(self.drs, self._draw_command, x, y)\n\n        if self.master and not in_idle():\n            self.master.mainloop()\n        else:\n            return self._visit(self.drs, x, y)\n\n    def _visit(self, expression, x, y):\n        \"\"\"\n        Return the bottom-rightmost point without actually drawing the item\n\n        :param expression: the item to visit\n        :param x: the top of the current drawing area\n        :param y: the left side of the current drawing area\n        :return: the bottom-rightmost point\n        \"\"\"\n        return self._handle(expression, self._visit_command, x, y)\n\n    def _draw_command(self, item, x, y):\n        \"\"\"\n        Draw the given item at the given location\n\n        :param item: the item to draw\n        :param x: the top of the current drawing area\n        :param y: the left side of the current drawing area\n        :return: the bottom-rightmost point\n        \"\"\"\n        if isinstance(item, string_types):\n            self.canvas.create_text(x, y, anchor='nw', font=self.canvas.font, text=item)\n        elif isinstance(item, tuple):\n            (right, bottom) = item\n            self.canvas.create_rectangle(x, y, right, bottom)\n            horiz_line_y = y + self._get_text_height() + (self.BUFFER * 2) #the line separating refs from conds\n            self.canvas.create_line(x, horiz_line_y, right, horiz_line_y)\n\n        return self._visit_command(item, x, y)\n\n    def _visit_command(self, item, x, y):\n        \"\"\"\n        Return the bottom-rightmost point without actually drawing the item\n\n        :param item: the item to visit\n        :param x: the top of the current drawing area\n        :param y: the left side of the current drawing area\n        :return: the bottom-rightmost point\n        \"\"\"\n        if isinstance(item, string_types):\n            return (x + self.canvas.font.measure(item), y + self._get_text_height())\n        elif isinstance(item, tuple):\n            return item\n\n    def _handle(self, expression, command, x=0, y=0):\n        \"\"\"\n        :param expression: the expression to handle\n        :param command: the function to apply, either _draw_command or _visit_command\n        :param x: the top of the current drawing area\n        :param y: the left side of the current drawing area\n        :return: the bottom-rightmost point\n        \"\"\"\n        if command == self._visit_command:\n            try:\n                right = expression._drawing_width + x\n                bottom = expression._drawing_height + y\n                return (right, bottom)\n            except AttributeError:\n                pass\n\n        if isinstance(expression, DrtAbstractVariableExpression):\n            factory = self._handle_VariableExpression\n        elif isinstance(expression, DRS):\n            factory = self._handle_DRS\n        elif isinstance(expression, DrtNegatedExpression):\n            factory = self._handle_NegatedExpression\n        elif isinstance(expression, DrtLambdaExpression):\n            factory = self._handle_LambdaExpression\n        elif isinstance(expression, BinaryExpression):\n            factory = self._handle_BinaryExpression\n        elif isinstance(expression, DrtApplicationExpression):\n            factory = self._handle_ApplicationExpression\n        elif isinstance(expression, PossibleAntecedents):\n            factory = self._handle_VariableExpression\n        elif isinstance(expression, DrtProposition):\n            factory = self._handle_DrtProposition\n        else:\n            raise Exception(expression.__class__.__name__)\n\n        (right, bottom) = factory(expression, command, x, y)\n\n        expression._drawing_width = right - x\n        expression._drawing_height = bottom - y\n\n        return (right, bottom)\n\n    def _handle_VariableExpression(self, expression, command, x, y):\n        return command(\"%s\" % expression, x, y)\n\n    def _handle_NegatedExpression(self, expression, command, x, y):\n        right = self._visit_command(DrtTokens.NOT, x, y)[0]\n\n        (right, bottom) = self._handle(expression.term, command, right, y)\n\n        command(DrtTokens.NOT, x, self._get_centered_top(y, bottom - y, self._get_text_height()))\n\n        return (right, bottom)\n\n    def _handle_DRS(self, expression, command, x, y):\n        left = x + self.BUFFER #indent the left side\n        bottom = y + self.BUFFER #indent the top\n\n        if expression.refs:\n            refs = ' '.join(\"%s\"%r for r in expression.refs)\n        else:\n            refs = '     '\n        (max_right, bottom) = command(refs, left, bottom)\n        bottom += (self.BUFFER * 2)\n\n        if expression.conds:\n            for cond in expression.conds:\n                (right, bottom) = self._handle(cond, command, left, bottom)\n                max_right = max(max_right, right)\n                bottom += self.BUFFER\n        else:\n            bottom += self._get_text_height() + self.BUFFER\n\n        max_right += self.BUFFER\n        return command((max_right, bottom), x, y)\n\n    def _handle_ApplicationExpression(self, expression, command, x, y):\n        function, args = expression.uncurry()\n        if not isinstance(function, DrtAbstractVariableExpression):\n            function = expression.function\n            args = [expression.argument]\n\n        function_bottom = self._visit(function, x, y)[1]\n        max_bottom = max([function_bottom] + [self._visit(arg, x, y)[1] for arg in args])\n\n        line_height = max_bottom - y\n\n        function_drawing_top = self._get_centered_top(y, line_height, function._drawing_height)\n        right = self._handle(function, command, x, function_drawing_top)[0]\n\n        centred_string_top = self._get_centered_top(y, line_height, self._get_text_height())\n        right = command(DrtTokens.OPEN, right, centred_string_top)[0]\n\n        for (i,arg) in enumerate(args):\n            arg_drawing_top = self._get_centered_top(y, line_height, arg._drawing_height)\n            right = self._handle(arg, command, right, arg_drawing_top)[0]\n\n            if i+1 < len(args):\n                right = command(DrtTokens.COMMA + ' ', right, centred_string_top)[0]\n\n        right = command(DrtTokens.CLOSE, right, centred_string_top)[0]\n\n        return (right, max_bottom)\n\n    def _handle_LambdaExpression(self, expression, command, x, y):\n        variables = DrtTokens.LAMBDA + \"%s\" % expression.variable + DrtTokens.DOT\n        right = self._visit_command(variables, x, y)[0]\n\n        (right, bottom) = self._handle(expression.term, command, right, y)\n\n        command(variables, x, self._get_centered_top(y, bottom - y, self._get_text_height()))\n\n        return (right, bottom)\n\n    def _handle_BinaryExpression(self, expression, command, x, y):\n        first_height = self._visit(expression.first, 0, 0)[1]\n        second_height = self._visit(expression.second, 0, 0)[1]\n        line_height = max(first_height, second_height)\n\n        centred_string_top = self._get_centered_top(y, line_height, self._get_text_height())\n        right = command(DrtTokens.OPEN, x, centred_string_top)[0]\n\n        first_height = expression.first._drawing_height\n        (right, first_bottom) = self._handle(expression.first, command, right, self._get_centered_top(y, line_height, first_height))\n\n        right = command(' %s ' % expression.getOp(), right, centred_string_top)[0]\n\n        second_height = expression.second._drawing_height\n        (right, second_bottom) = self._handle(expression.second, command, right, self._get_centered_top(y, line_height, second_height))\n\n        right = command(DrtTokens.CLOSE, right, centred_string_top)[0]\n\n        return (right, max(first_bottom, second_bottom))\n\n    def _handle_DrtProposition(self, expression, command, x, y):\n        right = command(expression.variable, x, y)[0]\n\n        (right, bottom) = self._handle(expression.term, command, right, y)\n\n        return (right, bottom)\n\n    def _get_centered_top(self, top, full_height, item_height):\n        \"\"\"Get the y-coordinate of the point that a figure should start at if\n        its height is 'item_height' and it needs to be centered in an area that\n        starts at 'top' and is 'full_height' tall.\"\"\"\n        return top + (full_height - item_height) / 2\n\n\ndef demo():\n    print('='*20 + 'TEST PARSE' + '='*20)\n    dexpr = DrtExpression.fromstring\n    print(dexpr(r'([x,y],[sees(x,y)])'))\n    print(dexpr(r'([x],[man(x), walks(x)])'))\n    print(dexpr(r'\\x.\\y.([],[sees(x,y)])'))\n    print(dexpr(r'\\x.([],[walks(x)])(john)'))\n    print(dexpr(r'(([x],[walks(x)]) + ([y],[runs(y)]))'))\n    print(dexpr(r'(([],[walks(x)]) -> ([],[runs(x)]))'))\n    print(dexpr(r'([x],[PRO(x), sees(John,x)])'))\n    print(dexpr(r'([x],[man(x), -([],[walks(x)])])'))\n    print(dexpr(r'([],[(([x],[man(x)]) -> ([],[walks(x)]))])'))\n\n    print('='*20 + 'Test fol()' + '='*20)\n    print(dexpr(r'([x,y],[sees(x,y)])').fol())\n\n    print('='*20 + 'Test alpha conversion and lambda expression equality' + '='*20)\n    e1 = dexpr(r'\\x.([],[P(x)])')\n    print(e1)\n    e2 = e1.alpha_convert(Variable('z'))\n    print(e2)\n    print(e1 == e2)\n\n    print('='*20 + 'Test resolve_anaphora()' + '='*20)\n    print(resolve_anaphora(dexpr(r'([x,y,z],[dog(x), cat(y), walks(z), PRO(z)])')))\n    print(resolve_anaphora(dexpr(r'([],[(([x],[dog(x)]) -> ([y],[walks(y), PRO(y)]))])')))\n    print(resolve_anaphora(dexpr(r'(([x,y],[]) + ([],[PRO(x)]))')))\n\n    print('='*20 + 'Test pretty_print()' + '='*20)\n    dexpr(r\"([],[])\").pretty_print()\n    dexpr(r\"([],[([x],[big(x), dog(x)]) -> ([],[bark(x)]) -([x],[walk(x)])])\").pretty_print()\n    dexpr(r\"([x,y],[x=y]) + ([z],[dog(z), walk(z)])\").pretty_print()\n    dexpr(r\"([],[([x],[]) | ([y],[]) | ([z],[dog(z), walk(z)])])\").pretty_print()\n    dexpr(r\"\\P.\\Q.(([x],[]) + P(x) + Q(x))(\\x.([],[dog(x)]))\").pretty_print()\n\n\ndef test_draw():\n    try:\n        from six.moves.tkinter import Tk\n    except ImportError:\n        from nose import SkipTest\n        raise SkipTest(\"tkinter is required, but it's not available.\")\n\n    expressions = [\n            r'x',\n            r'([],[])',\n            r'([x],[])',\n            r'([x],[man(x)])',\n\n            r'([x,y],[sees(x,y)])',\n            r'([x],[man(x), walks(x)])',\n            r'\\x.([],[man(x), walks(x)])',\n            r'\\x y.([],[sees(x,y)])',\n            r'([],[(([],[walks(x)]) + ([],[runs(x)]))])',\n\n            r'([x],[man(x), -([],[walks(x)])])',\n            r'([],[(([x],[man(x)]) -> ([],[walks(x)]))])'\n            ]\n\n    for e in expressions:\n        d = DrtExpression.fromstring(e)\n        d.draw()\n\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\sem\\drt_glue_demo": [".py", "\ntry:\n    from six.moves.tkinter import (Button, Frame, IntVar, Label, Listbox, Menu,\n                                   Scrollbar, Tk)\n    from six.moves.tkinter_font import Font\n    from nltk.draw.util import CanvasFrame, ShowText\n\nexcept ImportError:\n\nfrom nltk.util import in_idle\nfrom nltk.tag import RegexpTagger\nfrom nltk.parse import MaltParser\nfrom nltk.sem.logic import Variable\nfrom nltk.sem.drt import DrsDrawer, DrtVariableExpression\nfrom nltk.sem.glue import DrtGlue\n\n\nclass DrtGlueDemo(object):\n    def __init__(self, examples):\n        self._top = Tk()\n        self._top.title('DRT Glue Demo')\n\n        self._init_bindings()\n\n        self._init_fonts(self._top)\n\n        self._examples = examples\n        self._readingCache = [None for example in examples]\n\n        self._show_grammar = IntVar(self._top)\n        self._show_grammar.set(1)\n\n        self._curExample = -1\n        self._readings = []\n        self._drs = None\n        self._drsWidget = None\n        self._error = None\n\n        self._init_glue()\n\n        self._init_menubar(self._top)\n        self._init_buttons(self._top)\n        self._init_exampleListbox(self._top)\n        self._init_readingListbox(self._top)\n        self._init_canvas(self._top)\n\n        self._canvas.bind('<Configure>', self._configure)\n\n\n    def _init_glue(self):\n        tagger = RegexpTagger(\n            [('^(David|Mary|John)$', 'NNP'),\n             ('^(walks|sees|eats|chases|believes|gives|sleeps|chases|persuades|tries|seems|leaves)$', 'VB'),\n             ('^(go|order|vanish|find|approach)$', 'VB'),\n             ('^(a)$', 'ex_quant'),\n             ('^(every)$', 'univ_quant'),\n             ('^(sandwich|man|dog|pizza|unicorn|cat|senator)$', 'NN'),\n             ('^(big|gray|former)$', 'JJ'),\n             ('^(him|himself)$', 'PRP')\n        ])\n\n        depparser = MaltParser(tagger=tagger)\n        self._glue = DrtGlue(depparser=depparser, remove_duplicates=False)\n\n    def _init_fonts(self, root):\n        self._sysfont = Font(font=Button()[\"font\"])\n        root.option_add(\"*Font\", self._sysfont)\n\n        self._size = IntVar(root)\n        self._size.set(self._sysfont.cget('size'))\n\n        self._boldfont = Font(family='helvetica', weight='bold',\n                                    size=self._size.get())\n        self._font = Font(family='helvetica',\n                                    size=self._size.get())\n        if self._size.get() < 0: big = self._size.get()-2\n        else: big = self._size.get()+2\n        self._bigfont = Font(family='helvetica', weight='bold',\n                                    size=big)\n\n    def _init_exampleListbox(self, parent):\n        self._exampleFrame = listframe = Frame(parent)\n        self._exampleFrame.pack(fill='both', side='left', padx=2)\n        self._exampleList_label = Label(self._exampleFrame, font=self._boldfont,\n                                     text='Examples')\n        self._exampleList_label.pack()\n        self._exampleList = Listbox(self._exampleFrame, selectmode='single',\n                                 relief='groove', background='white',\n                                 foreground='#909090', font=self._font,\n                                 selectforeground='#004040',\n                                 selectbackground='#c0f0c0')\n\n        self._exampleList.pack(side='right', fill='both', expand=1)\n\n        for example in self._examples:\n            self._exampleList.insert('end', ('  %s' % example))\n        self._exampleList.config(height=min(len(self._examples), 25), width=40)\n\n        if len(self._examples) > 25:\n            listscroll = Scrollbar(self._exampleFrame,\n                                   orient='vertical')\n            self._exampleList.config(yscrollcommand = listscroll.set)\n            listscroll.config(command=self._exampleList.yview)\n            listscroll.pack(side='left', fill='y')\n\n        self._exampleList.bind('<<ListboxSelect>>', self._exampleList_select)\n\n    def _init_readingListbox(self, parent):\n        self._readingFrame = listframe = Frame(parent)\n        self._readingFrame.pack(fill='both', side='left', padx=2)\n        self._readingList_label = Label(self._readingFrame, font=self._boldfont,\n                                     text='Readings')\n        self._readingList_label.pack()\n        self._readingList = Listbox(self._readingFrame, selectmode='single',\n                                 relief='groove', background='white',\n                                 foreground='#909090', font=self._font,\n                                 selectforeground='#004040',\n                                 selectbackground='#c0f0c0')\n\n        self._readingList.pack(side='right', fill='both', expand=1)\n\n        listscroll = Scrollbar(self._readingFrame,\n                               orient='vertical')\n        self._readingList.config(yscrollcommand = listscroll.set)\n        listscroll.config(command=self._readingList.yview)\n        listscroll.pack(side='right', fill='y')\n\n        self._populate_readingListbox()\n\n    def _populate_readingListbox(self):\n        self._readingList.delete(0, 'end')\n        for i in range(len(self._readings)):\n            self._readingList.insert('end', ('  %s' % (i+1)))\n        self._readingList.config(height=min(len(self._readings), 25), width=5)\n\n        self._readingList.bind('<<ListboxSelect>>', self._readingList_select)\n\n    def _init_bindings(self):\n        self._top.bind('<Control-q>', self.destroy)\n        self._top.bind('<Control-x>', self.destroy)\n        self._top.bind('<Escape>', self.destroy)\n        self._top.bind('n', self.next)\n        self._top.bind('<space>', self.next)\n        self._top.bind('p', self.prev)\n        self._top.bind('<BackSpace>', self.prev)\n\n    def _init_buttons(self, parent):\n        self._buttonframe = buttonframe = Frame(parent)\n        buttonframe.pack(fill='none', side='bottom', padx=3, pady=2)\n        Button(buttonframe, text='Prev',\n               background='#90c0d0', foreground='black',\n               command=self.prev,).pack(side='left')\n        Button(buttonframe, text='Next',\n               background='#90c0d0', foreground='black',\n               command=self.next,).pack(side='left')\n\n    def _configure(self, event):\n        self._autostep = 0\n        (x1, y1, x2, y2) = self._cframe.scrollregion()\n        y2 = event.height - 6\n        self._canvas['scrollregion'] = '%d %d %d %d' % (x1,y1,x2,y2)\n        self._redraw()\n\n    def _init_canvas(self, parent):\n        self._cframe = CanvasFrame(parent, background='white',\n                                   closeenough=10,\n                                   border=2, relief='sunken')\n        self._cframe.pack(expand=1, fill='both', side='top', pady=2)\n        canvas = self._canvas = self._cframe.canvas()\n\n        self._tree = None\n        self._textwidgets = []\n        self._textline = None\n\n    def _init_menubar(self, parent):\n        menubar = Menu(parent)\n\n        filemenu = Menu(menubar, tearoff=0)\n        filemenu.add_command(label='Exit', underline=1,\n                             command=self.destroy, accelerator='q')\n        menubar.add_cascade(label='File', underline=0, menu=filemenu)\n\n        actionmenu = Menu(menubar, tearoff=0)\n        actionmenu.add_command(label='Next', underline=0,\n                               command=self.next, accelerator='n, Space')\n        actionmenu.add_command(label='Previous', underline=0,\n                               command=self.prev, accelerator='p, Backspace')\n        menubar.add_cascade(label='Action', underline=0, menu=actionmenu)\n\n        optionmenu = Menu(menubar, tearoff=0)\n        optionmenu.add_checkbutton(label='Remove Duplicates', underline=0,\n                                   variable=self._glue.remove_duplicates,\n                                   command=self._toggle_remove_duplicates,\n                                   accelerator='r')\n        menubar.add_cascade(label='Options', underline=0, menu=optionmenu)\n\n        viewmenu = Menu(menubar, tearoff=0)\n        viewmenu.add_radiobutton(label='Tiny', variable=self._size,\n                                 underline=0, value=10, command=self.resize)\n        viewmenu.add_radiobutton(label='Small', variable=self._size,\n                                 underline=0, value=12, command=self.resize)\n        viewmenu.add_radiobutton(label='Medium', variable=self._size,\n                                 underline=0, value=14, command=self.resize)\n        viewmenu.add_radiobutton(label='Large', variable=self._size,\n                                 underline=0, value=18, command=self.resize)\n        viewmenu.add_radiobutton(label='Huge', variable=self._size,\n                                 underline=0, value=24, command=self.resize)\n        menubar.add_cascade(label='View', underline=0, menu=viewmenu)\n\n        helpmenu = Menu(menubar, tearoff=0)\n        helpmenu.add_command(label='About', underline=0,\n                             command=self.about)\n        menubar.add_cascade(label='Help', underline=0, menu=helpmenu)\n\n        parent.config(menu=menubar)\n\n\n    def _redraw(self):\n        canvas = self._canvas\n\n        if self._drsWidget is not None:\n            self._drsWidget.clear()\n\n        if self._drs:\n            self._drsWidget = DrsWidget( self._canvas, self._drs )\n            self._drsWidget.draw()\n\n        if self._error:\n            self._drsWidget = DrsWidget( self._canvas, self._error )\n            self._drsWidget.draw()\n\n\n    def destroy(self, *e):\n        self._autostep = 0\n        if self._top is None: return\n        self._top.destroy()\n        self._top = None\n\n    def prev(self, *e):\n        selection = self._readingList.curselection()\n        readingListSize = self._readingList.size()\n\n        if readingListSize > 0:\n            if len(selection) == 1:\n                index = int(selection[0])\n\n                if index <= 0:\n                    self._select_previous_example()\n                else:\n                    self._readingList_store_selection(index-1)\n\n            else:\n                self._readingList_store_selection(readingListSize-1)\n\n        else:\n            self._select_previous_example()\n\n\n    def _select_previous_example(self):\n        if self._curExample > 0:\n            self._exampleList_store_selection(self._curExample-1)\n        else:\n            self._exampleList_store_selection(len(self._examples)-1)\n\n    def next(self, *e):\n        selection = self._readingList.curselection()\n        readingListSize = self._readingList.size()\n\n        if readingListSize > 0:\n            if len(selection) == 1:\n                index = int(selection[0])\n\n                if index >= (readingListSize-1):\n                    self._select_next_example()\n                else:\n                    self._readingList_store_selection(index+1)\n\n            else:\n                self._readingList_store_selection(0)\n\n        else:\n            self._select_next_example()\n\n    def _select_next_example(self):\n        if self._curExample < len(self._examples)-1:\n            self._exampleList_store_selection(self._curExample+1)\n        else:\n            self._exampleList_store_selection(0)\n\n\n    def about(self, *e):\n        ABOUT = (\"NLTK Discourse Representation Theory (DRT) Glue Semantics Demo\\n\"+\n                 \"Written by Daniel H. Garrette\")\n        TITLE = 'About: NLTK DRT Glue Demo'\n        try:\n            from six.moves.tkinter_messagebox import Message\n            Message(message=ABOUT, title=TITLE).show()\n        except:\n            ShowText(self._top, TITLE, ABOUT)\n\n    def postscript(self, *e):\n        self._autostep = 0\n        self._cframe.print_to_file()\n\n    def mainloop(self, *args, **kwargs):\n        if in_idle(): return\n        self._top.mainloop(*args, **kwargs)\n\n    def resize(self, size=None):\n        if size is not None: self._size.set(size)\n        size = self._size.get()\n        self._font.configure(size=-(abs(size)))\n        self._boldfont.configure(size=-(abs(size)))\n        self._sysfont.configure(size=-(abs(size)))\n        self._bigfont.configure(size=-(abs(size+2)))\n        self._redraw()\n\n    def _toggle_remove_duplicates(self):\n        self._glue.remove_duplicates = not self._glue.remove_duplicates\n\n        self._exampleList.selection_clear(0, 'end')\n        self._readings = []\n        self._populate_readingListbox()\n        self._readingCache = [None for ex in self._examples]\n        self._curExample = -1\n        self._error = None\n\n        self._drs = None\n        self._redraw()\n\n\n    def _exampleList_select(self, event):\n        selection = self._exampleList.curselection()\n        if len(selection) != 1: return\n        self._exampleList_store_selection(int(selection[0]))\n\n    def _exampleList_store_selection(self, index):\n        self._curExample = index\n        example = self._examples[index]\n\n        self._exampleList.selection_clear(0, 'end')\n        if example:\n            cache = self._readingCache[index]\n            if cache:\n                if isinstance(cache, list):\n                    self._readings = cache\n                    self._error = None\n                else:\n                    self._readings = []\n                    self._error = cache\n            else:\n                try:\n                    self._readings = self._glue.parse_to_meaning(example)\n                    self._error = None\n                    self._readingCache[index] = self._readings\n                except Exception as e:\n                    self._readings = []\n                    self._error = DrtVariableExpression(Variable('Error: ' + str(e)))\n                    self._readingCache[index] = self._error\n\n                    self._exampleList.delete(index)\n                    self._exampleList.insert(index, ('  %s *' % example))\n                    self._exampleList.config(height=min(len(self._examples), 25), width=40)\n\n            self._populate_readingListbox()\n\n            self._exampleList.selection_set(index)\n\n            self._drs = None\n            self._redraw()\n\n\n    def _readingList_select(self, event):\n        selection = self._readingList.curselection()\n        if len(selection) != 1: return\n        self._readingList_store_selection(int(selection[0]))\n\n    def _readingList_store_selection(self, index):\n        reading = self._readings[index]\n\n        self._readingList.selection_clear(0, 'end')\n        if reading:\n            self._readingList.selection_set(index)\n\n            self._drs = reading.simplify().normalize().resolve_anaphora()\n\n            self._redraw()\n\n\nclass DrsWidget(object):\n    def __init__(self, canvas, drs, **attribs):\n        self._drs = drs\n        self._canvas = canvas\n        canvas.font = Font(font=canvas.itemcget(canvas.create_text(0, 0, text=''), 'font'))\n        canvas._BUFFER = 3\n        self.bbox = (0, 0, 0, 0)\n\n    def draw(self):\n        (right, bottom) = DrsDrawer(self._drs, canvas=self._canvas).draw()\n        self.bbox = (0, 0, right+1, bottom+1)\n\n    def clear(self):\n        self._canvas.create_rectangle(self.bbox, fill=\"white\", width=\"0\" )\n\ndef demo():\n    examples = ['John walks',\n                'David sees Mary',\n                'David eats a sandwich',\n                'every man chases a dog',\n                'John chases himself',\n                ]\n    DrtGlueDemo(examples).mainloop()\n\nif __name__ == '__main__': demo()\n"], "nltk\\sem\\evaluate": [".py", "\n\nfrom __future__ import print_function, unicode_literals\n\nfrom pprint import pformat\nimport inspect\nimport textwrap\nimport re\nimport sys\n\nfrom six import string_types\n\nfrom nltk.decorators import decorator # this used in code that is commented out\nfrom nltk.compat import python_2_unicode_compatible\n\nfrom nltk.sem.logic import (AbstractVariableExpression, AllExpression, Expression,\n                            AndExpression, ApplicationExpression, EqualityExpression,\n                            ExistsExpression, IffExpression, ImpExpression,\n                            IndividualVariableExpression, LambdaExpression,\n                            NegatedExpression, OrExpression,\n                            Variable, is_indvar)\n\n\nclass Error(Exception): pass\n\nclass Undefined(Error):  pass\n\ndef trace(f, *args, **kw):\n    if sys.version_info[0] >= 3:\n        argspec = inspect.getfullargspec(f)\n    else:\n        argspec = inspect.getargspec(f)\n    d = dict(zip(argspec[0], args))\n    if d.pop('trace', None):\n        print()\n        for item in d.items():\n            print(\"%s => %s\" % item)\n    return f(*args, **kw)\n\ndef is_rel(s):\n    if len(s) == 0:\n        return True\n    elif all(isinstance(el, tuple) for el in s) and len(max(s))==len(min(s)):\n        return True\n    else:\n        raise ValueError(\"Set %r contains sequences of different lengths\" % s)\n\ndef set2rel(s):\n    new = set()\n    for elem in s:\n        if isinstance(elem, string_types):\n            new.add((elem,))\n        elif isinstance(elem, int):\n            new.add((str(elem,)))\n        else:\n            new.add(elem)\n    return new\n\ndef arity(rel):\n    if len(rel) == 0:\n        return 0\n    return len(list(rel)[0])\n\n\n@python_2_unicode_compatible\nclass Valuation(dict):\n    def __init__(self, xs):\n        super(Valuation, self).__init__()\n        for (sym, val) in xs:\n            if isinstance(val, string_types) or isinstance(val, bool):\n                self[sym] = val\n            elif isinstance(val, set):\n                self[sym] = set2rel(val)\n            else:\n                msg = textwrap.fill(\"Error in initializing Valuation. \"\n                                    \"Unrecognized value for symbol '%s':\\n%s\" % (sym, val), width=66)\n\n                raise ValueError(msg)\n\n    def __getitem__(self, key):\n        if key in self:\n            return dict.__getitem__(self, key)\n        else:\n            raise Undefined(\"Unknown expression: '%s'\" % key)\n\n    def __str__(self):\n        return pformat(self)\n\n    @property\n    def domain(self):\n        dom = []\n        for val in self.values():\n            if isinstance(val, string_types):\n                dom.append(val)\n            elif not isinstance(val, bool):\n                dom.extend([elem for tuple_ in val for elem in tuple_ if elem is not None])\n        return set(dom)\n\n    @property\n    def symbols(self):\n        return sorted(self.keys())\n\n    @classmethod\n    def fromstring(cls, s):\n        return read_valuation(s)\n\n\n_VAL_SPLIT_RE = re.compile(r'\\s*=+>\\s*')\n_ELEMENT_SPLIT_RE = re.compile(r'\\s*,\\s*')\n_TUPLES_RE = re.compile(r\"\"\"\\s*\n                                (\\([^)]+\\))  # tuple-expression\n                                \\s*\"\"\", re.VERBOSE)\n\ndef _read_valuation_line(s):\n    \"\"\"\n    Read a line in a valuation file.\n\n    Lines are expected to be of the form::\n\n      noosa => n\n      girl => {g1, g2}\n      chase => {(b1, g1), (b2, g1), (g1, d1), (g2, d2)}\n\n    :param s: input line\n    :type s: str\n    :return: a pair (symbol, value)\n    :rtype: tuple\n    \"\"\"\n    pieces = _VAL_SPLIT_RE.split(s)\n    symbol = pieces[0]\n    value = pieces[1]\n    if value.startswith('{'):\n        value = value[1:-1]\n        tuple_strings = _TUPLES_RE.findall(value)\n        if tuple_strings:\n            set_elements = []\n            for ts in tuple_strings:\n                ts = ts[1:-1]\n                element = tuple(_ELEMENT_SPLIT_RE.split(ts))\n                set_elements.append(element)\n        else:\n            set_elements = _ELEMENT_SPLIT_RE.split(value)\n        value = set(set_elements)\n    return symbol, value\n\ndef read_valuation(s, encoding=None):\n    \"\"\"\n    Convert a valuation string into a valuation.\n\n    :param s: a valuation string\n    :type s: str\n    :param encoding: the encoding of the input string, if it is binary\n    :type encoding: str\n    :return: a ``nltk.sem`` valuation\n    :rtype: Valuation\n    \"\"\"\n    if encoding is not None:\n        s = s.decode(encoding)\n    statements = []\n    for linenum, line in enumerate(s.splitlines()):\n        line = line.strip()\n        if line.startswith('#') or line=='': continue\n        try:\n            statements.append(_read_valuation_line(line))\n        except ValueError:\n            raise ValueError('Unable to parse line %s: %s' % (linenum, line))\n    return Valuation(statements)\n\n\n@python_2_unicode_compatible\nclass Assignment(dict):\n    \"\"\"\n    A dictionary which represents an assignment of values to variables.\n\n    An assigment can only assign values from its domain.\n\n    If an unknown expression *a* is passed to a model *M*\\ 's\n    interpretation function *i*, *i* will first check whether *M*\\ 's\n    valuation assigns an interpretation to *a* as a constant, and if\n    this fails, *i* will delegate the interpretation of *a* to\n    *g*. *g* only assigns values to individual variables (i.e.,\n    members of the class ``IndividualVariableExpression`` in the ``logic``\n    module. If a variable is not assigned a value by *g*, it will raise\n    an ``Undefined`` exception.\n\n    A variable *Assignment* is a mapping from individual variables to\n    entities in the domain. Individual variables are usually indicated\n    with the letters ``'x'``, ``'y'``, ``'w'`` and ``'z'``, optionally\n    followed by an integer (e.g., ``'x0'``, ``'y332'``).  Assignments are\n    created using the ``Assignment`` constructor, which also takes the\n    domain as a parameter.\n\n        >>> from nltk.sem.evaluate import Assignment\n        >>> dom = set(['u1', 'u2', 'u3', 'u4'])\n        >>> g3 = Assignment(dom, [('x', 'u1'), ('y', 'u2')])\n        >>> g3 == {'x': 'u1', 'y': 'u2'}\n        True\n\n    There is also a ``print`` format for assignments which uses a notation\n    closer to that in logic textbooks:\n\n        >>> print(g3)\n        g[u1/x][u2/y]\n\n    It is also possible to update an assignment using the ``add`` method:\n\n        >>> dom = set(['u1', 'u2', 'u3', 'u4'])\n        >>> g4 = Assignment(dom)\n        >>> g4.add('x', 'u1')\n        {'x': 'u1'}\n\n    With no arguments, ``purge()`` is equivalent to ``clear()`` on a dictionary:\n\n        >>> g4.purge()\n        >>> g4\n        {}\n\n    :param domain: the domain of discourse\n    :type domain: set\n    :param assign: a list of (varname, value) associations\n    :type assign: list\n    \"\"\"\n\n    def __init__(self, domain, assign=None):\n        super(Assignment, self).__init__()\n        self.domain = domain\n        if assign:\n            for (var, val) in assign:\n                assert val in self.domain,\\\n                       \"'%s' is not in the domain: %s\" % (val, self.domain)\n                assert is_indvar(var),\\\n                       \"Wrong format for an Individual Variable: '%s'\" % var\n                self[var] = val\n        self.variant = None\n        self._addvariant()\n\n    def __getitem__(self, key):\n        if key in self:\n            return dict.__getitem__(self, key)\n        else:\n            raise Undefined(\"Not recognized as a variable: '%s'\" % key)\n\n    def copy(self):\n        new = Assignment(self.domain)\n        new.update(self)\n        return new\n\n    def purge(self, var=None):\n        \"\"\"\n        Remove one or all keys (i.e. logic variables) from an\n        assignment, and update ``self.variant``.\n\n        :param var: a Variable acting as a key for the assignment.\n        \"\"\"\n        if var:\n            del self[var]\n        else:\n            self.clear()\n        self._addvariant()\n        return None\n\n    def __str__(self):\n        \"\"\"\n        Pretty printing for assignments. {'x', 'u'} appears as 'g[u/x]'\n        \"\"\"\n        gstring = \"g\"\n        variant = sorted(self.variant)\n        for (val, var) in variant:\n            gstring += \"[%s/%s]\" % (val, var)\n        return gstring\n\n    def _addvariant(self):\n        \"\"\"\n        Create a more pretty-printable version of the assignment.\n        \"\"\"\n        list_ = []\n        for item in self.items():\n            pair = (item[1], item[0])\n            list_.append(pair)\n        self.variant = list_\n        return None\n\n    def add(self, var, val):\n        \"\"\"\n        Add a new variable-value pair to the assignment, and update\n        ``self.variant``.\n\n        \"\"\"\n        assert val in self.domain,\\\n               \"%s is not in the domain %s\" % (val, self.domain)\n        assert is_indvar(var),\\\n               \"Wrong format for an Individual Variable: '%s'\" % var\n        self[var] = val\n        self._addvariant()\n        return self\n\n\n@python_2_unicode_compatible\nclass Model(object):\n    \"\"\"\n    A first order model is a domain *D* of discourse and a valuation *V*.\n\n    A domain *D* is a set, and a valuation *V* is a map that associates\n    expressions with values in the model.\n    The domain of *V* should be a subset of *D*.\n\n    Construct a new ``Model``.\n\n    :type domain: set\n    :param domain: A set of entities representing the domain of discourse of the model.\n    :type valuation: Valuation\n    :param valuation: the valuation of the model.\n    :param prop: If this is set, then we are building a propositional\\\n    model and don't require the domain of *V* to be subset of *D*.\n    \"\"\"\n\n    def __init__(self, domain, valuation):\n        assert isinstance(domain, set)\n        self.domain = domain\n        self.valuation = valuation\n        if not domain.issuperset(valuation.domain):\n            raise Error(\"The valuation domain, %s, must be a subset of the model's domain, %s\"\\\n                  % (valuation.domain, domain))\n\n    def __repr__(self):\n        return \"(%r, %r)\" % (self.domain, self.valuation)\n\n    def __str__(self):\n        return \"Domain = %s,\\nValuation = \\n%s\" % (self.domain, self.valuation)\n\n    def evaluate(self, expr, g, trace=None):\n        \"\"\"\n        Read input expressions, and provide a handler for ``satisfy``\n        that blocks further propagation of the ``Undefined`` error.\n        :param expr: An ``Expression`` of ``logic``.\n        :type g: Assignment\n        :param g: an assignment to individual variables.\n        :rtype: bool or 'Undefined'\n        \"\"\"\n        try:\n            parsed = Expression.fromstring(expr)\n            value = self.satisfy(parsed, g, trace=trace)\n            if trace:\n                print()\n                print(\"'%s' evaluates to %s under M, %s\" %  (expr, value, g))\n            return value\n        except Undefined:\n            if trace:\n                print()\n                print(\"'%s' is undefined under M, %s\" %  (expr, g))\n            return 'Undefined'\n\n\n    def satisfy(self, parsed, g, trace=None):\n        \"\"\"\n        Recursive interpretation function for a formula of first-order logic.\n\n        Raises an ``Undefined`` error when ``parsed`` is an atomic string\n        but is not a symbol or an individual variable.\n\n        :return: Returns a truth value or ``Undefined`` if ``parsed`` is\\\n        complex, and calls the interpretation function ``i`` if ``parsed``\\\n        is atomic.\n\n        :param parsed: An expression of ``logic``.\n        :type g: Assignment\n        :param g: an assignment to individual variables.\n        \"\"\"\n\n        if isinstance(parsed, ApplicationExpression):\n            function, arguments = parsed.uncurry()\n            if isinstance(function, AbstractVariableExpression):\n                funval = self.satisfy(function, g)\n                argvals = tuple(self.satisfy(arg, g) for arg in arguments)\n                return argvals in funval\n            else:\n                funval = self.satisfy(parsed.function, g)\n                argval = self.satisfy(parsed.argument, g)\n                return funval[argval]\n        elif isinstance(parsed, NegatedExpression):\n            return not self.satisfy(parsed.term, g)\n        elif isinstance(parsed, AndExpression):\n            return self.satisfy(parsed.first, g) and \\\n                   self.satisfy(parsed.second, g)\n        elif isinstance(parsed, OrExpression):\n            return self.satisfy(parsed.first, g) or \\\n                   self.satisfy(parsed.second, g)\n        elif isinstance(parsed, ImpExpression):\n            return (not self.satisfy(parsed.first, g)) or \\\n                   self.satisfy(parsed.second, g)\n        elif isinstance(parsed, IffExpression):\n            return self.satisfy(parsed.first, g) == \\\n                   self.satisfy(parsed.second, g)\n        elif isinstance(parsed, EqualityExpression):\n            return self.satisfy(parsed.first, g) == \\\n                   self.satisfy(parsed.second, g)\n        elif isinstance(parsed, AllExpression):\n            new_g = g.copy()\n            for u in self.domain:\n                new_g.add(parsed.variable.name, u)\n                if not self.satisfy(parsed.term, new_g):\n                    return False\n            return True\n        elif isinstance(parsed, ExistsExpression):\n            new_g = g.copy()\n            for u in self.domain:\n                new_g.add(parsed.variable.name, u)\n                if self.satisfy(parsed.term, new_g):\n                    return True\n            return False\n        elif isinstance(parsed, LambdaExpression):\n            cf = {}\n            var = parsed.variable.name\n            for u in self.domain:\n                val = self.satisfy(parsed.term, g.add(var, u))\n                cf[u] = val\n            return cf\n        else:\n            return self.i(parsed, g, trace)\n\n    def i(self, parsed, g, trace=False):\n        \"\"\"\n        An interpretation function.\n\n        Assuming that ``parsed`` is atomic:\n\n        - if ``parsed`` is a non-logical constant, calls the valuation *V*\n        - else if ``parsed`` is an individual variable, calls assignment *g*\n        - else returns ``Undefined``.\n\n        :param parsed: an ``Expression`` of ``logic``.\n        :type g: Assignment\n        :param g: an assignment to individual variables.\n        :return: a semantic value\n        \"\"\"\n        if parsed.variable.name in self.valuation.symbols:\n            return self.valuation[parsed.variable.name]\n        elif isinstance(parsed, IndividualVariableExpression):\n            return g[parsed.variable.name]\n\n        else:\n            raise Undefined(\"Can't find a value for %s\" % parsed)\n\n    def satisfiers(self, parsed, varex, g, trace=None, nesting=0):\n        \"\"\"\n        Generate the entities from the model's domain that satisfy an open formula.\n\n        :param parsed: an open formula\n        :type parsed: Expression\n        :param varex: the relevant free individual variable in ``parsed``.\n        :type varex: VariableExpression or str\n        :param g: a variable assignment\n        :type g:  Assignment\n        :return: a set of the entities that satisfy ``parsed``.\n        \"\"\"\n\n        spacer = '   '\n        indent = spacer + (spacer * nesting)\n        candidates = []\n\n        if isinstance(varex, string_types):\n            var = Variable(varex)\n        else:\n            var = varex\n\n        if var in parsed.free():\n            if trace:\n                print()\n                print((spacer * nesting) + \"Open formula is '%s' with assignment %s\" % (parsed, g))\n            for u in self.domain:\n                new_g = g.copy()\n                new_g.add(var.name, u)\n                if trace and trace > 1:\n                    lowtrace = trace-1\n                else:\n                    lowtrace = 0\n                value = self.satisfy(parsed, new_g, lowtrace)\n\n                if trace:\n                    print(indent + \"(trying assignment %s)\" % new_g)\n\n                if value == False:\n                    if trace:\n                        print(indent + \"value of '%s' under %s is False\" % (parsed, new_g))\n\n                else:\n                    candidates.append(u)\n                    if trace:\n                        print(indent + \"value of '%s' under %s is %s\" % (parsed, new_g, value))\n\n            result = set(c for c in candidates)\n        else:\n            raise Undefined(\"%s is not free in %s\" % (var.name, parsed))\n\n        return result\n\n\n\n\n\nmult = 30\n\ndef propdemo(trace=None):\n\n    global val1, dom1, m1, g1\n    val1 = Valuation([('P', True), ('Q', True), ('R', False)])\n    dom1 = set([])\n    m1 = Model(dom1, val1)\n    g1 = Assignment(dom1)\n\n    print()\n    print('*' * mult)\n    print(\"Propositional Formulas Demo\")\n    print('*' * mult)\n    print('(Propositional constants treated as nullary predicates)')\n    print()\n    print(\"Model m1:\\n\", m1)\n    print('*' * mult)\n    sentences = [\n    '(P & Q)',\n    '(P & R)',\n    '- P',\n    '- R',\n    '- - P',\n    '- (P & R)',\n    '(P | R)',\n    '(R | P)',\n    '(R | R)',\n    '(- P | R)',\n    '(P | - P)',\n    '(P -> Q)',\n    '(P -> R)',\n    '(R -> P)',\n    '(P <-> P)',\n    '(R <-> R)',\n    '(P <-> R)',\n    ]\n\n    for sent in sentences:\n        if trace:\n            print()\n            m1.evaluate(sent, g1, trace)\n        else:\n            print(\"The value of '%s' is: %s\" % (sent, m1.evaluate(sent, g1)))\n\n\ndef folmodel(quiet=False, trace=None):\n\n    global val2, v2, dom2, m2, g2\n\n    v2 = [('adam', 'b1'), ('betty', 'g1'), ('fido', 'd1'),\\\n         ('girl', set(['g1', 'g2'])), ('boy', set(['b1', 'b2'])), ('dog', set(['d1'])),\n         ('love', set([('b1', 'g1'), ('b2', 'g2'), ('g1', 'b1'), ('g2', 'b1')]))]\n    val2 = Valuation(v2)\n    dom2 = val2.domain\n    m2 = Model(dom2, val2)\n    g2 = Assignment(dom2, [('x', 'b1'), ('y', 'g2')])\n\n    if not quiet:\n        print()\n        print('*' * mult)\n        print(\"Models Demo\")\n        print(\"*\" * mult)\n        print(\"Model m2:\\n\", \"-\" * 14,\"\\n\", m2)\n        print(\"Variable assignment = \", g2)\n\n        exprs = ['adam', 'boy', 'love', 'walks', 'x', 'y', 'z']\n        parsed_exprs = [Expression.fromstring(e) for e in exprs]\n\n        print()\n        for parsed in parsed_exprs:\n            try:\n                print(\"The interpretation of '%s' in m2 is %s\" % (parsed, m2.i(parsed, g2)))\n            except Undefined:\n                print(\"The interpretation of '%s' in m2 is Undefined\" % parsed)\n\n\n        applications = [('boy', ('adam')), ('walks', ('adam',)), ('love', ('adam', 'y')), ('love', ('y', 'adam'))]\n\n        for (fun, args) in applications:\n            try:\n                funval = m2.i(Expression.fromstring(fun), g2)\n                argsval = tuple(m2.i(Expression.fromstring(arg), g2) for arg in args)\n                print(\"%s(%s) evaluates to %s\" % (fun, args, argsval in funval))\n            except Undefined:\n                print(\"%s(%s) evaluates to Undefined\" % (fun, args))\n\n\ndef foldemo(trace=None):\n    \"\"\"\n    Interpretation of closed expressions in a first-order model.\n    \"\"\"\n    folmodel(quiet=True)\n\n    print()\n    print('*' * mult)\n    print(\"FOL Formulas Demo\")\n    print('*' * mult)\n\n    formulas = [\n    'love (adam, betty)',\n    '(adam = mia)',\n    '\\\\x. (boy(x) | girl(x))',\n    '\\\\x. boy(x)(adam)',\n    '\\\\x y. love(x, y)',\n    '\\\\x y. love(x, y)(adam)(betty)',\n    '\\\\x y. love(x, y)(adam, betty)',\n    '\\\\x y. (boy(x) & love(x, y))',\n    '\\\\x. exists y. (boy(x) & love(x, y))',\n    'exists z1. boy(z1)',\n    'exists x. (boy(x) &  -(x = adam))',\n    'exists x. (boy(x) & all y. love(y, x))',\n    'all x. (boy(x) | girl(x))',\n    'all x. (girl(x) -> exists y. boy(y) & love(x, y))',    #Every girl loves exists boy.\n    'exists x. (boy(x) & all y. (girl(y) -> love(y, x)))',  #There is exists boy that every girl loves.\n    'exists x. (boy(x) & all y. (girl(y) -> love(x, y)))',  #exists boy loves every girl.\n    'all x. (dog(x) -> - girl(x))',\n    'exists x. exists y. (love(x, y) & love(x, y))'\n    ]\n\n\n    for fmla in formulas:\n        g2.purge()\n        if trace:\n            m2.evaluate(fmla, g2, trace)\n        else:\n            print(\"The value of '%s' is: %s\" % (fmla, m2.evaluate(fmla, g2)))\n\n\n\ndef satdemo(trace=None):\n\n    print()\n    print('*' * mult)\n    print(\"Satisfiers Demo\")\n    print('*' * mult)\n\n    folmodel(quiet=True)\n\n    formulas = [\n               'boy(x)',\n               '(x = x)',\n               '(boy(x) | girl(x))',\n               '(boy(x) & girl(x))',\n               'love(adam, x)',\n               'love(x, adam)',\n               '-(x = adam)',\n               'exists z22. love(x, z22)',\n               'exists y. love(y, x)',\n               'all y. (girl(y) -> love(x, y))',\n               'all y. (girl(y) -> love(y, x))',\n               'all y. (girl(y) -> (boy(x) & love(y, x)))',\n               '(boy(x) & all y. (girl(y) -> love(x, y)))',\n               '(boy(x) & all y. (girl(y) -> love(y, x)))',\n               '(boy(x) & exists y. (girl(y) & love(y, x)))',\n               '(girl(x) -> dog(x))',\n               'all y. (dog(y) -> (x = y))',\n               'exists y. love(y, x)',\n               'exists y. (love(adam, y) & love(y, x))'\n                ]\n\n    if trace:\n        print(m2)\n\n    for fmla in formulas:\n        print(fmla)\n        Expression.fromstring(fmla)\n\n    parsed = [Expression.fromstring(fmla) for fmla in formulas]\n\n    for p in parsed:\n        g2.purge()\n        print(\"The satisfiers of '%s' are: %s\" % (p, m2.satisfiers(p, 'x', g2, trace)))\n\n\ndef demo(num=0, trace=None):\n    \"\"\"\n    Run exists demos.\n\n     - num = 1: propositional logic demo\n     - num = 2: first order model demo (only if trace is set)\n     - num = 3: first order sentences demo\n     - num = 4: satisfaction of open formulas demo\n     - any other value: run all the demos\n\n    :param trace: trace = 1, or trace = 2 for more verbose tracing\n    \"\"\"\n    demos = {\n        1: propdemo,\n        2: folmodel,\n        3: foldemo,\n        4: satdemo}\n\n    try:\n        demos[num](trace=trace)\n    except KeyError:\n        for num in demos:\n            demos[num](trace=trace)\n\n\nif __name__ == \"__main__\":\n    demo(2, trace=0)\n"], "nltk\\sem\\glue": [".py", "from __future__ import print_function, division, unicode_literals\n\nimport os\nfrom itertools import chain\n\nfrom six import string_types\n\nimport nltk\nfrom nltk.internals import Counter\nfrom nltk.tag import UnigramTagger, BigramTagger, TrigramTagger, RegexpTagger\nfrom nltk.sem.logic import (Expression, Variable, VariableExpression,\n                            LambdaExpression, AbstractVariableExpression)\nfrom nltk.compat import python_2_unicode_compatible\nfrom nltk.sem import drt\nfrom nltk.sem import linearlogic\n\nSPEC_SEMTYPES = {'a'       : 'ex_quant',\n                 'an'      : 'ex_quant',\n                 'every'   : 'univ_quant',\n                 'the'     : 'def_art',\n                 'no'      : 'no_quant',\n                 'default' : 'ex_quant'}\n\nOPTIONAL_RELATIONSHIPS = ['nmod', 'vmod', 'punct']\n\n@python_2_unicode_compatible\nclass GlueFormula(object):\n    def __init__(self, meaning, glue, indices=None):\n        if not indices:\n            indices = set()\n\n        if isinstance(meaning, string_types):\n            self.meaning = Expression.fromstring(meaning)\n        elif isinstance(meaning, Expression):\n            self.meaning = meaning\n        else:\n            raise RuntimeError('Meaning term neither string or expression: %s, %s' % (meaning, meaning.__class__))\n\n        if isinstance(glue, string_types):\n            self.glue = linearlogic.LinearLogicParser().parse(glue)\n        elif isinstance(glue, linearlogic.Expression):\n            self.glue = glue\n        else:\n            raise RuntimeError('Glue term neither string or expression: %s, %s' % (glue, glue.__class__))\n\n        self.indices = indices\n\n    def applyto(self, arg):\n        if self.indices & arg.indices: # if the sets are NOT disjoint\n            raise linearlogic.LinearLogicApplicationException(\"'%s' applied to '%s'.  Indices are not disjoint.\" % (self, arg))\n        else: # if the sets ARE disjoint\n            return_indices = (self.indices | arg.indices)\n\n        try:\n            return_glue = linearlogic.ApplicationExpression(self.glue, arg.glue, arg.indices)\n        except linearlogic.LinearLogicApplicationException:\n            raise linearlogic.LinearLogicApplicationException(\"'%s' applied to '%s'\" % (self.simplify(), arg.simplify()))\n\n        arg_meaning_abstracted = arg.meaning\n        if return_indices:\n            for dep in self.glue.simplify().antecedent.dependencies[::-1]: # if self.glue is (A -o B), dep is in A.dependencies\n                arg_meaning_abstracted = self.make_LambdaExpression(Variable('v%s' % dep),\n                                                                    arg_meaning_abstracted)\n        return_meaning = self.meaning.applyto(arg_meaning_abstracted)\n\n        return self.__class__(return_meaning, return_glue, return_indices)\n\n    def make_VariableExpression(self, name):\n        return VariableExpression(name)\n\n    def make_LambdaExpression(self, variable, term):\n        return LambdaExpression(variable, term)\n\n    def lambda_abstract(self, other):\n        assert isinstance(other, GlueFormula)\n        assert isinstance(other.meaning, AbstractVariableExpression)\n        return self.__class__(self.make_LambdaExpression(other.meaning.variable,\n                                                         self.meaning),\n                              linearlogic.ImpExpression(other.glue, self.glue))\n\n    def compile(self, counter=None):\n        if not counter:\n            counter = Counter()\n        (compiled_glue, new_forms) = self.glue.simplify().compile_pos(counter, self.__class__)\n        return new_forms + [self.__class__(self.meaning, compiled_glue, set([counter.get()]))]\n\n    def simplify(self):\n        return self.__class__(self.meaning.simplify(), self.glue.simplify(), self.indices)\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__ and self.meaning == other.meaning and self.glue == other.glue\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __lt__(self, other):\n        return str(self) < str(other)\n\n    def __str__(self):\n        assert isinstance(self.indices, set)\n        accum = '%s : %s' % (self.meaning, self.glue)\n        if self.indices:\n            accum += ' : {' + ', '.join(str(index) for index in self.indices) + '}'\n        return accum\n\n    def __repr__(self):\n        return \"%s\" % self\n\n@python_2_unicode_compatible\nclass GlueDict(dict):\n    def __init__(self, filename, encoding=None):\n        self.filename = filename\n        self.file_encoding = encoding\n        self.read_file()\n\n    def read_file(self, empty_first=True):\n        if empty_first:\n            self.clear()\n\n        try:\n            contents = nltk.data.load(self.filename, format='text', encoding=self.file_encoding)\n        except LookupError as e:\n            try:\n                contents = nltk.data.load('file:' + self.filename, format='text', encoding=self.file_encoding)\n            except LookupError:\n                raise e\n        lines = contents.splitlines()\n\n        for line in lines:                          # example: 'n : (\\\\x.(<word> x), (v-or))'\n            line = line.strip()                     # remove trailing newline\n            if not len(line): continue              # skip empty lines\n            if line[0] == '#': continue             # skip commented out lines\n\n            parts = line.split(' : ', 2)            # ['verb', '(\\\\x.(<word> x), ( subj -o f ))', '[subj]']\n\n            glue_formulas = []\n            paren_count = 0\n            tuple_start = 0\n            tuple_comma = 0\n\n            relationships = None\n\n            if len(parts) > 1:\n                for (i, c) in enumerate(parts[1]):\n                    if c == '(':\n                        if paren_count == 0:             # if it's the first '(' of a tuple\n                            tuple_start = i+1           # then save the index\n                        paren_count += 1\n                    elif c == ')':\n                        paren_count -= 1\n                        if paren_count == 0:             # if it's the last ')' of a tuple\n                            meaning_term =  parts[1][tuple_start:tuple_comma]   # '\\\\x.(<word> x)'\n                            glue_term =     parts[1][tuple_comma+1:i]           # '(v-r)'\n                            glue_formulas.append([meaning_term, glue_term])     # add the GlueFormula to the list\n                    elif c == ',':\n                        if paren_count == 1:             # if it's a comma separating the parts of the tuple\n                            tuple_comma = i             # then save the index\n                    elif c == '#':                      # skip comments at the ends of lines\n                        if paren_count != 0:             # if the line hasn't parsed correctly so far\n                            raise RuntimeError('Formula syntax is incorrect for entry ' + line)\n                        break                           # break to the next line\n\n            if len(parts) > 2:                      #if there is a relationship entry at the end\n                rel_start = parts[2].index('[')+1\n                rel_end   = parts[2].index(']')\n                if rel_start == rel_end:\n                    relationships = frozenset()\n                else:\n                    relationships = frozenset(r.strip() for r in parts[2][rel_start:rel_end].split(','))\n\n            try:\n                start_inheritance = parts[0].index('(')\n                end_inheritance = parts[0].index(')')\n                sem = parts[0][:start_inheritance].strip()\n                supertype = parts[0][start_inheritance+1:end_inheritance]\n            except:\n                sem = parts[0].strip()\n                supertype = None\n\n            if sem not in self:\n                self[sem] = {}\n\n            if relationships is None: #if not specified for a specific relationship set\n                if supertype:\n                    for rels in self[supertype]:\n                        if rels not in self[sem]:\n                            self[sem][rels] = []\n                        glue = self[supertype][rels]\n                        self[sem][rels].extend(glue)\n                        self[sem][rels].extend(glue_formulas) # add the glue formulas to every rel entry\n                else:\n                    if None not in self[sem]:\n                        self[sem][None] = []\n                    self[sem][None].extend(glue_formulas) # add the glue formulas to every rel entry\n            else:\n                if relationships not in self[sem]:\n                    self[sem][relationships] = []\n                if supertype:\n                    self[sem][relationships].extend(self[supertype][relationships])\n                self[sem][relationships].extend(glue_formulas) # add the glue entry to the dictionary\n\n    def __str__(self):\n        accum = ''\n        for pos in self:\n            str_pos = \"%s\" % pos\n            for relset in self[pos]:\n                i = 1\n                for gf in self[pos][relset]:\n                    if i == 1:\n                        accum += str_pos + ': '\n                    else:\n                        accum += ' '*(len(str_pos)+2)\n                    accum += \"%s\" % gf\n                    if relset and i == len(self[pos][relset]):\n                        accum += ' : %s' % relset\n                    accum += '\\n'\n                    i += 1\n        return accum\n\n    def to_glueformula_list(self, depgraph, node=None, counter=None, verbose=False):\n        if node is None:\n            top = depgraph.nodes[0]\n            depList = list(chain(*top['deps'].values()))\n            root = depgraph.nodes[depList[0]]\n\n            return self.to_glueformula_list(depgraph, root, Counter(), verbose)\n\n        glueformulas = self.lookup(node, depgraph, counter)\n        for dep_idx in chain(*node['deps'].values()):\n            dep = depgraph.nodes[dep_idx]\n            glueformulas.extend(self.to_glueformula_list(depgraph, dep, counter, verbose))\n        return glueformulas\n\n    def lookup(self, node, depgraph, counter):\n        semtype_names = self.get_semtypes(node)\n\n        semtype = None\n        for name in semtype_names:\n            if name in self:\n                semtype = self[name]\n                break\n        if semtype is None:\n            return []\n\n        self.add_missing_dependencies(node, depgraph)\n\n        lookup = self._lookup_semtype_option(semtype, node, depgraph)\n\n        if not len(lookup):\n            raise KeyError(\n                \"There is no GlueDict entry for sem type of '%s' \"\n                \"with tag '%s', and rel '%s'\" %\n                (node['word'], node['tag'], node['rel'])\n                )\n\n        return self.get_glueformulas_from_semtype_entry(lookup, node['word'], node, depgraph, counter)\n\n    def add_missing_dependencies(self, node, depgraph):\n        rel = node['rel'].lower()\n\n        if rel == 'main':\n            headnode = depgraph.nodes[node['head']]\n            subj = self.lookup_unique('subj', headnode, depgraph)\n            relation = subj['rel']\n            node['deps'].setdefault(relation,[])\n            node['deps'][relation].append(subj['address'])\n\n    def _lookup_semtype_option(self, semtype, node, depgraph):\n        relationships = frozenset(\n            depgraph.nodes[dep]['rel'].lower()\n            for dep in chain(*node['deps'].values())\n            if depgraph.nodes[dep]['rel'].lower() not in OPTIONAL_RELATIONSHIPS\n        )\n\n        try:\n            lookup = semtype[relationships]\n        except KeyError:\n            best_match = frozenset()\n            for relset_option in set(semtype)-set([None]):\n                if len(relset_option) > len(best_match) and \\\n                   relset_option < relationships:\n                    best_match = relset_option\n            if not best_match:\n                if None in semtype:\n                    best_match = None\n                else:\n                    return None\n            lookup = semtype[best_match]\n\n        return lookup\n\n    def get_semtypes(self, node):\n        rel = node['rel'].lower()\n        word = node['word'].lower()\n\n        if rel == 'spec':\n            if word in SPEC_SEMTYPES:\n                return [SPEC_SEMTYPES[word]]\n            else:\n                return [SPEC_SEMTYPES['default']]\n        elif rel in ['nmod', 'vmod']:\n            return [node['tag'], rel]\n        else:\n            return [node['tag']]\n\n    def get_glueformulas_from_semtype_entry(self, lookup, word, node, depgraph, counter):\n        glueformulas = []\n\n        glueFormulaFactory = self.get_GlueFormula_factory()\n        for meaning, glue in lookup:\n            gf = glueFormulaFactory(self.get_meaning_formula(meaning, word), glue)\n            if not len(glueformulas):\n                gf.word = word\n            else:\n                gf.word = '%s%s' % (word, len(glueformulas)+1)\n\n            gf.glue = self.initialize_labels(gf.glue, node, depgraph, counter.get())\n\n            glueformulas.append(gf)\n        return glueformulas\n\n    def get_meaning_formula(self, generic, word):\n        word = word.replace('.', '')\n        return generic.replace('<word>', word)\n\n    def initialize_labels(self, expr, node, depgraph, unique_index):\n        if isinstance(expr, linearlogic.AtomicExpression):\n            name = self.find_label_name(expr.name.lower(), node, depgraph, unique_index)\n            if name[0].isupper():\n                return linearlogic.VariableExpression(name)\n            else:\n                return linearlogic.ConstantExpression(name)\n        else:\n            return linearlogic.ImpExpression(\n                self.initialize_labels(expr.antecedent, node, depgraph, unique_index),\n                self.initialize_labels(expr.consequent, node, depgraph, unique_index)\n            )\n\n    def find_label_name(self, name, node, depgraph, unique_index):\n        try:\n            dot = name.index('.')\n\n            before_dot = name[:dot]\n            after_dot = name[dot+1:]\n            if before_dot == 'super':\n                return self.find_label_name(after_dot, depgraph.nodes[node['head']], depgraph, unique_index)\n            else:\n                return self.find_label_name(after_dot, self.lookup_unique(before_dot, node, depgraph), depgraph, unique_index)\n        except ValueError:\n            lbl = self.get_label(node)\n            if name == 'f':\n                return lbl\n            elif name == 'v':\n                return '%sv' % lbl\n            elif name == 'r':\n                return '%sr' % lbl\n            elif name == 'super':\n                return self.get_label(depgraph.nodes[node['head']])\n            elif name == 'var':\n                return '%s%s' % (lbl.upper(), unique_index)\n            elif name == 'a':\n                return self.get_label(self.lookup_unique('conja', node, depgraph))\n            elif name == 'b':\n                return self.get_label(self.lookup_unique('conjb', node, depgraph))\n            else:\n                return self.get_label(self.lookup_unique(name, node, depgraph))\n\n    def get_label(self, node):\n        value = node['address']\n\n        letter = ['f','g','h','i','j','k','l','m','n','o','p','q','r','s',\n                  't','u','v','w','x','y','z','a','b','c','d','e'][value-1]\n        num = int(value) // 26\n        if num > 0:\n            return letter + str(num)\n        else:\n            return letter\n\n    def lookup_unique(self, rel, node, depgraph):\n        deps = [\n            depgraph.nodes[dep]\n            for dep in chain(*node['deps'].values())\n            if depgraph.nodes[dep]['rel'].lower() == rel.lower()\n        ]\n\n        if len(deps) == 0:\n            raise KeyError(\"'%s' doesn't contain a feature '%s'\" % (node['word'], rel))\n        elif len(deps) > 1:\n            raise KeyError(\"'%s' should only have one feature '%s'\" % (node['word'], rel))\n        else:\n            return deps[0]\n\n    def get_GlueFormula_factory(self):\n        return GlueFormula\n\n\nclass Glue(object):\n    def __init__(self, semtype_file=None, remove_duplicates=False,\n                 depparser=None, verbose=False):\n        self.verbose = verbose\n        self.remove_duplicates = remove_duplicates\n        self.depparser = depparser\n\n        from nltk import Prover9\n        self.prover = Prover9()\n\n        if semtype_file:\n            self.semtype_file = semtype_file\n        else:\n            self.semtype_file = os.path.join('grammars', 'sample_grammars','glue.semtype')\n\n    def train_depparser(self, depgraphs=None):\n        if depgraphs:\n            self.depparser.train(depgraphs)\n        else:\n            self.depparser.train_from_file(nltk.data.find(\n                os.path.join('grammars', 'sample_grammars',\n                             'glue_train.conll')))\n\n    def parse_to_meaning(self, sentence):\n        readings = []\n        for agenda in self.parse_to_compiled(sentence):\n            readings.extend(self.get_readings(agenda))\n        return readings\n\n    def get_readings(self, agenda):\n        readings = []\n        agenda_length = len(agenda)\n        atomics = dict()\n        nonatomics = dict()\n        while agenda: # is not empty\n            cur = agenda.pop()\n            glue_simp = cur.glue.simplify()\n            if isinstance(glue_simp, linearlogic.ImpExpression): # if cur.glue is non-atomic\n                for key in atomics:\n                    try:\n                        if isinstance(cur.glue, linearlogic.ApplicationExpression):\n                            bindings = cur.glue.bindings\n                        else:\n                            bindings = linearlogic.BindingDict()\n                        glue_simp.antecedent.unify(key, bindings)\n                        for atomic in atomics[key]:\n                            if not (cur.indices & atomic.indices): # if the sets of indices are disjoint\n                                try:\n                                    agenda.append(cur.applyto(atomic))\n                                except linearlogic.LinearLogicApplicationException:\n                                    pass\n                    except linearlogic.UnificationException:\n                        pass\n                try:\n                    nonatomics[glue_simp.antecedent].append(cur)\n                except KeyError:\n                    nonatomics[glue_simp.antecedent] = [cur]\n\n            else: # else cur.glue is atomic\n                for key in nonatomics:\n                    for nonatomic in nonatomics[key]:\n                        try:\n                            if isinstance(nonatomic.glue, linearlogic.ApplicationExpression):\n                                bindings = nonatomic.glue.bindings\n                            else:\n                                bindings = linearlogic.BindingDict()\n                            glue_simp.unify(key, bindings)\n                            if not (cur.indices & nonatomic.indices): # if the sets of indices are disjoint\n                                try:\n                                    agenda.append(nonatomic.applyto(cur))\n                                except linearlogic.LinearLogicApplicationException:\n                                    pass\n                        except linearlogic.UnificationException:\n                            pass\n                try:\n                    atomics[glue_simp].append(cur)\n                except KeyError:\n                    atomics[glue_simp] = [cur]\n\n        for entry in atomics:\n            for gf in atomics[entry]:\n                if len(gf.indices) == agenda_length:\n                    self._add_to_reading_list(gf, readings)\n        for entry in nonatomics:\n            for gf in nonatomics[entry]:\n                if len(gf.indices) == agenda_length:\n                    self._add_to_reading_list(gf, readings)\n        return readings\n\n    def _add_to_reading_list(self, glueformula, reading_list):\n        add_reading = True\n        if self.remove_duplicates:\n            for reading in reading_list:\n                try:\n                    if reading.equiv(glueformula.meaning, self.prover):\n                        add_reading = False\n                        break\n                except Exception as e:\n                    print('Error when checking logical equality of statements', e)\n                    pass\n        if add_reading:\n            reading_list.append(glueformula.meaning)\n\n    def parse_to_compiled(self, sentence):\n        gfls = [self.depgraph_to_glue(dg) for dg in self.dep_parse(sentence)]\n        return [self.gfl_to_compiled(gfl) for gfl in gfls]\n\n    def dep_parse(self, sentence):\n\n        if self.depparser is None:\n            from nltk.parse import MaltParser\n            self.depparser = MaltParser(tagger=self.get_pos_tagger())\n        if not self.depparser._trained:\n            self.train_depparser()\n        return self.depparser.parse(sentence, verbose=self.verbose)\n\n    def depgraph_to_glue(self, depgraph):\n        return self.get_glue_dict().to_glueformula_list(depgraph)\n\n    def get_glue_dict(self):\n        return GlueDict(self.semtype_file)\n\n    def gfl_to_compiled(self, gfl):\n        index_counter = Counter()\n        return_list = []\n        for gf in gfl:\n            return_list.extend(gf.compile(index_counter))\n\n        if self.verbose:\n            print('Compiled Glue Premises:')\n            for cgf in return_list:\n                print(cgf)\n\n        return return_list\n\n    def get_pos_tagger(self):\n        from nltk.corpus import brown\n        regexp_tagger = RegexpTagger(\n            [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers\n             (r'(The|the|A|a|An|an)$', 'AT'),   # articles\n             (r'.*able$', 'JJ'),                # adjectives\n             (r'.*ness$', 'NN'),                # nouns formed from adjectives\n             (r'.*ly$', 'RB'),                  # adverbs\n             (r'.*s$', 'NNS'),                  # plural nouns\n             (r'.*ing$', 'VBG'),                # gerunds\n             (r'.*ed$', 'VBD'),                 # past tense verbs\n             (r'.*', 'NN')                      # nouns (default)\n        ])\n        brown_train = brown.tagged_sents(categories='news')\n        unigram_tagger = UnigramTagger(brown_train, backoff=regexp_tagger)\n        bigram_tagger = BigramTagger(brown_train, backoff=unigram_tagger)\n        trigram_tagger = TrigramTagger(brown_train, backoff=bigram_tagger)\n\n        main_tagger = RegexpTagger(\n            [(r'(A|a|An|an)$', 'ex_quant'),\n             (r'(Every|every|All|all)$', 'univ_quant')\n        ], backoff=trigram_tagger)\n\n        return main_tagger\n\n\nclass DrtGlueFormula(GlueFormula):\n    def __init__(self, meaning, glue, indices=None):\n        if not indices:\n            indices = set()\n\n        if isinstance(meaning, string_types):\n            self.meaning = drt.DrtExpression.fromstring(meaning)\n        elif isinstance(meaning, drt.DrtExpression):\n            self.meaning = meaning\n        else:\n            raise RuntimeError('Meaning term neither string or expression: %s, %s' % (meaning, meaning.__class__))\n\n        if isinstance(glue, string_types):\n            self.glue = linearlogic.LinearLogicParser().parse(glue)\n        elif isinstance(glue, linearlogic.Expression):\n            self.glue = glue\n        else:\n            raise RuntimeError('Glue term neither string or expression: %s, %s' % (glue, glue.__class__))\n\n        self.indices = indices\n\n    def make_VariableExpression(self, name):\n        return drt.DrtVariableExpression(name)\n\n    def make_LambdaExpression(self, variable, term):\n        return drt.DrtLambdaExpression(variable, term)\n\nclass DrtGlueDict(GlueDict):\n    def get_GlueFormula_factory(self):\n        return DrtGlueFormula\n\nclass DrtGlue(Glue):\n    def __init__(self, semtype_file=None, remove_duplicates=False,\n                 depparser=None, verbose=False):\n        if not semtype_file:\n            semtype_file = os.path.join('grammars', 'sample_grammars','drt_glue.semtype')\n        Glue.__init__(self, semtype_file, remove_duplicates, depparser, verbose)\n\n    def get_glue_dict(self):\n        return DrtGlueDict(self.semtype_file)\n\n\ndef demo(show_example=-1):\n    from nltk.parse import MaltParser\n    examples = ['David sees Mary',\n                'David eats a sandwich',\n                'every man chases a dog',\n                'every man believes a dog sleeps',\n                'John gives David a sandwich',\n                'John chases himself']\n\n    print('============== DEMO ==============')\n\n    tagger = RegexpTagger(\n        [('^(David|Mary|John)$', 'NNP'),\n         ('^(sees|eats|chases|believes|gives|sleeps|chases|persuades|tries|seems|leaves)$', 'VB'),\n         ('^(go|order|vanish|find|approach)$', 'VB'),\n         ('^(a)$', 'ex_quant'),\n         ('^(every)$', 'univ_quant'),\n         ('^(sandwich|man|dog|pizza|unicorn|cat|senator)$', 'NN'),\n         ('^(big|gray|former)$', 'JJ'),\n         ('^(him|himself)$', 'PRP')\n    ])\n\n    depparser = MaltParser(tagger=tagger)\n    glue = Glue(depparser=depparser, verbose=False)\n\n    for (i, sentence) in enumerate(examples):\n        if i==show_example or show_example==-1:\n            print('[[[Example %s]]]  %s' % (i, sentence))\n            for reading in glue.parse_to_meaning(sentence.split()):\n                print(reading.simplify())\n            print('')\n\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\sem\\hole": [".py", "\nfrom __future__ import print_function, unicode_literals\n\nfrom functools import reduce\n\nfrom six import itervalues\n\nfrom nltk import compat\nfrom nltk.parse import load_parser\n\nfrom nltk.sem.skolemize import skolemize\nfrom nltk.sem.logic import (AllExpression, AndExpression, ApplicationExpression,\n                            ExistsExpression, IffExpression, ImpExpression,\n                            LambdaExpression, NegatedExpression, OrExpression)\n\n\n\nclass Constants(object):\n    ALL = 'ALL'\n    EXISTS = 'EXISTS'\n    NOT = 'NOT'\n    AND = 'AND'\n    OR = 'OR'\n    IMP = 'IMP'\n    IFF = 'IFF'\n    PRED = 'PRED'\n    LEQ = 'LEQ'\n    HOLE = 'HOLE'\n    LABEL = 'LABEL'\n\n    MAP = {ALL: lambda v, e: AllExpression(v.variable, e),\n           EXISTS: lambda v, e: ExistsExpression(v.variable, e),\n           NOT: NegatedExpression,\n           AND: AndExpression,\n           OR: OrExpression,\n           IMP: ImpExpression,\n           IFF: IffExpression,\n           PRED: ApplicationExpression}\n\n\nclass HoleSemantics(object):\n    def __init__(self, usr):\n        self.holes = set()\n        self.labels = set()\n        self.fragments = {}  # mapping of label -> formula fragment\n        self.constraints = set()  # set of Constraints\n        self._break_down(usr)\n        self.top_most_labels = self._find_top_most_labels()\n        self.top_hole = self._find_top_hole()\n\n    def is_node(self, x):\n        return x in (self.labels | self.holes)\n\n    def _break_down(self, usr):\n        if isinstance(usr, AndExpression):\n            self._break_down(usr.first)\n            self._break_down(usr.second)\n        elif isinstance(usr, ApplicationExpression):\n            func, args = usr.uncurry()\n            if func.variable.name == Constants.LEQ:\n                self.constraints.add(Constraint(args[0], args[1]))\n            elif func.variable.name == Constants.HOLE:\n                self.holes.add(args[0])\n            elif func.variable.name == Constants.LABEL:\n                self.labels.add(args[0])\n            else:\n                label = args[0]\n                assert label not in self.fragments\n                self.fragments[label] = (func, args[1:])\n        else:\n            raise ValueError(usr.label())\n\n    def _find_top_nodes(self, node_list):\n        top_nodes = node_list.copy()\n        for f in itervalues(self.fragments):\n            args = f[1]\n            for arg in args:\n                if arg in node_list:\n                    top_nodes.discard(arg)\n        return top_nodes\n\n    def _find_top_most_labels(self):\n        return self._find_top_nodes(self.labels)\n\n    def _find_top_hole(self):\n        top_holes = self._find_top_nodes(self.holes)\n        assert len(top_holes) == 1  # it must be unique\n        return top_holes.pop()\n\n    def pluggings(self):\n        record = []\n        self._plug_nodes([(self.top_hole, [])], self.top_most_labels, {}, record)\n        return record\n\n    def _plug_nodes(self, queue, potential_labels, plug_acc, record):\n        if queue != []:\n            (node, ancestors) = queue[0]\n            if node in self.holes:\n                self._plug_hole(node, ancestors, queue[1:], potential_labels, plug_acc, record)\n            else:\n                assert node in self.labels\n                args = self.fragments[node][1]\n                head = [(a, ancestors) for a in args if self.is_node(a)]\n                self._plug_nodes(head + queue[1:], potential_labels, plug_acc, record)\n        else:\n            raise Exception('queue empty')\n\n    def _plug_hole(self, hole, ancestors0, queue, potential_labels0,\n                   plug_acc0, record):\n        assert hole not in ancestors0\n        ancestors = [hole] + ancestors0\n\n        for l in potential_labels0:\n            if self._violates_constraints(l, ancestors):\n                continue\n\n            plug_acc = plug_acc0.copy()\n            plug_acc[hole] = l\n            potential_labels = potential_labels0.copy()\n            potential_labels.remove(l)\n\n            if len(potential_labels) == 0:\n                self._sanity_check_plugging(plug_acc, self.top_hole, [])\n                record.append(plug_acc)\n            else:\n                self._plug_nodes(queue + [(l, ancestors)], potential_labels, plug_acc, record)\n\n    def _violates_constraints(self, label, ancestors):\n        for c in self.constraints:\n            if c.lhs == label:\n                if c.rhs not in ancestors:\n                    return True\n        return False\n\n    def _sanity_check_plugging(self, plugging, node, ancestors):\n        if node in self.holes:\n            ancestors = [node] + ancestors\n            label = plugging[node]\n        else:\n            label = node\n        assert label in self.labels\n        for c in self.constraints:\n            if c.lhs == label:\n                assert c.rhs in ancestors\n        args = self.fragments[label][1]\n        for arg in args:\n            if self.is_node(arg):\n                self._sanity_check_plugging(plugging, arg, [label] + ancestors)\n\n    def formula_tree(self, plugging):\n        return self._formula_tree(plugging, self.top_hole)\n\n    def _formula_tree(self, plugging, node):\n        if node in plugging:\n            return self._formula_tree(plugging, plugging[node])\n        elif node in self.fragments:\n            pred, args = self.fragments[node]\n            children = [self._formula_tree(plugging, arg) for arg in args]\n            return reduce(Constants.MAP[pred.variable.name], children)\n        else:\n            return node\n\n\n@compat.python_2_unicode_compatible\nclass Constraint(object):\n    def __init__(self, lhs, rhs):\n        self.lhs = lhs\n        self.rhs = rhs\n\n    def __eq__(self, other):\n        if self.__class__ == other.__class__:\n            return self.lhs == other.lhs and self.rhs == other.rhs\n        else:\n            return False\n\n    def __ne__(self, other):\n        return not (self == other)\n\n    def __hash__(self):\n        return hash(repr(self))\n\n    def __repr__(self):\n        return '(%s < %s)' % (self.lhs, self.rhs)\n\n\ndef hole_readings(sentence, grammar_filename=None, verbose=False):\n    if not grammar_filename:\n        grammar_filename = 'grammars/sample_grammars/hole.fcfg'\n\n    if verbose:\n        print('Reading grammar file', grammar_filename)\n\n    parser = load_parser(grammar_filename)\n\n    tokens = sentence.split()\n    trees = list(parser.parse(tokens))\n    if verbose:\n        print('Got %d different parses' % len(trees))\n\n    all_readings = []\n    for tree in trees:\n        sem = tree.label()['SEM'].simplify()\n\n        if verbose:\n            print('Raw:       ', sem)\n\n        while isinstance(sem, LambdaExpression):\n            sem = sem.term\n        skolemized = skolemize(sem)\n\n        if verbose:\n            print('Skolemized:', skolemized)\n\n        hole_sem = HoleSemantics(skolemized)\n\n        if verbose:\n            print('Holes:       ', hole_sem.holes)\n            print('Labels:      ', hole_sem.labels)\n            print('Constraints: ', hole_sem.constraints)\n            print('Top hole:    ', hole_sem.top_hole)\n            print('Top labels:  ', hole_sem.top_most_labels)\n            print('Fragments:')\n            for l, f in hole_sem.fragments.items():\n                print('\\t%s: %s' % (l, f))\n\n        pluggings = hole_sem.pluggings()\n\n        readings = list(map(hole_sem.formula_tree, pluggings))\n\n        if verbose:\n            for i, r in enumerate(readings):\n                print()\n                print('%d. %s' % (i, r))\n            print()\n\n        all_readings.extend(readings)\n\n    return all_readings\n\n\nif __name__ == '__main__':\n    for r in hole_readings('a dog barks'):\n        print(r)\n    print()\n    for r in hole_readings('every girl chases a dog'):\n        print(r)\n"], "nltk\\sem\\lfg": [".py", "from __future__ import print_function, division, unicode_literals\n\nfrom itertools import chain\n\nfrom nltk.internals import Counter\nfrom nltk.compat import python_2_unicode_compatible\n\n\n@python_2_unicode_compatible\nclass FStructure(dict):\n    def safeappend(self, key, item):\n        if key not in self:\n            self[key] = []\n        self[key].append(item)\n\n    def __setitem__(self, key, value):\n        dict.__setitem__(self, key.lower(), value)\n\n    def __getitem__(self, key):\n        return dict.__getitem__(self, key.lower())\n\n    def __contains__(self, key):\n        return dict.__contains__(self, key.lower())\n\n    def to_glueformula_list(self, glue_dict):\n        depgraph = self.to_depgraph()\n        return glue_dict.to_glueformula_list(depgraph)\n\n    def to_depgraph(self, rel=None):\n        from nltk.parse.dependencygraph import DependencyGraph\n        depgraph = DependencyGraph()\n        nodes = depgraph.nodes\n\n        self._to_depgraph(nodes, 0, 'ROOT')\n\n        for address, node in nodes.items():\n            for n2 in (n for n in nodes.values() if n['rel'] != 'TOP'):\n                if n2['head'] == address:\n                    relation = n2['rel']\n                    node['deps'].setdefault(relation,[])\n                    node['deps'][relation].append(n2['address'])\n\n        depgraph.root = nodes[1]\n\n        return depgraph\n\n    def _to_depgraph(self, nodes, head, rel):\n        index = len(nodes)\n\n        nodes[index].update(\n            {\n                'address': index,\n                'word': self.pred[0],\n                'tag': self.pred[1],\n                'head': head,\n                'rel': rel,\n            }\n        )\n\n        for feature in sorted(self):\n            for item in sorted(self[feature]):\n                if isinstance(item, FStructure):\n                    item._to_depgraph(nodes, index, feature)\n                elif isinstance(item, tuple):\n                    new_index = len(nodes)\n                    nodes[new_index].update(\n                        {\n                            'address': new_index,\n                            'word': item[0],\n                            'tag': item[1],\n                            'head': index,\n                            'rel': feature,\n                        }\n                    )\n                elif isinstance(item, list):\n                    for n in item:\n                        n._to_depgraph(nodes, index, feature)\n                else:\n                    raise Exception('feature %s is not an FStruct, a list, or a tuple' % feature)\n\n    @staticmethod\n    def read_depgraph(depgraph):\n        return FStructure._read_depgraph(depgraph.root, depgraph)\n\n    @staticmethod\n    def _read_depgraph(node, depgraph, label_counter=None, parent=None):\n        if not label_counter:\n            label_counter = Counter()\n\n        if node['rel'].lower() in ['spec', 'punct']:\n            return (node['word'], node['tag'])\n\n        else:\n            fstruct = FStructure()\n            fstruct.pred = None\n            fstruct.label = FStructure._make_label(label_counter.get())\n\n            fstruct.parent = parent\n\n            word, tag = node['word'], node['tag']\n            if tag[:2] == 'VB':\n                if tag[2:3] == 'D':\n                    fstruct.safeappend('tense', ('PAST', 'tense'))\n                fstruct.pred = (word, tag[:2])\n\n            if not fstruct.pred:\n                fstruct.pred = (word, tag)\n\n            children = [depgraph.nodes[idx] for idx in chain(*node['deps'].values())]\n            for child in children:\n                fstruct.safeappend(child['rel'], FStructure._read_depgraph(child, depgraph, label_counter, fstruct))\n\n            return fstruct\n\n    @staticmethod\n    def _make_label(value):\n        letter = ['f','g','h','i','j','k','l','m','n','o','p','q','r','s',\n                  't','u','v','w','x','y','z','a','b','c','d','e'][value-1]\n        num = int(value) // 26\n        if num > 0:\n            return letter + str(num)\n        else:\n            return letter\n\n    def __repr__(self):\n        return self.__unicode__().replace('\\n', '')\n\n    def __str__(self):\n        return self.pretty_format()\n\n    def pretty_format(self, indent=3):\n        try:\n            accum = '%s:[' % self.label\n        except NameError:\n            accum = '['\n        try:\n            accum += 'pred \\'%s\\'' % (self.pred[0])\n        except NameError:\n            pass\n\n        for feature in sorted(self):\n            for item in self[feature]:\n                if isinstance(item, FStructure):\n                    next_indent = indent+len(feature)+3+len(self.label)\n                    accum += '\\n%s%s %s' % (' '*(indent), feature, item.pretty_format(next_indent))\n                elif isinstance(item, tuple):\n                    accum += '\\n%s%s \\'%s\\'' % (' '*(indent), feature, item[0])\n                elif isinstance(item, list):\n                    accum += '\\n%s%s {%s}' % (' '*(indent), feature, ('\\n%s' % (' '*(indent+len(feature)+2))).join(item))\n                else: # ERROR\n                    raise Exception('feature %s is not an FStruct, a list, or a tuple' % feature)\n        return accum+']'\n\n\n\ndef demo_read_depgraph():\n    from nltk.parse.dependencygraph import DependencyGraph\n    dg1 = DependencyGraph(\"\"\"\\\nEsso       NNP     2       SUB\nsaid       VBD     0       ROOT\nthe        DT      5       NMOD\nWhiting    NNP     5       NMOD\nfield      NN      6       SUB\nstarted    VBD     2       VMOD\nproduction NN      6       OBJ\nTuesday    NNP     6       VMOD\n\"\"\")\n    dg2 = DependencyGraph(\"\"\"\\\nJohn    NNP     2       SUB\nsees    VBP     0       ROOT\nMary    NNP     2       OBJ\n\"\"\")\n    dg3 = DependencyGraph(\"\"\"\\\na       DT      2       SPEC\nman     NN      3       SUBJ\nwalks   VB      0       ROOT\n\"\"\")\n    dg4 = DependencyGraph(\"\"\"\\\nevery   DT      2       SPEC\ngirl    NN      3       SUBJ\nchases  VB      0       ROOT\na       DT      5       SPEC\ndog     NN      3       OBJ\n\"\"\")\n\n    depgraphs = [dg1,dg2,dg3,dg4]\n    for dg in depgraphs:\n        print(FStructure.read_depgraph(dg))\n\nif __name__ == '__main__':\n    demo_read_depgraph()\n"], "nltk\\sem\\linearlogic": [".py", "from __future__ import print_function, unicode_literals\n\nfrom six import string_types\n\nfrom nltk.internals import Counter\nfrom nltk.compat import python_2_unicode_compatible\nfrom nltk.sem.logic import LogicParser, APP\n\n_counter = Counter()\n\nclass Tokens(object):\n    OPEN = '('\n    CLOSE = ')'\n\n    IMP = '-o'\n\n    PUNCT = [OPEN, CLOSE]\n    TOKENS = PUNCT + [IMP]\n\nclass LinearLogicParser(LogicParser):\n    def __init__(self):\n        LogicParser.__init__(self)\n\n        self.operator_precedence = {APP: 1, Tokens.IMP: 2, None: 3}\n        self.right_associated_operations += [Tokens.IMP]\n\n    def get_all_symbols(self):\n        return Tokens.TOKENS\n\n    def handle(self, tok, context):\n        if tok not in Tokens.TOKENS:\n            return self.handle_variable(tok, context)\n        elif tok == Tokens.OPEN:\n            return self.handle_open(tok, context)\n\n    def get_BooleanExpression_factory(self, tok):\n        if tok == Tokens.IMP:\n            return ImpExpression\n        else:\n            return None\n\n    def make_BooleanExpression(self, factory, first, second):\n        return factory(first, second)\n\n    def attempt_ApplicationExpression(self, expression, context):\n        :param name: str for the constant name\n        :param dependencies: list of int for the indices on which this atom is dependent\n        \"\"\"\n        assert isinstance(name, string_types)\n        self.name = name\n\n        if not dependencies:\n            dependencies = []\n        self.dependencies = dependencies\n\n    def simplify(self, bindings=None):\n        \"\"\"\n        If 'self' is bound by 'bindings', return the atomic to which it is bound.\n        Otherwise, return self.\n\n        :param bindings: ``BindingDict`` A dictionary of bindings used to simplify\n        :return: ``AtomicExpression``\n        \"\"\"\n        if bindings and self in bindings:\n            return bindings[self]\n        else:\n            return self\n\n    def compile_pos(self, index_counter, glueFormulaFactory):\n        \"\"\"\n        From Iddo Lev's PhD Dissertation p108-109\n\n        :param index_counter: ``Counter`` for unique indices\n        :param glueFormulaFactory: ``GlueFormula`` for creating new glue formulas\n        :return: (``Expression``,set) for the compiled linear logic and any newly created glue formulas\n        \"\"\"\n        self.dependencies = []\n        return (self, [])\n\n    def compile_neg(self, index_counter, glueFormulaFactory):\n        \"\"\"\n        From Iddo Lev's PhD Dissertation p108-109\n\n        :param index_counter: ``Counter`` for unique indices\n        :param glueFormulaFactory: ``GlueFormula`` for creating new glue formulas\n        :return: (``Expression``,set) for the compiled linear logic and any newly created glue formulas\n        \"\"\"\n        self.dependencies = []\n        return (self, [])\n\n    def initialize_labels(self, fstruct):\n        self.name = fstruct.initialize_label(self.name.lower())\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__ and self.name == other.name\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __str__(self):\n        accum = self.name\n        if self.dependencies:\n            accum += \"%s\" % self.dependencies\n        return accum\n\n    def __hash__(self):\n        return hash(self.name)\n\nclass ConstantExpression(AtomicExpression):\n    def unify(self, other, bindings):\n        \"\"\"\n        If 'other' is a constant, then it must be equal to 'self'.  If 'other' is a variable,\n        then it must not be bound to anything other than 'self'.\n\n        :param other: ``Expression``\n        :param bindings: ``BindingDict`` A dictionary of all current bindings\n        :return: ``BindingDict`` A new combined dictionary of of 'bindings' and any new binding\n        :raise UnificationException: If 'self' and 'other' cannot be unified in the context of 'bindings'\n        \"\"\"\n        assert isinstance(other, Expression)\n        if isinstance(other, VariableExpression):\n            try:\n                return bindings + BindingDict([(other, self)])\n            except VariableBindingException:\n                pass\n        elif self == other:\n            return bindings\n        raise UnificationException(self, other, bindings)\n\nclass VariableExpression(AtomicExpression):\n    def unify(self, other, bindings):\n        \"\"\"\n        'self' must not be bound to anything other than 'other'.\n\n        :param other: ``Expression``\n        :param bindings: ``BindingDict`` A dictionary of all current bindings\n        :return: ``BindingDict`` A new combined dictionary of of 'bindings' and the new binding\n        :raise UnificationException: If 'self' and 'other' cannot be unified in the context of 'bindings'\n        \"\"\"\n        assert isinstance(other, Expression)\n        try:\n            if self == other:\n                return bindings\n            else:\n                return bindings + BindingDict([(self, other)])\n        except VariableBindingException:\n            raise UnificationException(self, other, bindings)\n\n@python_2_unicode_compatible\nclass ImpExpression(Expression):\n    def __init__(self, antecedent, consequent):\n        \"\"\"\n        :param antecedent: ``Expression`` for the antecedent\n        :param consequent: ``Expression`` for the consequent\n        \"\"\"\n        assert isinstance(antecedent, Expression)\n        assert isinstance(consequent, Expression)\n        self.antecedent = antecedent\n        self.consequent = consequent\n\n    def simplify(self, bindings=None):\n        return self.__class__(self.antecedent.simplify(bindings), self.consequent.simplify(bindings))\n\n    def unify(self, other, bindings):\n        \"\"\"\n        Both the antecedent and consequent of 'self' and 'other' must unify.\n\n        :param other: ``ImpExpression``\n        :param bindings: ``BindingDict`` A dictionary of all current bindings\n        :return: ``BindingDict`` A new combined dictionary of of 'bindings' and any new bindings\n        :raise UnificationException: If 'self' and 'other' cannot be unified in the context of 'bindings'\n        \"\"\"\n        assert isinstance(other, ImpExpression)\n        try:\n            return bindings + self.antecedent.unify(other.antecedent, bindings) + self.consequent.unify(other.consequent, bindings)\n        except VariableBindingException:\n            raise UnificationException(self, other, bindings)\n\n    def compile_pos(self, index_counter, glueFormulaFactory):\n        \"\"\"\n        From Iddo Lev's PhD Dissertation p108-109\n\n        :param index_counter: ``Counter`` for unique indices\n        :param glueFormulaFactory: ``GlueFormula`` for creating new glue formulas\n        :return: (``Expression``,set) for the compiled linear logic and any newly created glue formulas\n        \"\"\"\n        (a, a_new) = self.antecedent.compile_neg(index_counter, glueFormulaFactory)\n        (c, c_new) = self.consequent.compile_pos(index_counter, glueFormulaFactory)\n        return (ImpExpression(a,c), a_new + c_new)\n\n    def compile_neg(self, index_counter, glueFormulaFactory):\n        \"\"\"\n        From Iddo Lev's PhD Dissertation p108-109\n\n        :param index_counter: ``Counter`` for unique indices\n        :param glueFormulaFactory: ``GlueFormula`` for creating new glue formulas\n        :return: (``Expression``,list of ``GlueFormula``) for the compiled linear logic and any newly created glue formulas\n        \"\"\"\n        (a, a_new) = self.antecedent.compile_pos(index_counter, glueFormulaFactory)\n        (c, c_new) = self.consequent.compile_neg(index_counter, glueFormulaFactory)\n        fresh_index = index_counter.get()\n        c.dependencies.append(fresh_index)\n        new_v = glueFormulaFactory('v%s' % fresh_index, a, set([fresh_index]))\n        return (c, a_new + c_new + [new_v])\n\n    def initialize_labels(self, fstruct):\n        self.antecedent.initialize_labels(fstruct)\n        self.consequent.initialize_labels(fstruct)\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__ and \\\n                self.antecedent == other.antecedent and self.consequent == other.consequent\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __str__(self):\n        return \"%s%s %s %s%s\" % (\n            Tokens.OPEN, self.antecedent, Tokens.IMP, self.consequent, Tokens.CLOSE)\n\n    def __hash__(self):\n        return hash('%s%s%s' % (hash(self.antecedent), Tokens.IMP, hash(self.consequent)))\n\n@python_2_unicode_compatible\nclass ApplicationExpression(Expression):\n    def __init__(self, function, argument, argument_indices=None):\n        \"\"\"\n        :param function: ``Expression`` for the function\n        :param argument: ``Expression`` for the argument\n        :param argument_indices: set for the indices of the glue formula from which the argument came\n        :raise LinearLogicApplicationException: If 'function' cannot be applied to 'argument' given 'argument_indices'.\n        \"\"\"\n        function_simp = function.simplify()\n        argument_simp = argument.simplify()\n\n        assert isinstance(function_simp, ImpExpression)\n        assert isinstance(argument_simp, Expression)\n\n        bindings = BindingDict()\n\n        try:\n            if isinstance(function, ApplicationExpression):\n                bindings += function.bindings\n            if isinstance(argument, ApplicationExpression):\n                bindings += argument.bindings\n            bindings += function_simp.antecedent.unify(argument_simp, bindings)\n        except UnificationException as e:\n            raise LinearLogicApplicationException('Cannot apply %s to %s. %s' % (function_simp, argument_simp, e))\n\n        if argument_indices:\n            if not set(function_simp.antecedent.dependencies) < argument_indices:\n                raise LinearLogicApplicationException('Dependencies unfulfilled when attempting to apply Linear Logic formula %s to %s' % (function_simp, argument_simp))\n            if set(function_simp.antecedent.dependencies) == argument_indices:\n                raise LinearLogicApplicationException('Dependencies not a proper subset of indices when attempting to apply Linear Logic formula %s to %s' % (function_simp, argument_simp))\n\n        self.function = function\n        self.argument = argument\n        self.bindings = bindings\n\n    def simplify(self, bindings=None):\n        \"\"\"\n        Since function is an implication, return its consequent.  There should be\n        no need to check that the application is valid since the checking is done\n        by the constructor.\n\n        :param bindings: ``BindingDict`` A dictionary of bindings used to simplify\n        :return: ``Expression``\n        \"\"\"\n        if not bindings:\n            bindings = self.bindings\n\n        return self.function.simplify(bindings).consequent\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__ and \\\n                self.function == other.function and self.argument == other.argument\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __str__(self):\n        return \"%s\" % self.function + Tokens.OPEN + \"%s\" % self.argument + Tokens.CLOSE\n\n    def __hash__(self):\n        return hash('%s%s%s' % (hash(self.antecedent), Tokens.OPEN, hash(self.consequent)))\n\n@python_2_unicode_compatible\nclass BindingDict(object):\n    def __init__(self, bindings=None):\n        \"\"\"\n        :param bindings:\n            list [(``VariableExpression``, ``AtomicExpression``)] to initialize the dictionary\n            dict {``VariableExpression``: ``AtomicExpression``} to initialize the dictionary\n        \"\"\"\n        self.d = {}\n\n        if isinstance(bindings, dict):\n            bindings = bindings.items()\n\n        if bindings:\n            for (v, b) in bindings:\n                self[v] = b\n\n    def __setitem__(self, variable, binding):\n        \"\"\"\n        A binding is consistent with the dict if its variable is not already bound, OR if its\n        variable is already bound to its argument.\n\n        :param variable: ``VariableExpression`` The variable bind\n        :param binding: ``Expression`` The expression to which 'variable' should be bound\n        :raise VariableBindingException: If the variable cannot be bound in this dictionary\n        \"\"\"\n        assert isinstance(variable, VariableExpression)\n        assert isinstance(binding, Expression)\n\n        assert variable != binding\n\n        existing = self.d.get(variable, None)\n\n        if not existing or binding == existing:\n            self.d[variable] = binding\n        else:\n            raise VariableBindingException('Variable %s already bound to another value' % (variable))\n\n    def __getitem__(self, variable):\n        \"\"\"\n        Return the expression to which 'variable' is bound\n        \"\"\"\n        assert isinstance(variable, VariableExpression)\n\n        intermediate = self.d[variable]\n        while intermediate:\n            try:\n                intermediate = self.d[intermediate]\n            except KeyError:\n                return intermediate\n\n    def __contains__(self, item):\n        return item in self.d\n\n    def __add__(self, other):\n        \"\"\"\n        :param other: ``BindingDict`` The dict with which to combine self\n        :return: ``BindingDict`` A new dict containing all the elements of both parameters\n        :raise VariableBindingException: If the parameter dictionaries are not consistent with each other\n        \"\"\"\n        try:\n            combined = BindingDict()\n            for v in self.d:\n                combined[v] = self.d[v]\n            for v in other.d:\n                combined[v] = other.d[v]\n            return combined\n        except VariableBindingException:\n            raise VariableBindingException('Attempting to add two contradicting'\\\n                        ' VariableBindingsLists: %s, %s' % (self, other))\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __eq__(self, other):\n        if not isinstance(other, BindingDict):\n            raise TypeError\n        return self.d == other.d\n\n    def __str__(self):\n        return '{' + ', '.join('%s: %s' % (v, self.d[v]) for v in self.d) + '}'\n\n    def __repr__(self):\n        return 'BindingDict: %s' % self\n\nclass VariableBindingException(Exception):\n    pass\n\nclass UnificationException(Exception):\n    def __init__(self, a, b, bindings):\n        Exception.__init__(self, 'Cannot unify %s with %s given %s' % (a, b, bindings))\n\nclass LinearLogicApplicationException(Exception):\n    pass\n\n\ndef demo():\n    lexpr = Expression.fromstring\n\n    print(lexpr(r'f'))\n    print(lexpr(r'(g -o f)'))\n    print(lexpr(r'((g -o G) -o G)'))\n    print(lexpr(r'g -o h -o f'))\n    print(lexpr(r'(g -o f)(g)').simplify())\n    print(lexpr(r'(H -o f)(g)').simplify())\n    print(lexpr(r'((g -o G) -o G)((g -o f))').simplify())\n    print(lexpr(r'(H -o H)((g -o f))').simplify())\n\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\sem\\logic": [".py", "\nfrom __future__ import print_function, unicode_literals\n\nimport re\nimport operator\nfrom collections import defaultdict\nfrom functools import reduce, total_ordering\n\nfrom six import string_types\n\nfrom nltk.util import Trie\nfrom nltk.internals import Counter\nfrom nltk.compat import python_2_unicode_compatible\n\nAPP = 'APP'\n\n_counter = Counter()\n\nclass Tokens(object):\n    LAMBDA = '\\\\';     LAMBDA_LIST = ['\\\\']\n\n    EXISTS = 'exists'; EXISTS_LIST = ['some', 'exists', 'exist']\n    ALL = 'all';       ALL_LIST = ['all', 'forall']\n\n    DOT = '.'\n    OPEN = '('\n    CLOSE = ')'\n    COMMA = ','\n\n    NOT = '-';         NOT_LIST = ['not', '-', '!']\n    AND = '&';         AND_LIST = ['and', '&', '^']\n    OR = '|';          OR_LIST = ['or', '|']\n    IMP = '->';        IMP_LIST = ['implies', '->', '=>']\n    IFF = '<->';       IFF_LIST = ['iff', '<->', '<=>']\n    EQ = '=';          EQ_LIST = ['=', '==']\n    NEQ = '!=';        NEQ_LIST = ['!=']\n\n    BINOPS = AND_LIST + OR_LIST + IMP_LIST + IFF_LIST\n    QUANTS = EXISTS_LIST + ALL_LIST\n    PUNCT = [DOT, OPEN, CLOSE, COMMA]\n\n    TOKENS = BINOPS + EQ_LIST + NEQ_LIST + QUANTS + LAMBDA_LIST + PUNCT + NOT_LIST\n\n    SYMBOLS = [x for x in TOKENS if re.match(r'^[-\\\\.(),!&^|>=<]*$', x)]\n\n\ndef boolean_ops():\n    names =  [\"negation\", \"conjunction\", \"disjunction\", \"implication\", \"equivalence\"]\n    for pair in zip(names, [Tokens.NOT, Tokens.AND, Tokens.OR, Tokens.IMP, Tokens.IFF]):\n        print(\"%-15s\\t%s\" %  pair)\n\ndef equality_preds():\n    names =  [\"equality\", \"inequality\"]\n    for pair in zip(names, [Tokens.EQ, Tokens.NEQ]):\n        print(\"%-15s\\t%s\" %  pair)\n\ndef binding_ops():\n    names =  [\"existential\", \"universal\", \"lambda\"]\n    for pair in zip(names, [Tokens.EXISTS, Tokens.ALL, Tokens.LAMBDA]):\n        print(\"%-15s\\t%s\" %  pair)\n\n\n@python_2_unicode_compatible\nclass LogicParser(object):\n\n    def __init__(self, type_check=False):\n        assert isinstance(type_check, bool)\n\n        self._currentIndex = 0\n        self._buffer = []\n        self.type_check = type_check\n\n        Parse the expression.\n\n        :param data: str for the input to be parsed\n        :param signature: ``dict<str, str>`` that maps variable names to type\n        strings\n        :returns: a parsed Expression\n        \"\"\"\n        data = data.rstrip()\n\n        self._currentIndex = 0\n        self._buffer, mapping = self.process(data)\n\n        try:\n            result = self.process_next_expression(None)\n            if self.inRange(0):\n                raise UnexpectedTokenException(self._currentIndex+1, self.token(0))\n        except LogicalExpressionException as e:\n            msg = '%s\\n%s\\n%s^' % (e, data, ' '*mapping[e.index-1])\n            raise LogicalExpressionException(None, msg)\n\n        if self.type_check:\n            result.typecheck(signature)\n\n        return result\n\n    def process(self, data):\n        out = []\n        mapping = {}\n        tokenTrie = Trie(self.get_all_symbols())\n        token = ''\n        data_idx = 0\n        token_start_idx = data_idx\n        while data_idx < len(data):\n            cur_data_idx = data_idx\n            quoted_token, data_idx = self.process_quoted_token(data_idx, data)\n            if quoted_token:\n                if not token:\n                    token_start_idx = cur_data_idx\n                token += quoted_token\n                continue\n\n            st = tokenTrie\n            c = data[data_idx]\n            symbol = ''\n            while c in st:\n                symbol += c\n                st = st[c]\n                if len(data)-data_idx > len(symbol):\n                    c = data[data_idx+len(symbol)]\n                else:\n                    break\n            if Trie.LEAF in st:\n                if token:\n                    mapping[len(out)] = token_start_idx\n                    out.append(token)\n                    token = ''\n                mapping[len(out)] = data_idx\n                out.append(symbol)\n                data_idx += len(symbol)\n            else:\n                if data[data_idx] in ' \\t\\n': #any whitespace\n                    if token:\n                        mapping[len(out)] = token_start_idx\n                        out.append(token)\n                        token = ''\n                else:\n                    if not token:\n                        token_start_idx = data_idx\n                    token += data[data_idx]\n                data_idx += 1\n        if token:\n            mapping[len(out)] = token_start_idx\n            out.append(token)\n        mapping[len(out)] = len(data)\n        mapping[len(out)+1] = len(data)+1\n        return out, mapping\n\n    def process_quoted_token(self, data_idx, data):\n        token = ''\n        c = data[data_idx]\n        i = data_idx\n        for start, end, escape, incl_quotes in self.quote_chars:\n            if c == start:\n                if incl_quotes:\n                    token += c\n                i += 1\n                while data[i] != end:\n                    if data[i] == escape:\n                        if incl_quotes:\n                            token += data[i]\n                        i += 1\n                        if len(data) == i: #if there are no more chars\n                            raise LogicalExpressionException(None, \"End of input reached.  \"\n                                    \"Escape character [%s] found at end.\"\n                                    % escape)\n                        token += data[i]\n                    else:\n                        token += data[i]\n                    i += 1\n                    if len(data) == i:\n                        raise LogicalExpressionException(None, \"End of input reached.  \"\n                                             \"Expected: [%s]\" % end)\n                if incl_quotes:\n                    token += data[i]\n                i += 1\n                if not token:\n                    raise LogicalExpressionException(None, 'Empty quoted token found')\n                break\n        return token, i\n\n    def get_all_symbols(self):\n        return Tokens.SYMBOLS\n\n    def inRange(self, location):\n        return self._currentIndex+location < len(self._buffer)\n\n    def token(self, location=None):\n        \"\"\"Get the next waiting token.  If a location is given, then\n        return the token at currentIndex+location without advancing\n        currentIndex; setting it gives lookahead/lookback capability.\"\"\"\n        try:\n            if location is None:\n                tok = self._buffer[self._currentIndex]\n                self._currentIndex += 1\n            else:\n                tok = self._buffer[self._currentIndex+location]\n            return tok\n        except IndexError:\n            raise ExpectedMoreTokensException(self._currentIndex+1)\n\n    def isvariable(self, tok):\n        return tok not in Tokens.TOKENS\n\n    def process_next_expression(self, context):\n        try:\n            tok = self.token()\n        except ExpectedMoreTokensException:\n            raise ExpectedMoreTokensException(self._currentIndex+1, message='Expression expected.')\n\n        accum = self.handle(tok, context)\n\n        if not accum:\n            raise UnexpectedTokenException(self._currentIndex, tok, message='Expression expected.')\n\n        return self.attempt_adjuncts(accum, context)\n\n    def handle(self, tok, context):\n        \"\"\"This method is intended to be overridden for logics that\n        use different operators or expressions\"\"\"\n        if self.isvariable(tok):\n            return self.handle_variable(tok, context)\n\n        elif tok in Tokens.NOT_LIST:\n            return self.handle_negation(tok, context)\n\n        elif tok in Tokens.LAMBDA_LIST:\n            return self.handle_lambda(tok, context)\n\n        elif tok in Tokens.QUANTS:\n            return self.handle_quant(tok, context)\n\n        elif tok == Tokens.OPEN:\n            return self.handle_open(tok, context)\n\n    def attempt_adjuncts(self, expression, context):\n        cur_idx = None\n        while cur_idx != self._currentIndex: #while adjuncts are added\n            cur_idx = self._currentIndex\n            expression = self.attempt_EqualityExpression(expression, context)\n            expression = self.attempt_ApplicationExpression(expression, context)\n            expression = self.attempt_BooleanExpression(expression, context)\n        return expression\n\n    def handle_negation(self, tok, context):\n        return self.make_NegatedExpression(self.process_next_expression(Tokens.NOT))\n\n    def make_NegatedExpression(self, expression):\n        return NegatedExpression(expression)\n\n    def handle_variable(self, tok, context):\n        accum = self.make_VariableExpression(tok)\n        if self.inRange(0) and self.token(0) == Tokens.OPEN:\n            if not isinstance(accum, FunctionVariableExpression) and \\\n               not isinstance(accum, ConstantExpression):\n                raise LogicalExpressionException(self._currentIndex,\n                                     \"'%s' is an illegal predicate name.  \"\n                                     \"Individual variables may not be used as \"\n                                     \"predicates.\" % tok)\n            self.token() #swallow the Open Paren\n\n            accum = self.make_ApplicationExpression(accum, self.process_next_expression(APP))\n            while self.inRange(0) and self.token(0) == Tokens.COMMA:\n                self.token() #swallow the comma\n                accum = self.make_ApplicationExpression(accum, self.process_next_expression(APP))\n            self.assertNextToken(Tokens.CLOSE)\n        return accum\n\n    def get_next_token_variable(self, description):\n        try:\n            tok = self.token()\n        except ExpectedMoreTokensException as e:\n            raise ExpectedMoreTokensException(e.index, 'Variable expected.')\n        if isinstance(self.make_VariableExpression(tok), ConstantExpression):\n            raise LogicalExpressionException(self._currentIndex,\n                                 \"'%s' is an illegal variable name.  \"\n                                 \"Constants may not be %s.\" % (tok, description))\n        return Variable(tok)\n\n    def handle_lambda(self, tok, context):\n        if not self.inRange(0):\n            raise ExpectedMoreTokensException(self._currentIndex+2,\n                                              message=\"Variable and Expression expected following lambda operator.\")\n        vars = [self.get_next_token_variable('abstracted')]\n        while True:\n            if not self.inRange(0) or (self.token(0) == Tokens.DOT and not self.inRange(1)):\n                raise ExpectedMoreTokensException(self._currentIndex+2, message=\"Expression expected.\")\n            if not self.isvariable(self.token(0)):\n                break\n            vars.append(self.get_next_token_variable('abstracted'))\n        if self.inRange(0) and self.token(0) == Tokens.DOT:\n            self.token() #swallow the dot\n\n        accum = self.process_next_expression(tok)\n        while vars:\n            accum = self.make_LambdaExpression(vars.pop(), accum)\n        return accum\n\n    def handle_quant(self, tok, context):\n        factory = self.get_QuantifiedExpression_factory(tok)\n\n        if not self.inRange(0):\n            raise ExpectedMoreTokensException(self._currentIndex+2,\n                                              message=\"Variable and Expression expected following quantifier '%s'.\" % tok)\n        vars = [self.get_next_token_variable('quantified')]\n        while True:\n            if not self.inRange(0) or (self.token(0) == Tokens.DOT and not self.inRange(1)):\n                raise ExpectedMoreTokensException(self._currentIndex+2, message=\"Expression expected.\")\n            if not self.isvariable(self.token(0)):\n                break\n            vars.append(self.get_next_token_variable('quantified'))\n        if self.inRange(0) and self.token(0) == Tokens.DOT:\n            self.token() #swallow the dot\n\n        accum = self.process_next_expression(tok)\n        while vars:\n            accum = self.make_QuanifiedExpression(factory, vars.pop(), accum)\n        return accum\n\n    def get_QuantifiedExpression_factory(self, tok):\n        \"\"\"This method serves as a hook for other logic parsers that\n        have different quantifiers\"\"\"\n        if tok in Tokens.EXISTS_LIST:\n            return ExistsExpression\n        elif tok in Tokens.ALL_LIST:\n            return AllExpression\n        else:\n            self.assertToken(tok, Tokens.QUANTS)\n\n    def make_QuanifiedExpression(self, factory, variable, term):\n        return factory(variable, term)\n\n    def handle_open(self, tok, context):\n        accum = self.process_next_expression(None)\n        self.assertNextToken(Tokens.CLOSE)\n        return accum\n\n    def attempt_EqualityExpression(self, expression, context):\n        \"\"\"Attempt to make an equality expression.  If the next token is an\n        equality operator, then an EqualityExpression will be returned.\n        Otherwise, the parameter will be returned.\"\"\"\n        if self.inRange(0):\n            tok = self.token(0)\n            if tok in Tokens.EQ_LIST + Tokens.NEQ_LIST and self.has_priority(tok, context):\n                self.token() #swallow the \"=\" or \"!=\"\n                expression = self.make_EqualityExpression(expression, self.process_next_expression(tok))\n                if tok in Tokens.NEQ_LIST:\n                    expression = self.make_NegatedExpression(expression)\n        return expression\n\n    def make_EqualityExpression(self, first, second):\n        \"\"\"This method serves as a hook for other logic parsers that\n        have different equality expression classes\"\"\"\n        return EqualityExpression(first, second)\n\n    def attempt_BooleanExpression(self, expression, context):\n        \"\"\"Attempt to make a boolean expression.  If the next token is a boolean\n        operator, then a BooleanExpression will be returned.  Otherwise, the\n        parameter will be returned.\"\"\"\n        while self.inRange(0):\n            tok = self.token(0)\n            factory = self.get_BooleanExpression_factory(tok)\n            if factory and self.has_priority(tok, context):\n                self.token() #swallow the operator\n                expression = self.make_BooleanExpression(factory, expression,\n                                                         self.process_next_expression(tok))\n            else:\n                break\n        return expression\n\n    def get_BooleanExpression_factory(self, tok):\n        \"\"\"This method serves as a hook for other logic parsers that\n        have different boolean operators\"\"\"\n        if tok in Tokens.AND_LIST:\n            return AndExpression\n        elif tok in Tokens.OR_LIST:\n            return OrExpression\n        elif tok in Tokens.IMP_LIST:\n            return ImpExpression\n        elif tok in Tokens.IFF_LIST:\n            return IffExpression\n        else:\n            return None\n\n    def make_BooleanExpression(self, factory, first, second):\n        return factory(first, second)\n\n    def attempt_ApplicationExpression(self, expression, context):\n        \"\"\"Attempt to make an application expression.  The next tokens are\n        a list of arguments in parens, then the argument expression is a\n        function being applied to the arguments.  Otherwise, return the\n        argument expression.\"\"\"\n        if self.has_priority(APP, context):\n            if self.inRange(0) and self.token(0) == Tokens.OPEN:\n                if not isinstance(expression, LambdaExpression) and \\\n                   not isinstance(expression, ApplicationExpression) and \\\n                   not isinstance(expression, FunctionVariableExpression) and \\\n                   not isinstance(expression, ConstantExpression):\n                    raise LogicalExpressionException(self._currentIndex,\n                                         (\"The function '%s\" % expression) +\n                                         \"' is not a Lambda Expression, an \"\n                                         \"Application Expression, or a \"\n                                         \"functional predicate, so it may \"\n                                         \"not take arguments.\")\n                self.token() #swallow then open paren\n                accum = self.make_ApplicationExpression(expression, self.process_next_expression(APP))\n                while self.inRange(0) and self.token(0) == Tokens.COMMA:\n                    self.token() #swallow the comma\n                    accum = self.make_ApplicationExpression(accum, self.process_next_expression(APP))\n                self.assertNextToken(Tokens.CLOSE)\n                return accum\n        return expression\n\n    def make_ApplicationExpression(self, function, argument):\n        return ApplicationExpression(function, argument)\n\n    def make_VariableExpression(self, name):\n        return VariableExpression(Variable(name))\n\n    def make_LambdaExpression(self, variable, term):\n        return LambdaExpression(variable, term)\n\n    def has_priority(self, operation, context):\n        return self.operator_precedence[operation] < self.operator_precedence[context] or \\\n               (operation in self.right_associated_operations and \\\n                self.operator_precedence[operation] == self.operator_precedence[context])\n\n    def assertNextToken(self, expected):\n        try:\n            tok = self.token()\n        except ExpectedMoreTokensException as e:\n            raise ExpectedMoreTokensException(e.index, message=\"Expected token '%s'.\" % expected)\n\n        if isinstance(expected, list):\n            if tok not in expected:\n                raise UnexpectedTokenException(self._currentIndex, tok, expected)\n        else:\n            if tok != expected:\n                raise UnexpectedTokenException(self._currentIndex, tok, expected)\n\n    def assertToken(self, tok, expected):\n        if isinstance(expected, list):\n            if tok not in expected:\n                raise UnexpectedTokenException(self._currentIndex, tok, expected)\n        else:\n            if tok != expected:\n                raise UnexpectedTokenException(self._currentIndex, tok, expected)\n\n    def __repr__(self):\n        if self.inRange(0):\n            msg = 'Next token: ' + self.token(0)\n        else:\n            msg = 'No more tokens'\n        return '<' + self.__class__.__name__ + ': ' + msg + '>'\n\n\ndef read_logic(s, logic_parser=None, encoding=None):\n    \"\"\"\n    Convert a file of First Order Formulas into a list of {Expression}s.\n\n    :param s: the contents of the file\n    :type s: str\n    :param logic_parser: The parser to be used to parse the logical expression\n    :type logic_parser: LogicParser\n    :param encoding: the encoding of the input string, if it is binary\n    :type encoding: str\n    :return: a list of parsed formulas.\n    :rtype: list(Expression)\n    \"\"\"\n    if encoding is not None:\n        s = s.decode(encoding)\n    if logic_parser is None:\n        logic_parser = LogicParser()\n\n    statements = []\n    for linenum, line in enumerate(s.splitlines()):\n        line = line.strip()\n        if line.startswith('#') or line=='': continue\n        try:\n            statements.append(logic_parser.parse(line))\n        except LogicalExpressionException:\n            raise ValueError('Unable to parse line %s: %s' % (linenum, line))\n    return statements\n\n\n@total_ordering\n@python_2_unicode_compatible\nclass Variable(object):\n    def __init__(self, name):\n        \"\"\"\n        :param name: the name of the variable\n        \"\"\"\n        assert isinstance(name, string_types), \"%s is not a string\" % name\n        self.name = name\n\n    def __eq__(self, other):\n        return isinstance(other, Variable) and self.name == other.name\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __lt__(self, other):\n        if not isinstance(other, Variable):\n            raise TypeError\n        return self.name < other.name\n\n    def substitute_bindings(self, bindings):\n        return bindings.get(self, self)\n\n    def __hash__(self):\n        return hash(self.name)\n\n    def __str__(self):\n        return self.name\n\n    def __repr__(self):\n        return \"Variable('%s')\" % self.name\n\n\ndef unique_variable(pattern=None, ignore=None):\n    \"\"\"\n    Return a new, unique variable.\n\n    :param pattern: ``Variable`` that is being replaced.  The new variable must\n        be the same type.\n    :param term: a set of ``Variable`` objects that should not be returned from\n        this function.\n    :rtype: Variable\n    \"\"\"\n    if pattern is not None:\n        if is_indvar(pattern.name):\n            prefix = 'z'\n        elif is_funcvar(pattern.name):\n            prefix = 'F'\n        elif is_eventvar(pattern.name):\n            prefix = 'e0'\n        else:\n            assert False, \"Cannot generate a unique constant\"\n    else:\n        prefix = 'z'\n\n    v = Variable(\"%s%s\" % (prefix, _counter.get()))\n    while ignore is not None and v in ignore:\n        v = Variable(\"%s%s\" % (prefix, _counter.get()))\n    return v\n\ndef skolem_function(univ_scope=None):\n    \"\"\"\n    Return a skolem function over the variables in univ_scope\n    param univ_scope\n    \"\"\"\n    skolem = VariableExpression(Variable('F%s' % _counter.get()))\n    if univ_scope:\n        for v in list(univ_scope):\n            skolem = skolem(VariableExpression(v))\n    return skolem\n\n\n@python_2_unicode_compatible\nclass Type(object):\n    def __repr__(self):\n        return \"%s\" % self\n\n    def __hash__(self):\n        return hash(\"%s\" % self)\n\n    @classmethod\n    def fromstring(cls, s):\n        return read_type(s)\n\n@python_2_unicode_compatible\nclass ComplexType(Type):\n    def __init__(self, first, second):\n        assert(isinstance(first, Type)), \"%s is not a Type\" % first\n        assert(isinstance(second, Type)), \"%s is not a Type\" % second\n        self.first = first\n        self.second = second\n\n    def __eq__(self, other):\n        return isinstance(other, ComplexType) and \\\n               self.first == other.first and \\\n               self.second == other.second\n\n    def __ne__(self, other):\n        return not self == other\n\n    __hash__ = Type.__hash__\n\n    def matches(self, other):\n        if isinstance(other, ComplexType):\n            return self.first.matches(other.first) and \\\n                   self.second.matches(other.second)\n        else:\n            return self == ANY_TYPE\n\n    def resolve(self, other):\n        if other == ANY_TYPE:\n            return self\n        elif isinstance(other, ComplexType):\n            f = self.first.resolve(other.first)\n            s = self.second.resolve(other.second)\n            if f and s:\n                return ComplexType(f,s)\n            else:\n                return None\n        elif self == ANY_TYPE:\n            return other\n        else:\n            return None\n\n    def __str__(self):\n        if self == ANY_TYPE:\n            return \"%s\" % ANY_TYPE\n        else:\n            return '<%s,%s>' % (self.first, self.second)\n\n    def str(self):\n        if self == ANY_TYPE:\n            return ANY_TYPE.str()\n        else:\n            return '(%s -> %s)' % (self.first.str(), self.second.str())\n\nclass BasicType(Type):\n    def __eq__(self, other):\n        return isinstance(other, BasicType) and (\"%s\" % self) == (\"%s\" % other)\n\n    def __ne__(self, other):\n        return not self == other\n\n    __hash__ = Type.__hash__\n\n    def matches(self, other):\n        return other == ANY_TYPE or self == other\n\n    def resolve(self, other):\n        if self.matches(other):\n            return self\n        else:\n            return None\n\n@python_2_unicode_compatible\nclass EntityType(BasicType):\n    def __str__(self):\n        return 'e'\n\n    def str(self):\n        return 'IND'\n\n@python_2_unicode_compatible\nclass TruthValueType(BasicType):\n    def __str__(self):\n        return 't'\n\n    def str(self):\n        return 'BOOL'\n\n@python_2_unicode_compatible\nclass EventType(BasicType):\n    def __str__(self):\n        return 'v'\n\n    def str(self):\n        return 'EVENT'\n\n@python_2_unicode_compatible\nclass AnyType(BasicType, ComplexType):\n    def __init__(self):\n        pass\n\n    @property\n    def first(self): return self\n\n    @property\n    def second(self): return self\n\n    def __eq__(self, other):\n        return isinstance(other, AnyType) or other.__eq__(self)\n\n    def __ne__(self, other):\n        return not self == other\n\n    __hash__ = Type.__hash__\n\n    def matches(self, other):\n        return True\n\n    def resolve(self, other):\n        return other\n\n    def __str__(self):\n        return '?'\n\n    def str(self):\n        return 'ANY'\n\n\nTRUTH_TYPE = TruthValueType()\nENTITY_TYPE = EntityType()\nEVENT_TYPE = EventType()\nANY_TYPE = AnyType()\n\n\ndef read_type(type_string):\n    assert isinstance(type_string, string_types)\n    type_string = type_string.replace(' ', '') #remove spaces\n\n    if type_string[0] == '<':\n        assert type_string[-1] == '>'\n        paren_count = 0\n        for i,char in enumerate(type_string):\n            if char == '<':\n                paren_count += 1\n            elif char == '>':\n                paren_count -= 1\n                assert paren_count > 0\n            elif char == ',':\n                if paren_count == 1:\n                    break\n        return ComplexType(read_type(type_string[1  :i ]),\n                           read_type(type_string[i+1:-1]))\n    elif type_string[0] == \"%s\" % ENTITY_TYPE:\n        return ENTITY_TYPE\n    elif type_string[0] == \"%s\" % TRUTH_TYPE:\n        return TRUTH_TYPE\n    elif type_string[0] == \"%s\" % ANY_TYPE:\n        return ANY_TYPE\n    else:\n        raise LogicalExpressionException(\"Unexpected character: '%s'.\" % type_string[0])\n\n\nclass TypeException(Exception):\n    def __init__(self, msg):\n        super(TypeException, self).__init__(msg)\n\nclass InconsistentTypeHierarchyException(TypeException):\n    def __init__(self, variable, expression=None):\n        if expression:\n            msg = \"The variable '%s' was found in multiple places with different\"\\\n                \" types in '%s'.\" % (variable, expression)\n        else:\n            msg = \"The variable '%s' was found in multiple places with different\"\\\n                \" types.\" % (variable)\n        super(InconsistentTypeHierarchyException, self).__init__(msg)\n\nclass TypeResolutionException(TypeException):\n    def __init__(self, expression, other_type):\n        super(TypeResolutionException, self).__init__(\n            \"The type of '%s', '%s', cannot be resolved with type '%s'\" %\n            (expression, expression.type, other_type))\n\nclass IllegalTypeException(TypeException):\n    def __init__(self, expression, other_type, allowed_type):\n        super(IllegalTypeException, self).__init__(\n            \"Cannot set type of %s '%s' to '%s'; must match type '%s'.\" %\n            (expression.__class__.__name__, expression, other_type,\n            allowed_type))\n\ndef typecheck(expressions, signature=None):\n    \"\"\"\n    Ensure correct typing across a collection of ``Expression`` objects.\n    :param expressions: a collection of expressions\n    :param signature: dict that maps variable names to types (or string\n    representations of types)\n    \"\"\"\n    for expression in expressions:\n        signature = expression.typecheck(signature)\n    for expression in expressions[:-1]:\n        expression.typecheck(signature)\n    return signature\n\n\nclass SubstituteBindingsI(object):\n    \"\"\"\n    An interface for classes that can perform substitutions for\n    variables.\n    \"\"\"\n    def substitute_bindings(self, bindings):\n        \"\"\"\n        :return: The object that is obtained by replacing\n            each variable bound by ``bindings`` with its values.\n            Aliases are already resolved. (maybe?)\n        :rtype: (any)\n        \"\"\"\n        raise NotImplementedError()\n\n    def variables(self):\n        \"\"\"\n        :return: A list of all variables in this object.\n        \"\"\"\n        raise NotImplementedError()\n\n\n@python_2_unicode_compatible\nclass Expression(SubstituteBindingsI):\n\n    _logic_parser = LogicParser()\n    _type_checking_logic_parser = LogicParser(type_check=True)\n\n    @classmethod\n    def fromstring(cls, s, type_check=False, signature=None):\n        if type_check:\n            return cls._type_checking_logic_parser.parse(s, signature)\n        else:\n            return cls._logic_parser.parse(s, signature)\n\n    def __call__(self, other, *additional):\n        accum = self.applyto(other)\n        for a in additional:\n            accum = accum(a)\n        return accum\n\n    def applyto(self, other):\n        assert isinstance(other, Expression), \"%s is not an Expression\" % other\n        return ApplicationExpression(self, other)\n\n    def __neg__(self):\n        return NegatedExpression(self)\n\n    def negate(self):\n        \"\"\"If this is a negated expression, remove the negation.\n        Otherwise add a negation.\"\"\"\n        return -self\n\n    def __and__(self, other):\n        if not isinstance(other, Expression):\n            raise TypeError(\"%s is not an Expression\" % other)\n        return AndExpression(self, other)\n\n    def __or__(self, other):\n        if not isinstance(other, Expression):\n            raise TypeError(\"%s is not an Expression\" % other)\n        return OrExpression(self, other)\n\n    def __gt__(self, other):\n        if not isinstance(other, Expression):\n            raise TypeError(\"%s is not an Expression\" % other)\n        return ImpExpression(self, other)\n\n    def __lt__(self, other):\n        if not isinstance(other, Expression):\n            raise TypeError(\"%s is not an Expression\" % other)\n        return IffExpression(self, other)\n\n    def __eq__(self, other):\n        raise NotImplementedError()\n\n    def __ne__(self, other):\n        return not self == other\n\n    def equiv(self, other, prover=None):\n        \"\"\"\n        Check for logical equivalence.\n        Pass the expression (self <-> other) to the theorem prover.\n        If the prover says it is valid, then the self and other are equal.\n\n        :param other: an ``Expression`` to check equality against\n        :param prover: a ``nltk.inference.api.Prover``\n        \"\"\"\n        assert isinstance(other, Expression), \"%s is not an Expression\" % other\n\n        if prover is None:\n            from nltk.inference import Prover9\n            prover = Prover9()\n        bicond = IffExpression(self.simplify(), other.simplify())\n        return prover.prove(bicond)\n\n    def __hash__(self):\n        return hash(repr(self))\n\n    def substitute_bindings(self, bindings):\n        expr = self\n        for var in expr.variables():\n            if var in bindings:\n                val = bindings[var]\n                if isinstance(val, Variable):\n                    val = self.make_VariableExpression(val)\n                elif not isinstance(val, Expression):\n                    raise ValueError('Can not substitute a non-expression '\n                                     'value into an expression: %r' % (val,))\n                val = val.substitute_bindings(bindings)\n                expr = expr.replace(var, val)\n        return expr.simplify()\n\n    def typecheck(self, signature=None):\n        \"\"\"\n        Infer and check types.  Raise exceptions if necessary.\n\n        :param signature: dict that maps variable names to types (or string\n            representations of types)\n        :return: the signature, plus any additional type mappings\n        \"\"\"\n        sig = defaultdict(list)\n        if signature:\n            for key in signature:\n                val = signature[key]\n                varEx = VariableExpression(Variable(key))\n                if isinstance(val, Type):\n                    varEx.type = val\n                else:\n                    varEx.type = read_type(val)\n                sig[key].append(varEx)\n\n        self._set_type(signature=sig)\n\n        return dict((key, sig[key][0].type) for key in sig)\n\n    def findtype(self, variable):\n        \"\"\"\n        Find the type of the given variable as it is used in this expression.\n        For example, finding the type of \"P\" in \"P(x) & Q(x,y)\" yields \"<e,t>\"\n\n        :param variable: Variable\n        \"\"\"\n        raise NotImplementedError()\n\n    def _set_type(self, other_type=ANY_TYPE, signature=None):\n        \"\"\"\n        Set the type of this expression to be the given type.  Raise type\n        exceptions where applicable.\n\n        :param other_type: Type\n        :param signature: dict(str -> list(AbstractVariableExpression))\n        \"\"\"\n        raise NotImplementedError()\n\n    def replace(self, variable, expression, replace_bound=False, alpha_convert=True):\n        \"\"\"\n        Replace every instance of 'variable' with 'expression'\n        :param variable: ``Variable`` The variable to replace\n        :param expression: ``Expression`` The expression with which to replace it\n        :param replace_bound: bool Should bound variables be replaced?\n        :param alpha_convert: bool Alpha convert automatically to avoid name clashes?\n        \"\"\"\n        assert isinstance(variable, Variable), \"%s is not a Variable\" % variable\n        assert isinstance(expression, Expression), \"%s is not an Expression\" % expression\n\n        return self.visit_structured(lambda e: e.replace(variable, expression,\n                                                         replace_bound, alpha_convert),\n                                     self.__class__)\n\n    def normalize(self, newvars=None):\n        def get_indiv_vars(e):\n            if isinstance(e, IndividualVariableExpression):\n                return set([e])\n            elif isinstance(e, AbstractVariableExpression):\n                return set()\n            else:\n                return e.visit(get_indiv_vars,\n                               lambda parts: reduce(operator.or_, parts, set()))\n\n        result = self\n        for i,e in enumerate(sorted(get_indiv_vars(self), key=lambda e: e.variable)):\n            if isinstance(e,EventVariableExpression):\n                newVar = e.__class__(Variable('e0%s' % (i+1)))\n            elif isinstance(e,IndividualVariableExpression):\n                newVar = e.__class__(Variable('z%s' % (i+1)))\n            else:\n                newVar = e\n            result = result.replace(e.variable, newVar, True)\n        return result\n\n    def visit(self, function, combinator):\n        \"\"\"\n        Recursively visit subexpressions.  Apply 'function' to each\n        subexpression and pass the result of each function application\n        to the 'combinator' for aggregation:\n\n            return combinator(map(function, self.subexpressions))\n\n        Bound variables are neither applied upon by the function nor given to\n        the combinator.\n        :param function: ``Function<Expression,T>`` to call on each subexpression\n        :param combinator: ``Function<list<T>,R>`` to combine the results of the\n        function calls\n        :return: result of combination ``R``\n        \"\"\"\n        raise NotImplementedError()\n\n    def visit_structured(self, function, combinator):\n        \"\"\"\n        Recursively visit subexpressions.  Apply 'function' to each\n        subexpression and pass the result of each function application\n        to the 'combinator' for aggregation.  The combinator must have\n        the same signature as the constructor.  The function is not\n        applied to bound variables, but they are passed to the\n        combinator.\n        :param function: ``Function`` to call on each subexpression\n        :param combinator: ``Function`` with the same signature as the\n        constructor, to combine the results of the function calls\n        :return: result of combination\n        \"\"\"\n        return self.visit(function, lambda parts: combinator(*parts))\n\n    def __repr__(self):\n        return '<%s %s>' % (self.__class__.__name__, self)\n\n    def __str__(self):\n        return self.str()\n\n    def variables(self):\n        \"\"\"\n        Return a set of all the variables for binding substitution.\n        The variables returned include all free (non-bound) individual\n        variables and any variable starting with '?' or '@'.\n        :return: set of ``Variable`` objects\n        \"\"\"\n        return self.free() | set(p for p in self.predicates()|self.constants()\n                                 if re.match('^[?@]', p.name))\n\n    def free(self):\n        \"\"\"\n        Return a set of all the free (non-bound) variables.  This includes\n        both individual and predicate variables, but not constants.\n        :return: set of ``Variable`` objects\n        \"\"\"\n        return self.visit(lambda e: e.free(),\n                          lambda parts: reduce(operator.or_, parts, set()))\n\n    def constants(self):\n        \"\"\"\n        Return a set of individual constants (non-predicates).\n        :return: set of ``Variable`` objects\n        \"\"\"\n        return self.visit(lambda e: e.constants(),\n                          lambda parts: reduce(operator.or_, parts, set()))\n\n    def predicates(self):\n        \"\"\"\n        Return a set of predicates (constants, not variables).\n        :return: set of ``Variable`` objects\n        \"\"\"\n        return self.visit(lambda e: e.predicates(),\n                          lambda parts: reduce(operator.or_, parts, set()))\n\n    def simplify(self):\n        \"\"\"\n        :return: beta-converted version of this expression\n        \"\"\"\n        return self.visit_structured(lambda e: e.simplify(), self.__class__)\n\n    def make_VariableExpression(self, variable):\n        return VariableExpression(variable)\n\n\n@python_2_unicode_compatible\nclass ApplicationExpression(Expression):\n    r\"\"\"\n    This class is used to represent two related types of logical expressions.\n\n    The first is a Predicate Expression, such as \"P(x,y)\".  A predicate\n    expression is comprised of a ``FunctionVariableExpression`` or\n    ``ConstantExpression`` as the predicate and a list of Expressions as the\n    arguments.\n\n    The second is a an application of one expression to another, such as\n    \"(\\x.dog(x))(fido)\".\n\n    The reason Predicate Expressions are treated as Application Expressions is\n    that the Variable Expression predicate of the expression may be replaced\n    with another Expression, such as a LambdaExpression, which would mean that\n    the Predicate should be thought of as being applied to the arguments.\n\n    The logical expression reader will always curry arguments in a application expression.\n    So, \"\\x y.see(x,y)(john,mary)\" will be represented internally as\n    \"((\\x y.(see(x))(y))(john))(mary)\".  This simplifies the internals since\n    there will always be exactly one argument in an application.\n\n    The str() method will usually print the curried forms of application\n    expressions.  The one exception is when the the application expression is\n    really a predicate expression (ie, underlying function is an\n    ``AbstractVariableExpression``).  This means that the example from above\n    will be returned as \"(\\x y.see(x,y)(john))(mary)\".\n    \"\"\"\n    def __init__(self, function, argument):\n        \"\"\"\n        :param function: ``Expression``, for the function expression\n        :param argument: ``Expression``, for the argument\n        \"\"\"\n        assert isinstance(function, Expression), \"%s is not an Expression\" % function\n        assert isinstance(argument, Expression), \"%s is not an Expression\" % argument\n        self.function = function\n        self.argument = argument\n\n    def simplify(self):\n        function = self.function.simplify()\n        argument = self.argument.simplify()\n        if isinstance(function, LambdaExpression):\n            return function.term.replace(function.variable, argument).simplify()\n        else:\n            return self.__class__(function, argument)\n\n    @property\n    def type(self):\n        if isinstance(self.function.type, ComplexType):\n            return self.function.type.second\n        else:\n            return ANY_TYPE\n\n    def _set_type(self, other_type=ANY_TYPE, signature=None):\n        assert isinstance(other_type, Type)\n\n        if signature is None:\n            signature = defaultdict(list)\n\n        self.argument._set_type(ANY_TYPE, signature)\n        try:\n            self.function._set_type(ComplexType(self.argument.type, other_type), signature)\n        except TypeResolutionException:\n            raise TypeException(\n                    \"The function '%s' is of type '%s' and cannot be applied \"\n                    \"to '%s' of type '%s'.  Its argument must match type '%s'.\"\n                    % (self.function, self.function.type, self.argument,\n                       self.argument.type, self.function.type.first))\n\n    def findtype(self, variable):\n        assert isinstance(variable, Variable), \"%s is not a Variable\" % variable\n        if self.is_atom():\n            function, args = self.uncurry()\n        else:\n            function = self.function\n            args = [self.argument]\n\n        found = [arg.findtype(variable) for arg in [function]+args]\n\n        unique = []\n        for f in found:\n            if f != ANY_TYPE:\n                if unique:\n                    for u in unique:\n                        if f.matches(u):\n                            break\n                else:\n                    unique.append(f)\n\n        if len(unique) == 1:\n            return list(unique)[0]\n        else:\n            return ANY_TYPE\n\n    def constants(self):\n        if isinstance(self.function, AbstractVariableExpression):\n            function_constants = set()\n        else:\n            function_constants = self.function.constants()\n        return function_constants | self.argument.constants()\n\n    def predicates(self):\n        if isinstance(self.function, ConstantExpression):\n            function_preds = set([self.function.variable])\n        else:\n            function_preds = self.function.predicates()\n        return function_preds | self.argument.predicates()\n\n    def visit(self, function, combinator):\n        return combinator([function(self.function), function(self.argument)])\n\n    def __eq__(self, other):\n        return isinstance(other, ApplicationExpression) and \\\n                self.function == other.function and \\\n                self.argument == other.argument\n\n    def __ne__(self, other):\n        return not self == other\n\n    __hash__ = Expression.__hash__\n\n    def __str__(self):\n        if self.is_atom():\n            function, args = self.uncurry()\n            arg_str = ','.join(\"%s\" % arg for arg in args)\n        else:\n            function = self.function\n            arg_str = \"%s\" % self.argument\n\n        function_str = \"%s\" % function\n        parenthesize_function = False\n        if isinstance(function, LambdaExpression):\n            if isinstance(function.term, ApplicationExpression):\n                if not isinstance(function.term.function,\n                                  AbstractVariableExpression):\n                    parenthesize_function = True\n            elif not isinstance(function.term, BooleanExpression):\n                parenthesize_function = True\n        elif isinstance(function, ApplicationExpression):\n            parenthesize_function = True\n\n        if parenthesize_function:\n            function_str = Tokens.OPEN + function_str + Tokens.CLOSE\n\n        return function_str + Tokens.OPEN + arg_str + Tokens.CLOSE\n\n    def uncurry(self):\n        \"\"\"\n        Uncurry this application expression\n\n        return: A tuple (base-function, arg-list)\n        \"\"\"\n        function = self.function\n        args = [self.argument]\n        while isinstance(function, ApplicationExpression):\n            args.insert(0, function.argument)\n            function = function.function\n        return (function, args)\n\n    @property\n    def pred(self):\n        \"\"\"\n        Return uncurried base-function.\n        If this is an atom, then the result will be a variable expression.\n        Otherwise, it will be a lambda expression.\n        \"\"\"\n        return self.uncurry()[0]\n\n    @property\n    def args(self):\n        \"\"\"\n        Return uncurried arg-list\n        \"\"\"\n        return self.uncurry()[1]\n\n    def is_atom(self):\n        \"\"\"\n        Is this expression an atom (as opposed to a lambda expression applied\n        to a term)?\n        \"\"\"\n        return isinstance(self.pred, AbstractVariableExpression)\n\n\n@total_ordering\n@python_2_unicode_compatible\nclass AbstractVariableExpression(Expression):\n    def __init__(self, variable):\n        \"\"\"\n        :param variable: ``Variable``, for the variable\n        \"\"\"\n        assert isinstance(variable, Variable), \"%s is not a Variable\" % variable\n        self.variable = variable\n\n    def simplify(self):\n        return self\n\n    def replace(self, variable, expression, replace_bound=False, alpha_convert=True):\n        assert isinstance(variable, Variable), \"%s is not an Variable\" % variable\n        assert isinstance(expression, Expression), \"%s is not an Expression\" % expression\n        if self.variable == variable:\n            return expression\n        else:\n            return self\n\n    def _set_type(self, other_type=ANY_TYPE, signature=None):\n        assert isinstance(other_type, Type)\n\n        if signature is None:\n            signature = defaultdict(list)\n\n        resolution = other_type\n        for varEx in signature[self.variable.name]:\n            resolution = varEx.type.resolve(resolution)\n            if not resolution:\n                raise InconsistentTypeHierarchyException(self)\n\n        signature[self.variable.name].append(self)\n        for varEx in signature[self.variable.name]:\n            varEx.type = resolution\n\n    def findtype(self, variable):\n        assert isinstance(variable, Variable), \"%s is not a Variable\" % variable\n        if self.variable == variable:\n            return self.type\n        else:\n            return ANY_TYPE\n\n    def predicates(self):\n        return set()\n\n    def __eq__(self, other):\n        \"\"\"Allow equality between instances of ``AbstractVariableExpression``\n        subtypes.\"\"\"\n        return isinstance(other, AbstractVariableExpression) and \\\n               self.variable == other.variable\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __lt__(self, other):\n        if not isinstance(other, AbstractVariableExpression):\n            raise TypeError\n        return self.variable < other.variable\n\n    __hash__ = Expression.__hash__\n\n    def __str__(self):\n        return \"%s\" % self.variable\n\nclass IndividualVariableExpression(AbstractVariableExpression):\n    \"\"\"This class represents variables that take the form of a single lowercase\n    character (other than 'e') followed by zero or more digits.\"\"\"\n    def _set_type(self, other_type=ANY_TYPE, signature=None):\n        assert isinstance(other_type, Type)\n\n        if signature is None:\n            signature = defaultdict(list)\n\n        if not other_type.matches(ENTITY_TYPE):\n            raise IllegalTypeException(self, other_type, ENTITY_TYPE)\n\n        signature[self.variable.name].append(self)\n\n    def _get_type(self): return ENTITY_TYPE\n    type = property(_get_type, _set_type)\n\n    def free(self):\n        return set([self.variable])\n\n    def constants(self):\n        return set()\n\nclass FunctionVariableExpression(AbstractVariableExpression):\n    \"\"\"This class represents variables that take the form of a single uppercase\n    character followed by zero or more digits.\"\"\"\n    type = ANY_TYPE\n\n    def free(self):\n        return set([self.variable])\n\n    def constants(self):\n        return set()\n\nclass EventVariableExpression(IndividualVariableExpression):\n    \"\"\"This class represents variables that take the form of a single lowercase\n    'e' character followed by zero or more digits.\"\"\"\n    type = EVENT_TYPE\n\nclass ConstantExpression(AbstractVariableExpression):\n    \"\"\"This class represents variables that do not take the form of a single\n    character followed by zero or more digits.\"\"\"\n    type = ENTITY_TYPE\n\n    def _set_type(self, other_type=ANY_TYPE, signature=None):\n        assert isinstance(other_type, Type)\n\n        if signature is None:\n            signature = defaultdict(list)\n\n        if other_type == ANY_TYPE:\n            resolution = ENTITY_TYPE\n        else:\n            resolution = other_type\n            if self.type != ENTITY_TYPE:\n                resolution = resolution.resolve(self.type)\n\n        for varEx in signature[self.variable.name]:\n            resolution = varEx.type.resolve(resolution)\n            if not resolution:\n                raise InconsistentTypeHierarchyException(self)\n\n        signature[self.variable.name].append(self)\n        for varEx in signature[self.variable.name]:\n            varEx.type = resolution\n\n    def free(self):\n        return set()\n\n    def constants(self):\n        return set([self.variable])\n\n\ndef VariableExpression(variable):\n    \"\"\"\n    This is a factory method that instantiates and returns a subtype of\n    ``AbstractVariableExpression`` appropriate for the given variable.\n    \"\"\"\n    assert isinstance(variable, Variable), \"%s is not a Variable\" % variable\n    if is_indvar(variable.name):\n        return IndividualVariableExpression(variable)\n    elif is_funcvar(variable.name):\n        return FunctionVariableExpression(variable)\n    elif is_eventvar(variable.name):\n        return EventVariableExpression(variable)\n    else:\n        return ConstantExpression(variable)\n\n\nclass VariableBinderExpression(Expression):\n    \"\"\"This an abstract class for any Expression that binds a variable in an\n    Expression.  This includes LambdaExpressions and Quantified Expressions\"\"\"\n    def __init__(self, variable, term):\n        \"\"\"\n        :param variable: ``Variable``, for the variable\n        :param term: ``Expression``, for the term\n        \"\"\"\n        assert isinstance(variable, Variable), \"%s is not a Variable\" % variable\n        assert isinstance(term, Expression), \"%s is not an Expression\" % term\n        self.variable = variable\n        self.term = term\n\n    def replace(self, variable, expression, replace_bound=False, alpha_convert=True):\n        assert isinstance(variable, Variable), \"%s is not a Variable\" % variable\n        assert isinstance(expression, Expression), \"%s is not an Expression\" % expression\n        if self.variable == variable:\n            if replace_bound:\n                assert isinstance(expression, AbstractVariableExpression),\\\n                       \"%s is not a AbstractVariableExpression\" % expression\n                return self.__class__(expression.variable,\n                                      self.term.replace(variable, expression, True, alpha_convert))\n            else:\n                return self\n        else:\n            if alpha_convert and self.variable in expression.free():\n                self = self.alpha_convert(unique_variable(pattern=self.variable))\n\n            return self.__class__(self.variable,\n                                  self.term.replace(variable, expression, replace_bound, alpha_convert))\n\n    def alpha_convert(self, newvar):\n        \"\"\"Rename all occurrences of the variable introduced by this variable\n        binder in the expression to ``newvar``.\n        :param newvar: ``Variable``, for the new variable\n        \"\"\"\n        assert isinstance(newvar, Variable), \"%s is not a Variable\" % newvar\n        return self.__class__(newvar,\n                              self.term.replace(self.variable,\n                                                VariableExpression(newvar),\n                                                True))\n\n    def free(self):\n        return self.term.free() - set([self.variable])\n\n    def findtype(self, variable):\n        assert isinstance(variable, Variable), \"%s is not a Variable\" % variable\n        if variable == self.variable:\n            return ANY_TYPE\n        else:\n            return self.term.findtype(variable)\n\n    def visit(self, function, combinator):\n        return combinator([function(self.term)])\n\n    def visit_structured(self, function, combinator):\n        return combinator(self.variable, function(self.term))\n\n    def __eq__(self, other):\n        r\"\"\"Defines equality modulo alphabetic variance.  If we are comparing\n        \\x.M  and \\y.N, then check equality of M and N[x/y].\"\"\"\n        if isinstance(self, other.__class__) or \\\n           isinstance(other, self.__class__):\n            if self.variable == other.variable:\n                return self.term == other.term\n            else:\n                varex = VariableExpression(self.variable)\n                return self.term == other.term.replace(other.variable, varex)\n        else:\n            return False\n\n    def __ne__(self, other):\n        return not self == other\n\n    __hash__ = Expression.__hash__\n\n\n@python_2_unicode_compatible\nclass LambdaExpression(VariableBinderExpression):\n    @property\n    def type(self):\n        return ComplexType(self.term.findtype(self.variable),\n                           self.term.type)\n\n    def _set_type(self, other_type=ANY_TYPE, signature=None):\n        assert isinstance(other_type, Type)\n\n        if signature is None:\n            signature = defaultdict(list)\n\n        self.term._set_type(other_type.second, signature)\n        if not self.type.resolve(other_type):\n            raise TypeResolutionException(self, other_type)\n\n    def __str__(self):\n        variables = [self.variable]\n        term = self.term\n        while term.__class__ == self.__class__:\n            variables.append(term.variable)\n            term = term.term\n        return Tokens.LAMBDA + ' '.join(\"%s\" % v for v in variables) + \\\n               Tokens.DOT + \"%s\" % term\n\n\n@python_2_unicode_compatible\nclass QuantifiedExpression(VariableBinderExpression):\n    @property\n    def type(self): return TRUTH_TYPE\n\n    def _set_type(self, other_type=ANY_TYPE, signature=None):\n        assert isinstance(other_type, Type)\n\n        if signature is None:\n            signature = defaultdict(list)\n\n        if not other_type.matches(TRUTH_TYPE):\n            raise IllegalTypeException(self, other_type, TRUTH_TYPE)\n        self.term._set_type(TRUTH_TYPE, signature)\n\n    def __str__(self):\n        variables = [self.variable]\n        term = self.term\n        while term.__class__ == self.__class__:\n            variables.append(term.variable)\n            term = term.term\n        return self.getQuantifier() + ' ' + ' '.join(\"%s\" % v for v in variables) + \\\n               Tokens.DOT + \"%s\" % term\n\nclass ExistsExpression(QuantifiedExpression):\n    def getQuantifier(self):\n        return Tokens.EXISTS\n\nclass AllExpression(QuantifiedExpression):\n    def getQuantifier(self):\n        return Tokens.ALL\n\n\n@python_2_unicode_compatible\nclass NegatedExpression(Expression):\n    def __init__(self, term):\n        assert isinstance(term, Expression), \"%s is not an Expression\" % term\n        self.term = term\n\n    @property\n    def type(self): return TRUTH_TYPE\n\n    def _set_type(self, other_type=ANY_TYPE, signature=None):\n        assert isinstance(other_type, Type)\n\n        if signature is None:\n            signature = defaultdict(list)\n\n        if not other_type.matches(TRUTH_TYPE):\n            raise IllegalTypeException(self, other_type, TRUTH_TYPE)\n        self.term._set_type(TRUTH_TYPE, signature)\n\n    def findtype(self, variable):\n        assert isinstance(variable, Variable), \"%s is not a Variable\" % variable\n        return self.term.findtype(variable)\n\n    def visit(self, function, combinator):\n        return combinator([function(self.term)])\n\n    def negate(self):\n        return self.term\n\n    def __eq__(self, other):\n        return isinstance(other, NegatedExpression) and self.term == other.term\n\n    def __ne__(self, other):\n        return not self == other\n\n    __hash__ = Expression.__hash__\n\n    def __str__(self):\n        return Tokens.NOT + \"%s\" % self.term\n\n\n@python_2_unicode_compatible\nclass BinaryExpression(Expression):\n    def __init__(self, first, second):\n        assert isinstance(first, Expression), \"%s is not an Expression\" % first\n        assert isinstance(second, Expression), \"%s is not an Expression\" % second\n        self.first = first\n        self.second = second\n\n    @property\n    def type(self): return TRUTH_TYPE\n\n    def findtype(self, variable):\n        assert isinstance(variable, Variable), \"%s is not a Variable\" % variable\n        f = self.first.findtype(variable)\n        s = self.second.findtype(variable)\n        if f == s or s == ANY_TYPE:\n            return f\n        elif f == ANY_TYPE:\n            return s\n        else:\n            return ANY_TYPE\n\n    def visit(self, function, combinator):\n        return combinator([function(self.first), function(self.second)])\n\n    def __eq__(self, other):\n        return (isinstance(self, other.__class__) or \\\n                isinstance(other, self.__class__)) and \\\n               self.first == other.first and self.second == other.second\n\n    def __ne__(self, other):\n        return not self == other\n\n    __hash__ = Expression.__hash__\n\n    def __str__(self):\n        first = self._str_subex(self.first)\n        second = self._str_subex(self.second)\n        return Tokens.OPEN + first + ' ' + self.getOp() \\\n                + ' ' + second + Tokens.CLOSE\n\n    def _str_subex(self, subex):\n        return \"%s\" % subex\n\n\nclass BooleanExpression(BinaryExpression):\n    def _set_type(self, other_type=ANY_TYPE, signature=None):\n        assert isinstance(other_type, Type)\n\n        if signature is None:\n            signature = defaultdict(list)\n\n        if not other_type.matches(TRUTH_TYPE):\n            raise IllegalTypeException(self, other_type, TRUTH_TYPE)\n        self.first._set_type(TRUTH_TYPE, signature)\n        self.second._set_type(TRUTH_TYPE, signature)\n\nclass AndExpression(BooleanExpression):\n    def getOp(self):\n        return Tokens.AND\n\n    def _str_subex(self, subex):\n        s = \"%s\" % subex\n        if isinstance(subex, AndExpression):\n            return s[1:-1]\n        return s\n\nclass OrExpression(BooleanExpression):\n    def getOp(self):\n        return Tokens.OR\n\n    def _str_subex(self, subex):\n        s = \"%s\" % subex\n        if isinstance(subex, OrExpression):\n            return s[1:-1]\n        return s\n\nclass ImpExpression(BooleanExpression):\n    def getOp(self):\n        return Tokens.IMP\n\nclass IffExpression(BooleanExpression):\n    def getOp(self):\n        return Tokens.IFF\n\n\nclass EqualityExpression(BinaryExpression):\n    def _set_type(self, other_type=ANY_TYPE, signature=None):\n        assert isinstance(other_type, Type)\n\n        if signature is None:\n            signature = defaultdict(list)\n\n        if not other_type.matches(TRUTH_TYPE):\n            raise IllegalTypeException(self, other_type, TRUTH_TYPE)\n        self.first._set_type(ENTITY_TYPE, signature)\n        self.second._set_type(ENTITY_TYPE, signature)\n\n    def getOp(self):\n        return Tokens.EQ\n\n\n\nclass LogicalExpressionException(Exception):\n    def __init__(self, index, message):\n        self.index = index\n        Exception.__init__(self, message)\n\nclass UnexpectedTokenException(LogicalExpressionException):\n    def __init__(self, index, unexpected=None, expected=None, message=None):\n        if unexpected and expected:\n            msg = \"Unexpected token: '%s'.  \" \\\n                  \"Expected token '%s'.\" % (unexpected, expected)\n        elif unexpected:\n            msg = \"Unexpected token: '%s'.\" % unexpected\n            if message:\n                msg += '  '+message\n        else:\n            msg = \"Expected token '%s'.\" % expected\n        LogicalExpressionException.__init__(self, index, msg)\n\nclass ExpectedMoreTokensException(LogicalExpressionException):\n    def __init__(self, index, message=None):\n        if not message:\n            message = 'More tokens expected.'\n        LogicalExpressionException.__init__(self, index, 'End of input found.  ' + message)\n\n\ndef is_indvar(expr):\n    \"\"\"\n    An individual variable must be a single lowercase character other than 'e',\n    followed by zero or more digits.\n\n    :param expr: str\n    :return: bool True if expr is of the correct form\n    \"\"\"\n    assert isinstance(expr, string_types), \"%s is not a string\" % expr\n    return re.match(r'^[a-df-z]\\d*$', expr) is not None\n\ndef is_funcvar(expr):\n    \"\"\"\n    A function variable must be a single uppercase character followed by\n    zero or more digits.\n\n    :param expr: str\n    :return: bool True if expr is of the correct form\n    \"\"\"\n    assert isinstance(expr, string_types), \"%s is not a string\" % expr\n    return re.match(r'^[A-Z]\\d*$', expr) is not None\n\ndef is_eventvar(expr):\n    \"\"\"\n    An event variable must be a single lowercase 'e' character followed by\n    zero or more digits.\n\n    :param expr: str\n    :return: bool True if expr is of the correct form\n    \"\"\"\n    assert isinstance(expr, string_types), \"%s is not a string\" % expr\n    return re.match(r'^e\\d*$', expr) is not None\n\n\ndef demo():\n    lexpr = Expression.fromstring\n    print('='*20 + 'Test reader' + '='*20)\n    print(lexpr(r'john'))\n    print(lexpr(r'man(x)'))\n    print(lexpr(r'-man(x)'))\n    print(lexpr(r'(man(x) & tall(x) & walks(x))'))\n    print(lexpr(r'exists x.(man(x) & tall(x) & walks(x))'))\n    print(lexpr(r'\\x.man(x)'))\n    print(lexpr(r'\\x.man(x)(john)'))\n    print(lexpr(r'\\x y.sees(x,y)'))\n    print(lexpr(r'\\x y.sees(x,y)(a,b)'))\n    print(lexpr(r'(\\x.exists y.walks(x,y))(x)'))\n    print(lexpr(r'exists x.x = y'))\n    print(lexpr(r'exists x.(x = y)'))\n    print(lexpr('P(x) & x=y & P(y)'))\n    print(lexpr(r'\\P Q.exists x.(P(x) & Q(x))'))\n    print(lexpr(r'man(x) <-> tall(x)'))\n\n    print('='*20 + 'Test simplify' + '='*20)\n    print(lexpr(r'\\x.\\y.sees(x,y)(john)(mary)').simplify())\n    print(lexpr(r'\\x.\\y.sees(x,y)(john, mary)').simplify())\n    print(lexpr(r'all x.(man(x) & (\\x.exists y.walks(x,y))(x))').simplify())\n    print(lexpr(r'(\\P.\\Q.exists x.(P(x) & Q(x)))(\\x.dog(x))(\\x.bark(x))').simplify())\n\n    print('='*20 + 'Test alpha conversion and binder expression equality' + '='*20)\n    e1 = lexpr('exists x.P(x)')\n    print(e1)\n    e2 = e1.alpha_convert(Variable('z'))\n    print(e2)\n    print(e1 == e2)\n\ndef demo_errors():\n    print('='*20 + 'Test reader errors' + '='*20)\n    demoException('(P(x) & Q(x)')\n    demoException('((P(x) &) & Q(x))')\n    demoException('P(x) -> ')\n    demoException('P(x')\n    demoException('P(x,')\n    demoException('P(x,)')\n    demoException('exists')\n    demoException('exists x.')\n    demoException('\\\\')\n    demoException('\\\\ x y.')\n    demoException('P(x)Q(x)')\n    demoException('(P(x)Q(x)')\n    demoException('exists x -> y')\n\ndef demoException(s):\n    try:\n        Expression.fromstring(s)\n    except LogicalExpressionException as e:\n        print(\"%s: %s\" % (e.__class__.__name__, e))\n\ndef printtype(ex):\n    print(\"%s : %s\" % (ex.str(), ex.type))\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\sem\\relextract": [".py", "\nfrom __future__ import print_function\n\n\nfrom collections import defaultdict\nimport re\n\nfrom six.moves import html_entities\n\nNE_CLASSES = {\n    'ieer': ['LOCATION', 'ORGANIZATION', 'PERSON', 'DURATION',\n            'DATE', 'CARDINAL', 'PERCENT', 'MONEY', 'MEASURE'],\n    'conll2002': ['LOC', 'PER', 'ORG'],\n    'ace': ['LOCATION', 'ORGANIZATION', 'PERSON', 'DURATION',\n            'DATE', 'CARDINAL', 'PERCENT', 'MONEY', 'MEASURE', 'FACILITY', 'GPE'],\n    }\n\nshort2long = dict(LOC = 'LOCATION', ORG = 'ORGANIZATION', PER = 'PERSON')\nlong2short = dict(LOCATION ='LOC', ORGANIZATION = 'ORG', PERSON = 'PER')\n\n\ndef _expand(type):\n    try:\n        return short2long[type]\n    except KeyError:\n        return type\n\ndef class_abbrev(type):\n    try:\n        return long2short[type]\n    except KeyError:\n        return type\n\n\ndef _join(lst, sep=' ', untag=False):\n    try:\n        return sep.join(lst)\n    except TypeError:\n        if untag:\n            return sep.join(tup[0] for tup in lst)\n        from nltk.tag import tuple2str\n        return sep.join(tuple2str(tup) for tup in lst)\n\ndef descape_entity(m, defs=html_entities.entitydefs):\n    try:\n        return defs[m.group(1)]\n\n    except KeyError:\n        return m.group(0) # use as is\n\ndef list2sym(lst):\n    sym = _join(lst, '_', untag=True)\n    sym = sym.lower()\n    ENT = re.compile(\"&(\\w+?);\")\n    sym = ENT.sub(descape_entity, sym)\n    sym = sym.replace('.', '')\n    return sym\n\ndef tree2semi_rel(tree):\n\n    from nltk.tree import Tree\n\n    semi_rels = []\n    semi_rel = [[], None]\n\n    for dtr in tree:\n        if not isinstance(dtr, Tree):\n            semi_rel[0].append(dtr)\n        else:\n            semi_rel[1] = dtr\n            semi_rels.append(semi_rel)\n            semi_rel = [[], None]\n    return semi_rels\n\n\ndef semi_rel2reldict(pairs, window=5, trace=False):\n    result = []\n    while len(pairs) > 2:\n        reldict = defaultdict(str)\n        reldict['lcon'] = _join(pairs[0][0][-window:])\n        reldict['subjclass'] = pairs[0][1].label()\n        reldict['subjtext'] = _join(pairs[0][1].leaves())\n        reldict['subjsym'] = list2sym(pairs[0][1].leaves())\n        reldict['filler'] = _join(pairs[1][0])\n        reldict['untagged_filler'] = _join(pairs[1][0], untag=True)\n        reldict['objclass'] = pairs[1][1].label()\n        reldict['objtext'] = _join(pairs[1][1].leaves())\n        reldict['objsym'] = list2sym(pairs[1][1].leaves())\n        reldict['rcon'] = _join(pairs[2][0][:window])\n        if trace:\n            print(\"(%s(%s, %s)\" % (reldict['untagged_filler'], reldict['subjclass'], reldict['objclass']))\n        result.append(reldict)\n        pairs = pairs[1:]\n    return result\n\ndef extract_rels(subjclass, objclass, doc, corpus='ace', pattern=None, window=10):\n\n    if subjclass and subjclass not in NE_CLASSES[corpus]:\n        if _expand(subjclass) in NE_CLASSES[corpus]:\n            subjclass = _expand(subjclass)\n        else:\n            raise ValueError(\"your value for the subject type has not been recognized: %s\" % subjclass)\n    if objclass and objclass not in NE_CLASSES[corpus]:\n        if _expand(objclass) in NE_CLASSES[corpus]:\n            objclass = _expand(objclass)\n        else:\n            raise ValueError(\"your value for the object type has not been recognized: %s\" % objclass)\n\n    if corpus == 'ace' or corpus == 'conll2002':\n        pairs = tree2semi_rel(doc)\n    elif corpus == 'ieer':\n        pairs = tree2semi_rel(doc.text) + tree2semi_rel(doc.headline)\n    else:\n        raise ValueError(\"corpus type not recognized\")\n\n    reldicts = semi_rel2reldict(pairs)\n\n    relfilter = lambda x: (x['subjclass'] == subjclass and\n                           len(x['filler'].split()) <= window and\n                           pattern.match(x['filler']) and\n                           x['objclass'] == objclass)\n\n    return list(filter(relfilter, reldicts))\n\n\ndef rtuple(reldict, lcon=False, rcon=False):\n    items = [class_abbrev(reldict['subjclass']), reldict['subjtext'], reldict['filler'], class_abbrev(reldict['objclass']), reldict['objtext']]\n    format = '[%s: %r] %r [%s: %r]'\n    if lcon:\n        items = [reldict['lcon']] + items\n        format = '...%r)' + format\n    if rcon:\n        items.append(reldict['rcon'])\n        format = format + '(%r...'\n    printargs = tuple(items)\n    return format % printargs\n\ndef clause(reldict, relsym):\n    items = (relsym, reldict['subjsym'], reldict['objsym'])\n    return \"%s(%r, %r)\" % items\n\n\n\ndef in_demo(trace=0, sql=True):\n    from nltk.corpus import ieer\n    if sql:\n        try:\n            import sqlite3\n            connection =  sqlite3.connect(\":memory:\")\n            connection.text_factory = sqlite3.OptimizedUnicode\n            cur = connection.cursor()\n            cur.execute(\"\"\"create table Locations\n            (OrgName text, LocationName text, DocID text)\"\"\")\n        except ImportError:\n            import warnings\n            warnings.warn(\"Cannot import sqlite; sql flag will be ignored.\")\n\n\n    IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n\n    print()\n    print(\"IEER: in(ORG, LOC) -- just the clauses:\")\n    print(\"=\" * 45)\n\n    for file in ieer.fileids():\n        for doc in ieer.parsed_docs(file):\n            if trace:\n                print(doc.docno)\n                print(\"=\" * 15)\n            for rel in extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern=IN):\n                print(clause(rel, relsym='IN'))\n                if sql:\n                    try:\n                        rtuple = (rel['subjtext'], rel['objtext'], doc.docno)\n                        cur.execute(\"\"\"insert into Locations\n                                    values (?, ?, ?)\"\"\", rtuple)\n                        connection.commit()\n                    except NameError:\n                        pass\n\n    if sql:\n        try:\n            cur.execute(\"\"\"select OrgName from Locations\n                        where LocationName = 'Atlanta'\"\"\")\n            print()\n            print(\"Extract data from SQL table: ORGs in Atlanta\")\n            print(\"-\" * 15)\n            for row in cur:\n                print(row)\n        except NameError:\n            pass\n\n\n\ndef roles_demo(trace=0):\n    from nltk.corpus import ieer\n    roles = \"\"\"\n    (.*(                   # assorted roles\n    analyst|\n    chair(wo)?man|\n    commissioner|\n    counsel|\n    director|\n    economist|\n    editor|\n    executive|\n    foreman|\n    governor|\n    head|\n    lawyer|\n    leader|\n    librarian).*)|\n    manager|\n    partner|\n    president|\n    producer|\n    professor|\n    researcher|\n    spokes(wo)?man|\n    writer|\n    ,\\sof\\sthe?\\s*  # \"X, of (the) Y\"\n    \"\"\"\n    ROLES = re.compile(roles, re.VERBOSE)\n\n    print()\n    print(\"IEER: has_role(PER, ORG) -- raw rtuples:\")\n    print(\"=\" * 45)\n\n    for file in ieer.fileids():\n        for doc in ieer.parsed_docs(file):\n            lcon = rcon = False\n            if trace:\n                print(doc.docno)\n                print(\"=\" * 15)\n                lcon = rcon = True\n            for rel in extract_rels('PER', 'ORG', doc, corpus='ieer', pattern=ROLES):\n                print(rtuple(rel, lcon=lcon, rcon=rcon))\n\n\n\n\ndef ieer_headlines():\n\n    from nltk.corpus import ieer\n    from nltk.tree import Tree\n\n    print(\"IEER: First 20 Headlines\")\n    print(\"=\" * 45)\n\n    trees = [(doc.docno, doc.headline) for file in ieer.fileids() for doc in ieer.parsed_docs(file)]\n    for tree in trees[:20]:\n        print()\n        print(\"%s:\\n%s\" % tree)\n\n\n\n\ndef conllned(trace=1):\n    \"\"\"\n    Find the copula+'van' relation ('of') in the Dutch tagged training corpus\n    from CoNLL 2002.\n    \"\"\"\n\n    from nltk.corpus import conll2002\n\n    vnv = \"\"\"\n    (\n    is/V|    # 3rd sing present and\n    was/V|   # past forms of the verb zijn ('be')\n    werd/V|  # and also present\n    wordt/V  # past of worden ('become)\n    )\n    .*       # followed by anything\n    van/Prep # followed by van ('of')\n    \"\"\"\n    VAN = re.compile(vnv, re.VERBOSE)\n\n    print()\n    print(\"Dutch CoNLL2002: van(PER, ORG) -- raw rtuples with context:\")\n    print(\"=\" * 45)\n\n\n    for doc in conll2002.chunked_sents('ned.train'):\n        lcon = rcon = False\n        if trace:\n                lcon = rcon = True\n        for rel in extract_rels('PER', 'ORG', doc, corpus='conll2002', pattern=VAN, window=10):\n            print(rtuple(rel, lcon=lcon, rcon=rcon))\n\n\ndef conllesp():\n    from nltk.corpus import conll2002\n\n    de = \"\"\"\n    .*\n    (\n    de/SP|\n    del/SP\n    )\n    \"\"\"\n    DE = re.compile(de, re.VERBOSE)\n\n    print()\n    print(\"Spanish CoNLL2002: de(ORG, LOC) -- just the first 10 clauses:\")\n    print(\"=\" * 45)\n    rels = [rel for doc in conll2002.chunked_sents('esp.train')\n            for rel in extract_rels('ORG', 'LOC', doc, corpus='conll2002', pattern = DE)]\n    for r in rels[:10]: print(clause(r, relsym='DE'))\n    print()\n\n\ndef ne_chunked():\n    print()\n    print(\"1500 Sentences from Penn Treebank, as processed by NLTK NE Chunker\")\n    print(\"=\" * 45)\n    ROLE = re.compile(r'.*(chairman|president|trader|scientist|economist|analyst|partner).*')\n    rels = []\n    for i, sent in enumerate(nltk.corpus.treebank.tagged_sents()[:1500]):\n        sent = nltk.ne_chunk(sent)\n        rels = extract_rels('PER', 'ORG', sent, corpus='ace', pattern=ROLE, window=7)\n        for rel in rels:\n            print('{0:<5}{1}'.format(i, rtuple(rel)))\n\n\nif __name__ == '__main__':\n    import nltk\n    from nltk.sem import relextract\n    in_demo(trace=0)\n    roles_demo(trace=0)\n    conllned()\n    conllesp()\n    ieer_headlines()\n    ne_chunked()\n"], "nltk\\sem\\skolemize": [".py", "\nfrom nltk.sem.logic import (AllExpression, AndExpression, ApplicationExpression,\n                            EqualityExpression, ExistsExpression, IffExpression,\n                            ImpExpression, NegatedExpression, OrExpression,\n                            VariableExpression, skolem_function, unique_variable)\n\ndef skolemize(expression, univ_scope=None, used_variables=None):\n    if univ_scope is None:\n        univ_scope = set()\n    if used_variables is None:\n        used_variables = set()\n\n    if isinstance(expression, AllExpression):\n        term = skolemize(expression.term, univ_scope|set([expression.variable]), used_variables|set([expression.variable]))\n        return term.replace(expression.variable, VariableExpression(unique_variable(ignore=used_variables)))\n    elif isinstance(expression, AndExpression):\n        return skolemize(expression.first, univ_scope, used_variables) &\\\n               skolemize(expression.second, univ_scope, used_variables)\n    elif isinstance(expression, OrExpression):\n        return to_cnf(skolemize(expression.first, univ_scope, used_variables),\n                      skolemize(expression.second, univ_scope, used_variables))\n    elif isinstance(expression, ImpExpression):\n        return to_cnf(skolemize(-expression.first, univ_scope, used_variables),\n                      skolemize(expression.second, univ_scope, used_variables))\n    elif isinstance(expression, IffExpression):\n        return to_cnf(skolemize(-expression.first, univ_scope, used_variables),\n                      skolemize(expression.second, univ_scope, used_variables)) &\\\n               to_cnf(skolemize(expression.first, univ_scope, used_variables),\n                      skolemize(-expression.second, univ_scope, used_variables))\n    elif isinstance(expression, EqualityExpression):\n        return expression\n    elif isinstance(expression, NegatedExpression):\n        negated = expression.term\n        if isinstance(negated, AllExpression):\n            term = skolemize(-negated.term, univ_scope, used_variables|set([negated.variable]))\n            if univ_scope:\n                return term.replace(negated.variable, skolem_function(univ_scope))\n            else:\n                skolem_constant = VariableExpression(unique_variable(ignore=used_variables))\n                return term.replace(negated.variable, skolem_constant)\n        elif isinstance(negated, AndExpression):\n            return to_cnf(skolemize(-negated.first, univ_scope, used_variables),\n                          skolemize(-negated.second, univ_scope, used_variables))\n        elif isinstance(negated, OrExpression):\n            return skolemize(-negated.first, univ_scope, used_variables) &\\\n                   skolemize(-negated.second, univ_scope, used_variables)\n        elif isinstance(negated, ImpExpression):\n            return skolemize(negated.first, univ_scope, used_variables) &\\\n                   skolemize(-negated.second, univ_scope, used_variables)\n        elif isinstance(negated, IffExpression):\n            return to_cnf(skolemize(-negated.first, univ_scope, used_variables),\n                          skolemize(-negated.second, univ_scope, used_variables)) &\\\n                   to_cnf(skolemize(negated.first, univ_scope, used_variables),\n                          skolemize(negated.second, univ_scope, used_variables))\n        elif isinstance(negated, EqualityExpression):\n            return expression\n        elif isinstance(negated, NegatedExpression):\n            return skolemize(negated.term, univ_scope, used_variables)\n        elif isinstance(negated, ExistsExpression):\n            term = skolemize(-negated.term, univ_scope|set([negated.variable]), used_variables|set([negated.variable]))\n            return term.replace(negated.variable, VariableExpression(unique_variable(ignore=used_variables)))\n        elif isinstance(negated, ApplicationExpression):\n            return expression\n        else:\n            raise Exception('\\'%s\\' cannot be skolemized' % expression)\n    elif isinstance(expression, ExistsExpression):\n        term = skolemize(expression.term, univ_scope, used_variables|set([expression.variable]))\n        if univ_scope:\n            return term.replace(expression.variable, skolem_function(univ_scope))\n        else:\n            skolem_constant = VariableExpression(unique_variable(ignore=used_variables))\n            return term.replace(expression.variable, skolem_constant)\n    elif isinstance(expression, ApplicationExpression):\n        return expression\n    else:\n        raise Exception('\\'%s\\' cannot be skolemized' % expression)\n\ndef to_cnf(first, second):\n    if isinstance(first, AndExpression):\n        r_first = to_cnf(first.first, second)\n        r_second = to_cnf(first.second, second)\n        return r_first & r_second\n    elif isinstance(second, AndExpression):\n        r_first = to_cnf(first, second.first)\n        r_second = to_cnf(first, second.second)\n        return r_first & r_second\n    else:\n        return first | second\n"], "nltk\\sem\\util": [".py", "\nfrom __future__ import print_function, unicode_literals\n\nimport codecs\nfrom nltk.sem import evaluate\n\n\n\ndef parse_sents(inputs, grammar, trace=0):\n    from nltk.grammar import FeatureGrammar\n    from nltk.parse import FeatureChartParser, load_parser\n\n    if isinstance(grammar, FeatureGrammar):\n        cp = FeatureChartParser(grammar)\n    else:\n        cp = load_parser(grammar, trace=trace)\n    parses = []\n    for sent in inputs:\n        tokens = sent.split() # use a tokenizer?\n        syntrees = list(cp.parse(tokens))\n        parses.append(syntrees)\n    return parses\n\ndef root_semrep(syntree, semkey='SEM'):\n    from nltk.grammar import FeatStructNonterminal\n\n    node = syntree.label()\n    assert isinstance(node, FeatStructNonterminal)\n    try:\n        return node[semkey]\n    except KeyError:\n        print(node, end=' ')\n        print(\"has no specification for the feature %s\" % semkey)\n    raise\n\ndef interpret_sents(inputs, grammar, semkey='SEM', trace=0):\n    return [[(syn, root_semrep(syn, semkey)) for syn in syntrees]\n            for syntrees in parse_sents(inputs, grammar, trace=trace)]\n\ndef evaluate_sents(inputs, grammar, model, assignment, trace=0):\n    return [[(syn, sem, model.evaluate(\"%s\" % sem, assignment, trace=trace))\n            for (syn, sem) in interpretations]\n            for interpretations in interpret_sents(inputs, grammar)]\n\n\ndef demo_model0():\n    global m0, g0\n    v = [('john', 'b1'),\n        ('mary', 'g1'),\n        ('suzie', 'g2'),\n        ('fido', 'd1'),\n        ('tess', 'd2'),\n        ('noosa', 'n'),\n        ('girl', set(['g1', 'g2'])),\n        ('boy', set(['b1', 'b2'])),\n        ('dog', set(['d1', 'd2'])),\n        ('bark', set(['d1', 'd2'])),\n        ('walk', set(['b1', 'g2', 'd1'])),\n        ('chase', set([('b1', 'g1'), ('b2', 'g1'), ('g1', 'd1'), ('g2', 'd2')])),\n        ('see', set([('b1', 'g1'), ('b2', 'd2'), ('g1', 'b1'),('d2', 'b1'), ('g2', 'n')])),\n        ('in', set([('b1', 'n'), ('b2', 'n'), ('d2', 'n')])),\n        ('with', set([('b1', 'g1'), ('g1', 'b1'), ('d1', 'b1'), ('b1', 'd1')]))\n     ]\n    val = evaluate.Valuation(v)\n    dom = val.domain\n    m0 = evaluate.Model(dom, val)\n    g0 = evaluate.Assignment(dom)\n\n\ndef read_sents(filename, encoding='utf8'):\n    with codecs.open(filename, 'r', encoding) as fp:\n        sents = [l.rstrip() for l in fp]\n\n    sents = [l for l in sents if len(l) > 0]\n    sents = [l for l in sents if not l[0] == '#']\n    return sents\n\ndef demo_legacy_grammar():\n    from nltk.grammar import FeatureGrammar\n\n    g = FeatureGrammar.fromstring(\"\"\"\n    % start S\n    S[sem=<hello>] -> 'hello'\n    \"\"\")\n    print(\"Reading grammar: %s\" % g)\n    print(\"*\" * 20)\n    for reading in interpret_sents(['hello'], g, semkey='sem'):\n        syn, sem = reading[0]\n        print()\n        print(\"output: \", sem)\n\ndef demo():\n    import sys\n    from optparse import OptionParser\n    description = \\\n    \"\"\"\n    Parse and evaluate some sentences.\n    \"\"\"\n\n    opts = OptionParser(description=description)\n\n    opts.set_defaults(evaluate=True, beta=True, syntrace=0,\n                      semtrace=0, demo='default', grammar='', sentences='')\n\n    opts.add_option(\"-d\", \"--demo\", dest=\"demo\",\n                    help=\"choose demo D; omit this for the default demo, or specify 'chat80'\", metavar=\"D\")\n    opts.add_option(\"-g\", \"--gram\", dest=\"grammar\",\n                    help=\"read in grammar G\", metavar=\"G\")\n    opts.add_option(\"-m\", \"--model\", dest=\"model\",\n                        help=\"import model M (omit '.py' suffix)\", metavar=\"M\")\n    opts.add_option(\"-s\", \"--sentences\", dest=\"sentences\",\n                        help=\"read in a file of test sentences S\", metavar=\"S\")\n    opts.add_option(\"-e\", \"--no-eval\", action=\"store_false\", dest=\"evaluate\",\n                    help=\"just do a syntactic analysis\")\n    opts.add_option(\"-b\", \"--no-beta-reduction\", action=\"store_false\",\n                    dest=\"beta\", help=\"don't carry out beta-reduction\")\n    opts.add_option(\"-t\", \"--syntrace\", action=\"count\", dest=\"syntrace\",\n                    help=\"set syntactic tracing on; requires '-e' option\")\n    opts.add_option(\"-T\", \"--semtrace\", action=\"count\", dest=\"semtrace\",\n                    help=\"set semantic tracing on\")\n\n    (options, args) = opts.parse_args()\n\n    SPACER = '-' * 30\n\n    demo_model0()\n\n    sents = [\n    'Fido sees a boy with Mary',\n    'John sees Mary',\n    'every girl chases a dog',\n    'every boy chases a girl',\n    'John walks with a girl in Noosa',\n    'who walks']\n\n    gramfile = 'grammars/sample_grammars/sem2.fcfg'\n\n    if options.sentences:\n        sentsfile = options.sentences\n    if options.grammar:\n        gramfile = options.grammar\n    if options.model:\n        exec(\"import %s as model\" % options.model)\n\n    if sents is None:\n        sents = read_sents(sentsfile)\n\n    model = m0\n    g = g0\n\n    if options.evaluate:\n        evaluations = \\\n            evaluate_sents(sents, gramfile, model, g, trace=options.semtrace)\n    else:\n        semreps = \\\n            interpret_sents(sents, gramfile, trace=options.syntrace)\n\n    for i, sent in enumerate(sents):\n        n = 1\n        print('\\nSentence: %s' % sent)\n        print(SPACER)\n        if options.evaluate:\n\n            for (syntree, semrep, value) in evaluations[i]:\n                if isinstance(value, dict):\n                    value = set(value.keys())\n                print('%d:  %s' % (n, semrep))\n                print(value)\n                n += 1\n        else:\n\n            for (syntree, semrep) in semreps[i]:\n                print('%d:  %s' % (n, semrep))\n                n += 1\n\nif __name__ == \"__main__\":\n    demo()\n    demo_legacy_grammar()\n"], "nltk\\sem\\__init__": [".py", "\n\nfrom nltk.sem.util import (parse_sents, interpret_sents, evaluate_sents,\n                           root_semrep)\nfrom nltk.sem.evaluate import (Valuation, Assignment, Model, Undefined,\n                               is_rel, set2rel, arity, read_valuation)\nfrom nltk.sem.logic import (boolean_ops, binding_ops, equality_preds,\n                           read_logic, Variable, Expression,\n                           ApplicationExpression, LogicalExpressionException)\nfrom nltk.sem.skolemize import skolemize\nfrom nltk.sem.lfg import FStructure\nfrom nltk.sem.relextract import (extract_rels, rtuple, clause)\nfrom nltk.sem.boxer import Boxer\nfrom nltk.sem.drt import DrtExpression, DRS\n\n\n", 1], "nltk\\sentiment\\sentiment_analyzer": [".py", "\n\nfrom __future__ import print_function\nfrom collections import defaultdict\n\nfrom nltk.classify.util import apply_features, accuracy as eval_accuracy\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import (BigramAssocMeasures, precision as eval_precision,\n    recall as eval_recall, f_measure as eval_f_measure)\n\nfrom nltk.probability import FreqDist\n\nfrom nltk.sentiment.util import save_file, timer\n\nclass SentimentAnalyzer(object):\n    def __init__(self, classifier=None):\n        self.feat_extractors = defaultdict(list)\n        self.classifier = classifier\n\n    def all_words(self, documents, labeled=None):\n        all_words = []\n        if labeled is None:\n            labeled = documents and isinstance(documents[0], tuple)\n        if labeled == True:\n            for words, sentiment in documents:\n                all_words.extend(words)\n        elif labeled == False:\n            for words in documents:\n                all_words.extend(words)\n        return all_words\n\n    def apply_features(self, documents, labeled=None):\n        return apply_features(self.extract_features, documents, labeled)\n\n    def unigram_word_feats(self, words, top_n=None, min_freq=0):\n        unigram_feats_freqs = FreqDist(word for word in words)\n        return [w for w, f in unigram_feats_freqs.most_common(top_n)\n                if unigram_feats_freqs[w] > min_freq]\n\n    def bigram_collocation_feats(self, documents, top_n=None, min_freq=3,\n                                 assoc_measure=BigramAssocMeasures.pmi):\n        finder = BigramCollocationFinder.from_documents(documents)\n        finder.apply_freq_filter(min_freq)\n        return finder.nbest(assoc_measure, top_n)\n\n    def classify(self, instance):\n        instance_feats = self.apply_features([instance], labeled=False)\n        return self.classifier.classify(instance_feats[0])\n\n    def add_feat_extractor(self, function, **kwargs):\n        self.feat_extractors[function].append(kwargs)\n\n    def extract_features(self, document):\n        all_features = {}\n        for extractor in self.feat_extractors:\n            for param_set in self.feat_extractors[extractor]:\n                feats = extractor(document, **param_set)\n            all_features.update(feats)\n        return all_features\n\n    def train(self, trainer, training_set, save_classifier=None, **kwargs):\n        print(\"Training classifier\")\n        self.classifier = trainer(training_set, **kwargs)\n        if save_classifier:\n            save_file(self.classifier, save_classifier)\n\n        return self.classifier\n\n    def evaluate(self, test_set, classifier=None, accuracy=True, f_measure=True,\n                 precision=True, recall=True, verbose=False):\n        if classifier is None:\n            classifier = self.classifier\n        print(\"Evaluating {0} results...\".format(type(classifier).__name__))\n        metrics_results = {}\n        if accuracy == True:\n            accuracy_score = eval_accuracy(classifier, test_set)\n            metrics_results['Accuracy'] = accuracy_score\n\n        gold_results = defaultdict(set)\n        test_results = defaultdict(set)\n        labels = set()\n        for i, (feats, label) in enumerate(test_set):\n            labels.add(label)\n            gold_results[label].add(i)\n            observed = classifier.classify(feats)\n            test_results[observed].add(i)\n\n        for label in labels:\n            if precision == True:\n                precision_score = eval_precision(gold_results[label],\n                    test_results[label])\n                metrics_results['Precision [{0}]'.format(label)] = precision_score\n            if recall == True:\n                recall_score = eval_recall(gold_results[label],\n                    test_results[label])\n                metrics_results['Recall [{0}]'.format(label)] = recall_score\n            if f_measure == True:\n                f_measure_score = eval_f_measure(gold_results[label],\n                    test_results[label])\n                metrics_results['F-measure [{0}]'.format(label)] = f_measure_score\n\n        if verbose == True:\n            for result in sorted(metrics_results):\n                print('{0}: {1}'.format(result, metrics_results[result]))\n\n        return metrics_results\n"], "nltk\\sentiment\\util": [".py", "\nfrom __future__ import division\n\nimport codecs\nimport csv\nimport json\nimport pickle\nimport random\nimport re\nimport sys\nimport time\nfrom copy import deepcopy\nfrom itertools import tee\n\nimport nltk\nfrom nltk.corpus import CategorizedPlaintextCorpusReader\nfrom nltk.data import load\nfrom nltk.tokenize.casual import EMOTICON_RE\nfrom nltk.twitter.common import outf_writer_compat, extract_fields\n\n\nNEGATION = r\"\"\"\n    (?:\n        ^(?:never|no|nothing|nowhere|noone|none|not|\n            havent|hasnt|hadnt|cant|couldnt|shouldnt|\n            wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint\n        )$\n    )\n    |\n    n't\"\"\"\n\nNEGATION_RE = re.compile(NEGATION, re.VERBOSE)\n\nCLAUSE_PUNCT = r'^[.:;!?]$'\nCLAUSE_PUNCT_RE = re.compile(CLAUSE_PUNCT)\n\n\nHAPPY = set([\n    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n    '<3'\n    ])\n\nSAD = set([\n    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n    ':c', ':{', '>:\\\\', ';('\n    ])\n\n\ndef timer(method):\n    \"\"\"\n    A timer decorator to measure execution performance of methods.\n    \"\"\"\n    def timed(*args, **kw):\n        start = time.time()\n        result = method(*args, **kw)\n        end = time.time()\n        tot_time = end - start\n        hours = tot_time // 3600\n        mins = tot_time // 60 % 60\n        secs = int(round(tot_time % 60))\n        if hours == 0 and mins == 0 and secs < 10:\n            print('[TIMER] {0}(): {:.3f} seconds'.format(method.__name__, tot_time))\n        else:\n            print('[TIMER] {0}(): {1}h {2}m {3}s'.format(method.__name__, hours, mins, secs))\n        return result\n    return timed\n\n\ndef pairwise(iterable):\n    a, b = tee(iterable)\n    next(b, None)\n    return zip(a, b)\n\n\"\"\"\nFeature extractor functions are declared outside the SentimentAnalyzer class.\nUsers should have the possibility to create their own feature extractors\nwithout modifying SentimentAnalyzer.\n\"\"\"\n\ndef extract_unigram_feats(document, unigrams, handle_negation=False):\n    \"\"\"\n    Populate a dictionary of unigram features, reflecting the presence/absence in\n    the document of each of the tokens in `unigrams`.\n\n    :param document: a list of words/tokens.\n    :param unigrams: a list of words/tokens whose presence/absence has to be\n        checked in `document`.\n    :param handle_negation: if `handle_negation == True` apply `mark_negation`\n        method to `document` before checking for unigram presence/absence.\n    :return: a dictionary of unigram features {unigram : boolean}.\n\n    >>> words = ['ice', 'police', 'riot']\n    >>> document = 'ice is melting due to global warming'.split()\n    >>> sorted(extract_unigram_feats(document, words).items())\n    [('contains(ice)', True), ('contains(police)', False), ('contains(riot)', False)]\n    \"\"\"\n    features = {}\n    if handle_negation:\n        document = mark_negation(document)\n    for word in unigrams:\n        features['contains({0})'.format(word)] = word in set(document)\n    return features\n\ndef extract_bigram_feats(document, bigrams):\n    \"\"\"\n    Populate a dictionary of bigram features, reflecting the presence/absence in\n    the document of each of the tokens in `bigrams`. This extractor function only\n    considers contiguous bigrams obtained by `nltk.bigrams`.\n\n    :param document: a list of words/tokens.\n    :param unigrams: a list of bigrams whose presence/absence has to be\n        checked in `document`.\n    :return: a dictionary of bigram features {bigram : boolean}.\n\n    >>> bigrams = [('global', 'warming'), ('police', 'prevented'), ('love', 'you')]\n    >>> document = 'ice is melting due to global warming'.split()\n    >>> sorted(extract_bigram_feats(document, bigrams).items())\n    [('contains(global - warming)', True), ('contains(love - you)', False),\n    ('contains(police - prevented)', False)]\n    \"\"\"\n    features = {}\n    for bigr in bigrams:\n        features['contains({0} - {1})'.format(bigr[0], bigr[1])] = bigr in nltk.bigrams(document)\n    return features\n\n\ndef mark_negation(document, double_neg_flip=False, shallow=False):\n    \"\"\"\n    Append _NEG suffix to words that appear in the scope between a negation\n    and a punctuation mark.\n\n    :param document: a list of words/tokens, or a tuple (words, label).\n    :param shallow: if True, the method will modify the original document in place.\n    :param double_neg_flip: if True, double negation is considered affirmation\n        (we activate/deactivate negation scope everytime we find a negation).\n    :return: if `shallow == True` the method will modify the original document\n        and return it. If `shallow == False` the method will return a modified\n        document, leaving the original unmodified.\n\n    >>> sent = \"I didn't like this movie . It was bad .\".split()\n    >>> mark_negation(sent)\n    ['I', \"didn't\", 'like_NEG', 'this_NEG', 'movie_NEG', '.', 'It', 'was', 'bad', '.']\n    \"\"\"\n    if not shallow:\n        document = deepcopy(document)\n    labeled = document and isinstance(document[0], (tuple, list))\n    if labeled:\n        doc = document[0]\n    else:\n        doc = document\n    neg_scope = False\n    for i, word in enumerate(doc):\n        if NEGATION_RE.search(word):\n            if not neg_scope or (neg_scope and double_neg_flip):\n                neg_scope = not neg_scope\n                continue\n            else:\n                doc[i] += '_NEG'\n        elif neg_scope and CLAUSE_PUNCT_RE.search(word):\n            neg_scope = not neg_scope\n        elif neg_scope and not CLAUSE_PUNCT_RE.search(word):\n            doc[i] += '_NEG'\n\n    return document\n\ndef output_markdown(filename, **kwargs):\n    \"\"\"\n    Write the output of an analysis to a file.\n    \"\"\"\n    with codecs.open(filename, 'at') as outfile:\n        text = '\\n*** \\n\\n'\n        text += '{0} \\n\\n'.format(time.strftime(\"%d/%m/%Y, %H:%M\"))\n        for k in sorted(kwargs):\n            if isinstance(kwargs[k], dict):\n                dictionary = kwargs[k]\n                text += '  - **{0}:**\\n'.format(k)\n                for entry in sorted(dictionary):\n                    text += '    - {0}: {1} \\n'.format(entry, dictionary[entry])\n            elif isinstance(kwargs[k], list):\n                text += '  - **{0}:**\\n'.format(k)\n                for entry in kwargs[k]:\n                    text += '    - {0}\\n'.format(entry)\n            else:\n                text += '  - **{0}:** {1} \\n'.format(k, kwargs[k])\n        outfile.write(text)\n\ndef save_file(content, filename):\n    \"\"\"\n    Store `content` in `filename`. Can be used to store a SentimentAnalyzer.\n    \"\"\"\n    print(\"Saving\", filename)\n    with codecs.open(filename, 'wb') as storage_file:\n        pickle.dump(content, storage_file, protocol=2)\n\ndef split_train_test(all_instances, n=None):\n    \"\"\"\n    Randomly split `n` instances of the dataset into train and test sets.\n\n    :param all_instances: a list of instances (e.g. documents) that will be split.\n    :param n: the number of instances to consider (in case we want to use only a\n        subset).\n    :return: two lists of instances. Train set is 8/10 of the total and test set\n        is 2/10 of the total.\n    \"\"\"\n    random.seed(12345)\n    random.shuffle(all_instances)\n    if not n or n > len(all_instances):\n        n = len(all_instances)\n    train_set = all_instances[:int(.8*n)]\n    test_set = all_instances[int(.8*n):n]\n\n    return train_set, test_set\n\ndef _show_plot(x_values, y_values, x_labels=None, y_labels=None):\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:\n        raise ImportError('The plot function requires matplotlib to be installed.'\n                         'See http://matplotlib.org/')\n\n    plt.locator_params(axis='y', nbins=3)\n    axes = plt.axes()\n    axes.yaxis.grid()\n    plt.plot(x_values, y_values, 'ro', color='red')\n    plt.ylim(ymin=-1.2, ymax=1.2)\n    plt.tight_layout(pad=5)\n    if x_labels:\n        plt.xticks(x_values, x_labels, rotation='vertical')\n    if y_labels:\n        plt.yticks([-1, 0, 1], y_labels, rotation='horizontal')\n    plt.margins(0.2)\n    plt.show()\n\n\ndef json2csv_preprocess(json_file, outfile, fields, encoding='utf8', errors='replace',\n            gzip_compress=False, skip_retweets=True, skip_tongue_tweets=True,\n            skip_ambiguous_tweets=True, strip_off_emoticons=True, remove_duplicates=True,\n            limit=None):\n    \"\"\"\n    Convert json file to csv file, preprocessing each row to obtain a suitable\n    dataset for tweets Semantic Analysis.\n\n    :param json_file: the original json file containing tweets.\n    :param outfile: the output csv filename.\n    :param fields: a list of fields that will be extracted from the json file and\n        kept in the output csv file.\n    :param encoding: the encoding of the files.\n    :param errors: the error handling strategy for the output writer.\n    :param gzip_compress: if True, create a compressed GZIP file.\n\n    :param skip_retweets: if True, remove retweets.\n    :param skip_tongue_tweets: if True, remove tweets containing \":P\" and \":-P\"\n        emoticons.\n    :param skip_ambiguous_tweets: if True, remove tweets containing both happy\n        and sad emoticons.\n    :param strip_off_emoticons: if True, strip off emoticons from all tweets.\n    :param remove_duplicates: if True, remove tweets appearing more than once.\n    :param limit: an integer to set the number of tweets to convert. After the\n        limit is reached the conversion will stop. It can be useful to create\n        subsets of the original tweets json data.\n    \"\"\"\n    with codecs.open(json_file, encoding=encoding) as fp:\n        (writer, outf) = outf_writer_compat(outfile, encoding, errors, gzip_compress)\n        writer.writerow(fields)\n\n        if remove_duplicates == True:\n            tweets_cache = []\n        i = 0\n        for line in fp:\n            tweet = json.loads(line)\n            row = extract_fields(tweet, fields)\n            try:\n                text = row[fields.index('text')]\n                if skip_retweets == True:\n                    if re.search(r'\\bRT\\b', text):\n                        continue\n                if skip_tongue_tweets == True:\n                    if re.search(r'\\:\\-?P\\b', text):\n                        continue\n                if skip_ambiguous_tweets == True:\n                    all_emoticons = EMOTICON_RE.findall(text)\n                    if all_emoticons:\n                        if (set(all_emoticons) & HAPPY) and (set(all_emoticons) & SAD):\n                            continue\n                if strip_off_emoticons == True:\n                    row[fields.index('text')] = re.sub(r'(?!\\n)\\s+', ' ', EMOTICON_RE.sub('', text))\n                if remove_duplicates == True:\n                    if row[fields.index('text')] in tweets_cache:\n                        continue\n                    else:\n                        tweets_cache.append(row[fields.index('text')])\n            except ValueError:\n                pass\n            writer.writerow(row)\n            i += 1\n            if limit and i >= limit:\n                break\n        outf.close()\n\ndef parse_tweets_set(filename, label, word_tokenizer=None, sent_tokenizer=None,\n                     skip_header=True):\n    \"\"\"\n    Parse csv file containing tweets and output data a list of (text, label) tuples.\n\n    :param filename: the input csv filename.\n    :param label: the label to be appended to each tweet contained in the csv file.\n    :param word_tokenizer: the tokenizer instance that will be used to tokenize\n        each sentence into tokens (e.g. WordPunctTokenizer() or BlanklineTokenizer()).\n        If no word_tokenizer is specified, tweets will not be tokenized.\n    :param sent_tokenizer: the tokenizer that will be used to split each tweet into\n        sentences.\n    :param skip_header: if True, skip the first line of the csv file (which usually\n        contains headers).\n\n    :return: a list of (text, label) tuples.\n    \"\"\"\n    tweets = []\n    if not sent_tokenizer:\n        sent_tokenizer = load('tokenizers/punkt/english.pickle')\n\n    if sys.version_info[0] == 3:\n        with codecs.open(filename, 'rt') as csvfile:\n            reader = csv.reader(csvfile)\n            if skip_header == True:\n                next(reader, None) # skip the header\n            i = 0\n            for tweet_id, text in reader:\n                i += 1\n                sys.stdout.write('Loaded {0} tweets\\r'.format(i))\n                if word_tokenizer:\n                    tweet = [w for sent in sent_tokenizer.tokenize(text)\n                                       for w in word_tokenizer.tokenize(sent)]\n                else:\n                    tweet = text\n                tweets.append((tweet, label))\n    elif sys.version_info[0] < 3:\n        with codecs.open(filename) as csvfile:\n            reader = csv.reader(csvfile)\n            if skip_header == True:\n                next(reader, None) # skip the header\n            i = 0\n            for row in reader:\n                unicode_row = [x.decode('utf8') for x in row]\n                text = unicode_row[1]\n                i += 1\n                sys.stdout.write('Loaded {0} tweets\\r'.format(i))\n                if word_tokenizer:\n                    tweet = [w.encode('utf8') for sent in sent_tokenizer.tokenize(text)\n                                       for w in word_tokenizer.tokenize(sent)]\n                else:\n                    tweet = text\n                tweets.append((tweet, label))\n    print(\"Loaded {0} tweets\".format(i))\n    return tweets\n\n\ndef demo_tweets(trainer, n_instances=None, output=None):\n    \"\"\"\n    Train and test Naive Bayes classifier on 10000 tweets, tokenized using\n    TweetTokenizer.\n    Features are composed of:\n        - 1000 most frequent unigrams\n        - 100 top bigrams (using BigramAssocMeasures.pmi)\n\n    :param trainer: `train` method of a classifier.\n    :param n_instances: the number of total tweets that have to be used for\n        training and testing. Tweets will be equally split between positive and\n        negative.\n    :param output: the output file where results have to be reported.\n    \"\"\"\n    from nltk.tokenize import TweetTokenizer\n    from nltk.sentiment import SentimentAnalyzer\n    from nltk.corpus import twitter_samples, stopwords\n\n    tokenizer = TweetTokenizer(preserve_case=False)\n\n    if n_instances is not None:\n        n_instances = int(n_instances/2)\n\n    fields = ['id', 'text']\n    positive_json = twitter_samples.abspath(\"positive_tweets.json\")\n    positive_csv = 'positive_tweets.csv'\n    json2csv_preprocess(positive_json, positive_csv, fields, limit=n_instances)\n\n    negative_json = twitter_samples.abspath(\"negative_tweets.json\")\n    negative_csv = 'negative_tweets.csv'\n    json2csv_preprocess(negative_json, negative_csv, fields, limit=n_instances)\n\n    neg_docs = parse_tweets_set(negative_csv, label='neg', word_tokenizer=tokenizer)\n    pos_docs = parse_tweets_set(positive_csv, label='pos', word_tokenizer=tokenizer)\n\n    train_pos_docs, test_pos_docs = split_train_test(pos_docs)\n    train_neg_docs, test_neg_docs = split_train_test(neg_docs)\n\n    training_tweets = train_pos_docs+train_neg_docs\n    testing_tweets = test_pos_docs+test_neg_docs\n\n    sentim_analyzer = SentimentAnalyzer()\n    all_words = [word for word in sentim_analyzer.all_words(training_tweets)]\n\n    unigram_feats = sentim_analyzer.unigram_word_feats(all_words, top_n=1000)\n    sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n\n    bigram_collocs_feats = sentim_analyzer.bigram_collocation_feats([tweet[0] for tweet in training_tweets],\n        top_n=100, min_freq=12)\n    sentim_analyzer.add_feat_extractor(extract_bigram_feats, bigrams=bigram_collocs_feats)\n\n    training_set = sentim_analyzer.apply_features(training_tweets)\n    test_set = sentim_analyzer.apply_features(testing_tweets)\n\n    classifier = sentim_analyzer.train(trainer, training_set)\n    try:\n        classifier.show_most_informative_features()\n    except AttributeError:\n        print('Your classifier does not provide a show_most_informative_features() method.')\n    results = sentim_analyzer.evaluate(test_set)\n\n    if output:\n        extr = [f.__name__ for f in sentim_analyzer.feat_extractors]\n        output_markdown(output, Dataset='labeled_tweets', Classifier=type(classifier).__name__,\n                        Tokenizer=tokenizer.__class__.__name__, Feats=extr,\n                        Results=results, Instances=n_instances)\n\ndef demo_movie_reviews(trainer, n_instances=None, output=None):\n    \"\"\"\n    Train classifier on all instances of the Movie Reviews dataset.\n    The corpus has been preprocessed using the default sentence tokenizer and\n    WordPunctTokenizer.\n    Features are composed of:\n        - most frequent unigrams\n\n    :param trainer: `train` method of a classifier.\n    :param n_instances: the number of total reviews that have to be used for\n        training and testing. Reviews will be equally split between positive and\n        negative.\n    :param output: the output file where results have to be reported.\n    \"\"\"\n    from nltk.corpus import movie_reviews\n    from nltk.sentiment import SentimentAnalyzer\n\n    if n_instances is not None:\n        n_instances = int(n_instances/2)\n\n    pos_docs = [(list(movie_reviews.words(pos_id)), 'pos') for pos_id in movie_reviews.fileids('pos')[:n_instances]]\n    neg_docs = [(list(movie_reviews.words(neg_id)), 'neg') for neg_id in movie_reviews.fileids('neg')[:n_instances]]\n    train_pos_docs, test_pos_docs = split_train_test(pos_docs)\n    train_neg_docs, test_neg_docs = split_train_test(neg_docs)\n\n    training_docs = train_pos_docs+train_neg_docs\n    testing_docs = test_pos_docs+test_neg_docs\n\n    sentim_analyzer = SentimentAnalyzer()\n    all_words = sentim_analyzer.all_words(training_docs)\n\n    unigram_feats = sentim_analyzer.unigram_word_feats(all_words, min_freq=4)\n    sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n    training_set = sentim_analyzer.apply_features(training_docs)\n    test_set = sentim_analyzer.apply_features(testing_docs)\n\n    classifier = sentim_analyzer.train(trainer, training_set)\n    try:\n        classifier.show_most_informative_features()\n    except AttributeError:\n        print('Your classifier does not provide a show_most_informative_features() method.')\n    results = sentim_analyzer.evaluate(test_set)\n\n    if output:\n        extr = [f.__name__ for f in sentim_analyzer.feat_extractors]\n        output_markdown(output, Dataset='Movie_reviews', Classifier=type(classifier).__name__,\n                        Tokenizer='WordPunctTokenizer', Feats=extr, Results=results,\n                        Instances=n_instances)\n\ndef demo_subjectivity(trainer, save_analyzer=False, n_instances=None, output=None):\n    \"\"\"\n    Train and test a classifier on instances of the Subjective Dataset by Pang and\n    Lee. The dataset is made of 5000 subjective and 5000 objective sentences.\n    All tokens (words and punctuation marks) are separated by a whitespace, so\n    we use the basic WhitespaceTokenizer to parse the data.\n\n    :param trainer: `train` method of a classifier.\n    :param save_analyzer: if `True`, store the SentimentAnalyzer in a pickle file.\n    :param n_instances: the number of total sentences that have to be used for\n        training and testing. Sentences will be equally split between positive\n        and negative.\n    :param output: the output file where results have to be reported.\n    \"\"\"\n    from nltk.sentiment import SentimentAnalyzer\n    from nltk.corpus import subjectivity\n\n    if n_instances is not None:\n        n_instances = int(n_instances/2)\n\n    subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n    obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n\n    train_subj_docs, test_subj_docs = split_train_test(subj_docs)\n    train_obj_docs, test_obj_docs = split_train_test(obj_docs)\n\n    training_docs = train_subj_docs+train_obj_docs\n    testing_docs = test_subj_docs+test_obj_docs\n\n    sentim_analyzer = SentimentAnalyzer()\n    all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n\n    unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n    sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n\n    training_set = sentim_analyzer.apply_features(training_docs)\n    test_set = sentim_analyzer.apply_features(testing_docs)\n\n    classifier = sentim_analyzer.train(trainer, training_set)\n    try:\n        classifier.show_most_informative_features()\n    except AttributeError:\n        print('Your classifier does not provide a show_most_informative_features() method.')\n    results = sentim_analyzer.evaluate(test_set)\n\n    if save_analyzer == True:\n        save_file(sentim_analyzer, 'sa_subjectivity.pickle')\n\n    if output:\n        extr = [f.__name__ for f in sentim_analyzer.feat_extractors]\n        output_markdown(output, Dataset='subjectivity', Classifier=type(classifier).__name__,\n                        Tokenizer='WhitespaceTokenizer', Feats=extr,\n                        Instances=n_instances, Results=results)\n\n    return sentim_analyzer\n\ndef demo_sent_subjectivity(text):\n    \"\"\"\n    Classify a single sentence as subjective or objective using a stored\n    SentimentAnalyzer.\n\n    :param text: a sentence whose subjectivity has to be classified.\n    \"\"\"\n    from nltk.classify import NaiveBayesClassifier\n    from nltk.tokenize import regexp\n    word_tokenizer = regexp.WhitespaceTokenizer()\n    try:\n        sentim_analyzer = load('sa_subjectivity.pickle')\n    except LookupError:\n        print('Cannot find the sentiment analyzer you want to load.')\n        print('Training a new one using NaiveBayesClassifier.')\n        sentim_analyzer = demo_subjectivity(NaiveBayesClassifier.train, True)\n\n    tokenized_text = [word.lower() for word in word_tokenizer.tokenize(text)]\n    print(sentim_analyzer.classify(tokenized_text))\n\ndef demo_liu_hu_lexicon(sentence, plot=False):\n    \"\"\"\n    Basic example of sentiment classification using Liu and Hu opinion lexicon.\n    This function simply counts the number of positive, negative and neutral words\n    in the sentence and classifies it depending on which polarity is more represented.\n    Words that do not appear in the lexicon are considered as neutral.\n\n    :param sentence: a sentence whose polarity has to be classified.\n    :param plot: if True, plot a visual representation of the sentence polarity.\n    \"\"\"\n    from nltk.corpus import opinion_lexicon\n    from nltk.tokenize import treebank\n\n    tokenizer = treebank.TreebankWordTokenizer()\n    pos_words = 0\n    neg_words = 0\n    tokenized_sent = [word.lower() for word in tokenizer.tokenize(sentence)]\n\n    x = list(range(len(tokenized_sent))) # x axis for the plot\n    y = []\n\n    for word in tokenized_sent:\n        if word in opinion_lexicon.positive():\n            pos_words += 1\n            y.append(1) # positive\n        elif word in opinion_lexicon.negative():\n            neg_words += 1\n            y.append(-1) # negative\n        else:\n            y.append(0) # neutral\n\n    if pos_words > neg_words:\n        print('Positive')\n    elif pos_words < neg_words:\n        print('Negative')\n    elif pos_words == neg_words:\n        print('Neutral')\n\n    if plot == True:\n        _show_plot(x, y, x_labels=tokenized_sent, y_labels=['Negative', 'Neutral', 'Positive'])\n\ndef demo_vader_instance(text):\n    \"\"\"\n    Output polarity scores for a text using Vader approach.\n\n    :param text: a text whose polarity has to be evaluated.\n    \"\"\"\n    from nltk.sentiment import SentimentIntensityAnalyzer\n    vader_analyzer = SentimentIntensityAnalyzer()\n    print(vader_analyzer.polarity_scores(text))\n\ndef demo_vader_tweets(n_instances=None, output=None):\n    \"\"\"\n    Classify 10000 positive and negative tweets using Vader approach.\n\n    :param n_instances: the number of total tweets that have to be classified.\n    :param output: the output file where results have to be reported.\n    \"\"\"\n    from collections import defaultdict\n    from nltk.corpus import twitter_samples\n    from nltk.sentiment import SentimentIntensityAnalyzer\n    from nltk.metrics import (accuracy as eval_accuracy, precision as eval_precision,\n        recall as eval_recall, f_measure as eval_f_measure)\n\n    if n_instances is not None:\n        n_instances = int(n_instances/2)\n\n    fields = ['id', 'text']\n    positive_json = twitter_samples.abspath(\"positive_tweets.json\")\n    positive_csv = 'positive_tweets.csv'\n    json2csv_preprocess(positive_json, positive_csv, fields, strip_off_emoticons=False,\n                        limit=n_instances)\n\n    negative_json = twitter_samples.abspath(\"negative_tweets.json\")\n    negative_csv = 'negative_tweets.csv'\n    json2csv_preprocess(negative_json, negative_csv, fields, strip_off_emoticons=False,\n                        limit=n_instances)\n\n    pos_docs = parse_tweets_set(positive_csv, label='pos')\n    neg_docs = parse_tweets_set(negative_csv, label='neg')\n\n    train_pos_docs, test_pos_docs = split_train_test(pos_docs)\n    train_neg_docs, test_neg_docs = split_train_test(neg_docs)\n\n    training_tweets = train_pos_docs+train_neg_docs\n    testing_tweets = test_pos_docs+test_neg_docs\n\n    vader_analyzer = SentimentIntensityAnalyzer()\n\n    gold_results = defaultdict(set)\n    test_results = defaultdict(set)\n    acc_gold_results = []\n    acc_test_results = []\n    labels = set()\n    num = 0\n    for i, (text, label) in enumerate(testing_tweets):\n        labels.add(label)\n        gold_results[label].add(i)\n        acc_gold_results.append(label)\n        score = vader_analyzer.polarity_scores(text)['compound']\n        if score > 0:\n            observed = 'pos'\n        else:\n            observed = 'neg'\n        num += 1\n        acc_test_results.append(observed)\n        test_results[observed].add(i)\n    metrics_results = {}\n    for label in labels:\n        accuracy_score = eval_accuracy(acc_gold_results,\n            acc_test_results)\n        metrics_results['Accuracy'] = accuracy_score\n        precision_score = eval_precision(gold_results[label],\n            test_results[label])\n        metrics_results['Precision [{0}]'.format(label)] = precision_score\n        recall_score = eval_recall(gold_results[label],\n            test_results[label])\n        metrics_results['Recall [{0}]'.format(label)] = recall_score\n        f_measure_score = eval_f_measure(gold_results[label],\n            test_results[label])\n        metrics_results['F-measure [{0}]'.format(label)] = f_measure_score\n\n    for result in sorted(metrics_results):\n            print('{0}: {1}'.format(result, metrics_results[result]))\n\n    if output:\n        output_markdown(output, Approach='Vader', Dataset='labeled_tweets',\n            Instances=n_instances, Results=metrics_results)\n\nif __name__ == '__main__':\n    from nltk.classify import NaiveBayesClassifier, MaxentClassifier\n    from nltk.classify.scikitlearn import SklearnClassifier\n    from sklearn.svm import LinearSVC\n\n    naive_bayes = NaiveBayesClassifier.train\n    svm = SklearnClassifier(LinearSVC()).train\n    maxent = MaxentClassifier.train\n\n    demo_tweets(naive_bayes)\n"], "nltk\\sentiment\\vader": [".py", "\n\nimport codecs\nimport math\nimport re\nimport string\nfrom itertools import product\nimport nltk.data\nfrom .util import pairwise\n\n\nB_INCR = 0.293\nB_DECR = -0.293\n\nC_INCR = 0.733\n\nN_SCALAR = -0.74\n\nREGEX_REMOVE_PUNCTUATION = re.compile('[{0}]'.format(re.escape(string.punctuation)))\n\nPUNC_LIST = [\".\", \"!\", \"?\", \",\", \";\", \":\", \"-\", \"'\", \"\\\"\",\n             \"!!\", \"!!!\", \"??\", \"???\", \"?!?\", \"!?!\", \"?!?!\", \"!?!?\"]\nNEGATE = {\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"}\n\n\nBOOSTER_DICT = \\\n{\"absolutely\": B_INCR, \"amazingly\": B_INCR, \"awfully\": B_INCR, \"completely\": B_INCR, \"considerably\": B_INCR,\n \"decidedly\": B_INCR, \"deeply\": B_INCR, \"effing\": B_INCR, \"enormously\": B_INCR,\n \"entirely\": B_INCR, \"especially\": B_INCR, \"exceptionally\": B_INCR, \"extremely\": B_INCR,\n \"fabulously\": B_INCR, \"flipping\": B_INCR, \"flippin\": B_INCR,\n \"fricking\": B_INCR, \"frickin\": B_INCR, \"frigging\": B_INCR, \"friggin\": B_INCR, \"fully\": B_INCR, \"fucking\": B_INCR,\n \"greatly\": B_INCR, \"hella\": B_INCR, \"highly\": B_INCR, \"hugely\": B_INCR, \"incredibly\": B_INCR,\n \"intensely\": B_INCR, \"majorly\": B_INCR, \"more\": B_INCR, \"most\": B_INCR, \"particularly\": B_INCR,\n \"purely\": B_INCR, \"quite\": B_INCR, \"really\": B_INCR, \"remarkably\": B_INCR,\n \"so\": B_INCR, \"substantially\": B_INCR,\n \"thoroughly\": B_INCR, \"totally\": B_INCR, \"tremendously\": B_INCR,\n \"uber\": B_INCR, \"unbelievably\": B_INCR, \"unusually\": B_INCR, \"utterly\": B_INCR,\n \"very\": B_INCR,\n \"almost\": B_DECR, \"barely\": B_DECR, \"hardly\": B_DECR, \"just enough\": B_DECR,\n \"kind of\": B_DECR, \"kinda\": B_DECR, \"kindof\": B_DECR, \"kind-of\": B_DECR,\n \"less\": B_DECR, \"little\": B_DECR, \"marginally\": B_DECR, \"occasionally\": B_DECR, \"partly\": B_DECR,\n \"scarcely\": B_DECR, \"slightly\": B_DECR, \"somewhat\": B_DECR,\n \"sort of\": B_DECR, \"sorta\": B_DECR, \"sortof\": B_DECR, \"sort-of\": B_DECR}\n\nSPECIAL_CASE_IDIOMS = {\"the shit\": 3, \"the bomb\": 3, \"bad ass\": 1.5, \"yeah right\": -2,\n                       \"cut the mustard\": 2, \"kiss of death\": -1.5, \"hand to mouth\": -2}\n\n\n\ndef negated(input_words, include_nt=True):\n    neg_words = NEGATE\n    if any(word.lower() in neg_words for word in input_words):\n        return True\n    if include_nt:\n        if any(\"n't\" in word.lower() for word in input_words):\n            return True\n    for first, second in pairwise(input_words):\n        if second.lower() == \"least\" and first.lower() != 'at':\n            return True\n    return False\n\n\ndef normalize(score, alpha=15):\n    norm_score = score/math.sqrt((score*score) + alpha)\n    return norm_score\n\n\ndef allcap_differential(words):\n    is_different = False\n    allcap_words = 0\n    for word in words:\n        if word.isupper():\n            allcap_words += 1\n    cap_differential = len(words) - allcap_words\n    if cap_differential > 0 and cap_differential < len(words):\n        is_different = True\n    return is_different\n\n\ndef scalar_inc_dec(word, valence, is_cap_diff):\n    scalar = 0.0\n    word_lower = word.lower()\n    if word_lower in BOOSTER_DICT:\n        scalar = BOOSTER_DICT[word_lower]\n        if valence < 0:\n            scalar *= -1\n        if word.isupper() and is_cap_diff:\n            if valence > 0:\n                scalar += C_INCR\n            else: scalar -= C_INCR\n    return scalar\n\nclass SentiText(object):\n    def __init__(self, text):\n        if not isinstance(text, str):\n            text = str(text.encode('utf-8'))\n        self.text = text\n        self.words_and_emoticons = self._words_and_emoticons()\n        self.is_cap_diff = allcap_differential(self.words_and_emoticons)\n\n    def _words_plus_punc(self):\n        no_punc_text = REGEX_REMOVE_PUNCTUATION.sub('', self.text)\n        words_only = no_punc_text.split()\n        words_only = set( w for w in words_only if len(w) > 1 )\n        punc_before = {''.join(p): p[1] for p in product(PUNC_LIST, words_only)}\n        punc_after = {''.join(p): p[0] for p in product(words_only, PUNC_LIST)}\n        words_punc_dict = punc_before\n        words_punc_dict.update(punc_after)\n        return words_punc_dict\n\n    def _words_and_emoticons(self):\n        wes = self.text.split()\n        words_punc_dict = self._words_plus_punc()\n        wes = [we for we in wes if len(we) > 1]\n        for i, we in enumerate(wes):\n            if we in words_punc_dict:\n                wes[i] = words_punc_dict[we]\n        return wes\n\nclass SentimentIntensityAnalyzer(object):\n    def __init__(self, lexicon_file=\"sentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\"):\n        self.lexicon_file = nltk.data.load(lexicon_file)\n        self.lexicon = self.make_lex_dict()\n\n    def make_lex_dict(self):\n        lex_dict = {}\n        for line in self.lexicon_file.split('\\n'):\n            (word, measure) = line.strip().split('\\t')[0:2]\n            lex_dict[word] = float(measure)\n        return lex_dict\n\n    def polarity_scores(self, text):\n        sentitext = SentiText(text)\n\n        sentiments = []\n        words_and_emoticons = sentitext.words_and_emoticons\n        for item in words_and_emoticons:\n            valence = 0\n            i = words_and_emoticons.index(item)\n            if (i < len(words_and_emoticons) - 1 and item.lower() == \"kind\" and \\\n                words_and_emoticons[i+1].lower() == \"of\") or \\\n                item.lower() in BOOSTER_DICT:\n                sentiments.append(valence)\n                continue\n\n            sentiments = self.sentiment_valence(valence, sentitext, item, i, sentiments)\n\n        sentiments = self._but_check(words_and_emoticons, sentiments)\n\n        return self.score_valence(sentiments, text)\n\n    def sentiment_valence(self, valence, sentitext, item, i, sentiments):\n        is_cap_diff = sentitext.is_cap_diff\n        words_and_emoticons = sentitext.words_and_emoticons\n        item_lowercase = item.lower()\n        if item_lowercase in self.lexicon:\n            valence = self.lexicon[item_lowercase]\n\n            if item.isupper() and is_cap_diff:\n                if valence > 0:\n                    valence += C_INCR\n                else:\n                    valence -= C_INCR\n\n            for start_i in range(0,3):\n                if i > start_i and words_and_emoticons[i-(start_i+1)].lower() not in self.lexicon:\n                    s = scalar_inc_dec(words_and_emoticons[i-(start_i+1)], valence, is_cap_diff)\n                    if start_i == 1 and s != 0:\n                        s = s*0.95\n                    if start_i == 2 and s != 0:\n                        s = s*0.9\n                    valence = valence+s\n                    valence = self._never_check(valence, words_and_emoticons, start_i, i)\n                    if start_i == 2:\n                        valence = self._idioms_check(valence, words_and_emoticons, i)\n\n\n            valence = self._least_check(valence, words_and_emoticons, i)\n\n        sentiments.append(valence)\n        return sentiments\n\n    def _least_check(self, valence, words_and_emoticons, i):\n        if i > 1 and words_and_emoticons[i-1].lower() not in self.lexicon \\\n           and words_and_emoticons[i-1].lower() == \"least\":\n            if words_and_emoticons[i-2].lower() != \"at\" and words_and_emoticons[i-2].lower() != \"very\":\n                valence = valence*N_SCALAR\n        elif i > 0 and words_and_emoticons[i-1].lower() not in self.lexicon \\\n             and words_and_emoticons[i-1].lower() == \"least\":\n            valence = valence*N_SCALAR\n        return valence\n\n    def _but_check(self, words_and_emoticons, sentiments):\n        if 'but' in words_and_emoticons or 'BUT' in words_and_emoticons:\n            try:\n                bi = words_and_emoticons.index('but')\n            except ValueError:\n                bi = words_and_emoticons.index('BUT')\n            for sentiment in sentiments:\n                si = sentiments.index(sentiment)\n                if si < bi:\n                    sentiments.pop(si)\n                    sentiments.insert(si, sentiment*0.5)\n                elif si > bi:\n                    sentiments.pop(si)\n                    sentiments.insert(si, sentiment*1.5)\n        return sentiments\n\n    def _idioms_check(self, valence, words_and_emoticons, i):\n        onezero = \"{0} {1}\".format(words_and_emoticons[i-1], words_and_emoticons[i])\n\n        twoonezero = \"{0} {1} {2}\".format(words_and_emoticons[i-2],\n                                       words_and_emoticons[i-1], words_and_emoticons[i])\n\n        twoone = \"{0} {1}\".format(words_and_emoticons[i-2], words_and_emoticons[i-1])\n\n        threetwoone = \"{0} {1} {2}\".format(words_and_emoticons[i-3],\n                                        words_and_emoticons[i-2], words_and_emoticons[i-1])\n\n        threetwo = \"{0} {1}\".format(words_and_emoticons[i-3], words_and_emoticons[i-2])\n\n        sequences = [onezero, twoonezero, twoone, threetwoone, threetwo]\n\n        for seq in sequences:\n            if seq in SPECIAL_CASE_IDIOMS:\n                valence = SPECIAL_CASE_IDIOMS[seq]\n                break\n\n        if len(words_and_emoticons)-1 > i:\n            zeroone = \"{0} {1}\".format(words_and_emoticons[i], words_and_emoticons[i+1])\n            if zeroone in SPECIAL_CASE_IDIOMS:\n                valence = SPECIAL_CASE_IDIOMS[zeroone]\n        if len(words_and_emoticons)-1 > i+1:\n            zeroonetwo = \"{0} {1} {2}\".format(words_and_emoticons[i], words_and_emoticons[i+1], words_and_emoticons[i+2])\n            if zeroonetwo in SPECIAL_CASE_IDIOMS:\n                valence = SPECIAL_CASE_IDIOMS[zeroonetwo]\n\n        if threetwo in BOOSTER_DICT or twoone in BOOSTER_DICT:\n            valence = valence+B_DECR\n        return valence\n\n    def _never_check(self, valence, words_and_emoticons, start_i, i):\n        if start_i == 0:\n            if negated([words_and_emoticons[i-1]]):\n                    valence = valence*N_SCALAR\n        if start_i == 1:\n            if words_and_emoticons[i-2] == \"never\" and\\\n               (words_and_emoticons[i-1] == \"so\" or\n                words_and_emoticons[i-1] == \"this\"):\n                valence = valence*1.5\n            elif negated([words_and_emoticons[i-(start_i+1)]]):\n                valence = valence*N_SCALAR\n        if start_i == 2:\n            if words_and_emoticons[i-3] == \"never\" and \\\n               (words_and_emoticons[i-2] == \"so\" or words_and_emoticons[i-2] == \"this\") or \\\n               (words_and_emoticons[i-1] == \"so\" or words_and_emoticons[i-1] == \"this\"):\n                valence = valence*1.25\n            elif negated([words_and_emoticons[i-(start_i+1)]]):\n                valence = valence*N_SCALAR\n        return valence\n\n    def _punctuation_emphasis(self, sum_s, text):\n        ep_amplifier = self._amplify_ep(text)\n        qm_amplifier = self._amplify_qm(text)\n        punct_emph_amplifier = ep_amplifier+qm_amplifier\n        return punct_emph_amplifier\n\n    def _amplify_ep(self, text):\n        ep_count = text.count(\"!\")\n        if ep_count > 4:\n            ep_count = 4\n        ep_amplifier = ep_count*0.292\n        return ep_amplifier\n\n    def _amplify_qm(self, text):\n        qm_count = text.count(\"?\")\n        qm_amplifier = 0\n        if qm_count > 1:\n            if qm_count <= 3:\n                qm_amplifier = qm_count*0.18\n            else:\n                qm_amplifier = 0.96\n        return qm_amplifier\n\n    def _sift_sentiment_scores(self, sentiments):\n        pos_sum = 0.0\n        neg_sum = 0.0\n        neu_count = 0\n        for sentiment_score in sentiments:\n            if sentiment_score > 0:\n                pos_sum += (float(sentiment_score) +1) # compensates for neutral words that are counted as 1\n            if sentiment_score < 0:\n                neg_sum += (float(sentiment_score) -1) # when used with math.fabs(), compensates for neutrals\n            if sentiment_score == 0:\n                neu_count += 1\n        return pos_sum, neg_sum, neu_count\n\n    def score_valence(self, sentiments, text):\n        if sentiments:\n            sum_s = float(sum(sentiments))\n            punct_emph_amplifier = self._punctuation_emphasis(sum_s, text)\n            if sum_s > 0:\n                sum_s += punct_emph_amplifier\n            elif  sum_s < 0:\n                sum_s -= punct_emph_amplifier\n\n            compound = normalize(sum_s)\n            pos_sum, neg_sum, neu_count = self._sift_sentiment_scores(sentiments)\n\n            if pos_sum > math.fabs(neg_sum):\n                pos_sum += (punct_emph_amplifier)\n            elif pos_sum < math.fabs(neg_sum):\n                neg_sum -= (punct_emph_amplifier)\n\n            total = pos_sum + math.fabs(neg_sum) + neu_count\n            pos = math.fabs(pos_sum / total)\n            neg = math.fabs(neg_sum / total)\n            neu = math.fabs(neu_count / total)\n\n        else:\n            compound = 0.0\n            pos = 0.0\n            neg = 0.0\n            neu = 0.0\n\n        sentiment_dict = \\\n            {\"neg\" : round(neg, 3),\n             \"neu\" : round(neu, 3),\n             \"pos\" : round(pos, 3),\n             \"compound\" : round(compound, 4)}\n\n        return sentiment_dict\n"], "nltk\\sentiment\\__init__": [".py", "\nfrom nltk.sentiment.sentiment_analyzer import SentimentAnalyzer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n", 1], "nltk\\stem\\api": [".py", "\nfrom abc import ABCMeta, abstractmethod\nfrom six import add_metaclass\n\n\n@add_metaclass(ABCMeta)\nclass StemmerI(object):\n    @abstractmethod\n    def stem(self, token):\n"], "nltk\\stem\\arlstem": [".py", "\n\nfrom __future__ import unicode_literals\nimport re\n\nfrom nltk.stem.api import StemmerI\n\n\nclass ARLSTem(StemmerI):\n    '''\n    ARLSTem stemmer : a light Arabic Stemming algorithm without any dictionary.\n    Department of Telecommunication & Information Processing. USTHB University,\n    Algiers, Algeria.\n    ARLSTem.stem(token) returns the Arabic stem for the input token.\n    The ARLSTem Stemmer requires that all tokens are encoded using Unicode\n    encoding.\n    '''\n\n    def __init__(self):\n        self.re_hamzated_alif = re.compile(r'[\\u0622\\u0623\\u0625]')\n        self.re_alifMaqsura = re.compile(r'[\\u0649]')\n        self.re_diacritics = re.compile(r'[\\u064B-\\u065F]')\n\n        self.pr2 = [\n            '\\u0627\\u0644', '\\u0644\\u0644',\n            '\\u0641\\u0644', '\\u0641\\u0628'\n            ]\n        self.pr3 = [\n            '\\u0628\\u0627\\u0644',\n            '\\u0643\\u0627\\u0644',\n            '\\u0648\\u0627\\u0644'\n            ]\n        self.pr32 = ['\\u0641\\u0644\\u0644', '\\u0648\\u0644\\u0644']\n        self.pr4 = [\n            '\\u0641\\u0628\\u0627\\u0644',\n            '\\u0648\\u0628\\u0627\\u0644',\n            '\\u0641\\u0643\\u0627\\u0644'\n            ]\n\n        self.su2 = [\n            '\\u0643\\u064A',\n            '\\u0643\\u0645'\n            ]\n        self.su22 = ['\\u0647\\u0627', '\\u0647\\u0645']\n        self.su3 = ['\\u0643\\u0645\\u0627', '\\u0643\\u0646\\u0651']\n        self.su32 = ['\\u0647\\u0645\\u0627', '\\u0647\\u0646\\u0651']\n\n        self.pl_si2 = ['\\u0627\\u0646', '\\u064A\\u0646', '\\u0648\\u0646']\n        self.pl_si3 = ['\\u062A\\u0627\\u0646', '\\u062A\\u064A\\u0646']\n\n        self.verb_su2 = ['\\u0627\\u0646', '\\u0648\\u0646']\n        self.verb_pr2 = ['\\u0633\\u062A', '\\u0633\\u064A']\n        self.verb_pr22 = ['\\u0633\\u0627', '\\u0633\\u0646']\n        self.verb_pr33 = ['\\u0644\\u0646', '\\u0644\\u062A', '\\u0644\\u064A', '\\u0644\\u0623']\n        self.verb_suf3 = ['\\u062A\\u0645\\u0627', '\\u062A\\u0646\\u0651']\n        self.verb_suf2 = [\n            '\\u0646\\u0627', '\\u062A\\u0645',\n            '\\u062A\\u0627', '\\u0648\\u0627'\n            ]\n        self.verb_suf1 = ['\\u062A', '\\u0627', '\\u0646']\n\n    def stem(self, token):\n        try:\n            if token is None:\n                raise ValueError(\"The word could not be stemmed, because \\\n                                 it is empty !\")\n            token = self.norm(token)\n            pre = self.pref(token)\n            if pre is not None:\n                token = pre\n            token = self.suff(token)\n            ps = self.plur2sing(token)\n            if ps is None:\n                fm = self.fem2masc(token)\n                if fm is not None:\n                    return fm\n                else:\n                    if pre is None:  # if the prefixes are not stripped\n                        return self.verb(token)\n            else:\n                return ps\n            return token\n        except ValueError as e:\n            print(e)\n\n    def norm(self, token):\n        token = self.re_diacritics.sub('', token)\n        token = self.re_hamzated_alif.sub('\\u0627', token)\n        token = self.re_alifMaqsura.sub('\\u064A', token)\n        if token.startswith('\\u0648') and len(token) > 3:\n            token = token[1:]\n        return token\n\n    def pref(self, token):\n        if len(token) > 5:\n            for p3 in self.pr3:\n                if token.startswith(p3):\n                    return token[3:]\n        if len(token) > 6:\n            for p4 in self.pr4:\n                if token.startswith(p4):\n                    return token[4:]\n        if len(token) > 5:\n            for p3 in self.pr32:\n                if token.startswith(p3):\n                    return token[3:]\n        if len(token) > 4:\n            for p2 in self.pr2:\n                if token.startswith(p2):\n                    return token[2:]\n\n    def suff(self, token):\n        if token.endswith('\\u0643') and len(token) > 3:\n            return token[:-1]\n        if len(token) > 4:\n            for s2 in self.su2:\n                if token.endswith(s2):\n                    return token[:-2]\n        if len(token) > 5:\n            for s3 in self.su3:\n                if token.endswith(s3):\n                    return token[:-3]\n        if token.endswith('\\u0647') and len(token) > 3:\n            token = token[:-1]\n            return token\n        if len(token) > 4:\n            for s2 in self.su22:\n                if token.endswith(s2):\n                    return token[:-2]\n        if len(token) > 5:\n            for s3 in self.su32:\n                if token.endswith(s3):\n                    return token[:-3]\n        if token.endswith('\\u0646\\u0627') and len(token) > 4:\n            return token[:-2]\n        return token\n\n    def fem2masc(self, token):\n        if token.endswith('\\u0629') and len(token) > 3:\n            return token[:-1]\n\n    def plur2sing(self, token):\n        if len(token) > 4:\n            for ps2 in self.pl_si2:\n                if token.endswith(ps2):\n                    return token[:-2]\n        if len(token) > 5:\n            for ps3 in self.pl_si3:\n                if token.endswith(ps3):\n                    return token[:-3]\n        if len(token) > 3 and token.endswith('\\u0627\\u062A'):\n            return token[:-2]\n        if (len(token) > 3 and token.startswith('\\u0627')\n           and token[2] == '\\u0627'):\n            return token[:2] + token[3:]\n        if (len(token) > 4 and token.startswith('\\u0627')\n           and token[-2] == '\\u0627'):\n            return token[1:-2] + token[-1]\n\n    def verb(self, token):\n        vb = self.verb_t1(token)\n        if vb is not None:\n            return vb\n        vb = self.verb_t2(token)\n        if vb is not None:\n            return vb\n        vb = self.verb_t3(token)\n        if vb is not None:\n            return vb\n        vb = self.verb_t4(token)\n        if vb is not None:\n            return vb\n        vb = self.verb_t5(token)\n        if vb is not None:\n            return vb\n        return self.verb_t6(token)\n\n    def verb_t1(self, token):\n        if len(token) > 5 and token.startswith('\\u062A'):  # Taa\n            for s2 in self.pl_si2:\n                if token.endswith(s2):\n                    return token[1:-2]\n        if len(token) > 5 and token.startswith('\\u064A'):  # Yaa\n            for s2 in self.verb_su2:\n                if token.endswith(s2):\n                    return token[1:-2]\n        if len(token) > 4 and token.startswith('\\u0627'):  # Alif\n            if len(token) > 5 and token.endswith('\\u0648\\u0627'):\n                return token[1:-2]\n            if token.endswith('\\u064A'):\n                return token[1:-1]\n            if token.endswith('\\u0627'):\n                return token[1:-1]\n            if token.endswith('\\u0646'):\n                return token[1:-1]\n        if (len(token) > 4\n           and token.startswith('\\u064A')\n           and token.endswith('\\u0646')):\n            return token[1:-1]\n        if (len(token) > 4\n           and token.startswith('\\u062A')\n           and token.endswith('\\u0646')):\n            return token[1:-1]\n\n    def verb_t2(self, token):\n        if len(token) > 6:\n            for s2 in self.pl_si2:\n                if (token.startswith(self.verb_pr2[0])\n                   and token.endswith(s2)):\n                    return token[2:-2]\n            if (token.startswith(self.verb_pr2[1])\n               and token.endswith(self.pl_si2[0])):\n                return token[2:-2]\n            if (token.startswith(self.verb_pr2[1])\n               and token.endswith(self.pl_si2[2])):\n                return token[2:-2]\n        if (len(token) > 5\n           and token.startswith(self.verb_pr2[0])\n           and token.endswith('\\u0646')):\n            return token[2:-1]\n        if (len(token) > 5\n           and token.startswith(self.verb_pr2[1])\n           and token.endswith('\\u0646')):\n            return token[2:-1]\n\n    def verb_t3(self, token):\n        if len(token) > 5:\n            for su3 in self.verb_suf3:\n                if(token.endswith(su3)):\n                    return token[:-3]\n        if len(token) > 4:\n            for su2 in self.verb_suf2:\n                if token.endswith(su2):\n                    return token[:-2]\n        if len(token) > 3:\n            for su1 in self.verb_suf1:\n                if token.endswith(su1):\n                    return token[:-1]\n\n    def verb_t4(self, token):\n        if len(token) > 3:\n            for pr1 in self.verb_suf1:\n                if token.startswith(pr1):\n                    return token[1:]\n            if token.startswith('\\u064A'):\n                return token[1:]\n\n    def verb_t5(self, token):\n        if len(token) > 4:\n            for pr2 in self.verb_pr22:\n                if token.startswith(pr2):\n                    return token[2:]\n            for pr2 in self.verb_pr2:\n                if token.startswith(pr2):\n                    return token[2:]\n        return token\n\n    def verb_t6(self, token):\n        if len(token) > 4:\n            for pr3 in self.verb_pr33:\n                if token.startswith(pr3):\n                    return token[2:]\n        return token\n"], "nltk\\stem\\isri": [".py", "\nfrom __future__ import unicode_literals\nimport re\n\nfrom nltk.stem.api import StemmerI\n\n\nclass ISRIStemmer(StemmerI):\n    '''\n    ISRI Arabic stemmer based on algorithm: Arabic Stemming without a root dictionary.\n    Information Science Research Institute. University of Nevada, Las Vegas, USA.\n\n    A few minor modifications have been made to ISRI basic algorithm.\n    See the source code of this module for more information.\n\n    isri.stem(token) returns Arabic root for the given token.\n\n    The ISRI Stemmer requires that all tokens have Unicode string types.\n    If you use Python IDLE on Arabic Windows you have to decode text first\n    using Arabic '1256' coding.\n    '''\n\n    def __init__(self):\n        self.p3 = ['\\u0643\\u0627\\u0644', '\\u0628\\u0627\\u0644',\n                   '\\u0648\\u0644\\u0644', '\\u0648\\u0627\\u0644']\n\n        self.p2 = ['\\u0627\\u0644', '\\u0644\\u0644']\n\n        self.p1 = ['\\u0644', '\\u0628', '\\u0641', '\\u0633', '\\u0648',\n                   '\\u064a', '\\u062a', '\\u0646', '\\u0627']\n\n        self.s3 = ['\\u062a\\u0645\\u0644', '\\u0647\\u0645\\u0644',\n                   '\\u062a\\u0627\\u0646', '\\u062a\\u064a\\u0646',\n                   '\\u0643\\u0645\\u0644']\n\n        self.s2 = ['\\u0648\\u0646', '\\u0627\\u062a', '\\u0627\\u0646',\n                   '\\u064a\\u0646', '\\u062a\\u0646', '\\u0643\\u0645',\n                   '\\u0647\\u0646', '\\u0646\\u0627', '\\u064a\\u0627',\n                   '\\u0647\\u0627', '\\u062a\\u0645', '\\u0643\\u0646',\n                   '\\u0646\\u064a', '\\u0648\\u0627', '\\u0645\\u0627',\n                   '\\u0647\\u0645']\n\n        self.s1 = ['\\u0629', '\\u0647', '\\u064a', '\\u0643', '\\u062a',\n                   '\\u0627', '\\u0646']\n\n        self.pr4 = {0: ['\\u0645'], 1: ['\\u0627'],\n                    2: ['\\u0627', '\\u0648', '\\u064A'], 3: ['\\u0629']}\n\n        self.pr53 = {0: ['\\u0627', '\\u062a'],\n                     1: ['\\u0627', '\\u064a', '\\u0648'],\n                     2: ['\\u0627', '\\u062a', '\\u0645'],\n                     3: ['\\u0645', '\\u064a', '\\u062a'],\n                     4: ['\\u0645', '\\u062a'],\n                     5: ['\\u0627', '\\u0648'],\n                     6: ['\\u0627', '\\u0645']}\n\n        self.re_short_vowels = re.compile(r'[\\u064B-\\u0652]')\n        self.re_hamza = re.compile(r'[\\u0621\\u0624\\u0626]')\n        self.re_initial_hamza = re.compile(r'^[\\u0622\\u0623\\u0625]')\n\n        self.stop_words = ['\\u064a\\u0643\\u0648\\u0646',\n                           '\\u0648\\u0644\\u064a\\u0633',\n                           '\\u0648\\u0643\\u0627\\u0646',\n                           '\\u0643\\u0630\\u0644\\u0643',\n                           '\\u0627\\u0644\\u062a\\u064a',\n                           '\\u0648\\u0628\\u064a\\u0646',\n                           '\\u0639\\u0644\\u064a\\u0647\\u0627',\n                           '\\u0645\\u0633\\u0627\\u0621',\n                           '\\u0627\\u0644\\u0630\\u064a',\n                           '\\u0648\\u0643\\u0627\\u0646\\u062a',\n                           '\\u0648\\u0644\\u0643\\u0646',\n                           '\\u0648\\u0627\\u0644\\u062a\\u064a',\n                           '\\u062a\\u0643\\u0648\\u0646',\n                           '\\u0627\\u0644\\u064a\\u0648\\u0645',\n                           '\\u0627\\u0644\\u0644\\u0630\\u064a\\u0646',\n                           '\\u0639\\u0644\\u064a\\u0647',\n                           '\\u0643\\u0627\\u0646\\u062a',\n                           '\\u0644\\u0630\\u0644\\u0643',\n                           '\\u0623\\u0645\\u0627\\u0645',\n                           '\\u0647\\u0646\\u0627\\u0643',\n                           '\\u0645\\u0646\\u0647\\u0627',\n                           '\\u0645\\u0627\\u0632\\u0627\\u0644',\n                           '\\u0644\\u0627\\u0632\\u0627\\u0644',\n                           '\\u0644\\u0627\\u064a\\u0632\\u0627\\u0644',\n                           '\\u0645\\u0627\\u064a\\u0632\\u0627\\u0644',\n                           '\\u0627\\u0635\\u0628\\u062d',\n                           '\\u0623\\u0635\\u0628\\u062d',\n                           '\\u0623\\u0645\\u0633\\u0649',\n                           '\\u0627\\u0645\\u0633\\u0649',\n                           '\\u0623\\u0636\\u062d\\u0649',\n                           '\\u0627\\u0636\\u062d\\u0649',\n                           '\\u0645\\u0627\\u0628\\u0631\\u062d',\n                           '\\u0645\\u0627\\u0641\\u062a\\u0626',\n                           '\\u0645\\u0627\\u0627\\u0646\\u0641\\u0643',\n                           '\\u0644\\u0627\\u0633\\u064a\\u0645\\u0627',\n                           '\\u0648\\u0644\\u0627\\u064a\\u0632\\u0627\\u0644',\n                           '\\u0627\\u0644\\u062d\\u0627\\u0644\\u064a',\n                           '\\u0627\\u0644\\u064a\\u0647\\u0627',\n                           '\\u0627\\u0644\\u0630\\u064a\\u0646',\n                           '\\u0641\\u0627\\u0646\\u0647',\n                           '\\u0648\\u0627\\u0644\\u0630\\u064a',\n                           '\\u0648\\u0647\\u0630\\u0627',\n                           '\\u0644\\u0647\\u0630\\u0627',\n                           '\\u0641\\u0643\\u0627\\u0646',\n                           '\\u0633\\u062a\\u0643\\u0648\\u0646',\n                           '\\u0627\\u0644\\u064a\\u0647',\n                           '\\u064a\\u0645\\u0643\\u0646',\n                           '\\u0628\\u0647\\u0630\\u0627',\n                           '\\u0627\\u0644\\u0630\\u0649']\n\n    def stem(self, token):\n        token = self.norm(token, 1)   # remove diacritics which representing Arabic short vowels\n        if token in self.stop_words:\n            return token              # exclude stop words from being processed\n        token = self.pre32(token)     # remove length three and length two prefixes in this order\n        token = self.suf32(token)     # remove length three and length two suffixes in this order\n        token = self.waw(token)       # remove connective \u2018\u0648\u2019 if it precedes a word beginning with \u2018\u0648\u2019\n        token = self.norm(token, 2)   # normalize initial hamza to bare alif\n        if len(token) == 4:           # length 4 word\n            token = self.pro_w4(token)\n        elif len(token) == 5:         # length 5 word\n            token = self.pro_w53(token)\n            token = self.end_w5(token)\n        elif len(token) == 6:         # length 6 word\n            token = self.pro_w6(token)\n            token = self.end_w6(token)\n        elif len(token) == 7:         # length 7 word\n            token = self.suf1(token)\n            if len(token) == 7:\n                token = self.pre1(token)\n            if len(token) == 6:\n                token = self.pro_w6(token)\n                token = self.end_w6(token)\n        return token\n\n    def norm(self, word, num=3):\n        if num == 1:\n            word = self.re_short_vowels.sub('', word)\n        elif num == 2:\n            word = self.re_initial_hamza.sub('\\u0627', word)\n        elif num == 3:\n            word = self.re_short_vowels.sub('', word)\n            word = self.re_initial_hamza.sub('\\u0627', word)\n        return word\n\n    def pre32(self, word):\n        if len(word) >= 6:\n            for pre3 in self.p3:\n                if word.startswith(pre3):\n                    return word[3:]\n        if len(word) >= 5:\n            for pre2 in self.p2:\n                if word.startswith(pre2):\n                    return word[2:]\n        return word\n\n    def suf32(self, word):\n        if len(word) >= 6:\n            for suf3 in self.s3:\n                if word.endswith(suf3):\n                    return word[:-3]\n        if len(word) >= 5:\n            for suf2 in self.s2:\n                if word.endswith(suf2):\n                    return word[:-2]\n        return word\n\n    def waw(self, word):\n        if len(word) >= 4 and word[:2] == '\\u0648\\u0648':\n            word = word[1:]\n        return word\n\n    def pro_w4(self, word):\n        if word[0] in self.pr4[0]:      # \u0645\u0641\u0639\u0644\n            word = word[1:]\n        elif word[1] in self.pr4[1]:    # \u0641\u0627\u0639\u0644\n            word = word[:1] + word[2:]\n        elif word[2] in self.pr4[2]:    # \u0641\u0639\u0627\u0644 - \u0641\u0639\u0648\u0644 - \u0641\u0639\u064a\u0644\n            word = word[:2] + word[3]\n        elif word[3] in self.pr4[3]:    # \u0641\u0639\u0644\u0629\n            word = word[:-1]\n        else:\n            word = self.suf1(word)      # do - normalize short sufix\n            if len(word) == 4:\n                word = self.pre1(word)  # do - normalize short prefix\n        return word\n\n    def pro_w53(self, word):\n        if word[2] in self.pr53[0] and word[0] == '\\u0627':    # \u0627\u0641\u062a\u0639\u0644 - \u0627\u0641\u0627\u0639\u0644\n            word = word[1] + word[3:]\n        elif word[3] in self.pr53[1] and word[0] == '\\u0645':  # \u0645\u0641\u0639\u0648\u0644 - \u0645\u0641\u0639\u0627\u0644 - \u0645\u0641\u0639\u064a\u0644\n            word = word[1:3] + word[4]\n        elif word[0] in self.pr53[2] and word[4] == '\\u0629':  # \u0645\u0641\u0639\u0644\u0629 - \u062a\u0641\u0639\u0644\u0629 - \u0627\u0641\u0639\u0644\u0629\n            word = word[1:4]\n        elif word[0] in self.pr53[3] and word[2] == '\\u062a':  # \u0645\u0641\u062a\u0639\u0644 - \u064a\u0641\u062a\u0639\u0644 - \u062a\u0641\u062a\u0639\u0644\n            word = word[1] + word[3:]\n        elif word[0] in self.pr53[4] and word[2] == '\\u0627':  # \u0645\u0641\u0627\u0639\u0644 - \u062a\u0641\u0627\u0639\u0644\n            word = word[1] + word[3:]\n        elif word[2] in self.pr53[5] and word[4] == '\\u0629':  # \u0641\u0639\u0648\u0644\u0629 - \u0641\u0639\u0627\u0644\u0629\n            word = word[:2] + word[3]\n        elif word[0] in self.pr53[6] and word[1] == '\\u0646':  # \u0627\u0646\u0641\u0639\u0644 - \u0645\u0646\u0641\u0639\u0644\n            word = word[2:]\n        elif word[3] == '\\u0627' and word[0] == '\\u0627':      # \u0627\u0641\u0639\u0627\u0644\n            word = word[1:3] + word[4]\n        elif word[4] == '\\u0646' and word[3] == '\\u0627':      # \u0641\u0639\u0644\u0627\u0646\n            word = word[:3]\n        elif word[3] == '\\u064a' and word[0] == '\\u062a':      # \u062a\u0641\u0639\u064a\u0644\n            word = word[1:3] + word[4]\n        elif word[3] == '\\u0648' and word[1] == '\\u0627':      # \u0641\u0627\u0639\u0648\u0644\n            word = word[0] + word[2] + word[4]\n        elif word[2] == '\\u0627' and word[1] == '\\u0648':      # \u0641\u0648\u0627\u0639\u0644\n            word = word[0] + word[3:]\n        elif word[3] == '\\u0626' and word[2] == '\\u0627':      # \u0641\u0639\u0627\u0626\u0644\n            word = word[:2] + word[4]\n        elif word[4] == '\\u0629' and word[1] == '\\u0627':      # \u0641\u0627\u0639\u0644\u0629\n            word = word[0] + word[2:4]\n        elif word[4] == '\\u064a' and word[2] == '\\u0627':      # \u0641\u0639\u0627\u0644\u064a\n            word = word[:2] + word[3]\n        else:\n            word = self.suf1(word)      # do - normalize short sufix\n            if len(word) == 5:\n                word = self.pre1(word)  # do - normalize short prefix\n        return word\n\n    def pro_w54(self, word):\n        if word[0] in self.pr53[2]:  # \u062a\u0641\u0639\u0644\u0644 - \u0627\u0641\u0639\u0644\u0644 - \u0645\u0641\u0639\u0644\u0644\n            word = word[1:]\n        elif word[4] == '\\u0629':    # \u0641\u0639\u0644\u0644\u0629\n            word = word[:4]\n        elif word[2] == '\\u0627':    # \u0641\u0639\u0627\u0644\u0644\n            word = word[:2] + word[3:]\n        return word\n\n    def end_w5(self, word):\n        if len(word) == 4:\n            word = self.pro_w4(word)\n        elif len(word) == 5:\n            word = self.pro_w54(word)\n        return word\n\n    def pro_w6(self, word):\n        if word.startswith('\\u0627\\u0633\\u062a') or word.startswith('\\u0645\\u0633\\u062a'):  # \u0645\u0633\u062a\u0641\u0639\u0644 - \u0627\u0633\u062a\u0641\u0639\u0644\n            word = word[3:]\n        elif word[0] == '\\u0645' and word[3] == '\\u0627' and word[5] == '\\u0629':           # \u0645\u0641\u0639\u0627\u0644\u0629\n            word = word[1:3] + word[4]\n        elif word[0] == '\\u0627' and word[2] == '\\u062a' and word[4] == '\\u0627':           # \u0627\u0641\u062a\u0639\u0627\u0644\n            word = word[1] + word[3] + word[5]\n        elif word[0] == '\\u0627' and word[3] == '\\u0648' and word[2] == word[4]:            # \u0627\u0641\u0639\u0648\u0639\u0644\n            word = word[1] + word[4:]\n        elif word[0] == '\\u062a' and word[2] == '\\u0627' and word[4] == '\\u064a':           # \u062a\u0641\u0627\u0639\u064a\u0644   new pattern\n            word = word[1] + word[3] + word[5]\n        else:\n            word = self.suf1(word)      # do - normalize short sufix\n            if len(word) == 6:\n                word = self.pre1(word)  # do - normalize short prefix\n        return word\n\n    def pro_w64(self, word):\n        if word[0] == '\\u0627' and word[4] == '\\u0627':  # \u0627\u0641\u0639\u0644\u0627\u0644\n            word = word[1:4] + word[5]\n        elif word.startswith('\\u0645\\u062a'):            # \u0645\u062a\u0641\u0639\u0644\u0644\n            word = word[2:]\n        return word\n\n    def end_w6(self, word):\n        if len(word) == 5:\n            word = self.pro_w53(word)\n            word = self.end_w5(word)\n        elif len(word) == 6:\n            word = self.pro_w64(word)\n        return word\n\n    def suf1(self, word):\n        for sf1 in self.s1:\n            if word.endswith(sf1):\n                return word[:-1]\n        return word\n\n    def pre1(self, word):\n        for sp1 in self.p1:\n            if word.startswith(sp1):\n                return word[1:]\n        return word\n\n\n"], "nltk\\stem\\lancaster": [".py", "\nfrom __future__ import unicode_literals\nimport re\n\nfrom nltk.stem.api import StemmerI\nfrom nltk.compat import python_2_unicode_compatible\n\n@python_2_unicode_compatible\nclass LancasterStemmer(StemmerI):\n\n    default_rule_tuple = (\n        \"ai*2.\",     # -ia > -   if intact\n        \"a*1.\",      # -a > -    if intact\n        \"bb1.\",      # -bb > -b\n        \"city3s.\",   # -ytic > -ys\n        \"ci2>\",      # -ic > -\n        \"cn1t>\",     # -nc > -nt\n        \"dd1.\",      # -dd > -d\n        \"dei3y>\",    # -ied > -y\n        \"deec2ss.\",  # -ceed >\", -cess\n        \"dee1.\",     # -eed > -ee\n        \"de2>\",      # -ed > -\n        \"dooh4>\",    # -hood > -\n        \"e1>\",       # -e > -\n        \"feil1v.\",   # -lief > -liev\n        \"fi2>\",      # -if > -\n        \"gni3>\",     # -ing > -\n        \"gai3y.\",    # -iag > -y\n        \"ga2>\",      # -ag > -\n        \"gg1.\",      # -gg > -g\n        \"ht*2.\",     # -th > -   if intact\n        \"hsiug5ct.\", # -guish > -ct\n        \"hsi3>\",     # -ish > -\n        \"i*1.\",      # -i > -    if intact\n        \"i1y>\",      # -i > -y\n        \"ji1d.\",     # -ij > -id   --  see nois4j> & vis3j>\n        \"juf1s.\",    # -fuj > -fus\n        \"ju1d.\",     # -uj > -ud\n        \"jo1d.\",     # -oj > -od\n        \"jeh1r.\",    # -hej > -her\n        \"jrev1t.\",   # -verj > -vert\n        \"jsim2t.\",   # -misj > -mit\n        \"jn1d.\",     # -nj > -nd\n        \"j1s.\",      # -j > -s\n        \"lbaifi6.\",  # -ifiabl > -\n        \"lbai4y.\",   # -iabl > -y\n        \"lba3>\",     # -abl > -\n        \"lbi3.\",     # -ibl > -\n        \"lib2l>\",    # -bil > -bl\n        \"lc1.\",      # -cl > c\n        \"lufi4y.\",   # -iful > -y\n        \"luf3>\",     # -ful > -\n        \"lu2.\",      # -ul > -\n        \"lai3>\",     # -ial > -\n        \"lau3>\",     # -ual > -\n        \"la2>\",      # -al > -\n        \"ll1.\",      # -ll > -l\n        \"mui3.\",     # -ium > -\n        \"mu*2.\",     # -um > -   if intact\n        \"msi3>\",     # -ism > -\n        \"mm1.\",      # -mm > -m\n        \"nois4j>\",   # -sion > -j\n        \"noix4ct.\",  # -xion > -ct\n        \"noi3>\",     # -ion > -\n        \"nai3>\",     # -ian > -\n        \"na2>\",      # -an > -\n        \"nee0.\",     # protect  -een\n        \"ne2>\",      # -en > -\n        \"nn1.\",      # -nn > -n\n        \"pihs4>\",    # -ship > -\n        \"pp1.\",      # -pp > -p\n        \"re2>\",      # -er > -\n        \"rae0.\",     # protect  -ear\n        \"ra2.\",      # -ar > -\n        \"ro2>\",      # -or > -\n        \"ru2>\",      # -ur > -\n        \"rr1.\",      # -rr > -r\n        \"rt1>\",      # -tr > -t\n        \"rei3y>\",    # -ier > -y\n        \"sei3y>\",    # -ies > -y\n        \"sis2.\",     # -sis > -s\n        \"si2>\",      # -is > -\n        \"ssen4>\",    # -ness > -\n        \"ss0.\",      # protect  -ss\n        \"suo3>\",     # -ous > -\n        \"su*2.\",     # -us > -   if intact\n        \"s*1>\",      # -s > -    if intact\n        \"s0.\",       # -s > -s\n        \"tacilp4y.\", # -plicat > -ply\n        \"ta2>\",      # -at > -\n        \"tnem4>\",    # -ment > -\n        \"tne3>\",     # -ent > -\n        \"tna3>\",     # -ant > -\n        \"tpir2b.\",   # -ript > -rib\n        \"tpro2b.\",   # -orpt > -orb\n        \"tcud1.\",    # -duct > -duc\n        \"tpmus2.\",   # -sumpt > -sum\n        \"tpec2iv.\",  # -cept > -ceiv\n        \"tulo2v.\",   # -olut > -olv\n        \"tsis0.\",    # protect  -sist\n        \"tsi3>\",     # -ist > -\n        \"tt1.\",      # -tt > -t\n        \"uqi3.\",     # -iqu > -\n        \"ugo1.\",     # -ogu > -og\n        \"vis3j>\",    # -siv > -j\n        \"vie0.\",     # protect  -eiv\n        \"vi2>\",      # -iv > -\n        \"ylb1>\",     # -bly > -bl\n        \"yli3y>\",    # -ily > -y\n        \"ylp0.\",     # protect  -ply\n        \"yl2>\",      # -ly > -\n        \"ygo1.\",     # -ogy > -og\n        \"yhp1.\",     # -phy > -ph\n        \"ymo1.\",     # -omy > -om\n        \"ypo1.\",     # -opy > -op\n        \"yti3>\",     # -ity > -\n        \"yte3>\",     # -ety > -\n        \"ytl2.\",     # -lty > -l\n        \"yrtsi5.\",   # -istry > -\n        \"yra3>\",     # -ary > -\n        \"yro3>\",     # -ory > -\n        \"yfi3.\",     # -ify > -\n        \"ycn2t>\",    # -ncy > -nt\n        \"yca3>\",     # -acy > -\n        \"zi2>\",      # -iz > -\n        \"zy1s.\"      # -yz > -ys\n    )\n\n    def __init__(self, rule_tuple=None, strip_prefix_flag=False):\n        self.rule_dictionary = {}\n        self._strip_prefix = strip_prefix_flag\n        self._rule_tuple = rule_tuple if rule_tuple else self.default_rule_tuple\n\n    def parseRules(self, rule_tuple=None):\n        rule_tuple = rule_tuple if rule_tuple else self._rule_tuple\n        valid_rule = re.compile(\"^[a-z]+\\*?\\d[a-z]*[>\\.]?$\")\n        self.rule_dictionary = {}\n\n        for rule in rule_tuple:\n            if not valid_rule.match(rule):\n                raise ValueError(\"The rule {0} is invalid\".format(rule))\n            first_letter = rule[0:1]\n            if first_letter in self.rule_dictionary:\n                self.rule_dictionary[first_letter].append(rule)\n            else:\n                self.rule_dictionary[first_letter] = [rule]\n\n    def stem(self, word):\n        word = word.lower()\n        word = self.__stripPrefix(word) if self._strip_prefix else word\n\n        intact_word = word\n\n        if not self.rule_dictionary:\n            self.parseRules()\n\n        return self.__doStemming(word, intact_word)\n\n    def __doStemming(self, word, intact_word):\n\n        valid_rule = re.compile(\"^([a-z]+)(\\*?)(\\d)([a-z]*)([>\\.]?)$\")\n\n        proceed = True\n\n        while proceed:\n\n            last_letter_position = self.__getLastLetter(word)\n\n            if last_letter_position < 0 or word[last_letter_position] not in self.rule_dictionary:\n                proceed = False\n\n            else:\n                rule_was_applied = False\n\n                for rule in self.rule_dictionary[word[last_letter_position]]:\n                    rule_match = valid_rule.match(rule)\n                    if rule_match:\n                        (ending_string,\n                         intact_flag,\n                         remove_total,\n                         append_string,\n                         cont_flag) = rule_match.groups()\n\n                        remove_total = int(remove_total)\n\n                        if word.endswith(ending_string[::-1]):\n                            if intact_flag:\n                                if (word == intact_word and\n                                    self.__isAcceptable(word, remove_total)):\n                                    word = self.__applyRule(word,\n                                                            remove_total,\n                                                            append_string)\n                                    rule_was_applied = True\n                                    if cont_flag == '.':\n                                        proceed = False\n                                    break\n                            elif self.__isAcceptable(word, remove_total):\n                                word = self.__applyRule(word,\n                                                        remove_total,\n                                                        append_string)\n                                rule_was_applied = True\n                                if cont_flag == '.':\n                                    proceed = False\n                                break\n                if rule_was_applied == False:\n                    proceed = False\n        return word\n\n    def __getLastLetter(self, word):\n        last_letter = -1\n        for position in range(len(word)):\n            if word[position].isalpha():\n                last_letter = position\n            else:\n                break\n        return last_letter\n\n    def __isAcceptable(self, word, remove_total):\n        word_is_acceptable = False\n        if word[0] in \"aeiouy\":\n            if (len(word) - remove_total >= 2):\n                word_is_acceptable = True\n        elif (len(word) - remove_total >= 3):\n            if word[1] in \"aeiouy\":\n                word_is_acceptable = True\n            elif word[2] in \"aeiouy\":\n                word_is_acceptable = True\n        return word_is_acceptable\n\n\n    def __applyRule(self, word, remove_total, append_string):\n        new_word_length = len(word) - remove_total\n        word = word[0:new_word_length]\n\n        if append_string:\n            word += append_string\n        return word\n\n    def __stripPrefix(self, word):\n        for prefix in (\"kilo\", \"micro\", \"milli\", \"intra\", \"ultra\", \"mega\",\n                       \"nano\", \"pico\", \"pseudo\"):\n            if word.startswith(prefix):\n                return word[len(prefix):]\n        return word\n\n    def __repr__(self):\n        return '<LancasterStemmer>'\n"], "nltk\\stem\\porter": [".py", "\nfrom __future__ import print_function, unicode_literals\n\n__docformat__ = 'plaintext'\n\nimport re\n\nfrom nltk.stem.api import StemmerI\nfrom nltk.compat import python_2_unicode_compatible\n\n@python_2_unicode_compatible\nclass PorterStemmer(StemmerI):\n    \n    NLTK_EXTENSIONS = 'NLTK_EXTENSIONS'\n    MARTIN_EXTENSIONS = 'MARTIN_EXTENSIONS'\n    ORIGINAL_ALGORITHM = 'ORIGINAL_ALGORITHM'\n\n    def __init__(self, mode=NLTK_EXTENSIONS):\n        if mode not in (\n            self.NLTK_EXTENSIONS,\n            self.MARTIN_EXTENSIONS,\n            self.ORIGINAL_ALGORITHM\n        ):\n            raise ValueError(\n                \"Mode must be one of PorterStemmer.NLTK_EXTENSIONS, \"\n                \"PorterStemmer.MARTIN_EXTENSIONS, or \"\n                \"PorterStemmer.ORIGINAL_ALGORITHM\"\n            )\n        \n        self.mode = mode\n        \n        if self.mode == self.NLTK_EXTENSIONS:\n            irregular_forms = {\n                \"sky\" :     [\"sky\", \"skies\"],\n                \"die\" :     [\"dying\"],\n                \"lie\" :     [\"lying\"],\n                \"tie\" :     [\"tying\"],\n                \"news\" :    [\"news\"],\n                \"inning\" :  [\"innings\", \"inning\"],\n                \"outing\" :  [\"outings\", \"outing\"],\n                \"canning\" : [\"cannings\", \"canning\"],\n                \"howe\" :    [\"howe\"],\n                \"proceed\" : [\"proceed\"],\n                \"exceed\"  : [\"exceed\"],\n                \"succeed\" : [\"succeed\"],\n            }\n\n            self.pool = {}\n            for key in irregular_forms:\n                for val in irregular_forms[key]:\n                    self.pool[val] = key\n\n        self.vowels = frozenset(['a', 'e', 'i', 'o', 'u'])\n\n    def _is_consonant(self, word, i):\n        if word[i] in self.vowels:\n            return False\n        if word[i] == 'y':\n            if i == 0:\n                return True\n            else:\n                return (not self._is_consonant(word, i - 1))\n        return True\n        \n    def _measure(self, stem):\n        cv_sequence = ''\n        \n        for i in range(len(stem)):\n            if self._is_consonant(stem, i):\n                cv_sequence += 'c'\n            else:\n                cv_sequence += 'v'\n                \n        return cv_sequence.count('vc')\n        \n    def _has_positive_measure(self, stem):\n        return self._measure(stem) > 0\n\n    def _contains_vowel(self, stem):\n        for i in range(len(stem)):\n            if not self._is_consonant(stem, i):\n                return True\n        return False\n        \n    def _ends_double_consonant(self, word):\n        return (\n            len(word) >= 2 and\n            word[-1] == word[-2] and\n            self._is_consonant(word, len(word)-1)\n        )\n\n    def _ends_cvc(self, word):\n        return (\n            len(word) >= 3 and\n            self._is_consonant(word, len(word) - 3) and\n            not self._is_consonant(word, len(word) - 2) and\n            self._is_consonant(word, len(word) - 1) and\n            word[-1] not in ('w', 'x', 'y')\n        ) or (\n            self.mode == self.NLTK_EXTENSIONS and\n            len(word) == 2 and\n            not self._is_consonant(word, 0) and\n            self._is_consonant(word, 1)\n        )\n        \n    def _replace_suffix(self, word, suffix, replacement):\n        assert word.endswith(suffix), \"Given word doesn't end with given suffix\"\n        if suffix == '':\n            return word + replacement\n        else:\n            return word[:-len(suffix)] + replacement\n                \n    def _apply_rule_list(self, word, rules):\n        for rule in rules:\n            suffix, replacement, condition = rule\n            if suffix == '*d' and self._ends_double_consonant(word):\n                stem = word[:-2]\n                if condition is None or condition(stem):\n                    return stem + replacement\n                else:\n                    return word\n            if word.endswith(suffix):\n                stem = self._replace_suffix(word, suffix, '')\n                if condition is None or condition(stem):\n                    return stem + replacement\n                else:\n                    return word\n                \n        return word\n        \n    def _step1a(self, word):\n        if self.mode == self.NLTK_EXTENSIONS:\n            if word.endswith('ies') and len(word) == 4:\n                return self._replace_suffix(word, 'ies', 'ie')\n            \n        return self._apply_rule_list(word, [\n            ('sses', 'ss', None), # SSES -> SS\n            ('ies', 'i', None),   # IES  -> I\n            ('ss', 'ss', None),   # SS   -> SS\n            ('s', '', None),      # S    ->\n        ])\n        \n    def _step1b(self, word):\n        if self.mode == self.NLTK_EXTENSIONS:\n            if word.endswith('ied'):\n                if len(word) == 4:\n                    return self._replace_suffix(word, 'ied', 'ie')\n                else:\n                    return self._replace_suffix(word, 'ied', 'i')\n        \n        if word.endswith('eed'):\n            stem = self._replace_suffix(word, 'eed', '')\n            if self._measure(stem) > 0:\n                return stem + 'ee'\n            else:\n                return word\n            \n        rule_2_or_3_succeeded = False\n        \n        for suffix in ['ed', 'ing']:\n            if word.endswith(suffix):\n                intermediate_stem = self._replace_suffix(word, suffix, '')\n                if self._contains_vowel(intermediate_stem):\n                    rule_2_or_3_succeeded = True\n                    break\n                \n        if not rule_2_or_3_succeeded:\n            return word\n\n        return self._apply_rule_list(intermediate_stem, [\n            ('at', 'ate', None), # AT -> ATE\n            ('bl', 'ble', None), # BL -> BLE\n            ('iz', 'ize', None), # IZ -> IZE\n            (\n                '*d',\n                intermediate_stem[-1],\n                lambda stem: intermediate_stem[-1] not in ('l', 's', 'z')\n            ),\n            (\n                '',\n                'e',\n                lambda stem: (self._measure(stem) == 1 and\n                              self._ends_cvc(stem))\n            ),\n        ])\n    \n    def _step1c(self, word):\n        def nltk_condition(stem):\n            return len(stem) > 1 and self._is_consonant(stem, len(stem) - 1)\n        \n        def original_condition(stem):\n            return self._contains_vowel(stem)\n        \n        return self._apply_rule_list(word, [\n            (\n                'y',\n                'i',\n                nltk_condition if self.mode == self.NLTK_EXTENSIONS\n                               else original_condition\n            )\n        ])\n\n    def _step2(self, word):\n\n        if self.mode == self.NLTK_EXTENSIONS:\n            if (\n                word.endswith('alli') and\n                self._has_positive_measure(\n                    self._replace_suffix(word, 'alli', '')\n                )\n            ):\n                return self._step2(\n                    self._replace_suffix(word, 'alli', 'al')\n                )\n        \n        bli_rule = ('bli', 'ble', self._has_positive_measure)\n        abli_rule = ('abli', 'able', self._has_positive_measure)\n        \n        rules = [\n            ('ational', 'ate', self._has_positive_measure),\n            ('tional', 'tion', self._has_positive_measure),\n            ('enci', 'ence', self._has_positive_measure),\n            ('anci', 'ance', self._has_positive_measure),\n            ('izer', 'ize', self._has_positive_measure),\n            \n            abli_rule if self.mode == self.ORIGINAL_ALGORITHM else bli_rule,\n            \n            ('alli', 'al', self._has_positive_measure),\n            ('entli', 'ent', self._has_positive_measure),\n            ('eli', 'e', self._has_positive_measure),\n            ('ousli', 'ous', self._has_positive_measure),\n            ('ization', 'ize', self._has_positive_measure),\n            ('ation', 'ate', self._has_positive_measure),\n            ('ator', 'ate', self._has_positive_measure),\n            ('alism', 'al', self._has_positive_measure),\n            ('iveness', 'ive', self._has_positive_measure),\n            ('fulness', 'ful', self._has_positive_measure),\n            ('ousness', 'ous', self._has_positive_measure),\n            ('aliti', 'al', self._has_positive_measure),\n            ('iviti', 'ive', self._has_positive_measure),\n            ('biliti', 'ble', self._has_positive_measure),\n        ]\n        \n        if self.mode == self.NLTK_EXTENSIONS:\n            rules.append(\n                ('fulli', 'ful', self._has_positive_measure)\n            )\n            \n            rules.append((\n                \"logi\",\n                \"log\",\n                lambda stem: self._has_positive_measure(word[:-3])\n            ))\n\n        if self.mode == self.MARTIN_EXTENSIONS:\n            rules.append(\n                (\"logi\", \"log\", self._has_positive_measure)\n            )\n        \n        return self._apply_rule_list(word, rules)\n\n    def _step3(self, word):\n        return self._apply_rule_list(word, [\n            ('icate', 'ic', self._has_positive_measure),\n            ('ative', '', self._has_positive_measure),\n            ('alize', 'al', self._has_positive_measure),\n            ('iciti', 'ic', self._has_positive_measure),\n            ('ical', 'ic', self._has_positive_measure),\n            ('ful', '', self._has_positive_measure),\n            ('ness', '', self._has_positive_measure),\n        ])\n\n    def _step4(self, word):\n        measure_gt_1 = lambda stem: self._measure(stem) > 1\n        \n        return self._apply_rule_list(word, [\n            ('al', '', measure_gt_1),\n            ('ance', '', measure_gt_1),\n            ('ence', '', measure_gt_1),\n            ('er', '', measure_gt_1),\n            ('ic', '', measure_gt_1),\n            ('able', '', measure_gt_1),\n            ('ible', '', measure_gt_1),\n            ('ant', '', measure_gt_1),\n            ('ement', '', measure_gt_1),\n            ('ment', '', measure_gt_1),\n            ('ent', '', measure_gt_1),\n            \n            (\n                'ion',\n                '',\n                lambda stem: self._measure(stem) > 1 and stem[-1] in ('s', 't')\n            ),\n            \n            ('ou', '', measure_gt_1),\n            ('ism', '', measure_gt_1),\n            ('ate', '', measure_gt_1),\n            ('iti', '', measure_gt_1),\n            ('ous', '', measure_gt_1),\n            ('ive', '', measure_gt_1),\n            ('ize', '', measure_gt_1),\n        ])\n        \n    def _step5a(self, word):\n        if word.endswith('e'):\n            stem = self._replace_suffix(word, 'e', '')\n            if self._measure(stem) > 1:\n                return stem\n            if self._measure(stem) == 1 and not self._ends_cvc(stem):\n                return stem\n        return word\n\n    def _step5b(self, word):\n        return self._apply_rule_list(word, [\n            ('ll', 'l', lambda stem: self._measure(word[:-1]) > 1)\n        ])\n\n    def stem(self, word):\n        stem = word.lower()\n        \n        if self.mode == self.NLTK_EXTENSIONS and word in self.pool:\n            return self.pool[word]\n\n        if self.mode != self.ORIGINAL_ALGORITHM and len(word) <= 2:\n            return word\n\n        stem = self._step1a(stem)\n        stem = self._step1b(stem)\n        stem = self._step1c(stem)\n        stem = self._step2(stem)\n        stem = self._step3(stem)\n        stem = self._step4(stem)\n        stem = self._step5a(stem)\n        stem = self._step5b(stem)\n        \n        return stem\n\n    def __repr__(self):\n        return '<PorterStemmer>'\n\ndef demo():\n\n    from nltk.corpus import treebank\n    from nltk import stem\n\n    stemmer = stem.PorterStemmer()\n\n    orig = []\n    stemmed = []\n    for item in treebank.fileids()[:3]:\n        for (word, tag) in treebank.tagged_words(item):\n            orig.append(word)\n            stemmed.append(stemmer.stem(word))\n\n    results = ' '.join(stemmed)\n    results = re.sub(r\"(.{,70})\\s\", r'\\1\\n', results+' ').rstrip()\n\n    original = ' '.join(orig)\n    original = re.sub(r\"(.{,70})\\s\", r'\\1\\n', original+' ').rstrip()\n\n    print('-Original-'.center(70).replace(' ', '*').replace('-', ' '))\n    print(original)\n    print('-Results-'.center(70).replace(' ', '*').replace('-', ' '))\n    print(results)\n    print('*'*70)\n"], "nltk\\stem\\regexp": [".py", "from __future__ import unicode_literals\nimport re\n\nfrom nltk.stem.api import StemmerI\nfrom nltk.compat import python_2_unicode_compatible\n\n@python_2_unicode_compatible\nclass RegexpStemmer(StemmerI):\n    def __init__(self, regexp, min=0):\n\n        if not hasattr(regexp, 'pattern'):\n            regexp = re.compile(regexp)\n        self._regexp = regexp\n        self._min = min\n\n    def stem(self, word):\n        if len(word) < self._min:\n            return word\n        else:\n            return self._regexp.sub('', word)\n\n    def __repr__(self):\n        return '<RegexpStemmer: {!r}>'.format(self._regexp.pattern)\n\n\n\n\n"], "nltk\\stem\\rslp": [".py", "\n\n\nfrom __future__ import print_function, unicode_literals\nfrom nltk.data import load\n\nfrom nltk.stem.api import StemmerI\n\nclass RSLPStemmer(StemmerI):\n\n    def __init__ (self):\n        self._model = []\n\n        self._model.append( self.read_rule(\"step0.pt\") )\n        self._model.append( self.read_rule(\"step1.pt\") )\n        self._model.append( self.read_rule(\"step2.pt\") )\n        self._model.append( self.read_rule(\"step3.pt\") )\n        self._model.append( self.read_rule(\"step4.pt\") )\n        self._model.append( self.read_rule(\"step5.pt\") )\n        self._model.append( self.read_rule(\"step6.pt\") )\n\n    def read_rule (self, filename):\n        rules = load('nltk:stemmers/rslp/' + filename, format='raw').decode(\"utf8\")\n        lines = rules.split(\"\\n\")\n\n        lines = [line for line in lines if line != \"\"]     # remove blank lines\n        lines = [line for line in lines if line[0] != \"#\"]  # remove comments\n\n        lines = [line.replace(\"\\t\\t\", \"\\t\") for line in lines]\n\n        rules = []\n        for line in lines:\n            rule = []\n            tokens = line.split(\"\\t\")\n\n            rule.append( tokens[0][1:-1] ) # remove quotes\n\n            rule.append( int(tokens[1]) )\n\n            rule.append( tokens[2][1:-1] ) # remove quotes\n\n            rule.append( [token[1:-1] for token in tokens[3].split(\",\")] )\n\n            rules.append(rule)\n\n        return rules\n\n    def stem(self, word):\n        word = word.lower()\n\n        if word[-1] == \"s\":\n            word = self.apply_rule(word, 0)\n\n        if word[-1] == \"a\":\n            word = self.apply_rule(word, 1)\n\n        word = self.apply_rule(word, 3)\n\n        word = self.apply_rule(word, 2)\n\n        prev_word = word\n        word = self.apply_rule(word, 4)\n        if word == prev_word:\n            prev_word = word\n            word = self.apply_rule(word, 5)\n            if word == prev_word:\n                word = self.apply_rule(word, 6)\n\n        return word\n\n    def apply_rule(self, word, rule_index):\n        rules = self._model[rule_index]\n        for rule in rules:\n            suffix_length = len(rule[0])\n            if word[-suffix_length:] == rule[0]:       # if suffix matches\n                if len(word) >= suffix_length + rule[1]: # if we have minimum size\n                    if word not in rule[3]:                # if not an exception\n                        word = word[:-suffix_length] + rule[2]\n                        break\n\n        return word\n\n\n\n"], "nltk\\stem\\snowball": [".py", "\nfrom __future__ import unicode_literals, print_function\n\nfrom six.moves import input\nimport re\n\nfrom nltk import compat\nfrom nltk.corpus import stopwords\nfrom nltk.stem import porter\nfrom nltk.stem.util import suffix_replace, prefix_replace\n\nfrom nltk.stem.api import StemmerI\n\n\nclass SnowballStemmer(StemmerI):\n\n\n    languages = (\"arabic\", \"danish\", \"dutch\", \"english\", \"finnish\", \"french\", \"german\",\n                 \"hungarian\", \"italian\", \"norwegian\", \"porter\", \"portuguese\",\n                 \"romanian\", \"russian\", \"spanish\", \"swedish\")\n\n    def __init__(self, language, ignore_stopwords=False):\n        if language not in self.languages:\n            raise ValueError(\"The language '{0}' is not supported.\".format(language))\n        stemmerclass = globals()[language.capitalize() + \"Stemmer\"]\n        self.stemmer = stemmerclass(ignore_stopwords)\n        self.stem = self.stemmer.stem\n        self.stopwords = self.stemmer.stopwords\n    \n    def stem(self, token):\n        return self.stemmer.stem(self, token)\n\n\n@compat.python_2_unicode_compatible\nclass _LanguageSpecificStemmer(StemmerI):\n\n\n    def __init__(self, ignore_stopwords=False):\n        language = type(self).__name__.lower()\n        if language.endswith(\"stemmer\"):\n            language = language[:-7]\n\n        self.stopwords = set()\n        if ignore_stopwords:\n            try:\n                for word in stopwords.words(language):\n                    self.stopwords.add(word)\n            except IOError:\n                raise ValueError(\"{!r} has no list of stopwords. Please set\"\n                                 \" 'ignore_stopwords' to 'False'.\".format(self))\n\n    def __repr__(self):\n        return \"<{0}>\".format(type(self).__name__)\n\n\nclass PorterStemmer(_LanguageSpecificStemmer, porter.PorterStemmer):\n    def __init__(self, ignore_stopwords=False):\n        _LanguageSpecificStemmer.__init__(self, ignore_stopwords)\n        porter.PorterStemmer.__init__(self)\n\n\nclass _ScandinavianStemmer(_LanguageSpecificStemmer):\n\n\n    def _r1_scandinavian(self, word, vowels):\n        r1 = \"\"\n        for i in range(1, len(word)):\n            if word[i] not in vowels and word[i-1] in vowels:\n                if len(word[:i+1]) < 3 and len(word[:i+1]) > 0:\n                    r1 = word[3:]\n                elif len(word[:i+1]) >= 3:\n                    r1 = word[i+1:]\n                else:\n                    return word\n                break\n\n        return r1\n\n\nclass _StandardStemmer(_LanguageSpecificStemmer):\n\n\n    def _r1r2_standard(self, word, vowels):\n        r1 = \"\"\n        r2 = \"\"\n        for i in range(1, len(word)):\n            if word[i] not in vowels and word[i-1] in vowels:\n                r1 = word[i+1:]\n                break\n\n        for i in range(1, len(r1)):\n            if r1[i] not in vowels and r1[i-1] in vowels:\n                r2 = r1[i+1:]\n                break\n\n        return (r1, r2)\n\n\n\n    def _rv_standard(self, word, vowels):\n        rv = \"\"\n        if len(word) >= 2:\n            if word[1] not in vowels:\n                for i in range(2, len(word)):\n                    if word[i] in vowels:\n                        rv = word[i+1:]\n                        break\n\n            elif word[0] in vowels and word[1] in vowels:\n                for i in range(2, len(word)):\n                    if word[i] not in vowels:\n                        rv = word[i+1:]\n                        break\n            else:\n                rv = word[3:]\n\n        return rv\n\nclass ArabicStemmer(_LanguageSpecificStemmer):\n    __vocalization = re.compile(r'[\\u064b-\\u064c-\\u064d-\\u064e-\\u064f-\\u0650-\\u0651-\\u0652]') # \u064b\u060c \u064c\u060c \u064d\u060c \u064e\u060c \u064f\u060c \u0650\u060c \u0651\u060c \u0652\n\n    __kasheeda = re.compile(r'[\\u0640]') # \u0640 tatweel/kasheeda\n\n    __arabic_punctuation_marks = re.compile(r'[\\u060C-\\u061B-\\u061F]') #  \u061b \u060c \u061f\n\n    __last_hamzat = ('\\u0623', '\\u0625', '\\u0622', '\\u0624', '\\u0626') # \u0623\u060c \u0625\u060c \u0622\u060c \u0624\u060c \u0626\n\n    __initial_hamzat = re.compile(r'^[\\u0622\\u0623\\u0625]') #  \u0623\u060c \u0625\u060c \u0622\n\n    __waw_hamza = re.compile(r'[\\u0624]') # \u0624\n\n    __yeh_hamza = re.compile(r'[\\u0626]') # \u0626\n\n    __alefat = re.compile(r'[\\u0623\\u0622\\u0625]') #  \u0623\u060c \u0625\u060c \u0622\n\n    __checks1 = ('\\u0643\\u0627\\u0644', '\\u0628\\u0627\\u0644',  # \u0628\u0627\u0644\u060c \u0643\u0627\u0644\n                 '\\u0627\\u0644', '\\u0644\\u0644' # \u0644\u0644\u060c \u0627\u0644\n                 )\n\n    __checks2 = ('\\u0629', # \u0629\n                 '\\u0627\\u062a'  #  female plural \u0627\u062a\n                 )\n\n    __suffix_noun_step1a = ('\\u064a', '\\u0643', '\\u0647', # \u064a\u060c \u0643\u060c \u0647\n                            '\\u0646\\u0627', '\\u0643\\u0645', '\\u0647\\u0627', '\\u0647\\u0646', '\\u0647\\u0645', # \u0646\u0627\u060c \u0643\u0645\u060c \u0647\u0627\u060c \u0647\u0646\u060c \u0647\u0645\n                            '\\u0643\\u0645\\u0627', '\\u0647\\u0645\\u0627' # \u0643\u0645\u0627\u060c \u0647\u0645\u0627\n                            )\n\n    __suffix_noun_step1b = ('\\u0646') # \u0646\n\n    __suffix_noun_step2a = ('\\u0627', '\\u064a', '\\u0648') # \u0627\u060c \u064a\u060c \u0648\n\n    __suffix_noun_step2b = ('\\u0627\\u062a') # \u0627\u062a\n\n    __suffix_noun_step2c1 = ('\\u062a') # \u062a\n\n    __suffix_noun_step2c2 = ('\\u0629') # \u0629\n\n    __suffix_noun_step3 = ('\\u064a') # \u064a\n\n    __suffix_verb_step1 = ('\\u0647', '\\u0643', # \u0647\u060c \u0643\n                           '\\u0646\\u064a', '\\u0646\\u0627', '\\u0647\\u0627', '\\u0647\\u0645', # \u0646\u064a\u060c \u0646\u0627\u060c \u0647\u0627\u060c \u0647\u0645\n                           '\\u0647\\u0646', '\\u0643\\u0645', '\\u0643\\u0646', # \u0647\u0646\u060c \u0643\u0645\u060c \u0643\u0646\n                           '\\u0647\\u0645\\u0627', '\\u0643\\u0645\\u0627', '\\u0643\\u0645\\u0648' # \u0647\u0645\u0627\u060c \u0643\u0645\u0627\u060c \u0643\u0645\u0648\n                          )\n\n    __suffix_verb_step2a = ( '\\u062a', '\\u0627', '\\u0646' , '\\u064a', # \u062a\u060c \u0627\u060c \u0646\u060c \u064a\n                             '\\u0646\\u0627', '\\u062a\\u0627', '\\u062a\\u0646', # \u0646\u0627\u060c \u062a\u0627\u060c \u062a\u0646 Past\n                             '\\u0627\\u0646', '\\u0648\\u0646', '\\u064a\\u0646', # \u0627\u0646\u060c \u0647\u0646\u060c \u064a\u0646 Present\n                             '\\u062a\\u0645\\u0627' # \u062a\u0645\u0627\n                           )\n\n    __suffix_verb_step2b = ('\\u0648\\u0627','\\u062a\\u0645') # \u0648\u0627\u060c \u062a\u0645\n\n    __suffix_verb_step2c = ('\\u0648', # \u0648\n                            '\\u062a\\u0645\\u0648' # \u062a\u0645\u0648\n                           )\n\n    __suffix_all_alef_maqsura = ('\\u0649') # \u0649\n\n    __prefix_step1 = ('\\u0623', # \u0623\n                      '\\u0623\\u0623', '\\u0623\\u0622', '\\u0623\\u0624', '\\u0623\\u0627', '\\u0623\\u0625', # \u0623\u0623\u060c \u0623\u0622\u060c \u0623\u0624\u060c \u0623\u0627\u060c \u0623\u0625\n                      )\n\n    __prefix_step2a = ('\\u0641\\u0627\\u0644', '\\u0648\\u0627\\u0644') # \u0641\u0627\u0644\u060c \u0648\u0627\u0644\n\n    __prefix_step2b = ('\\u0641', '\\u0648') # \u0641\u060c \u0648\n\n    __prefix_step3a_noun = ('\\u0627\\u0644', '\\u0644\\u0644', # \u0644\u0644\u060c \u0627\u0644\n                            '\\u0643\\u0627\\u0644', '\\u0628\\u0627\\u0644', # \u0628\u0627\u0644\u060c \u0643\u0627\u0644\n                            )\n\n    __prefix_step3b_noun = ('\\u0628', '\\u0643', '\\u0644', # \u0628\u060c \u0643\u060c \u0644\n                            '\\u0628\\u0628', '\\u0643\\u0643' # \u0628\u0628\u060c \u0643\u0643\n                           )\n\n    __prefix_step3_verb = ('\\u0633\\u064a', '\\u0633\\u062a', '\\u0633\\u0646', '\\u0633\\u0623') # \u0633\u064a\u060c \u0633\u062a\u060c \u0633\u0646\u060c \u0633\u0623\n\n    __prefix_step4_verb = ('\\u064a\\u0633\\u062a', '\\u0646\\u0633\\u062a', '\\u062a\\u0633\\u062a') # \u064a\u0633\u062a\u060c \u0646\u0633\u062a\u060c \u062a\u0633\u062a\n\n    __conjugation_suffix_verb_1 = ('\\u0647', '\\u0643') # \u0647\u060c \u0643\n\n    __conjugation_suffix_verb_2 = ('\\u0646\\u064a', '\\u0646\\u0627','\\u0647\\u0627', # \u0646\u064a\u060c \u0646\u0627\u060c \u0647\u0627\n                                   '\\u0647\\u0645', '\\u0647\\u0646', '\\u0643\\u0645', # \u0647\u0645\u060c \u0647\u0646\u060c \u0643\u0645\n                                   '\\u0643\\u0646' # \u0643\u0646\n                                   )\n    __conjugation_suffix_verb_3 = ('\\u0647\\u0645\\u0627', '\\u0643\\u0645\\u0627', '\\u0643\\u0645\\u0648') # \u0647\u0645\u0627\u060c \u0643\u0645\u0627\u060c \u0643\u0645\u0648\n\n    __conjugation_suffix_verb_4 = ('\\u0627', '\\u0646', '\\u064a') # \u0627\u060c \u0646\u060c \u064a\n\n    __conjugation_suffix_verb_past = ('\\u0646\\u0627', '\\u062a\\u0627', '\\u062a\\u0646') # \u0646\u0627\u060c \u062a\u0627\u060c \u062a\u0646\n\n    __conjugation_suffix_verb_present = ('\\u0627\\u0646', '\\u0648\\u0646', '\\u064a\\u0646') # \u0627\u0646\u060c \u0648\u0646\u060c \u064a\u0646\n\n    __conjugation_suffix_noun_1 = ('\\u064a', '\\u0643', '\\u0647') # \u064a\u060c \u0643\u060c \u0647\n\n    __conjugation_suffix_noun_2 = ('\\u0646\\u0627', '\\u0643\\u0645', # \u0646\u0627\u060c \u0643\u0645\n                                   '\\u0647\\u0627', '\\u0647\\u0646', '\\u0647\\u0645' # \u0647\u0627\u060c \u0647\u0646\u060c \u0647\u0645\n                                   )\n\n    __conjugation_suffix_noun_3 = ('\\u0643\\u0645\\u0627', '\\u0647\\u0645\\u0627') # \u0643\u0645\u0627\u060c \u0647\u0645\u0627\n\n    __prefixes1 = ('\\u0648\\u0627', '\\u0641\\u0627') # \u0641\u0627\u060c \u0648\u0627\n\n    __articles_3len = ('\\u0643\\u0627\\u0644', '\\u0628\\u0627\\u0644')  # \u0628\u0627\u0644 \u0643\u0627\u0644\n\n    __articles_2len = ('\\u0627\\u0644', '\\u0644\\u0644')  # \u0627\u0644 \u0644\u0644\n\n    __prepositions1 = ('\\u0643', '\\u0644') # \u0643\u060c \u0644\n    __prepositions2 = ('\\u0628\\u0628', '\\u0643\\u0643') # \u0628\u0628\u060c \u0643\u0643\n\n    is_verb = True\n    is_noun = True\n    is_defined = False\n\n    suffixes_verb_step1_success = False\n    suffix_verb_step2a_success = False\n    suffix_verb_step2b_success = False\n    suffix_noun_step2c2_success = False\n    suffix_noun_step1a_success = False\n    suffix_noun_step2a_success = False\n    suffix_noun_step2b_success = False\n    suffixe_noun_step1b_success = False\n    prefix_step2a_success = False\n    prefix_step3a_noun_success = False\n    prefix_step3b_noun_success = False\n\n    def __normalize_pre(self, token):\n        token = self.__vocalization.sub('', token)\n        token = self.__kasheeda.sub('', token)\n        token = self.__arabic_punctuation_marks.sub('', token)\n        return token\n\n    def __normalize_post(self, token):\n        for hamza in self.__last_hamzat:\n            if token.endswith(hamza):\n                token = suffix_replace(token, hamza, '\\u0621')\n                break\n        token = self.__initial_hamzat.sub('\\u0627', token)\n        token = self.__waw_hamza.sub('\\u0648', token)\n        token = self.__yeh_hamza.sub('\\u064a', token)\n        token = self.__alefat.sub('\\u0627', token)\n        return  token\n\n    def __checks_1(self, token):\n        for prefix in self.__checks1 :\n            if token.startswith(prefix):\n                if prefix in self.__articles_3len and len(token) > 4 :\n                    self.is_noun = True\n                    self.is_verb = False\n                    self.is_defined = True\n                    break\n\n                if prefix in self.__articles_2len and len(token) > 3 :\n                    self.is_noun = True\n                    self.is_verb = False\n                    self.is_defined = True\n                    break\n\n    def __checks_2(self, token):\n        for suffix in self.__checks2:\n            if token.endswith(suffix):\n                if suffix == '\\u0629' and len(token) > 2:\n                    self.is_noun = True\n                    self.is_verb = False\n                    break\n\n                if suffix == '\\u0627\\u062a' and len(token) > 3:\n                    self.is_noun = True\n                    self.is_verb = False\n                    break\n\n    def __Suffix_Verb_Step1(self, token):\n        for suffix in self.__suffix_verb_step1:\n            if token.endswith(suffix):\n                if suffix in self.__conjugation_suffix_verb_1 and len(token) >= 4:\n                    token = token[:-1]\n                    self.suffixes_verb_step1_success = True\n                    break\n\n                if suffix in self.__conjugation_suffix_verb_2 and len(token) >= 5:\n                    token = token[:-2]\n                    self.suffixes_verb_step1_success = True\n                    break\n\n                if suffix in self.__conjugation_suffix_verb_3 and len(token) >= 6:\n                    token = token[:-3]\n                    self.suffixes_verb_step1_success = True\n                    break\n        return token\n\n    def __Suffix_Verb_Step2a(self, token):\n        for suffix in self.__suffix_verb_step2a:\n            if token.endswith(suffix):\n                if suffix == '\\u062a' and len(token) >= 4:\n                    token = token[:-1]\n                    self.suffix_verb_step2a_success = True\n                    break\n\n                if suffix in self.__conjugation_suffix_verb_4 and len(token) >= 4:\n                    token = token[:-1]\n                    self.suffix_verb_step2a_success = True\n                    break\n\n                if suffix in self.__conjugation_suffix_verb_past and len(token) >= 5:\n                    token = token[:-2]  # past\n                    self.suffix_verb_step2a_success = True\n                    break\n\n                if suffix in self.__conjugation_suffix_verb_present and len(token) > 5:\n                    token = token[:-2]  # present\n                    self.suffix_verb_step2a_success = True\n                    break\n\n                if suffix == '\\u062a\\u0645\\u0627' and len(token) >= 6:\n                    token = token[:-3]\n                    self.suffix_verb_step2a_success = True\n                    break\n        return  token\n\n    def __Suffix_Verb_Step2c(self, token):\n        for suffix in self.__suffix_verb_step2c:\n            if token.endswith(suffix):\n                if suffix == '\\u062a\\u0645\\u0648' and len(token) >= 6:\n                    token = token[:-3]\n                    break\n\n                if suffix == '\\u0648' and len(token) >= 4:\n                    token = token[:-1]\n                    break\n        return token\n\n    def __Suffix_Verb_Step2b(self, token):\n        for suffix in self.__suffix_verb_step2b:\n            if token.endswith(suffix) and len(token) >= 5:\n                token = token[:-2]\n                self.suffix_verb_step2b_success = True\n                break\n        return  token\n\n    def __Suffix_Noun_Step2c2(self, token):\n        for suffix in self.__suffix_noun_step2c2:\n            if token.endswith(suffix) and len(token) >= 3:\n                token = token[:-1]\n                self.suffix_noun_step2c2_success = True\n                break\n        return token\n\n    def __Suffix_Noun_Step1a(self, token):\n        for suffix in self.__suffix_noun_step1a:\n            if token.endswith(suffix):\n                if suffix in self.__conjugation_suffix_noun_1 and len(token) >= 4:\n                    token = token[:-1]\n                    self.suffix_noun_step1a_success = True\n                    break\n\n                if suffix in self.__conjugation_suffix_noun_2 and len(token) >= 5:\n                    token = token[:-2]\n                    self.suffix_noun_step1a_success = True\n                    break\n\n                if suffix in self.__conjugation_suffix_noun_3 and len(token) >= 6:\n                    token = token[:-3]\n                    self.suffix_noun_step1a_success = True\n                    break\n        return token\n\n    def __Suffix_Noun_Step2a(self, token):\n        for suffix in self.__suffix_noun_step2a:\n            if token.endswith(suffix) and len(token) > 4:\n                token = token[:-1]\n                self.suffix_noun_step2a_success = True\n                break\n        return token\n\n    def __Suffix_Noun_Step2b(self, token):\n        for suffix in self.__suffix_noun_step2b:\n            if token.endswith(suffix) and len(token) >= 5:\n                token = token[:-2]\n                self.suffix_noun_step2b_success = True\n                break\n        return  token\n\n    def __Suffix_Noun_Step2c1(self, token):\n        for suffix in self.__suffix_noun_step2c1:\n            if token.endswith(suffix) and len(token) >= 4:\n                token = token[:-1]\n                break\n        return token\n\n    def __Suffix_Noun_Step1b(self, token):\n        for suffix in self.__suffix_noun_step1b:\n            if token.endswith(suffix) and len(token) > 5:\n                token = token[:-1]\n                self.suffixe_noun_step1b_success = True\n                break\n        return token\n\n    def __Suffix_Noun_Step3(self, token):\n        for suffix in self.__suffix_noun_step3:\n            if token.endswith(suffix) and len(token) >= 3:\n                token = token[:-1]  # ya' nisbiya\n                break\n        return token\n\n    def __Suffix_All_alef_maqsura(self, token):\n        for suffix in self.__suffix_all_alef_maqsura:\n            if token.endswith(suffix):\n                token = suffix_replace(token, suffix, '\\u064a')\n        return  token\n\n    def __Prefix_Step1(self, token):\n        for prefix in self.__prefix_step1:\n            if token.startswith(prefix) and len(token) > 3:\n                if prefix == '\\u0623\\u0623':\n                    token = prefix_replace(token, prefix, '\\u0623')\n                    break\n\n                elif prefix == '\\u0623\\u0622':\n                    token = prefix_replace(token, prefix, '\\u0622')\n                    break\n\n                elif prefix == '\\u0623\\u0624':\n                    token = prefix_replace(token, prefix, '\\u0624')\n                    break\n\n                elif prefix == '\\u0623\\u0627' :\n                    token = prefix_replace(token, prefix, '\\u0627')\n                    break\n\n                elif prefix == '\\u0623\\u0625' :\n                    token = prefix_replace(token, prefix, '\\u0625')\n                    break\n        return token\n\n    def __Prefix_Step2a(self, token):\n        for prefix in self.__prefix_step2a:\n            if token.startswith(prefix) and len(token) > 5:\n                token = token[len(prefix):]\n                self.prefix_step2a_success = True\n                break\n        return  token\n\n    def __Prefix_Step2b(self, token):\n        for prefix in self.__prefix_step2b:\n            if token.startswith(prefix) and len(token) > 3 :\n                if token[:2] not in self.__prefixes1:\n                    token = token[len(prefix):]\n                    break\n        return token\n\n    def __Prefix_Step3a_Noun(self, token):\n        for prefix in self.__prefix_step3a_noun:\n            if token.startswith(prefix):\n                if prefix in self.__articles_2len and len(token) > 4:\n                    token =  token[len(prefix):]\n                    self.prefix_step3a_noun_success = True\n                    break\n                if prefix in self.__articles_3len  and len(token) > 5:\n                    token = token[len(prefix):]\n                    break\n        return token\n\n    def __Prefix_Step3b_Noun(self, token):\n        for prefix in self.__prefix_step3b_noun:\n            if token.startswith(prefix):\n                if len(token) > 3:\n                    if prefix == '\\u0628':\n                        token = token[len(prefix):]\n                        self.prefix_step3b_noun_success = True\n                        break\n\n                    if prefix in self.__prepositions2:\n                        token = prefix_replace(token, prefix, prefix[1])\n                        self.prefix_step3b_noun_success = True\n                        break\n\n                if prefix in self.__prepositions1 and len(token) > 4:\n                    token = token[len(prefix):]  # BUG: cause confusion\n                    self.prefix_step3b_noun_success = True\n                    break\n        return token\n\n    def __Prefix_Step3_Verb(self, token):\n        for prefix in self.__prefix_step3_verb:\n            if token.startswith(prefix) and len(token) > 4:\n                token = prefix_replace(token, prefix, prefix[1])\n                break\n        return token\n\n    def __Prefix_Step4_Verb(self, token):\n        for prefix in self.__prefix_step4_verb:\n            if token.startswith(prefix) and len(token) > 4:\n                token = prefix_replace(token, prefix, '\\u0627\\u0633\\u062a')\n                self.is_verb = True\n                self.is_noun = False\n                break\n        return token\n\n    def stem(self, word):\n        self.is_verb = True\n        self.is_noun = True\n        self.is_defined = False\n\n        self.suffix_verb_step2a_success = False\n        self.suffix_verb_step2b_success = False\n        self.suffix_noun_step2c2_success = False\n        self.suffix_noun_step1a_success = False\n        self.suffix_noun_step2a_success = False\n        self.suffix_noun_step2b_success = False\n        self.suffixe_noun_step1b_success = False\n        self.prefix_step2a_success = False\n        self.prefix_step3a_noun_success = False\n        self.prefix_step3b_noun_success = False\n\n        modified_word = word\n        self.__checks_1(modified_word)\n        self.__checks_2(modified_word)\n        modified_word = self.__normalize_pre(modified_word)\n        if self.is_verb:\n            modified_word = self.__Suffix_Verb_Step1(modified_word)\n            if  self.suffixes_verb_step1_success:\n                modified_word = self.__Suffix_Verb_Step2a(modified_word)\n                if not self.suffix_verb_step2a_success :\n                    modified_word = self.__Suffix_Verb_Step2c(modified_word)\n            else:\n                modified_word = self.__Suffix_Verb_Step2b(modified_word)\n                if not self.suffix_verb_step2b_success:\n                    modified_word = self.__Suffix_Verb_Step2a(modified_word)\n        if self.is_noun:\n            modified_word = self.__Suffix_Noun_Step2c2(modified_word)\n            if not self.suffix_noun_step2c2_success:\n                if not self.is_defined:\n                    modified_word = self.__Suffix_Noun_Step1a(modified_word)\n                    modified_word = self.__Suffix_Noun_Step2a(modified_word)\n                    if not self.suffix_noun_step2a_success:\n                         modified_word = self.__Suffix_Noun_Step2b(modified_word)\n                    if not self.suffix_noun_step2b_success and not self.suffix_noun_step2a_success:\n                        modified_word = self.__Suffix_Noun_Step2c1(modified_word)\n                else:\n                    modified_word =  self.__Suffix_Noun_Step1b(modified_word)\n                    if self.suffixe_noun_step1b_success:\n                        modified_word = self.__Suffix_Noun_Step2a(modified_word)\n                        if not self.suffix_noun_step2a_success:\n                            modified_word = self.__Suffix_Noun_Step2b(modified_word)\n                        if not self.suffix_noun_step2b_success and not self.suffix_noun_step2a_success:\n                            modified_word = self.__Suffix_Noun_Step2c1(modified_word)\n                    else:\n                        if not self.is_defined:\n                            modified_word = self.__Suffix_Noun_Step2a(modified_word)\n                        modified_word = self.__Suffix_Noun_Step2b(modified_word)\n            modified_word = self.__Suffix_Noun_Step3(modified_word)\n        if not self.is_noun and self.is_verb:\n            modified_word = self.__Suffix_All_alef_maqsura(modified_word)\n\n        modified_word = self.__Prefix_Step1(modified_word)\n        modified_word = self.__Prefix_Step2a(modified_word)\n        if not self.prefix_step2a_success:\n            modified_word = self.__Prefix_Step2b(modified_word)\n        modified_word = self.__Prefix_Step3a_Noun(modified_word)\n        if not self.prefix_step3a_noun_success and self.is_noun:\n            modified_word = self.__Prefix_Step3b_Noun(modified_word)\n        else:\n            if not self.prefix_step3b_noun_success and self.is_verb:\n                modified_word = self.__Prefix_Step3_Verb(modified_word)\n                modified_word = self.__Prefix_Step4_Verb(modified_word)\n\n        modified_word = self.__normalize_post(modified_word)\n        stemmed_word = modified_word\n        return stemmed_word\n\nclass DanishStemmer(_ScandinavianStemmer):\n\n\n    __vowels = \"aeiouy\\xE6\\xE5\\xF8\"\n    __consonants = \"bcdfghjklmnpqrstvwxz\"\n    __double_consonants = (\"bb\", \"cc\", \"dd\", \"ff\", \"gg\", \"hh\", \"jj\",\n                           \"kk\", \"ll\", \"mm\", \"nn\", \"pp\", \"qq\", \"rr\",\n                           \"ss\", \"tt\", \"vv\", \"ww\", \"xx\", \"zz\")\n    __s_ending = \"abcdfghjklmnoprtvyz\\xE5\"\n\n    __step1_suffixes = (\"erendes\", \"erende\", \"hedens\", \"ethed\",\n                        \"erede\", \"heden\", \"heder\", \"endes\",\n                        \"ernes\", \"erens\", \"erets\", \"ered\",\n                        \"ende\", \"erne\", \"eren\", \"erer\", \"heds\",\n                        \"enes\", \"eres\", \"eret\", \"hed\", \"ene\", \"ere\",\n                        \"ens\", \"ers\", \"ets\", \"en\", \"er\", \"es\", \"et\",\n                        \"e\", \"s\")\n    __step2_suffixes = (\"gd\", \"dt\", \"gt\", \"kt\")\n    __step3_suffixes = (\"elig\", \"l\\xF8st\", \"lig\", \"els\", \"ig\")\n\n    def stem(self, word):\n        word = word.lower()\n\n        if word in self.stopwords:\n            return word\n\n        r1 = self._r1_scandinavian(word, self.__vowels)\n\n\n        for suffix in self.__step1_suffixes:\n            if r1.endswith(suffix):\n                if suffix == \"s\":\n                    if word[-2] in self.__s_ending:\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                else:\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                break\n\n        for suffix in self.__step2_suffixes:\n            if r1.endswith(suffix):\n                word = word[:-1]\n                r1 = r1[:-1]\n                break\n\n        if r1.endswith(\"igst\"):\n            word = word[:-2]\n            r1 = r1[:-2]\n\n        for suffix in self.__step3_suffixes:\n            if r1.endswith(suffix):\n                if suffix == \"l\\xF8st\":\n                    word = word[:-1]\n                    r1 = r1[:-1]\n                else:\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n\n                    if r1.endswith(self.__step2_suffixes):\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                break\n\n        for double_cons in self.__double_consonants:\n            if word.endswith(double_cons) and len(word) > 3:\n                word = word[:-1]\n                break\n\n\n        return word\n\n\nclass DutchStemmer(_StandardStemmer):\n\n\n    __vowels = \"aeiouy\\xE8\"\n    __step1_suffixes = (\"heden\", \"ene\", \"en\", \"se\", \"s\")\n    __step3b_suffixes = (\"baar\", \"lijk\", \"bar\", \"end\", \"ing\", \"ig\")\n\n    def stem(self, word):\n        word = word.lower()\n\n        if word in self.stopwords:\n            return word\n\n        step2_success = False\n\n        word = (word.replace(\"\\xE4\", \"a\").replace(\"\\xE1\", \"a\")\n                    .replace(\"\\xEB\", \"e\").replace(\"\\xE9\", \"e\")\n                    .replace(\"\\xED\", \"i\").replace(\"\\xEF\", \"i\")\n                    .replace(\"\\xF6\", \"o\").replace(\"\\xF3\", \"o\")\n                    .replace(\"\\xFC\", \"u\").replace(\"\\xFA\", \"u\"))\n\n        if word.startswith(\"y\"):\n            word = \"\".join((\"Y\", word[1:]))\n\n        for i in range(1, len(word)):\n            if word[i-1] in self.__vowels and word[i] == \"y\":\n                word = \"\".join((word[:i], \"Y\", word[i+1:]))\n\n        for i in range(1, len(word)-1):\n            if (word[i-1] in self.__vowels and word[i] == \"i\" and\n               word[i+1] in self.__vowels):\n                word = \"\".join((word[:i], \"I\", word[i+1:]))\n\n        r1, r2 = self._r1r2_standard(word, self.__vowels)\n\n        for i in range(1, len(word)):\n            if word[i] not in self.__vowels and word[i-1] in self.__vowels:\n                if len(word[:i+1]) < 3 and len(word[:i+1]) > 0:\n                    r1 = word[3:]\n                elif len(word[:i+1]) == 0:\n                    return word\n                break\n\n        for suffix in self.__step1_suffixes:\n            if r1.endswith(suffix):\n                if suffix == \"heden\":\n                    word = suffix_replace(word, suffix, \"heid\")\n                    r1 = suffix_replace(r1, suffix, \"heid\")\n                    if r2.endswith(\"heden\"):\n                        r2 = suffix_replace(r2, suffix, \"heid\")\n\n                elif (suffix in (\"ene\", \"en\") and\n                      not word.endswith(\"heden\") and\n                      word[-len(suffix)-1] not in self.__vowels and\n                      word[-len(suffix)-3:-len(suffix)] != \"gem\"):\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                    if word.endswith((\"kk\", \"dd\", \"tt\")):\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                        r2 = r2[:-1]\n\n                elif (suffix in (\"se\", \"s\") and\n                      word[-len(suffix)-1] not in self.__vowels and\n                      word[-len(suffix)-1] != \"j\"):\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                break\n\n        if r1.endswith(\"e\") and word[-2] not in self.__vowels:\n            step2_success = True\n            word = word[:-1]\n            r1 = r1[:-1]\n            r2 = r2[:-1]\n\n            if word.endswith((\"kk\", \"dd\", \"tt\")):\n                word = word[:-1]\n                r1 = r1[:-1]\n                r2 = r2[:-1]\n\n        if r2.endswith(\"heid\") and word[-5] != \"c\":\n            word = word[:-4]\n            r1 = r1[:-4]\n            r2 = r2[:-4]\n\n            if (r1.endswith(\"en\") and word[-3] not in self.__vowels and\n                word[-5:-2] != \"gem\"):\n                word = word[:-2]\n                r1 = r1[:-2]\n                r2 = r2[:-2]\n\n                if word.endswith((\"kk\", \"dd\", \"tt\")):\n                    word = word[:-1]\n                    r1 = r1[:-1]\n                    r2 = r2[:-1]\n\n        for suffix in self.__step3b_suffixes:\n            if r2.endswith(suffix):\n                if suffix in (\"end\", \"ing\"):\n                    word = word[:-3]\n                    r2 = r2[:-3]\n\n                    if r2.endswith(\"ig\") and word[-3] != \"e\":\n                        word = word[:-2]\n                    else:\n                        if word.endswith((\"kk\", \"dd\", \"tt\")):\n                            word = word[:-1]\n\n                elif suffix == \"ig\" and word[-3] != \"e\":\n                    word = word[:-2]\n\n                elif suffix == \"lijk\":\n                    word = word[:-4]\n                    r1 = r1[:-4]\n\n                    if r1.endswith(\"e\") and word[-2] not in self.__vowels:\n                        word = word[:-1]\n                        if word.endswith((\"kk\", \"dd\", \"tt\")):\n                            word = word[:-1]\n\n                elif suffix == \"baar\":\n                    word = word[:-4]\n\n                elif suffix == \"bar\" and step2_success:\n                    word = word[:-3]\n                break\n\n        if len(word) >= 4:\n            if word[-1] not in self.__vowels and word[-1] != \"I\":\n                if word[-3:-1] in (\"aa\", \"ee\", \"oo\", \"uu\"):\n                    if word[-4] not in self.__vowels:\n                        word = \"\".join((word[:-3], word[-3], word[-1]))\n\n        word = word.replace(\"I\", \"i\").replace(\"Y\", \"y\")\n\n\n        return word\n\n\n\nclass EnglishStemmer(_StandardStemmer):\n\n\n    __vowels = \"aeiouy\"\n    __double_consonants = (\"bb\", \"dd\", \"ff\", \"gg\", \"mm\", \"nn\",\n                           \"pp\", \"rr\", \"tt\")\n    __li_ending = \"cdeghkmnrt\"\n    __step0_suffixes = (\"'s'\", \"'s\", \"'\")\n    __step1a_suffixes = (\"sses\", \"ied\", \"ies\", \"us\", \"ss\", \"s\")\n    __step1b_suffixes = (\"eedly\", \"ingly\", \"edly\", \"eed\", \"ing\", \"ed\")\n    __step2_suffixes = ('ization', 'ational', 'fulness', 'ousness',\n                        'iveness', 'tional', 'biliti', 'lessli',\n                        'entli', 'ation', 'alism', 'aliti', 'ousli',\n                        'iviti', 'fulli', 'enci', 'anci', 'abli',\n                        'izer', 'ator', 'alli', 'bli', 'ogi', 'li')\n    __step3_suffixes = ('ational', 'tional', 'alize', 'icate', 'iciti',\n                        'ative', 'ical', 'ness', 'ful')\n    __step4_suffixes = ('ement', 'ance', 'ence', 'able', 'ible', 'ment',\n                        'ant', 'ent', 'ism', 'ate', 'iti', 'ous',\n                        'ive', 'ize', 'ion', 'al', 'er', 'ic')\n    __step5_suffixes = (\"e\", \"l\")\n    __special_words = {\"skis\" : \"ski\",\n                       \"skies\" : \"sky\",\n                       \"dying\" : \"die\",\n                       \"lying\" : \"lie\",\n                       \"tying\" : \"tie\",\n                       \"idly\" : \"idl\",\n                       \"gently\" : \"gentl\",\n                       \"ugly\" : \"ugli\",\n                       \"early\" : \"earli\",\n                       \"only\" : \"onli\",\n                       \"singly\" : \"singl\",\n                       \"sky\" : \"sky\",\n                       \"news\" : \"news\",\n                       \"howe\" : \"howe\",\n                       \"atlas\" : \"atlas\",\n                       \"cosmos\" : \"cosmos\",\n                       \"bias\" : \"bias\",\n                       \"andes\" : \"andes\",\n                       \"inning\" : \"inning\",\n                       \"innings\" : \"inning\",\n                       \"outing\" : \"outing\",\n                       \"outings\" : \"outing\",\n                       \"canning\" : \"canning\",\n                       \"cannings\" : \"canning\",\n                       \"herring\" : \"herring\",\n                       \"herrings\" : \"herring\",\n                       \"earring\" : \"earring\",\n                       \"earrings\" : \"earring\",\n                       \"proceed\" : \"proceed\",\n                       \"proceeds\" : \"proceed\",\n                       \"proceeded\" : \"proceed\",\n                       \"proceeding\" : \"proceed\",\n                       \"exceed\" : \"exceed\",\n                       \"exceeds\" : \"exceed\",\n                       \"exceeded\" : \"exceed\",\n                       \"exceeding\" : \"exceed\",\n                       \"succeed\" : \"succeed\",\n                       \"succeeds\" : \"succeed\",\n                       \"succeeded\" : \"succeed\",\n                       \"succeeding\" : \"succeed\"}\n\n    def stem(self, word):\n\n        word = word.lower()\n\n        if word in self.stopwords or len(word) <= 2:\n            return word\n\n        elif word in self.__special_words:\n            return self.__special_words[word]\n\n        word = (word.replace(\"\\u2019\", \"\\x27\")\n                    .replace(\"\\u2018\", \"\\x27\")\n                    .replace(\"\\u201B\", \"\\x27\"))\n\n        if word.startswith(\"\\x27\"):\n            word = word[1:]\n\n        if word.startswith(\"y\"):\n            word = \"\".join((\"Y\", word[1:]))\n\n        for i in range(1, len(word)):\n            if word[i-1] in self.__vowels and word[i] == \"y\":\n                word = \"\".join((word[:i], \"Y\", word[i+1:]))\n\n        step1a_vowel_found = False\n        step1b_vowel_found = False\n\n        r1 = \"\"\n        r2 = \"\"\n\n        if word.startswith((\"gener\", \"commun\", \"arsen\")):\n            if word.startswith((\"gener\", \"arsen\")):\n                r1 = word[5:]\n            else:\n                r1 = word[6:]\n\n            for i in range(1, len(r1)):\n                if r1[i] not in self.__vowels and r1[i-1] in self.__vowels:\n                    r2 = r1[i+1:]\n                    break\n        else:\n            r1, r2 = self._r1r2_standard(word, self.__vowels)\n\n\n        for suffix in self.__step0_suffixes:\n            if word.endswith(suffix):\n                word = word[:-len(suffix)]\n                r1 = r1[:-len(suffix)]\n                r2 = r2[:-len(suffix)]\n                break\n\n        for suffix in self.__step1a_suffixes:\n            if word.endswith(suffix):\n\n                if suffix == \"sses\":\n                    word = word[:-2]\n                    r1 = r1[:-2]\n                    r2 = r2[:-2]\n\n                elif suffix in (\"ied\", \"ies\"):\n                    if len(word[:-len(suffix)]) > 1:\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n                    else:\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                        r2 = r2[:-1]\n\n                elif suffix == \"s\":\n                    for letter in word[:-2]:\n                        if letter in self.__vowels:\n                            step1a_vowel_found = True\n                            break\n\n                    if step1a_vowel_found:\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                        r2 = r2[:-1]\n                break\n\n        for suffix in self.__step1b_suffixes:\n            if word.endswith(suffix):\n                if suffix in (\"eed\", \"eedly\"):\n\n                    if r1.endswith(suffix):\n                        word = suffix_replace(word, suffix, \"ee\")\n\n                        if len(r1) >= len(suffix):\n                            r1 = suffix_replace(r1, suffix, \"ee\")\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= len(suffix):\n                            r2 = suffix_replace(r2, suffix, \"ee\")\n                        else:\n                            r2 = \"\"\n                else:\n                    for letter in word[:-len(suffix)]:\n                        if letter in self.__vowels:\n                            step1b_vowel_found = True\n                            break\n\n                    if step1b_vowel_found:\n                        word = word[:-len(suffix)]\n                        r1 = r1[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n\n                        if word.endswith((\"at\", \"bl\", \"iz\")):\n                            word = \"\".join((word, \"e\"))\n                            r1 = \"\".join((r1, \"e\"))\n\n                            if len(word) > 5 or len(r1) >=3:\n                                r2 = \"\".join((r2, \"e\"))\n\n                        elif word.endswith(self.__double_consonants):\n                            word = word[:-1]\n                            r1 = r1[:-1]\n                            r2 = r2[:-1]\n\n                        elif ((r1 == \"\" and len(word) >= 3 and\n                               word[-1] not in self.__vowels and\n                               word[-1] not in \"wxY\" and\n                               word[-2] in self.__vowels and\n                               word[-3] not in self.__vowels)\n                              or\n                              (r1 == \"\" and len(word) == 2 and\n                               word[0] in self.__vowels and\n                               word[1] not in self.__vowels)):\n\n                            word = \"\".join((word, \"e\"))\n\n                            if len(r1) > 0:\n                                r1 = \"\".join((r1, \"e\"))\n\n                            if len(r2) > 0:\n                                r2 = \"\".join((r2, \"e\"))\n                break\n\n        if len(word) > 2 and word[-1] in \"yY\" and word[-2] not in self.__vowels:\n            word = \"\".join((word[:-1], \"i\"))\n            if len(r1) >= 1:\n                r1 = \"\".join((r1[:-1], \"i\"))\n            else:\n                r1 = \"\"\n\n            if len(r2) >= 1:\n                r2 = \"\".join((r2[:-1], \"i\"))\n            else:\n                r2 = \"\"\n\n        for suffix in self.__step2_suffixes:\n            if word.endswith(suffix):\n                if r1.endswith(suffix):\n                    if suffix == \"tional\":\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n\n                    elif suffix in (\"enci\", \"anci\", \"abli\"):\n                        word = \"\".join((word[:-1], \"e\"))\n\n                        if len(r1) >= 1:\n                            r1 = \"\".join((r1[:-1], \"e\"))\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= 1:\n                            r2 = \"\".join((r2[:-1], \"e\"))\n                        else:\n                            r2 = \"\"\n\n                    elif suffix == \"entli\":\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n\n                    elif suffix in (\"izer\", \"ization\"):\n                        word = suffix_replace(word, suffix, \"ize\")\n\n                        if len(r1) >= len(suffix):\n                            r1 = suffix_replace(r1, suffix, \"ize\")\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= len(suffix):\n                            r2 = suffix_replace(r2, suffix, \"ize\")\n                        else:\n                            r2 = \"\"\n\n                    elif suffix in (\"ational\", \"ation\", \"ator\"):\n                        word = suffix_replace(word, suffix, \"ate\")\n\n                        if len(r1) >= len(suffix):\n                            r1 = suffix_replace(r1, suffix, \"ate\")\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= len(suffix):\n                            r2 = suffix_replace(r2, suffix, \"ate\")\n                        else:\n                            r2 = \"e\"\n\n                    elif suffix in (\"alism\", \"aliti\", \"alli\"):\n                        word = suffix_replace(word, suffix, \"al\")\n\n                        if len(r1) >= len(suffix):\n                            r1 = suffix_replace(r1, suffix, \"al\")\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= len(suffix):\n                            r2 = suffix_replace(r2, suffix, \"al\")\n                        else:\n                            r2 = \"\"\n\n                    elif suffix == \"fulness\":\n                        word = word[:-4]\n                        r1 = r1[:-4]\n                        r2 = r2[:-4]\n\n                    elif suffix in (\"ousli\", \"ousness\"):\n                        word = suffix_replace(word, suffix, \"ous\")\n\n                        if len(r1) >= len(suffix):\n                            r1 = suffix_replace(r1, suffix, \"ous\")\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= len(suffix):\n                            r2 = suffix_replace(r2, suffix, \"ous\")\n                        else:\n                            r2 = \"\"\n\n                    elif suffix in (\"iveness\", \"iviti\"):\n                        word = suffix_replace(word, suffix, \"ive\")\n\n                        if len(r1) >= len(suffix):\n                            r1 = suffix_replace(r1, suffix, \"ive\")\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= len(suffix):\n                            r2 = suffix_replace(r2, suffix, \"ive\")\n                        else:\n                            r2 = \"e\"\n\n                    elif suffix in (\"biliti\", \"bli\"):\n                        word = suffix_replace(word, suffix, \"ble\")\n\n                        if len(r1) >= len(suffix):\n                            r1 = suffix_replace(r1, suffix, \"ble\")\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= len(suffix):\n                            r2 = suffix_replace(r2, suffix, \"ble\")\n                        else:\n                            r2 = \"\"\n\n                    elif suffix == \"ogi\" and word[-4] == \"l\":\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                        r2 = r2[:-1]\n\n                    elif suffix in (\"fulli\", \"lessli\"):\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n\n                    elif suffix == \"li\" and word[-3] in self.__li_ending:\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n                break\n\n        for suffix in self.__step3_suffixes:\n            if word.endswith(suffix):\n                if r1.endswith(suffix):\n                    if suffix == \"tional\":\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n\n                    elif suffix == \"ational\":\n                        word = suffix_replace(word, suffix, \"ate\")\n\n                        if len(r1) >= len(suffix):\n                            r1 = suffix_replace(r1, suffix, \"ate\")\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= len(suffix):\n                            r2 = suffix_replace(r2, suffix, \"ate\")\n                        else:\n                            r2 = \"\"\n\n                    elif suffix == \"alize\":\n                        word = word[:-3]\n                        r1 = r1[:-3]\n                        r2 = r2[:-3]\n\n                    elif suffix in (\"icate\", \"iciti\", \"ical\"):\n                        word = suffix_replace(word, suffix, \"ic\")\n\n                        if len(r1) >= len(suffix):\n                            r1 = suffix_replace(r1, suffix, \"ic\")\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= len(suffix):\n                            r2 = suffix_replace(r2, suffix, \"ic\")\n                        else:\n                            r2 = \"\"\n\n                    elif suffix in (\"ful\", \"ness\"):\n                        word = word[:-len(suffix)]\n                        r1 = r1[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n\n                    elif suffix == \"ative\" and r2.endswith(suffix):\n                        word = word[:-5]\n                        r1 = r1[:-5]\n                        r2 = r2[:-5]\n                break\n\n        for suffix in self.__step4_suffixes:\n            if word.endswith(suffix):\n                if r2.endswith(suffix):\n                    if suffix == \"ion\":\n                        if word[-4] in \"st\":\n                            word = word[:-3]\n                            r1 = r1[:-3]\n                            r2 = r2[:-3]\n                    else:\n                        word = word[:-len(suffix)]\n                        r1 = r1[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                break\n\n        if r2.endswith(\"l\") and word[-2] == \"l\":\n            word = word[:-1]\n        elif r2.endswith(\"e\"):\n            word = word[:-1]\n        elif r1.endswith(\"e\"):\n            if len(word) >= 4 and (word[-2] in self.__vowels or\n                                   word[-2] in \"wxY\" or\n                                   word[-3] not in self.__vowels or\n                                   word[-4] in self.__vowels):\n                word = word[:-1]\n\n\n        word = word.replace(\"Y\", \"y\")\n\n\n        return word\n\n\n\nclass FinnishStemmer(_StandardStemmer):\n\n\n    __vowels = \"aeiouy\\xE4\\xF6\"\n    __restricted_vowels = \"aeiou\\xE4\\xF6\"\n    __long_vowels = (\"aa\", \"ee\", \"ii\", \"oo\", \"uu\", \"\\xE4\\xE4\",\n                     \"\\xF6\\xF6\")\n    __consonants = \"bcdfghjklmnpqrstvwxz\"\n    __double_consonants = (\"bb\", \"cc\", \"dd\", \"ff\", \"gg\", \"hh\", \"jj\",\n                           \"kk\", \"ll\", \"mm\", \"nn\", \"pp\", \"qq\", \"rr\",\n                           \"ss\", \"tt\", \"vv\", \"ww\", \"xx\", \"zz\")\n    __step1_suffixes = ('kaan', 'k\\xE4\\xE4n', 'sti', 'kin', 'han',\n                        'h\\xE4n', 'ko', 'k\\xF6', 'pa', 'p\\xE4')\n    __step2_suffixes = ('nsa', 'ns\\xE4', 'mme', 'nne', 'si', 'ni',\n                        'an', '\\xE4n', 'en')\n    __step3_suffixes = ('siin', 'tten', 'seen', 'han', 'hen', 'hin',\n                        'hon', 'h\\xE4n', 'h\\xF6n', 'den', 'tta',\n                        'tt\\xE4', 'ssa', 'ss\\xE4', 'sta',\n                        'st\\xE4', 'lla', 'll\\xE4', 'lta',\n                        'lt\\xE4', 'lle', 'ksi', 'ine', 'ta',\n                        't\\xE4', 'na', 'n\\xE4', 'a', '\\xE4',\n                        'n')\n    __step4_suffixes = ('impi', 'impa', 'imp\\xE4', 'immi', 'imma',\n                        'imm\\xE4', 'mpi', 'mpa', 'mp\\xE4', 'mmi',\n                        'mma', 'mm\\xE4', 'eja', 'ej\\xE4')\n\n    def stem(self, word):\n        word = word.lower()\n\n        if word in self.stopwords:\n            return word\n\n        step3_success = False\n\n        r1, r2 = self._r1r2_standard(word, self.__vowels)\n\n        for suffix in self.__step1_suffixes:\n            if r1.endswith(suffix):\n                if suffix == \"sti\":\n                    if suffix in r2:\n                        word = word[:-3]\n                        r1 = r1[:-3]\n                        r2 = r2[:-3]\n                else:\n                    if word[-len(suffix)-1] in \"ntaeiouy\\xE4\\xF6\":\n                        word = word[:-len(suffix)]\n                        r1 = r1[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                break\n\n        for suffix in self.__step2_suffixes:\n            if r1.endswith(suffix):\n                if suffix == \"si\":\n                    if word[-3] != \"k\":\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n\n                elif suffix == \"ni\":\n                    word = word[:-2]\n                    r1 = r1[:-2]\n                    r2 = r2[:-2]\n                    if word.endswith(\"kse\"):\n                        word = suffix_replace(word, \"kse\", \"ksi\")\n\n                    if r1.endswith(\"kse\"):\n                        r1 = suffix_replace(r1, \"kse\", \"ksi\")\n\n                    if r2.endswith(\"kse\"):\n                        r2 = suffix_replace(r2, \"kse\", \"ksi\")\n\n                elif suffix == \"an\":\n                    if (word[-4:-2] in (\"ta\", \"na\") or\n                        word[-5:-2] in (\"ssa\", \"sta\", \"lla\", \"lta\")):\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n\n                elif suffix == \"\\xE4n\":\n                    if (word[-4:-2] in (\"t\\xE4\", \"n\\xE4\") or\n                        word[-5:-2] in (\"ss\\xE4\", \"st\\xE4\",\n                                        \"ll\\xE4\", \"lt\\xE4\")):\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n\n                elif suffix == \"en\":\n                    if word[-5:-2] in (\"lle\", \"ine\"):\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n                else:\n                    word = word[:-3]\n                    r1 = r1[:-3]\n                    r2 = r2[:-3]\n                break\n\n        for suffix in self.__step3_suffixes:\n            if r1.endswith(suffix):\n                if suffix in (\"han\", \"hen\", \"hin\", \"hon\", \"h\\xE4n\",\n                              \"h\\xF6n\"):\n                    if ((suffix == \"han\" and word[-4] == \"a\") or\n                        (suffix == \"hen\" and word[-4] == \"e\") or\n                        (suffix == \"hin\" and word[-4] == \"i\") or\n                        (suffix == \"hon\" and word[-4] == \"o\") or\n                        (suffix == \"h\\xE4n\" and word[-4] == \"\\xE4\") or\n                        (suffix == \"h\\xF6n\" and word[-4] == \"\\xF6\")):\n                        word = word[:-3]\n                        r1 = r1[:-3]\n                        r2 = r2[:-3]\n                        step3_success = True\n\n                elif suffix in (\"siin\", \"den\", \"tten\"):\n                    if (word[-len(suffix)-1] == \"i\" and\n                        word[-len(suffix)-2] in self.__restricted_vowels):\n                        word = word[:-len(suffix)]\n                        r1 = r1[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                        step3_success = True\n                    else:\n                        continue\n\n                elif suffix == \"seen\":\n                    if word[-6:-4] in self.__long_vowels:\n                        word = word[:-4]\n                        r1 = r1[:-4]\n                        r2 = r2[:-4]\n                        step3_success = True\n                    else:\n                        continue\n\n                elif suffix in (\"a\", \"\\xE4\"):\n                    if word[-2] in self.__vowels and word[-3] in self.__consonants:\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                        r2 = r2[:-1]\n                        step3_success = True\n\n                elif suffix in (\"tta\", \"tt\\xE4\"):\n                    if word[-4] == \"e\":\n                        word = word[:-3]\n                        r1 = r1[:-3]\n                        r2 = r2[:-3]\n                        step3_success = True\n\n                elif suffix == \"n\":\n                    word = word[:-1]\n                    r1 = r1[:-1]\n                    r2 = r2[:-1]\n                    step3_success = True\n\n                    if word[-2:] == \"ie\" or word[-2:] in self.__long_vowels:\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                        r2 = r2[:-1]\n                else:\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                    step3_success = True\n                break\n\n        for suffix in self.__step4_suffixes:\n            if r2.endswith(suffix):\n                if suffix in (\"mpi\", \"mpa\", \"mp\\xE4\", \"mmi\", \"mma\",\n                              \"mm\\xE4\"):\n                    if word[-5:-3] != \"po\":\n                        word = word[:-3]\n                        r1 = r1[:-3]\n                        r2 = r2[:-3]\n                else:\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                break\n\n        if step3_success and len(r1) >= 1 and r1[-1] in \"ij\":\n            word = word[:-1]\n            r1 = r1[:-1]\n\n        elif (not step3_success and len(r1) >= 2 and\n              r1[-1] == \"t\" and r1[-2] in self.__vowels):\n            word = word[:-1]\n            r1 = r1[:-1]\n            r2 = r2[:-1]\n            if r2.endswith(\"imma\"):\n                word = word[:-4]\n                r1 = r1[:-4]\n            elif r2.endswith(\"mma\") and r2[-5:-3] != \"po\":\n                word = word[:-3]\n                r1 = r1[:-3]\n\n        if r1[-2:] in self.__long_vowels:\n            word = word[:-1]\n            r1 = r1[:-1]\n\n        if (len(r1) >= 2 and r1[-2] in self.__consonants and\n            r1[-1] in \"a\\xE4ei\"):\n            word = word[:-1]\n            r1 = r1[:-1]\n\n        if r1.endswith((\"oj\", \"uj\")):\n            word = word[:-1]\n            r1 = r1[:-1]\n\n        if r1.endswith(\"jo\"):\n            word = word[:-1]\n            r1 = r1[:-1]\n\n        for i in range(1, len(word)):\n            if word[-i] in self.__vowels:\n                continue\n            else:\n                if i == 1:\n                    if word[-i-1:] in self.__double_consonants:\n                        word = word[:-1]\n                else:\n                    if word[-i-1:-i+1] in self.__double_consonants:\n                        word = \"\".join((word[:-i], word[-i+1:]))\n                break\n\n\n        return word\n\n\n\nclass FrenchStemmer(_StandardStemmer):\n\n\n    __vowels = \"aeiouy\\xE2\\xE0\\xEB\\xE9\\xEA\\xE8\\xEF\\xEE\\xF4\\xFB\\xF9\"\n    __step1_suffixes = ('issements', 'issement', 'atrices', 'atrice',\n                        'ateurs', 'ations', 'logies', 'usions',\n                        'utions', 'ements', 'amment', 'emment',\n                        'ances', 'iqUes', 'ismes', 'ables', 'istes',\n                        'ateur', 'ation', 'logie', 'usion', 'ution',\n                        'ences', 'ement', 'euses', 'ments', 'ance',\n                        'iqUe', 'isme', 'able', 'iste', 'ence',\n                        'it\\xE9s', 'ives', 'eaux', 'euse', 'ment',\n                        'eux', 'it\\xE9', 'ive', 'ifs', 'aux', 'if')\n    __step2a_suffixes = ('issaIent', 'issantes', 'iraIent', 'issante',\n                         'issants', 'issions', 'irions', 'issais',\n                         'issait', 'issant', 'issent', 'issiez', 'issons',\n                         'irais', 'irait', 'irent', 'iriez', 'irons',\n                         'iront', 'isses', 'issez', '\\xEEmes',\n                         '\\xEEtes', 'irai', 'iras', 'irez', 'isse',\n                         'ies', 'ira', '\\xEEt', 'ie', 'ir', 'is',\n                         'it', 'i')\n    __step2b_suffixes = ('eraIent', 'assions', 'erions', 'assent',\n                         'assiez', '\\xE8rent', 'erais', 'erait',\n                         'eriez', 'erons', 'eront', 'aIent', 'antes',\n                         'asses', 'ions', 'erai', 'eras', 'erez',\n                         '\\xE2mes', '\\xE2tes', 'ante', 'ants',\n                         'asse', '\\xE9es', 'era', 'iez', 'ais',\n                         'ait', 'ant', '\\xE9e', '\\xE9s', 'er',\n                         'ez', '\\xE2t', 'ai', 'as', '\\xE9', 'a')\n    __step4_suffixes = ('i\\xE8re', 'I\\xE8re', 'ion', 'ier', 'Ier',\n                        'e', '\\xEB')\n\n    def stem(self, word):\n        word = word.lower()\n\n        if word in self.stopwords:\n            return word\n\n        step1_success = False\n        rv_ending_found = False\n        step2a_success = False\n        step2b_success = False\n\n        for i in range(1, len(word)):\n            if word[i-1] == \"q\" and word[i] == \"u\":\n                word = \"\".join((word[:i], \"U\", word[i+1:]))\n\n        for i in range(1, len(word)-1):\n            if word[i-1] in self.__vowels and word[i+1] in self.__vowels:\n                if word[i] == \"u\":\n                    word = \"\".join((word[:i], \"U\", word[i+1:]))\n\n                elif word[i] == \"i\":\n                    word = \"\".join((word[:i], \"I\", word[i+1:]))\n\n            if word[i-1] in self.__vowels or word[i+1] in self.__vowels:\n                if word[i] == \"y\":\n                    word = \"\".join((word[:i], \"Y\", word[i+1:]))\n\n        r1, r2 = self._r1r2_standard(word, self.__vowels)\n        rv = self.__rv_french(word, self.__vowels)\n\n        for suffix in self.__step1_suffixes:\n            if word.endswith(suffix):\n                if suffix == \"eaux\":\n                    word = word[:-1]\n                    step1_success = True\n\n                elif suffix in (\"euse\", \"euses\"):\n                    if suffix in r2:\n                        word = word[:-len(suffix)]\n                        step1_success = True\n\n                    elif suffix in r1:\n                        word = suffix_replace(word, suffix, \"eux\")\n                        step1_success = True\n\n                elif suffix in (\"ement\", \"ements\") and suffix in rv:\n                    word = word[:-len(suffix)]\n                    step1_success = True\n\n                    if word[-2:] == \"iv\" and \"iv\" in r2:\n                        word = word[:-2]\n\n                        if word[-2:] == \"at\" and \"at\" in r2:\n                            word = word[:-2]\n\n                    elif word[-3:] == \"eus\":\n                        if \"eus\" in r2:\n                            word = word[:-3]\n                        elif \"eus\" in r1:\n                            word = \"\".join((word[:-1], \"x\"))\n\n                    elif word[-3:] in (\"abl\", \"iqU\"):\n                        if \"abl\" in r2 or \"iqU\" in r2:\n                            word = word[:-3]\n\n                    elif word[-3:] in (\"i\\xE8r\", \"I\\xE8r\"):\n                        if \"i\\xE8r\" in rv or \"I\\xE8r\" in rv:\n                            word = \"\".join((word[:-3], \"i\"))\n\n                elif suffix == \"amment\" and suffix in rv:\n                    word = suffix_replace(word, \"amment\", \"ant\")\n                    rv = suffix_replace(rv, \"amment\", \"ant\")\n                    rv_ending_found = True\n\n                elif suffix == \"emment\" and suffix in rv:\n                    word = suffix_replace(word, \"emment\", \"ent\")\n                    rv_ending_found = True\n\n                elif (suffix in (\"ment\", \"ments\") and suffix in rv and\n                      not rv.startswith(suffix) and\n                      rv[rv.rindex(suffix)-1] in self.__vowels):\n                    word = word[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n                    rv_ending_found = True\n\n                elif suffix == \"aux\" and suffix in r1:\n                    word = \"\".join((word[:-2], \"l\"))\n                    step1_success = True\n\n                elif (suffix in (\"issement\", \"issements\") and suffix in r1\n                      and word[-len(suffix)-1] not in self.__vowels):\n                    word = word[:-len(suffix)]\n                    step1_success = True\n\n                elif suffix in (\"ance\", \"iqUe\", \"isme\", \"able\", \"iste\",\n                              \"eux\", \"ances\", \"iqUes\", \"ismes\",\n                              \"ables\", \"istes\") and suffix in r2:\n                    word = word[:-len(suffix)]\n                    step1_success = True\n\n                elif suffix in (\"atrice\", \"ateur\", \"ation\", \"atrices\",\n                                \"ateurs\", \"ations\") and suffix in r2:\n                    word = word[:-len(suffix)]\n                    step1_success = True\n\n                    if word[-2:] == \"ic\":\n                        if \"ic\" in r2:\n                            word = word[:-2]\n                        else:\n                            word = \"\".join((word[:-2], \"iqU\"))\n\n                elif suffix in (\"logie\", \"logies\") and suffix in r2:\n                    word = suffix_replace(word, suffix, \"log\")\n                    step1_success = True\n\n                elif (suffix in (\"usion\", \"ution\", \"usions\", \"utions\") and\n                      suffix in r2):\n                    word = suffix_replace(word, suffix, \"u\")\n                    step1_success = True\n\n                elif suffix in (\"ence\", \"ences\") and suffix in r2:\n                    word = suffix_replace(word, suffix, \"ent\")\n                    step1_success = True\n\n                elif suffix in (\"it\\xE9\", \"it\\xE9s\") and suffix in r2:\n                    word = word[:-len(suffix)]\n                    step1_success = True\n\n                    if word[-4:] == \"abil\":\n                        if \"abil\" in r2:\n                            word = word[:-4]\n                        else:\n                            word = \"\".join((word[:-2], \"l\"))\n\n                    elif word[-2:] == \"ic\":\n                        if \"ic\" in r2:\n                            word = word[:-2]\n                        else:\n                            word = \"\".join((word[:-2], \"iqU\"))\n\n                    elif word[-2:] == \"iv\":\n                        if \"iv\" in r2:\n                            word = word[:-2]\n\n                elif (suffix in (\"if\", \"ive\", \"ifs\", \"ives\") and\n                      suffix in r2):\n                    word = word[:-len(suffix)]\n                    step1_success = True\n\n                    if word[-2:] == \"at\" and \"at\" in r2:\n                        word = word[:-2]\n\n                        if word[-2:] == \"ic\":\n                            if \"ic\" in r2:\n                                word = word[:-2]\n                            else:\n                                word = \"\".join((word[:-2], \"iqU\"))\n                break\n\n        if not step1_success or rv_ending_found:\n            for suffix in self.__step2a_suffixes:\n                if word.endswith(suffix):\n                    if (suffix in rv and len(rv) > len(suffix) and\n                        rv[rv.rindex(suffix)-1] not in self.__vowels):\n                        word = word[:-len(suffix)]\n                        step2a_success = True\n                    break\n\n            if not step2a_success:\n                for suffix in self.__step2b_suffixes:\n                    if rv.endswith(suffix):\n                        if suffix == \"ions\" and \"ions\" in r2:\n                            word = word[:-4]\n                            step2b_success = True\n\n                        elif suffix in ('eraIent', 'erions', '\\xE8rent',\n                                        'erais', 'erait', 'eriez',\n                                        'erons', 'eront', 'erai', 'eras',\n                                        'erez', '\\xE9es', 'era', 'iez',\n                                        '\\xE9e', '\\xE9s', 'er', 'ez',\n                                        '\\xE9'):\n                            word = word[:-len(suffix)]\n                            step2b_success = True\n\n                        elif suffix in ('assions', 'assent', 'assiez',\n                                        'aIent', 'antes', 'asses',\n                                        '\\xE2mes', '\\xE2tes', 'ante',\n                                        'ants', 'asse', 'ais', 'ait',\n                                        'ant', '\\xE2t', 'ai', 'as',\n                                        'a'):\n                            word = word[:-len(suffix)]\n                            rv = rv[:-len(suffix)]\n                            step2b_success = True\n                            if rv.endswith(\"e\"):\n                                word = word[:-1]\n                        break\n\n        if step1_success or step2a_success or step2b_success:\n            if word[-1] == \"Y\":\n                word = \"\".join((word[:-1], \"i\"))\n            elif word[-1] == \"\\xE7\":\n                word = \"\".join((word[:-1], \"c\"))\n\n        else:\n            if (len(word) >= 2 and word[-1] == \"s\" and\n                word[-2] not in \"aiou\\xE8s\"):\n                word = word[:-1]\n\n            for suffix in self.__step4_suffixes:\n                if word.endswith(suffix):\n                    if suffix in rv:\n                        if (suffix == \"ion\" and suffix in r2 and\n                            rv[-4] in \"st\"):\n                            word = word[:-3]\n\n                        elif suffix in (\"ier\", \"i\\xE8re\", \"Ier\",\n                                        \"I\\xE8re\"):\n                            word = suffix_replace(word, suffix, \"i\")\n\n                        elif suffix == \"e\":\n                            word = word[:-1]\n\n                        elif suffix == \"\\xEB\" and word[-3:-1] == \"gu\":\n                            word = word[:-1]\n                        break\n\n        if word.endswith((\"enn\", \"onn\", \"ett\", \"ell\", \"eill\")):\n            word = word[:-1]\n\n        for i in range(1, len(word)):\n            if word[-i] not in self.__vowels:\n                i += 1\n            else:\n                if i != 1 and word[-i] in (\"\\xE9\", \"\\xE8\"):\n                    word = \"\".join((word[:-i], \"e\", word[-i+1:]))\n                break\n\n        word = (word.replace(\"I\", \"i\")\n                    .replace(\"U\", \"u\")\n                    .replace(\"Y\", \"y\"))\n\n\n        return word\n\n\n\n    def __rv_french(self, word, vowels):\n        rv = \"\"\n        if len(word) >= 2:\n            if (word.startswith((\"par\", \"col\", \"tap\")) or\n                (word[0] in vowels and word[1] in vowels)):\n                rv = word[3:]\n            else:\n                for i in range(1, len(word)):\n                    if word[i] in vowels:\n                        rv = word[i+1:]\n                        break\n\n        return rv\n\n\n\nclass GermanStemmer(_StandardStemmer):\n\n\n    __vowels = \"aeiouy\\xE4\\xF6\\xFC\"\n    __s_ending = \"bdfghklmnrt\"\n    __st_ending = \"bdfghklmnt\"\n\n    __step1_suffixes = (\"ern\", \"em\", \"er\", \"en\", \"es\", \"e\", \"s\")\n    __step2_suffixes = (\"est\", \"en\", \"er\", \"st\")\n    __step3_suffixes = (\"isch\", \"lich\", \"heit\", \"keit\",\n                          \"end\", \"ung\", \"ig\", \"ik\")\n\n    def stem(self, word):\n        word = word.lower()\n\n        if word in self.stopwords:\n            return word\n\n        word = word.replace(\"\\xDF\", \"ss\")\n\n        for i in range(1, len(word)-1):\n            if word[i-1] in self.__vowels and word[i+1] in self.__vowels:\n                if word[i] == \"u\":\n                    word = \"\".join((word[:i], \"U\", word[i+1:]))\n\n                elif word[i] == \"y\":\n                    word = \"\".join((word[:i], \"Y\", word[i+1:]))\n\n        r1, r2 = self._r1r2_standard(word, self.__vowels)\n\n        for i in range(1, len(word)):\n            if word[i] not in self.__vowels and word[i-1] in self.__vowels:\n                if len(word[:i+1]) < 3 and len(word[:i+1]) > 0:\n                    r1 = word[3:]\n                elif len(word[:i+1]) == 0:\n                    return word\n                break\n\n        for suffix in self.__step1_suffixes:\n            if r1.endswith(suffix):\n                if (suffix in (\"en\", \"es\", \"e\") and\n                    word[-len(suffix)-4:-len(suffix)] == \"niss\"):\n                    word = word[:-len(suffix)-1]\n                    r1 = r1[:-len(suffix)-1]\n                    r2 = r2[:-len(suffix)-1]\n\n                elif suffix == \"s\":\n                    if word[-2] in self.__s_ending:\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                        r2 = r2[:-1]\n                else:\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                break\n\n        for suffix in self.__step2_suffixes:\n            if r1.endswith(suffix):\n                if suffix == \"st\":\n                    if word[-3] in self.__st_ending and len(word[:-3]) >= 3:\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n                else:\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                break\n\n        for suffix in self.__step3_suffixes:\n            if r2.endswith(suffix):\n                if suffix in (\"end\", \"ung\"):\n                    if (\"ig\" in r2[-len(suffix)-2:-len(suffix)] and\n                        \"e\" not in r2[-len(suffix)-3:-len(suffix)-2]):\n                        word = word[:-len(suffix)-2]\n                    else:\n                        word = word[:-len(suffix)]\n\n                elif (suffix in (\"ig\", \"ik\", \"isch\") and\n                      \"e\" not in r2[-len(suffix)-1:-len(suffix)]):\n                    word = word[:-len(suffix)]\n\n                elif suffix in (\"lich\", \"heit\"):\n                    if (\"er\" in r1[-len(suffix)-2:-len(suffix)] or\n                        \"en\" in r1[-len(suffix)-2:-len(suffix)]):\n                        word = word[:-len(suffix)-2]\n                    else:\n                        word = word[:-len(suffix)]\n\n                elif suffix == \"keit\":\n                    if \"lich\" in r2[-len(suffix)-4:-len(suffix)]:\n                        word = word[:-len(suffix)-4]\n\n                    elif \"ig\" in r2[-len(suffix)-2:-len(suffix)]:\n                        word = word[:-len(suffix)-2]\n                    else:\n                        word = word[:-len(suffix)]\n                break\n\n        word = (word.replace(\"\\xE4\", \"a\").replace(\"\\xF6\", \"o\")\n                    .replace(\"\\xFC\", \"u\").replace(\"U\", \"u\")\n                    .replace(\"Y\", \"y\"))\n\n\n        return word\n\n\n\nclass HungarianStemmer(_LanguageSpecificStemmer):\n\n\n    __vowels = \"aeiou\\xF6\\xFC\\xE1\\xE9\\xED\\xF3\\xF5\\xFA\\xFB\"\n    __digraphs = (\"cs\", \"dz\", \"dzs\", \"gy\", \"ly\", \"ny\", \"ty\", \"zs\")\n    __double_consonants = (\"bb\", \"cc\", \"ccs\", \"dd\", \"ff\", \"gg\",\n                             \"ggy\", \"jj\", \"kk\", \"ll\", \"lly\", \"mm\",\n                             \"nn\", \"nny\", \"pp\", \"rr\", \"ss\", \"ssz\",\n                             \"tt\", \"tty\", \"vv\", \"zz\", \"zzs\")\n\n    __step1_suffixes = (\"al\", \"el\")\n    __step2_suffixes = ('k\\xE9ppen', 'onk\\xE9nt', 'enk\\xE9nt',\n                        'ank\\xE9nt', 'k\\xE9pp', 'k\\xE9nt', 'ban',\n                        'ben', 'nak', 'nek', 'val', 'vel', 't\\xF3l',\n                        't\\xF5l', 'r\\xF3l', 'r\\xF5l', 'b\\xF3l',\n                        'b\\xF5l', 'hoz', 'hez', 'h\\xF6z',\n                        'n\\xE1l', 'n\\xE9l', '\\xE9rt', 'kor',\n                        'ba', 'be', 'ra', 're', 'ig', 'at', 'et',\n                        'ot', '\\xF6t', 'ul', '\\xFCl', 'v\\xE1',\n                        'v\\xE9', 'en', 'on', 'an', '\\xF6n',\n                        'n', 't')\n    __step3_suffixes = (\"\\xE1nk\\xE9nt\", \"\\xE1n\", \"\\xE9n\")\n    __step4_suffixes = ('astul', 'est\\xFCl', '\\xE1stul',\n                        '\\xE9st\\xFCl', 'stul', 'st\\xFCl')\n    __step5_suffixes = (\"\\xE1\", \"\\xE9\")\n    __step6_suffixes = ('ok\\xE9', '\\xF6k\\xE9', 'ak\\xE9',\n                        'ek\\xE9', '\\xE1k\\xE9', '\\xE1\\xE9i',\n                        '\\xE9k\\xE9', '\\xE9\\xE9i', 'k\\xE9',\n                        '\\xE9i', '\\xE9\\xE9', '\\xE9')\n    __step7_suffixes = ('\\xE1juk', '\\xE9j\\xFCk', '\\xFCnk',\n                        'unk', 'juk', 'j\\xFCk', '\\xE1nk',\n                        '\\xE9nk', 'nk', 'uk', '\\xFCk', 'em',\n                        'om', 'am', 'od', 'ed', 'ad', '\\xF6d',\n                        'ja', 'je', '\\xE1m', '\\xE1d', '\\xE9m',\n                        '\\xE9d', 'm', 'd', 'a', 'e', 'o',\n                        '\\xE1', '\\xE9')\n    __step8_suffixes = ('jaitok', 'jeitek', 'jaink', 'jeink', 'aitok',\n                        'eitek', '\\xE1itok', '\\xE9itek', 'jaim',\n                        'jeim', 'jaid', 'jeid', 'eink', 'aink',\n                        'itek', 'jeik', 'jaik', '\\xE1ink',\n                        '\\xE9ink', 'aim', 'eim', 'aid', 'eid',\n                        'jai', 'jei', 'ink', 'aik', 'eik',\n                        '\\xE1im', '\\xE1id', '\\xE1ik', '\\xE9im',\n                        '\\xE9id', '\\xE9ik', 'im', 'id', 'ai',\n                        'ei', 'ik', '\\xE1i', '\\xE9i', 'i')\n    __step9_suffixes = (\"\\xE1k\", \"\\xE9k\", \"\\xF6k\", \"ok\",\n                        \"ek\", \"ak\", \"k\")\n\n    def stem(self, word):\n        word = word.lower()\n\n        if word in self.stopwords:\n            return word\n\n        r1 = self.__r1_hungarian(word, self.__vowels, self.__digraphs)\n\n        if r1.endswith(self.__step1_suffixes):\n            for double_cons in self.__double_consonants:\n                if word[-2-len(double_cons):-2] == double_cons:\n                    word = \"\".join((word[:-4], word[-3]))\n\n                    if r1[-2-len(double_cons):-2] == double_cons:\n                        r1 = \"\".join((r1[:-4], r1[-3]))\n                    break\n\n        for suffix in self.__step2_suffixes:\n            if word.endswith(suffix):\n                if r1.endswith(suffix):\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n\n                    if r1.endswith(\"\\xE1\"):\n                        word = \"\".join((word[:-1], \"a\"))\n                        r1 = suffix_replace(r1, \"\\xE1\", \"a\")\n\n                    elif r1.endswith(\"\\xE9\"):\n                        word = \"\".join((word[:-1], \"e\"))\n                        r1 = suffix_replace(r1, \"\\xE9\", \"e\")\n                break\n\n        for suffix in self.__step3_suffixes:\n            if r1.endswith(suffix):\n                if suffix == \"\\xE9n\":\n                    word = suffix_replace(word, suffix, \"e\")\n                    r1 = suffix_replace(r1, suffix, \"e\")\n                else:\n                    word = suffix_replace(word, suffix, \"a\")\n                    r1 = suffix_replace(r1, suffix, \"a\")\n                break\n\n        for suffix in self.__step4_suffixes:\n            if r1.endswith(suffix):\n                if suffix == \"\\xE1stul\":\n                    word = suffix_replace(word, suffix, \"a\")\n                    r1 = suffix_replace(r1, suffix, \"a\")\n\n                elif suffix == \"\\xE9st\\xFCl\":\n                    word = suffix_replace(word, suffix, \"e\")\n                    r1 = suffix_replace(r1, suffix, \"e\")\n                else:\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                break\n\n        for suffix in self.__step5_suffixes:\n            if r1.endswith(suffix):\n                for double_cons in self.__double_consonants:\n                    if word[-1-len(double_cons):-1] == double_cons:\n                        word = \"\".join((word[:-3], word[-2]))\n\n                        if r1[-1-len(double_cons):-1] == double_cons:\n                            r1 = \"\".join((r1[:-3], r1[-2]))\n                        break\n\n        for suffix in self.__step6_suffixes:\n            if r1.endswith(suffix):\n                if suffix in (\"\\xE1k\\xE9\", \"\\xE1\\xE9i\"):\n                    word = suffix_replace(word, suffix, \"a\")\n                    r1 = suffix_replace(r1, suffix, \"a\")\n\n                elif suffix in (\"\\xE9k\\xE9\", \"\\xE9\\xE9i\",\n                                \"\\xE9\\xE9\"):\n                    word = suffix_replace(word, suffix, \"e\")\n                    r1 = suffix_replace(r1, suffix, \"e\")\n                else:\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                break\n\n        for suffix in self.__step7_suffixes:\n            if word.endswith(suffix):\n                if r1.endswith(suffix):\n                    if suffix in (\"\\xE1nk\", \"\\xE1juk\", \"\\xE1m\",\n                                  \"\\xE1d\", \"\\xE1\"):\n                        word = suffix_replace(word, suffix, \"a\")\n                        r1 = suffix_replace(r1, suffix, \"a\")\n\n                    elif suffix in (\"\\xE9nk\", \"\\xE9j\\xFCk\",\n                                    \"\\xE9m\", \"\\xE9d\", \"\\xE9\"):\n                        word = suffix_replace(word, suffix, \"e\")\n                        r1 = suffix_replace(r1, suffix, \"e\")\n                    else:\n                        word = word[:-len(suffix)]\n                        r1 = r1[:-len(suffix)]\n                break\n\n        for suffix in self.__step8_suffixes:\n            if word.endswith(suffix):\n                if r1.endswith(suffix):\n                    if suffix in (\"\\xE1im\", \"\\xE1id\", \"\\xE1i\",\n                                  \"\\xE1ink\", \"\\xE1itok\", \"\\xE1ik\"):\n                        word = suffix_replace(word, suffix, \"a\")\n                        r1 = suffix_replace(r1, suffix, \"a\")\n\n                    elif suffix in (\"\\xE9im\", \"\\xE9id\", \"\\xE9i\",\n                                    \"\\xE9ink\", \"\\xE9itek\", \"\\xE9ik\"):\n                        word = suffix_replace(word, suffix, \"e\")\n                        r1 = suffix_replace(r1, suffix, \"e\")\n                    else:\n                        word = word[:-len(suffix)]\n                        r1 = r1[:-len(suffix)]\n                break\n\n        for suffix in self.__step9_suffixes:\n            if word.endswith(suffix):\n                if r1.endswith(suffix):\n                    if suffix == \"\\xE1k\":\n                        word = suffix_replace(word, suffix, \"a\")\n                    elif suffix == \"\\xE9k\":\n                        word = suffix_replace(word, suffix, \"e\")\n                    else:\n                        word = word[:-len(suffix)]\n                break\n\n\n        return word\n\n\n\n    def __r1_hungarian(self, word, vowels, digraphs):\n        r1 = \"\"\n        if word[0] in vowels:\n            for digraph in digraphs:\n                if digraph in word[1:]:\n                    r1 = word[word.index(digraph[-1])+1:]\n                    return r1\n\n            for i in range(1, len(word)):\n                if word[i] not in vowels:\n                    r1 = word[i+1:]\n                    break\n        else:\n            for i in range(1, len(word)):\n                if word[i] in vowels:\n                    r1 = word[i+1:]\n                    break\n\n        return r1\n\n\n\nclass ItalianStemmer(_StandardStemmer):\n\n\n    __vowels = \"aeiou\\xE0\\xE8\\xEC\\xF2\\xF9\"\n    __step0_suffixes = ('gliela', 'gliele', 'glieli', 'glielo',\n                        'gliene', 'sene', 'mela', 'mele', 'meli',\n                        'melo', 'mene', 'tela', 'tele', 'teli',\n                        'telo', 'tene', 'cela', 'cele', 'celi',\n                        'celo', 'cene', 'vela', 'vele', 'veli',\n                        'velo', 'vene', 'gli', 'ci', 'la', 'le',\n                        'li', 'lo', 'mi', 'ne', 'si', 'ti', 'vi')\n    __step1_suffixes = ('atrice', 'atrici', 'azione', 'azioni',\n                        'uzione', 'uzioni', 'usione', 'usioni',\n                        'amento', 'amenti', 'imento', 'imenti',\n                        'amente', 'abile', 'abili', 'ibile', 'ibili',\n                        'mente', 'atore', 'atori', 'logia', 'logie',\n                        'anza', 'anze', 'iche', 'ichi', 'ismo',\n                        'ismi', 'ista', 'iste', 'isti', 'ist\\xE0',\n                        'ist\\xE8', 'ist\\xEC', 'ante', 'anti',\n                        'enza', 'enze', 'ico', 'ici', 'ica', 'ice',\n                        'oso', 'osi', 'osa', 'ose', 'it\\xE0',\n                        'ivo', 'ivi', 'iva', 'ive')\n    __step2_suffixes = ('erebbero', 'irebbero', 'assero', 'assimo',\n                        'eranno', 'erebbe', 'eremmo', 'ereste',\n                        'eresti', 'essero', 'iranno', 'irebbe',\n                        'iremmo', 'ireste', 'iresti', 'iscano',\n                        'iscono', 'issero', 'arono', 'avamo', 'avano',\n                        'avate', 'eremo', 'erete', 'erono', 'evamo',\n                        'evano', 'evate', 'iremo', 'irete', 'irono',\n                        'ivamo', 'ivano', 'ivate', 'ammo', 'ando',\n                        'asse', 'assi', 'emmo', 'enda', 'ende',\n                        'endi', 'endo', 'erai', 'erei', 'Yamo',\n                        'iamo', 'immo', 'irai', 'irei', 'isca',\n                        'isce', 'isci', 'isco', 'ano', 'are', 'ata',\n                        'ate', 'ati', 'ato', 'ava', 'avi', 'avo',\n                        'er\\xE0', 'ere', 'er\\xF2', 'ete', 'eva',\n                        'evi', 'evo', 'ir\\xE0', 'ire', 'ir\\xF2',\n                        'ita', 'ite', 'iti', 'ito', 'iva', 'ivi',\n                        'ivo', 'ono', 'uta', 'ute', 'uti', 'uto',\n                        'ar', 'ir')\n\n    def stem(self, word):\n        word = word.lower()\n\n        if word in self.stopwords:\n            return word\n\n        step1_success = False\n\n        word = (word.replace(\"\\xE1\", \"\\xE0\")\n                    .replace(\"\\xE9\", \"\\xE8\")\n                    .replace(\"\\xED\", \"\\xEC\")\n                    .replace(\"\\xF3\", \"\\xF2\")\n                    .replace(\"\\xFA\", \"\\xF9\"))\n\n        for i in range(1, len(word)):\n            if word[i-1] == \"q\" and word[i] == \"u\":\n                word = \"\".join((word[:i], \"U\", word[i+1:]))\n\n        for i in range(1, len(word)-1):\n            if word[i-1] in self.__vowels and word[i+1] in self.__vowels:\n                if word[i] == \"u\":\n                    word = \"\".join((word[:i], \"U\", word[i+1:]))\n\n                elif word [i] == \"i\":\n                    word = \"\".join((word[:i], \"I\", word[i+1:]))\n\n        r1, r2 = self._r1r2_standard(word, self.__vowels)\n        rv = self._rv_standard(word, self.__vowels)\n\n        for suffix in self.__step0_suffixes:\n            if rv.endswith(suffix):\n                if rv[-len(suffix)-4:-len(suffix)] in (\"ando\", \"endo\"):\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n\n                elif (rv[-len(suffix)-2:-len(suffix)] in\n                      (\"ar\", \"er\", \"ir\")):\n                    word = suffix_replace(word, suffix, \"e\")\n                    r1 = suffix_replace(r1, suffix, \"e\")\n                    r2 = suffix_replace(r2, suffix, \"e\")\n                    rv = suffix_replace(rv, suffix, \"e\")\n                break\n\n        for suffix in self.__step1_suffixes:\n            if word.endswith(suffix):\n                if suffix == \"amente\" and r1.endswith(suffix):\n                    step1_success = True\n                    word = word[:-6]\n                    r2 = r2[:-6]\n                    rv = rv[:-6]\n\n                    if r2.endswith(\"iv\"):\n                        word = word[:-2]\n                        r2 = r2[:-2]\n                        rv = rv[:-2]\n\n                        if r2.endswith(\"at\"):\n                            word = word[:-2]\n                            rv = rv[:-2]\n\n                    elif r2.endswith((\"os\", \"ic\")):\n                        word = word[:-2]\n                        rv = rv[:-2]\n\n                    elif r2 .endswith(\"abil\"):\n                        word = word[:-4]\n                        rv = rv[:-4]\n\n                elif (suffix in (\"amento\", \"amenti\",\n                                 \"imento\", \"imenti\") and\n                      rv.endswith(suffix)):\n                    step1_success = True\n                    word = word[:-6]\n                    rv = rv[:-6]\n\n                elif r2.endswith(suffix):\n                    step1_success = True\n                    if suffix in (\"azione\", \"azioni\", \"atore\", \"atori\"):\n                        word = word[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n\n                        if r2.endswith(\"ic\"):\n                            word = word[:-2]\n                            rv = rv[:-2]\n\n                    elif suffix in (\"logia\", \"logie\"):\n                        word = word[:-2]\n                        rv = word[:-2]\n\n                    elif suffix in (\"uzione\", \"uzioni\",\n                                    \"usione\", \"usioni\"):\n                        word = word[:-5]\n                        rv = rv[:-5]\n\n                    elif suffix in (\"enza\", \"enze\"):\n                        word = suffix_replace(word, suffix, \"te\")\n                        rv = suffix_replace(rv, suffix, \"te\")\n\n                    elif suffix == \"it\\xE0\":\n                        word = word[:-3]\n                        r2 = r2[:-3]\n                        rv = rv[:-3]\n\n                        if r2.endswith((\"ic\", \"iv\")):\n                            word = word[:-2]\n                            rv = rv[:-2]\n\n                        elif r2.endswith(\"abil\"):\n                            word = word[:-4]\n                            rv = rv[:-4]\n\n                    elif suffix in (\"ivo\", \"ivi\", \"iva\", \"ive\"):\n                        word = word[:-3]\n                        r2 = r2[:-3]\n                        rv = rv[:-3]\n\n                        if r2.endswith(\"at\"):\n                            word = word[:-2]\n                            r2 = r2[:-2]\n                            rv = rv[:-2]\n\n                            if r2.endswith(\"ic\"):\n                                word = word[:-2]\n                                rv = rv[:-2]\n                    else:\n                        word = word[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n                break\n\n        if not step1_success:\n            for suffix in self.__step2_suffixes:\n                if rv.endswith(suffix):\n                    word = word[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n                    break\n\n        if rv.endswith((\"a\", \"e\", \"i\", \"o\", \"\\xE0\", \"\\xE8\",\n                        \"\\xEC\", \"\\xF2\")):\n            word = word[:-1]\n            rv = rv[:-1]\n\n            if rv.endswith(\"i\"):\n                word = word[:-1]\n                rv = rv[:-1]\n\n        if rv.endswith((\"ch\", \"gh\")):\n            word = word[:-1]\n\n        word = word.replace(\"I\", \"i\").replace(\"U\", \"u\")\n\n\n        return word\n\n\n\nclass NorwegianStemmer(_ScandinavianStemmer):\n\n\n    __vowels = \"aeiouy\\xE6\\xE5\\xF8\"\n    __s_ending = \"bcdfghjlmnoprtvyz\"\n    __step1_suffixes = (\"hetenes\", \"hetene\", \"hetens\", \"heter\",\n                        \"heten\", \"endes\", \"ande\", \"ende\", \"edes\",\n                        \"enes\", \"erte\", \"ede\", \"ane\", \"ene\", \"ens\",\n                        \"ers\", \"ets\", \"het\", \"ast\", \"ert\", \"en\",\n                        \"ar\", \"er\", \"as\", \"es\", \"et\", \"a\", \"e\", \"s\")\n\n    __step2_suffixes = (\"dt\", \"vt\")\n\n    __step3_suffixes = (\"hetslov\", \"eleg\", \"elig\", \"elov\", \"slov\",\n                          \"leg\", \"eig\", \"lig\", \"els\", \"lov\", \"ig\")\n\n    def stem(self, word):\n        word = word.lower()\n\n        if word in self.stopwords:\n            return word\n\n        r1 = self._r1_scandinavian(word, self.__vowels)\n\n        for suffix in self.__step1_suffixes:\n            if r1.endswith(suffix):\n                if suffix in (\"erte\", \"ert\"):\n                    word = suffix_replace(word, suffix, \"er\")\n                    r1 = suffix_replace(r1, suffix, \"er\")\n\n                elif suffix == \"s\":\n                    if (word[-2] in self.__s_ending or\n                        (word[-2] == \"k\" and word[-3] not in self.__vowels)):\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                else:\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                break\n\n        for suffix in self.__step2_suffixes:\n            if r1.endswith(suffix):\n                word = word[:-1]\n                r1 = r1[:-1]\n                break\n\n        for suffix in self.__step3_suffixes:\n            if r1.endswith(suffix):\n                word = word[:-len(suffix)]\n                break\n\n\n        return word\n\n\n\nclass PortugueseStemmer(_StandardStemmer):\n\n\n    __vowels = \"aeiou\\xE1\\xE9\\xED\\xF3\\xFA\\xE2\\xEA\\xF4\"\n    __step1_suffixes = ('amentos', 'imentos', 'u\u00e7o~es', 'amento',\n                        'imento', 'adoras', 'adores', 'a\\xE7o~es',\n                        'logias', '\\xEAncias', 'amente',\n                        'idades', 'an\\xE7as', 'ismos', 'istas', 'adora',\n                        'a\\xE7a~o', 'antes', '\\xE2ncia',\n                        'logia', 'u\u00e7a~o', '\\xEAncia',\n                        'mente', 'idade', 'an\\xE7a', 'ezas', 'icos', 'icas',\n                        'ismo', '\\xE1vel', '\\xEDvel', 'ista',\n                        'osos', 'osas', 'ador', 'ante', 'ivas',\n                        'ivos', 'iras', 'eza', 'ico', 'ica',\n                        'oso', 'osa', 'iva', 'ivo', 'ira')\n    __step2_suffixes = ('ar\\xEDamos', 'er\\xEDamos', 'ir\\xEDamos',\n                        '\\xE1ssemos', '\\xEAssemos', '\\xEDssemos',\n                        'ar\\xEDeis', 'er\\xEDeis', 'ir\\xEDeis',\n                        '\\xE1sseis', '\\xE9sseis', '\\xEDsseis',\n                        '\\xE1ramos', '\\xE9ramos', '\\xEDramos',\n                        '\\xE1vamos', 'aremos', 'eremos', 'iremos',\n                        'ariam', 'eriam', 'iriam', 'assem', 'essem',\n                        'issem', 'ara~o', 'era~o', 'ira~o', 'arias',\n                        'erias', 'irias', 'ardes', 'erdes', 'irdes',\n                        'asses', 'esses', 'isses', 'astes', 'estes',\n                        'istes', '\\xE1reis', 'areis', '\\xE9reis',\n                        'ereis', '\\xEDreis', 'ireis', '\\xE1veis',\n                        '\\xEDamos', 'armos', 'ermos', 'irmos',\n                        'aria', 'eria', 'iria', 'asse', 'esse',\n                        'isse', 'aste', 'este', 'iste', 'arei',\n                        'erei', 'irei', 'aram', 'eram', 'iram',\n                        'avam', 'arem', 'erem', 'irem',\n                        'ando', 'endo', 'indo', 'adas', 'idas',\n                        'ar\\xE1s', 'aras', 'er\\xE1s', 'eras',\n                        'ir\\xE1s', 'avas', 'ares', 'eres', 'ires',\n                        '\\xEDeis', 'ados', 'idos', '\\xE1mos',\n                        'amos', 'emos', 'imos', 'iras', 'ada', 'ida',\n                        'ar\\xE1', 'ara', 'er\\xE1', 'era',\n                        'ir\\xE1', 'ava', 'iam', 'ado', 'ido',\n                        'ias', 'ais', 'eis', 'ira', 'ia', 'ei', 'am',\n                        'em', 'ar', 'er', 'ir', 'as',\n                        'es', 'is', 'eu', 'iu', 'ou')\n    __step4_suffixes = (\"os\", \"a\", \"i\", \"o\", \"\\xE1\",\n                        \"\\xED\", \"\\xF3\")\n\n    def stem(self, word):\n        word = word.lower()\n\n        if word in self.stopwords:\n            return word\n\n        step1_success = False\n        step2_success = False\n\n        word = (word.replace(\"\\xE3\", \"a~\")\n                    .replace(\"\\xF5\", \"o~\")\n                    .replace(\"q\\xFC\", \"qu\")\n                    .replace(\"g\\xFC\", \"gu\"))\n\n        r1, r2 = self._r1r2_standard(word, self.__vowels)\n        rv = self._rv_standard(word, self.__vowels)\n\n        for suffix in self.__step1_suffixes:\n            if word.endswith(suffix):\n                if suffix == \"amente\" and r1.endswith(suffix):\n                    step1_success = True\n\n                    word = word[:-6]\n                    r2 = r2[:-6]\n                    rv = rv[:-6]\n\n                    if r2.endswith(\"iv\"):\n                        word = word[:-2]\n                        r2 = r2[:-2]\n                        rv = rv[:-2]\n\n                        if r2.endswith(\"at\"):\n                            word = word[:-2]\n                            rv = rv[:-2]\n\n                    elif r2.endswith((\"os\", \"ic\", \"ad\")):\n                        word = word[:-2]\n                        rv = rv[:-2]\n\n                elif (suffix in (\"ira\", \"iras\") and rv.endswith(suffix) and\n                      word[-len(suffix)-1:-len(suffix)] == \"e\"):\n                    step1_success = True\n\n                    word = suffix_replace(word, suffix, \"ir\")\n                    rv = suffix_replace(rv, suffix, \"ir\")\n\n                elif r2.endswith(suffix):\n                    step1_success = True\n\n                    if suffix in (\"logia\", \"logias\"):\n                        word = suffix_replace(word, suffix, \"log\")\n                        rv = suffix_replace(rv, suffix, \"log\")\n\n                    elif suffix in (\"u\u00e7a~o\", \"u\u00e7o~es\"):\n                        word = suffix_replace(word, suffix, \"u\")\n                        rv = suffix_replace(rv, suffix, \"u\")\n\n                    elif suffix in (\"\\xEAncia\", \"\\xEAncias\"):\n                        word = suffix_replace(word, suffix, \"ente\")\n                        rv = suffix_replace(rv, suffix, \"ente\")\n\n                    elif suffix == \"mente\":\n                        word = word[:-5]\n                        r2 = r2[:-5]\n                        rv = rv[:-5]\n\n                        if r2.endswith((\"ante\", \"avel\", \"ivel\")):\n                            word = word[:-4]\n                            rv = rv[:-4]\n\n                    elif suffix in (\"idade\", \"idades\"):\n                        word = word[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n\n                        if r2.endswith((\"ic\", \"iv\")):\n                            word = word[:-2]\n                            rv = rv[:-2]\n\n                        elif r2.endswith(\"abil\"):\n                            word = word[:-4]\n                            rv = rv[:-4]\n\n                    elif suffix in (\"iva\", \"ivo\", \"ivas\", \"ivos\"):\n                        word = word[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n\n                        if r2.endswith(\"at\"):\n                            word = word[:-2]\n                            rv = rv[:-2]\n                    else:\n                        word = word[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n                break\n\n        if not step1_success:\n            for suffix in self.__step2_suffixes:\n                if rv.endswith(suffix):\n                    step2_success = True\n\n                    word = word[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n                    break\n\n        if step1_success or step2_success:\n            if rv.endswith(\"i\") and word[-2] == \"c\":\n                word = word[:-1]\n                rv = rv[:-1]\n\n        if not step1_success and not step2_success:\n            for suffix in self.__step4_suffixes:\n                if rv.endswith(suffix):\n                    word = word[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n                    break\n\n        if rv.endswith((\"e\", \"\\xE9\", \"\\xEA\")):\n            word = word[:-1]\n            rv = rv[:-1]\n\n            if ((word.endswith(\"gu\") and rv.endswith(\"u\")) or\n                (word.endswith(\"ci\") and rv.endswith(\"i\"))):\n                word = word[:-1]\n\n        elif word.endswith(\"\\xE7\"):\n            word = suffix_replace(word, \"\\xE7\", \"c\")\n\n        word = word.replace(\"a~\", \"\\xE3\").replace(\"o~\", \"\\xF5\")\n\n\n        return word\n\n\n\nclass RomanianStemmer(_StandardStemmer):\n\n\n    __vowels = \"aeiou\\u0103\\xE2\\xEE\"\n    __step0_suffixes = ('iilor', 'ului', 'elor', 'iile', 'ilor',\n                        'atei', 'a\\u0163ie', 'a\\u0163ia', 'aua',\n                        'ele', 'iua', 'iei', 'ile', 'ul', 'ea',\n                        'ii')\n    __step1_suffixes = ('abilitate', 'abilitati', 'abilit\\u0103\\u0163i',\n                        'ibilitate', 'abilit\\u0103i', 'ivitate',\n                        'ivitati', 'ivit\\u0103\\u0163i', 'icitate',\n                        'icitati', 'icit\\u0103\\u0163i', 'icatori',\n                        'ivit\\u0103i', 'icit\\u0103i', 'icator',\n                        'a\\u0163iune', 'atoare', '\\u0103toare',\n                        'i\\u0163iune', 'itoare', 'iciva', 'icive',\n                        'icivi', 'iciv\\u0103', 'icala', 'icale',\n                        'icali', 'ical\\u0103', 'ativa', 'ative',\n                        'ativi', 'ativ\\u0103', 'atori', '\\u0103tori',\n                        'itiva', 'itive', 'itivi', 'itiv\\u0103',\n                        'itori', 'iciv', 'ical', 'ativ', 'ator',\n                        '\\u0103tor', 'itiv', 'itor')\n    __step2_suffixes = ('abila', 'abile', 'abili', 'abil\\u0103',\n                        'ibila', 'ibile', 'ibili', 'ibil\\u0103',\n                        'atori', 'itate', 'itati', 'it\\u0103\\u0163i',\n                        'abil', 'ibil', 'oasa', 'oas\\u0103', 'oase',\n                        'anta', 'ante', 'anti', 'ant\\u0103', 'ator',\n                        'it\\u0103i', 'iune', 'iuni', 'isme', 'ista',\n                        'iste', 'isti', 'ist\\u0103', 'i\\u015Fti',\n                        'ata', 'at\\u0103', 'ati', 'ate', 'uta',\n                        'ut\\u0103', 'uti', 'ute', 'ita', 'it\\u0103',\n                        'iti', 'ite', 'ica', 'ice', 'ici', 'ic\\u0103',\n                        'osi', 'o\\u015Fi', 'ant', 'iva', 'ive', 'ivi',\n                        'iv\\u0103', 'ism', 'ist', 'at', 'ut', 'it',\n                        'ic', 'os', 'iv')\n    __step3_suffixes = ('seser\\u0103\\u0163i', 'aser\\u0103\\u0163i',\n                        'iser\\u0103\\u0163i', '\\xE2ser\\u0103\\u0163i',\n                        'user\\u0103\\u0163i', 'seser\\u0103m',\n                        'aser\\u0103m', 'iser\\u0103m', '\\xE2ser\\u0103m',\n                        'user\\u0103m', 'ser\\u0103\\u0163i', 'sese\\u015Fi',\n                        'seser\\u0103', 'easc\\u0103', 'ar\\u0103\\u0163i',\n                        'ur\\u0103\\u0163i', 'ir\\u0103\\u0163i',\n                        '\\xE2r\\u0103\\u0163i', 'ase\\u015Fi',\n                        'aser\\u0103', 'ise\\u015Fi', 'iser\\u0103',\n                        '\\xe2se\\u015Fi', '\\xE2ser\\u0103',\n                        'use\\u015Fi', 'user\\u0103', 'ser\\u0103m',\n                        'sesem', 'indu', '\\xE2ndu', 'eaz\\u0103',\n                        'e\\u015Fti', 'e\\u015Fte', '\\u0103\\u015Fti',\n                        '\\u0103\\u015Fte', 'ea\\u0163i', 'ia\\u0163i',\n                        'ar\\u0103m', 'ur\\u0103m', 'ir\\u0103m',\n                        '\\xE2r\\u0103m', 'asem', 'isem',\n                        '\\xE2sem', 'usem', 'se\\u015Fi', 'ser\\u0103',\n                        'sese', 'are', 'ere', 'ire', '\\xE2re',\n                        'ind', '\\xE2nd', 'eze', 'ezi', 'esc',\n                        '\\u0103sc', 'eam', 'eai', 'eau', 'iam',\n                        'iai', 'iau', 'a\\u015Fi', 'ar\\u0103',\n                        'u\\u015Fi', 'ur\\u0103', 'i\\u015Fi', 'ir\\u0103',\n                        '\\xE2\\u015Fi', '\\xe2r\\u0103', 'ase',\n                        'ise', '\\xE2se', 'use', 'a\\u0163i',\n                        'e\\u0163i', 'i\\u0163i', '\\xe2\\u0163i', 'sei',\n                        'ez', 'am', 'ai', 'au', 'ea', 'ia', 'ui',\n                        '\\xE2i', '\\u0103m', 'em', 'im', '\\xE2m',\n                        'se')\n\n    def stem(self, word):\n        word = word.lower()\n\n        if word in self.stopwords:\n            return word\n\n        step1_success = False\n        step2_success = False\n\n        for i in range(1, len(word)-1):\n            if word[i-1] in self.__vowels and word[i+1] in self.__vowels:\n                if word[i] == \"u\":\n                    word = \"\".join((word[:i], \"U\", word[i+1:]))\n\n                elif word[i] == \"i\":\n                    word = \"\".join((word[:i], \"I\", word[i+1:]))\n\n        r1, r2 = self._r1r2_standard(word, self.__vowels)\n        rv = self._rv_standard(word, self.__vowels)\n\n        for suffix in self.__step0_suffixes:\n            if word.endswith(suffix):\n                if suffix in r1:\n                    if suffix in (\"ul\", \"ului\"):\n                        word = word[:-len(suffix)]\n\n                        if suffix in rv:\n                            rv = rv[:-len(suffix)]\n                        else:\n                            rv = \"\"\n\n                    elif (suffix == \"aua\" or suffix == \"atei\" or\n                          (suffix == \"ile\" and word[-5:-3] != \"ab\")):\n                        word = word[:-2]\n\n                    elif suffix in (\"ea\", \"ele\", \"elor\"):\n                        word = suffix_replace(word, suffix, \"e\")\n\n                        if suffix in rv:\n                            rv = suffix_replace(rv, suffix, \"e\")\n                        else:\n                            rv = \"\"\n\n                    elif suffix in (\"ii\", \"iua\", \"iei\",\n                                    \"iile\", \"iilor\", \"ilor\"):\n                        word = suffix_replace(word, suffix, \"i\")\n\n                        if suffix in rv:\n                            rv = suffix_replace(rv, suffix, \"i\")\n                        else:\n                            rv = \"\"\n\n                    elif suffix in (\"a\\u0163ie\", \"a\\u0163ia\"):\n                        word = word[:-1]\n                break\n\n        while True:\n\n            replacement_done = False\n\n            for suffix in self.__step1_suffixes:\n                if word.endswith(suffix):\n                    if suffix in r1:\n                        step1_success = True\n                        replacement_done = True\n\n                        if suffix in (\"abilitate\", \"abilitati\",\n                                      \"abilit\\u0103i\",\n                                      \"abilit\\u0103\\u0163i\"):\n                            word = suffix_replace(word, suffix, \"abil\")\n\n                        elif suffix == \"ibilitate\":\n                            word = word[:-5]\n\n                        elif suffix in (\"ivitate\", \"ivitati\",\n                                        \"ivit\\u0103i\",\n                                        \"ivit\\u0103\\u0163i\"):\n                            word = suffix_replace(word, suffix, \"iv\")\n\n                        elif suffix in (\"icitate\", \"icitati\", \"icit\\u0103i\",\n                                        \"icit\\u0103\\u0163i\", \"icator\",\n                                        \"icatori\", \"iciv\", \"iciva\",\n                                        \"icive\", \"icivi\", \"iciv\\u0103\",\n                                        \"ical\", \"icala\", \"icale\", \"icali\",\n                                        \"ical\\u0103\"):\n                            word = suffix_replace(word, suffix, \"ic\")\n\n                        elif suffix in (\"ativ\", \"ativa\", \"ative\", \"ativi\",\n                                        \"ativ\\u0103\", \"a\\u0163iune\",\n                                        \"atoare\", \"ator\", \"atori\",\n                                        \"\\u0103toare\",\n                                        \"\\u0103tor\", \"\\u0103tori\"):\n                            word = suffix_replace(word, suffix, \"at\")\n\n                            if suffix in r2:\n                                r2 = suffix_replace(r2, suffix, \"at\")\n\n                        elif suffix in (\"itiv\", \"itiva\", \"itive\", \"itivi\",\n                                        \"itiv\\u0103\", \"i\\u0163iune\",\n                                        \"itoare\", \"itor\", \"itori\"):\n                            word = suffix_replace(word, suffix, \"it\")\n\n                            if suffix in r2:\n                                r2 = suffix_replace(r2, suffix, \"it\")\n                    else:\n                        step1_success = False\n                    break\n\n            if not replacement_done:\n                break\n\n        for suffix in self.__step2_suffixes:\n            if word.endswith(suffix):\n                if suffix in r2:\n                    step2_success = True\n\n                    if suffix in (\"iune\", \"iuni\"):\n                        if word[-5] == \"\\u0163\":\n                            word = \"\".join((word[:-5], \"t\"))\n\n                    elif suffix in (\"ism\", \"isme\", \"ist\", \"ista\", \"iste\",\n                                    \"isti\", \"ist\\u0103\", \"i\\u015Fti\"):\n                        word = suffix_replace(word, suffix, \"ist\")\n\n                    else:\n                        word = word[:-len(suffix)]\n                break\n\n        if not step1_success and not step2_success:\n            for suffix in self.__step3_suffixes:\n                if word.endswith(suffix):\n                    if suffix in rv:\n                        if suffix in ('seser\\u0103\\u0163i', 'seser\\u0103m',\n                                      'ser\\u0103\\u0163i', 'sese\\u015Fi',\n                                      'seser\\u0103', 'ser\\u0103m', 'sesem',\n                                      'se\\u015Fi', 'ser\\u0103', 'sese',\n                                      'a\\u0163i', 'e\\u0163i', 'i\\u0163i',\n                                      '\\xE2\\u0163i', 'sei', '\\u0103m',\n                                      'em', 'im', '\\xE2m', 'se'):\n                            word = word[:-len(suffix)]\n                            rv = rv[:-len(suffix)]\n                        else:\n                            if (not rv.startswith(suffix) and\n                                rv[rv.index(suffix)-1] not in\n                                \"aeio\\u0103\\xE2\\xEE\"):\n                                word = word[:-len(suffix)]\n                        break\n\n        for suffix in (\"ie\", \"a\", \"e\", \"i\", \"\\u0103\"):\n            if word.endswith(suffix):\n                if suffix in rv:\n                    word = word[:-len(suffix)]\n                break\n\n        word = word.replace(\"I\", \"i\").replace(\"U\", \"u\")\n\n\n        return word\n\n\n\nclass RussianStemmer(_LanguageSpecificStemmer):\n\n\n    __perfective_gerund_suffixes = (\"ivshis'\", \"yvshis'\", \"vshis'\",\n                                      \"ivshi\", \"yvshi\", \"vshi\", \"iv\",\n                                      \"yv\", \"v\")\n    __adjectival_suffixes = ('ui^ushchi^ui^u', 'ui^ushchi^ai^a',\n                               'ui^ushchimi', 'ui^ushchymi', 'ui^ushchego',\n                               'ui^ushchogo', 'ui^ushchemu', 'ui^ushchomu',\n                               'ui^ushchikh', 'ui^ushchykh',\n                               'ui^ushchui^u', 'ui^ushchaia',\n                               'ui^ushchoi^u', 'ui^ushchei^u',\n                               'i^ushchi^ui^u', 'i^ushchi^ai^a',\n                               'ui^ushchee', 'ui^ushchie',\n                               'ui^ushchye', 'ui^ushchoe', 'ui^ushchei`',\n                               'ui^ushchii`', 'ui^ushchyi`',\n                               'ui^ushchoi`', 'ui^ushchem', 'ui^ushchim',\n                               'ui^ushchym', 'ui^ushchom', 'i^ushchimi',\n                               'i^ushchymi', 'i^ushchego', 'i^ushchogo',\n                               'i^ushchemu', 'i^ushchomu', 'i^ushchikh',\n                               'i^ushchykh', 'i^ushchui^u', 'i^ushchai^a',\n                               'i^ushchoi^u', 'i^ushchei^u', 'i^ushchee',\n                               'i^ushchie', 'i^ushchye', 'i^ushchoe',\n                               'i^ushchei`', 'i^ushchii`',\n                               'i^ushchyi`', 'i^ushchoi`', 'i^ushchem',\n                               'i^ushchim', 'i^ushchym', 'i^ushchom',\n                               'shchi^ui^u', 'shchi^ai^a', 'ivshi^ui^u',\n                               'ivshi^ai^a', 'yvshi^ui^u', 'yvshi^ai^a',\n                               'shchimi', 'shchymi', 'shchego', 'shchogo',\n                               'shchemu', 'shchomu', 'shchikh', 'shchykh',\n                               'shchui^u', 'shchai^a', 'shchoi^u',\n                               'shchei^u', 'ivshimi', 'ivshymi',\n                               'ivshego', 'ivshogo', 'ivshemu', 'ivshomu',\n                               'ivshikh', 'ivshykh', 'ivshui^u',\n                               'ivshai^a', 'ivshoi^u', 'ivshei^u',\n                               'yvshimi', 'yvshymi', 'yvshego', 'yvshogo',\n                               'yvshemu', 'yvshomu', 'yvshikh', 'yvshykh',\n                               'yvshui^u', 'yvshai^a', 'yvshoi^u',\n                               'yvshei^u', 'vshi^ui^u', 'vshi^ai^a',\n                               'shchee', 'shchie', 'shchye', 'shchoe',\n                               'shchei`', 'shchii`', 'shchyi`', 'shchoi`',\n                               'shchem', 'shchim', 'shchym', 'shchom',\n                               'ivshee', 'ivshie', 'ivshye', 'ivshoe',\n                               'ivshei`', 'ivshii`', 'ivshyi`',\n                               'ivshoi`', 'ivshem', 'ivshim', 'ivshym',\n                               'ivshom', 'yvshee', 'yvshie', 'yvshye',\n                               'yvshoe', 'yvshei`', 'yvshii`',\n                               'yvshyi`', 'yvshoi`', 'yvshem',\n                               'yvshim', 'yvshym', 'yvshom', 'vshimi',\n                               'vshymi', 'vshego', 'vshogo', 'vshemu',\n                               'vshomu', 'vshikh', 'vshykh', 'vshui^u',\n                               'vshai^a', 'vshoi^u', 'vshei^u',\n                               'emi^ui^u', 'emi^ai^a', 'nni^ui^u',\n                               'nni^ai^a', 'vshee',\n                               'vshie', 'vshye', 'vshoe', 'vshei`',\n                               'vshii`', 'vshyi`', 'vshoi`',\n                               'vshem', 'vshim', 'vshym', 'vshom',\n                               'emimi', 'emymi', 'emego', 'emogo',\n                               'ememu', 'emomu', 'emikh', 'emykh',\n                               'emui^u', 'emai^a', 'emoi^u', 'emei^u',\n                               'nnimi', 'nnymi', 'nnego', 'nnogo',\n                               'nnemu', 'nnomu', 'nnikh', 'nnykh',\n                               'nnui^u', 'nnai^a', 'nnoi^u', 'nnei^u',\n                               'emee', 'emie', 'emye', 'emoe',\n                               'emei`', 'emii`', 'emyi`',\n                               'emoi`', 'emem', 'emim', 'emym',\n                               'emom', 'nnee', 'nnie', 'nnye', 'nnoe',\n                               'nnei`', 'nnii`', 'nnyi`',\n                               'nnoi`', 'nnem', 'nnim', 'nnym',\n                               'nnom', 'i^ui^u', 'i^ai^a', 'imi', 'ymi',\n                               'ego', 'ogo', 'emu', 'omu', 'ikh',\n                               'ykh', 'ui^u', 'ai^a', 'oi^u', 'ei^u',\n                               'ee', 'ie', 'ye', 'oe', 'ei`',\n                               'ii`', 'yi`', 'oi`', 'em',\n                               'im', 'ym', 'om')\n    __reflexive_suffixes = (\"si^a\", \"s'\")\n    __verb_suffixes = (\"esh'\", 'ei`te', 'ui`te', 'ui^ut',\n                         \"ish'\", 'ete', 'i`te', 'i^ut', 'nno',\n                         'ila', 'yla', 'ena', 'ite', 'ili', 'yli',\n                         'ilo', 'ylo', 'eno', 'i^at', 'uet', 'eny',\n                         \"it'\", \"yt'\", 'ui^u', 'la', 'na', 'li',\n                         'em', 'lo', 'no', 'et', 'ny', \"t'\",\n                         'ei`', 'ui`', 'il', 'yl', 'im',\n                         'ym', 'en', 'it', 'yt', 'i^u', 'i`',\n                         'l', 'n')\n    __noun_suffixes = ('ii^ami', 'ii^akh', 'i^ami', 'ii^am', 'i^akh',\n                         'ami', 'iei`', 'i^am', 'iem', 'akh',\n                         'ii^u', \"'i^u\", 'ii^a', \"'i^a\", 'ev', 'ov',\n                         'ie', \"'e\", 'ei', 'ii', 'ei`',\n                         'oi`', 'ii`', 'em', 'am', 'om',\n                         'i^u', 'i^a', 'a', 'e', 'i', 'i`',\n                         'o', 'u', 'y', \"'\")\n    __superlative_suffixes = (\"ei`she\", \"ei`sh\")\n    __derivational_suffixes = (\"ost'\", \"ost\")\n\n    def stem(self, word):\n        if word in self.stopwords:\n            return word\n\n        chr_exceeded = False\n        for i in range(len(word)):\n            if ord(word[i]) > 255:\n                chr_exceeded = True\n                break\n\n        if chr_exceeded:\n            word = self.__cyrillic_to_roman(word)\n\n        step1_success = False\n        adjectival_removed = False\n        verb_removed = False\n        undouble_success = False\n        superlative_removed = False\n\n        rv, r2 = self.__regions_russian(word)\n\n        for suffix in self.__perfective_gerund_suffixes:\n            if rv.endswith(suffix):\n                if suffix in (\"v\", \"vshi\", \"vshis'\"):\n                    if (rv[-len(suffix)-3:-len(suffix)] == \"i^a\" or\n                        rv[-len(suffix)-1:-len(suffix)] == \"a\"):\n                        word = word[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n                        step1_success = True\n                        break\n                else:\n                    word = word[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n                    step1_success = True\n                    break\n\n        if not step1_success:\n            for suffix in self.__reflexive_suffixes:\n                if rv.endswith(suffix):\n                    word = word[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n                    break\n\n            for suffix in self.__adjectival_suffixes:\n                if rv.endswith(suffix):\n                    if suffix in ('i^ushchi^ui^u', 'i^ushchi^ai^a',\n                              'i^ushchui^u', 'i^ushchai^a', 'i^ushchoi^u',\n                              'i^ushchei^u', 'i^ushchimi', 'i^ushchymi',\n                              'i^ushchego', 'i^ushchogo', 'i^ushchemu',\n                              'i^ushchomu', 'i^ushchikh', 'i^ushchykh',\n                              'shchi^ui^u', 'shchi^ai^a', 'i^ushchee',\n                              'i^ushchie', 'i^ushchye', 'i^ushchoe',\n                              'i^ushchei`', 'i^ushchii`', 'i^ushchyi`',\n                              'i^ushchoi`', 'i^ushchem', 'i^ushchim',\n                              'i^ushchym', 'i^ushchom', 'vshi^ui^u',\n                              'vshi^ai^a', 'shchui^u', 'shchai^a',\n                              'shchoi^u', 'shchei^u', 'emi^ui^u',\n                              'emi^ai^a', 'nni^ui^u', 'nni^ai^a',\n                              'shchimi', 'shchymi', 'shchego', 'shchogo',\n                              'shchemu', 'shchomu', 'shchikh', 'shchykh',\n                              'vshui^u', 'vshai^a', 'vshoi^u', 'vshei^u',\n                              'shchee', 'shchie', 'shchye', 'shchoe',\n                              'shchei`', 'shchii`', 'shchyi`', 'shchoi`',\n                              'shchem', 'shchim', 'shchym', 'shchom',\n                              'vshimi', 'vshymi', 'vshego', 'vshogo',\n                              'vshemu', 'vshomu', 'vshikh', 'vshykh',\n                              'emui^u', 'emai^a', 'emoi^u', 'emei^u',\n                              'nnui^u', 'nnai^a', 'nnoi^u', 'nnei^u',\n                              'vshee', 'vshie', 'vshye', 'vshoe',\n                              'vshei`', 'vshii`', 'vshyi`', 'vshoi`',\n                              'vshem', 'vshim', 'vshym', 'vshom',\n                              'emimi', 'emymi', 'emego', 'emogo',\n                              'ememu', 'emomu', 'emikh', 'emykh',\n                              'nnimi', 'nnymi', 'nnego', 'nnogo',\n                              'nnemu', 'nnomu', 'nnikh', 'nnykh',\n                              'emee', 'emie', 'emye', 'emoe', 'emei`',\n                              'emii`', 'emyi`', 'emoi`', 'emem', 'emim',\n                              'emym', 'emom', 'nnee', 'nnie', 'nnye',\n                              'nnoe', 'nnei`', 'nnii`', 'nnyi`', 'nnoi`',\n                              'nnem', 'nnim', 'nnym', 'nnom'):\n                        if (rv[-len(suffix)-3:-len(suffix)] == \"i^a\" or\n                            rv[-len(suffix)-1:-len(suffix)] == \"a\"):\n                            word = word[:-len(suffix)]\n                            r2 = r2[:-len(suffix)]\n                            rv = rv[:-len(suffix)]\n                            adjectival_removed = True\n                            break\n                    else:\n                        word = word[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n                        adjectival_removed = True\n                        break\n\n            if not adjectival_removed:\n                for suffix in self.__verb_suffixes:\n                    if rv.endswith(suffix):\n                        if suffix in (\"la\", \"na\", \"ete\", \"i`te\", \"li\",\n                                      \"i`\", \"l\", \"em\", \"n\", \"lo\", \"no\",\n                                      \"et\", \"i^ut\", \"ny\", \"t'\", \"esh'\",\n                                      \"nno\"):\n                            if (rv[-len(suffix)-3:-len(suffix)] == \"i^a\" or\n                                rv[-len(suffix)-1:-len(suffix)] == \"a\"):\n                                word = word[:-len(suffix)]\n                                r2 = r2[:-len(suffix)]\n                                rv = rv[:-len(suffix)]\n                                verb_removed = True\n                                break\n                        else:\n                            word = word[:-len(suffix)]\n                            r2 = r2[:-len(suffix)]\n                            rv = rv[:-len(suffix)]\n                            verb_removed = True\n                            break\n\n            if not adjectival_removed and not verb_removed:\n                for suffix in self.__noun_suffixes:\n                    if rv.endswith(suffix):\n                        word = word[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n                        break\n\n        if rv.endswith(\"i\"):\n            word = word[:-1]\n            r2 = r2[:-1]\n\n        for suffix in self.__derivational_suffixes:\n            if r2.endswith(suffix):\n                word = word[:-len(suffix)]\n                break\n\n        if word.endswith(\"nn\"):\n            word = word[:-1]\n            undouble_success = True\n\n        if not undouble_success:\n            for suffix in self.__superlative_suffixes:\n                if word.endswith(suffix):\n                    word = word[:-len(suffix)]\n                    superlative_removed = True\n                    break\n            if word.endswith(\"nn\"):\n                word = word[:-1]\n\n        if not undouble_success and not superlative_removed:\n            if word.endswith(\"'\"):\n                word = word[:-1]\n\n        if chr_exceeded:\n            word = self.__roman_to_cyrillic(word)\n\n\n        return word\n\n\n\n    def __regions_russian(self, word):\n        r1 = \"\"\n        r2 = \"\"\n        rv = \"\"\n\n        vowels = (\"A\", \"U\", \"E\", \"a\", \"e\", \"i\", \"o\", \"u\", \"y\")\n        word = (word.replace(\"i^a\", \"A\")\n                    .replace(\"i^u\", \"U\")\n                    .replace(\"e`\", \"E\"))\n\n        for i in range(1, len(word)):\n            if word[i] not in vowels and word[i-1] in vowels:\n                r1 = word[i+1:]\n                break\n\n        for i in range(1, len(r1)):\n            if r1[i] not in vowels and r1[i-1] in vowels:\n                r2 = r1[i+1:]\n                break\n\n        for i in range(len(word)):\n            if word[i] in vowels:\n                rv = word[i+1:]\n                break\n\n        r2 = (r2.replace(\"A\", \"i^a\")\n                .replace(\"U\", \"i^u\")\n                .replace(\"E\", \"e`\"))\n        rv = (rv.replace(\"A\", \"i^a\")\n              .replace(\"U\", \"i^u\")\n              .replace(\"E\", \"e`\"))\n\n\n        return (rv, r2)\n\n\n\n    def __cyrillic_to_roman(self, word):\n        word = (word.replace(\"\\u0410\", \"a\").replace(\"\\u0430\", \"a\")\n                    .replace(\"\\u0411\", \"b\").replace(\"\\u0431\", \"b\")\n                    .replace(\"\\u0412\", \"v\").replace(\"\\u0432\", \"v\")\n                    .replace(\"\\u0413\", \"g\").replace(\"\\u0433\", \"g\")\n                    .replace(\"\\u0414\", \"d\").replace(\"\\u0434\", \"d\")\n                    .replace(\"\\u0415\", \"e\").replace(\"\\u0435\", \"e\")\n                    .replace(\"\\u0401\", \"e\").replace(\"\\u0451\", \"e\")\n                    .replace(\"\\u0416\", \"zh\").replace(\"\\u0436\", \"zh\")\n                    .replace(\"\\u0417\", \"z\").replace(\"\\u0437\", \"z\")\n                    .replace(\"\\u0418\", \"i\").replace(\"\\u0438\", \"i\")\n                    .replace(\"\\u0419\", \"i`\").replace(\"\\u0439\", \"i`\")\n                    .replace(\"\\u041A\", \"k\").replace(\"\\u043A\", \"k\")\n                    .replace(\"\\u041B\", \"l\").replace(\"\\u043B\", \"l\")\n                    .replace(\"\\u041C\", \"m\").replace(\"\\u043C\", \"m\")\n                    .replace(\"\\u041D\", \"n\").replace(\"\\u043D\", \"n\")\n                    .replace(\"\\u041E\", \"o\").replace(\"\\u043E\", \"o\")\n                    .replace(\"\\u041F\", \"p\").replace(\"\\u043F\", \"p\")\n                    .replace(\"\\u0420\", \"r\").replace(\"\\u0440\", \"r\")\n                    .replace(\"\\u0421\", \"s\").replace(\"\\u0441\", \"s\")\n                    .replace(\"\\u0422\", \"t\").replace(\"\\u0442\", \"t\")\n                    .replace(\"\\u0423\", \"u\").replace(\"\\u0443\", \"u\")\n                    .replace(\"\\u0424\", \"f\").replace(\"\\u0444\", \"f\")\n                    .replace(\"\\u0425\", \"kh\").replace(\"\\u0445\", \"kh\")\n                    .replace(\"\\u0426\", \"t^s\").replace(\"\\u0446\", \"t^s\")\n                    .replace(\"\\u0427\", \"ch\").replace(\"\\u0447\", \"ch\")\n                    .replace(\"\\u0428\", \"sh\").replace(\"\\u0448\", \"sh\")\n                    .replace(\"\\u0429\", \"shch\").replace(\"\\u0449\", \"shch\")\n                    .replace(\"\\u042A\", \"''\").replace(\"\\u044A\", \"''\")\n                    .replace(\"\\u042B\", \"y\").replace(\"\\u044B\", \"y\")\n                    .replace(\"\\u042C\", \"'\").replace(\"\\u044C\", \"'\")\n                    .replace(\"\\u042D\", \"e`\").replace(\"\\u044D\", \"e`\")\n                    .replace(\"\\u042E\", \"i^u\").replace(\"\\u044E\", \"i^u\")\n                    .replace(\"\\u042F\", \"i^a\").replace(\"\\u044F\", \"i^a\"))\n\n\n        return word\n\n\n\n    def __roman_to_cyrillic(self, word):\n        word = (word.replace(\"i^u\", \"\\u044E\").replace(\"i^a\", \"\\u044F\")\n                    .replace(\"shch\", \"\\u0449\").replace(\"kh\", \"\\u0445\")\n                    .replace(\"t^s\", \"\\u0446\").replace(\"ch\", \"\\u0447\")\n                    .replace(\"e`\", \"\\u044D\").replace(\"i`\", \"\\u0439\")\n                    .replace(\"sh\", \"\\u0448\").replace(\"k\", \"\\u043A\")\n                    .replace(\"e\", \"\\u0435\").replace(\"zh\", \"\\u0436\")\n                    .replace(\"a\", \"\\u0430\").replace(\"b\", \"\\u0431\")\n                    .replace(\"v\", \"\\u0432\").replace(\"g\", \"\\u0433\")\n                    .replace(\"d\", \"\\u0434\").replace(\"e\", \"\\u0435\")\n                    .replace(\"z\", \"\\u0437\").replace(\"i\", \"\\u0438\")\n                    .replace(\"l\", \"\\u043B\").replace(\"m\", \"\\u043C\")\n                    .replace(\"n\", \"\\u043D\").replace(\"o\", \"\\u043E\")\n                    .replace(\"p\", \"\\u043F\").replace(\"r\", \"\\u0440\")\n                    .replace(\"s\", \"\\u0441\").replace(\"t\", \"\\u0442\")\n                    .replace(\"u\", \"\\u0443\").replace(\"f\", \"\\u0444\")\n                    .replace(\"''\", \"\\u044A\").replace(\"y\", \"\\u044B\")\n                    .replace(\"'\", \"\\u044C\"))\n\n\n        return word\n\n\nclass SpanishStemmer(_StandardStemmer):\n\n\n    __vowels = \"aeiou\\xE1\\xE9\\xED\\xF3\\xFA\\xFC\"\n    __step0_suffixes = (\"selas\", \"selos\", \"sela\", \"selo\", \"las\",\n                        \"les\", \"los\", \"nos\", \"me\", \"se\", \"la\", \"le\",\n                        \"lo\")\n    __step1_suffixes = ('amientos', 'imientos', 'amiento', 'imiento',\n                        'aciones', 'uciones', 'adoras', 'adores',\n                        'ancias', 'log\\xEDas', 'encias', 'amente',\n                        'idades', 'anzas', 'ismos', 'ables', 'ibles',\n                        'istas', 'adora', 'aci\\xF3n', 'antes',\n                        'ancia', 'log\\xEDa', 'uci\\xf3n', 'encia',\n                        'mente', 'anza', 'icos', 'icas', 'ismo',\n                        'able', 'ible', 'ista', 'osos', 'osas',\n                        'ador', 'ante', 'idad', 'ivas', 'ivos',\n                        'ico',\n                        'ica', 'oso', 'osa', 'iva', 'ivo')\n    __step2a_suffixes = ('yeron', 'yendo', 'yamos', 'yais', 'yan',\n                         'yen', 'yas', 'yes', 'ya', 'ye', 'yo',\n                         'y\\xF3')\n    __step2b_suffixes = ('ar\\xEDamos', 'er\\xEDamos', 'ir\\xEDamos',\n                         'i\\xE9ramos', 'i\\xE9semos', 'ar\\xEDais',\n                         'aremos', 'er\\xEDais', 'eremos',\n                         'ir\\xEDais', 'iremos', 'ierais', 'ieseis',\n                         'asteis', 'isteis', '\\xE1bamos',\n                         '\\xE1ramos', '\\xE1semos', 'ar\\xEDan',\n                         'ar\\xEDas', 'ar\\xE9is', 'er\\xEDan',\n                         'er\\xEDas', 'er\\xE9is', 'ir\\xEDan',\n                         'ir\\xEDas', 'ir\\xE9is',\n                         'ieran', 'iesen', 'ieron', 'iendo', 'ieras',\n                         'ieses', 'abais', 'arais', 'aseis',\n                         '\\xE9amos', 'ar\\xE1n', 'ar\\xE1s',\n                         'ar\\xEDa', 'er\\xE1n', 'er\\xE1s',\n                         'er\\xEDa', 'ir\\xE1n', 'ir\\xE1s',\n                         'ir\\xEDa', 'iera', 'iese', 'aste', 'iste',\n                         'aban', 'aran', 'asen', 'aron', 'ando',\n                         'abas', 'adas', 'idas', 'aras', 'ases',\n                         '\\xEDais', 'ados', 'idos', 'amos', 'imos',\n                         'emos', 'ar\\xE1', 'ar\\xE9', 'er\\xE1',\n                         'er\\xE9', 'ir\\xE1', 'ir\\xE9', 'aba',\n                         'ada', 'ida', 'ara', 'ase', '\\xEDan',\n                         'ado', 'ido', '\\xEDas', '\\xE1is',\n                         '\\xE9is', '\\xEDa', 'ad', 'ed', 'id',\n                         'an', 'i\\xF3', 'ar', 'er', 'ir', 'as',\n                         '\\xEDs', 'en', 'es')\n    __step3_suffixes = (\"os\", \"a\", \"e\", \"o\", \"\\xE1\",\n                        \"\\xE9\", \"\\xED\", \"\\xF3\")\n\n    def stem(self, word):\n        word = word.lower()\n\n        if word in self.stopwords:\n            return word\n\n        step1_success = False\n\n        r1, r2 = self._r1r2_standard(word, self.__vowels)\n        rv = self._rv_standard(word, self.__vowels)\n\n        for suffix in self.__step0_suffixes:\n            if not (word.endswith(suffix) and rv.endswith(suffix)):\n                continue\n\n            if ((rv[:-len(suffix)].endswith((\"ando\", \"\\xE1ndo\",\n                                             \"ar\", \"\\xE1r\",\n                                             \"er\", \"\\xE9r\",\n                                             \"iendo\", \"i\\xE9ndo\",\n                                             \"ir\", \"\\xEDr\"))) or\n                (rv[:-len(suffix)].endswith(\"yendo\") and\n                    word[:-len(suffix)].endswith(\"uyendo\"))):\n\n                word = self.__replace_accented(word[:-len(suffix)])\n                r1 = self.__replace_accented(r1[:-len(suffix)])\n                r2 = self.__replace_accented(r2[:-len(suffix)])\n                rv = self.__replace_accented(rv[:-len(suffix)])\n            break\n\n        for suffix in self.__step1_suffixes:\n            if not word.endswith(suffix):\n                continue\n\n            if suffix == \"amente\" and r1.endswith(suffix):\n                step1_success = True\n                word = word[:-6]\n                r2 = r2[:-6]\n                rv = rv[:-6]\n\n                if r2.endswith(\"iv\"):\n                    word = word[:-2]\n                    r2 = r2[:-2]\n                    rv = rv[:-2]\n\n                    if r2.endswith(\"at\"):\n                        word = word[:-2]\n                        rv = rv[:-2]\n\n                elif r2.endswith((\"os\", \"ic\", \"ad\")):\n                    word = word[:-2]\n                    rv = rv[:-2]\n\n            elif r2.endswith(suffix):\n                step1_success = True\n                if suffix in (\"adora\", \"ador\", \"aci\\xF3n\", \"adoras\",\n                              \"adores\", \"aciones\", \"ante\", \"antes\",\n                              \"ancia\", \"ancias\"):\n                    word = word[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n\n                    if r2.endswith(\"ic\"):\n                        word = word[:-2]\n                        rv = rv[:-2]\n\n                elif suffix in (\"log\\xEDa\", \"log\\xEDas\"):\n                    word = suffix_replace(word, suffix, \"log\")\n                    rv = suffix_replace(rv, suffix, \"log\")\n\n                elif suffix in (\"uci\\xF3n\", \"uciones\"):\n                    word = suffix_replace(word, suffix, \"u\")\n                    rv = suffix_replace(rv, suffix, \"u\")\n\n                elif suffix in (\"encia\", \"encias\"):\n                    word = suffix_replace(word, suffix, \"ente\")\n                    rv = suffix_replace(rv, suffix, \"ente\")\n\n                elif suffix == \"mente\":\n                    word = word[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n\n                    if r2.endswith((\"ante\", \"able\", \"ible\")):\n                        word = word[:-4]\n                        rv = rv[:-4]\n\n                elif suffix in (\"idad\", \"idades\"):\n                    word = word[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n\n                    for pre_suff in (\"abil\", \"ic\", \"iv\"):\n                        if r2.endswith(pre_suff):\n                            word = word[:-len(pre_suff)]\n                            rv = rv[:-len(pre_suff)]\n\n                elif suffix in (\"ivo\", \"iva\", \"ivos\", \"ivas\"):\n                    word = word[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n                    if r2.endswith(\"at\"):\n                        word = word[:-2]\n                        rv = rv[:-2]\n                else:\n                    word = word[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n            break\n\n        if not step1_success:\n            for suffix in self.__step2a_suffixes:\n                if (rv.endswith(suffix) and\n                        word[-len(suffix)-1:-len(suffix)] == \"u\"):\n                    word = word[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n                    break\n\n            for suffix in self.__step2b_suffixes:\n                if rv.endswith(suffix):\n                    word = word[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n                    if suffix in (\"en\", \"es\", \"\\xE9is\", \"emos\"):\n                        if word.endswith(\"gu\"):\n                            word = word[:-1]\n\n                        if rv.endswith(\"gu\"):\n                            rv = rv[:-1]\n                    break\n\n        for suffix in self.__step3_suffixes:\n            if rv.endswith(suffix):\n                word = word[:-len(suffix)]\n                if suffix in (\"e\", \"\\xE9\"):\n                    rv = rv[:-len(suffix)]\n\n                    if word[-2:] == \"gu\" and rv.endswith(\"u\"):\n                        word = word[:-1]\n                break\n\n        word = self.__replace_accented(word)\n\n        return word\n\n    def __replace_accented(self, word):\n        return (word.replace(\"\\xE1\", \"a\")\n                .replace(\"\\xE9\", \"e\")\n                .replace(\"\\xED\", \"i\")\n                .replace(\"\\xF3\", \"o\")\n                .replace(\"\\xFA\", \"u\"))\n\n\nclass SwedishStemmer(_ScandinavianStemmer):\n\n\n    __vowels = \"aeiouy\\xE4\\xE5\\xF6\"\n    __s_ending = \"bcdfghjklmnoprtvy\"\n    __step1_suffixes = (\"heterna\", \"hetens\", \"heter\", \"heten\",\n                        \"anden\", \"arnas\", \"ernas\", \"ornas\", \"andes\",\n                        \"andet\", \"arens\", \"arna\", \"erna\", \"orna\",\n                        \"ande\", \"arne\", \"aste\", \"aren\", \"ades\",\n                        \"erns\", \"ade\", \"are\", \"ern\", \"ens\", \"het\",\n                        \"ast\", \"ad\", \"en\", \"ar\", \"er\", \"or\", \"as\",\n                        \"es\", \"at\", \"a\", \"e\", \"s\")\n    __step2_suffixes = (\"dd\", \"gd\", \"nn\", \"dt\", \"gt\", \"kt\", \"tt\")\n    __step3_suffixes = (\"fullt\", \"l\\xF6st\", \"els\", \"lig\", \"ig\")\n\n    def stem(self, word):\n        word = word.lower()\n\n        if word in self.stopwords:\n            return word\n\n        r1 = self._r1_scandinavian(word, self.__vowels)\n\n        for suffix in self.__step1_suffixes:\n            if r1.endswith(suffix):\n                if suffix == \"s\":\n                    if word[-2] in self.__s_ending:\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                else:\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                break\n\n        for suffix in self.__step2_suffixes:\n            if r1.endswith(suffix):\n                word = word[:-1]\n                r1 = r1[:-1]\n                break\n\n        for suffix in self.__step3_suffixes:\n            if r1.endswith(suffix):\n                if suffix in (\"els\", \"lig\", \"ig\"):\n                    word = word[:-len(suffix)]\n                elif suffix in (\"fullt\", \"l\\xF6st\"):\n                    word = word[:-1]\n                break\n\n        return word\n\n\ndef demo():\n\n    import re\n    from nltk.corpus import udhr\n\n    udhr_corpus = {\"arabic\":     \"Arabic_Alarabia-Arabic\",\n                   \"danish\":     \"Danish_Dansk-Latin1\",\n                   \"dutch\":      \"Dutch_Nederlands-Latin1\",\n                   \"english\":    \"English-Latin1\",\n                   \"finnish\":    \"Finnish_Suomi-Latin1\",\n                   \"french\":     \"French_Francais-Latin1\",\n                   \"german\":     \"German_Deutsch-Latin1\",\n                   \"hungarian\":  \"Hungarian_Magyar-UTF8\",\n                   \"italian\":    \"Italian_Italiano-Latin1\",\n                   \"norwegian\":  \"Norwegian-Latin1\",\n                   \"porter\":     \"English-Latin1\",\n                   \"portuguese\": \"Portuguese_Portugues-Latin1\",\n                   \"romanian\":   \"Romanian_Romana-Latin2\",\n                   \"russian\":    \"Russian-UTF8\",\n                   \"spanish\":    \"Spanish-Latin1\",\n                   \"swedish\":    \"Swedish_Svenska-Latin1\",\n                   }\n\n    print(\"\\n\")\n    print(\"******************************\")\n    print(\"Demo for the Snowball stemmers\")\n    print(\"******************************\")\n\n    while True:\n\n        language = input(\"Please enter the name of the language \" +\n                             \"to be demonstrated\\n\" +\n                             \"/\".join(SnowballStemmer.languages) +\n                             \"\\n\" +\n                             \"(enter 'exit' in order to leave): \")\n\n        if language == \"exit\":\n            break\n\n        if language not in SnowballStemmer.languages:\n            print((\"\\nOops, there is no stemmer for this language. \" +\n                   \"Please try again.\\n\"))\n            continue\n\n        stemmer = SnowballStemmer(language)\n        excerpt = udhr.words(udhr_corpus[language]) [:300]\n\n        stemmed = \" \".join(stemmer.stem(word) for word in excerpt)\n        stemmed = re.sub(r\"(.{,70})\\s\", r'\\1\\n', stemmed+' ').rstrip()\n        excerpt = \" \".join(excerpt)\n        excerpt = re.sub(r\"(.{,70})\\s\", r'\\1\\n', excerpt+' ').rstrip()\n\n        print(\"\\n\")\n        print('-' * 70)\n        print('ORIGINAL'.center(70))\n        print(excerpt)\n        print(\"\\n\\n\")\n        print('STEMMED RESULTS'.center(70))\n        print(stemmed)\n        print('-' * 70)\n        print(\"\\n\")\n"], "nltk\\stem\\util": [".py", "\ndef suffix_replace(original, old, new):\n    return original[:-len(old)] + new\n\ndef prefix_replace(original, old, new):\n    return new + original[len(old):]"], "nltk\\stem\\wordnet": [".py", "from __future__ import unicode_literals\n\nfrom nltk.corpus.reader.wordnet import NOUN\nfrom nltk.corpus import wordnet\nfrom nltk.compat import python_2_unicode_compatible\n\n@python_2_unicode_compatible\nclass WordNetLemmatizer(object):\n\n    def __init__(self):\n        pass\n\n    def lemmatize(self, word, pos=NOUN):\n        lemmas = wordnet._morphy(word, pos)\n        return min(lemmas, key=len) if lemmas else word\n\n    def __repr__(self):\n        return '<WordNetLemmatizer>'\n\n\ndef teardown_module(module=None):\n    from nltk.corpus import wordnet\n    wordnet._unload()\n\n"], "nltk\\stem\\__init__": [".py", "\n\nfrom nltk.stem.api import StemmerI\nfrom nltk.stem.regexp import RegexpStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom nltk.stem.isri import ISRIStemmer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem.rslp import RSLPStemmer\n", 1], "nltk\\tag\\api": [".py", "\nfrom abc import ABCMeta, abstractmethod\nfrom six import add_metaclass\nfrom itertools import chain\n\nfrom nltk.internals import overridden\nfrom nltk.metrics import accuracy\n\nfrom nltk.tag.util import untag\n\n\n@add_metaclass(ABCMeta)\nclass TaggerI(object):\n    @abstractmethod\n    def tag(self, tokens):\n        if overridden(self.tag_sents):\n            return self.tag_sents([tokens])[0]\n\n    def tag_sents(self, sentences):\n        return [self.tag(sent) for sent in sentences]\n\n    def evaluate(self, gold):\n\n        tagged_sents = self.tag_sents(untag(sent) for sent in gold)\n        gold_tokens = list(chain(*gold))\n        test_tokens = list(chain(*tagged_sents))\n        return accuracy(gold_tokens, test_tokens)\n\n    def _check_params(self, train, model):\n        if (train and model) or (not train and not model):\n            raise ValueError(\n                    'Must specify either training data or trained model.')\n\n\nclass FeaturesetTaggerI(TaggerI):\n"], "nltk\\tag\\brill": [".py", "\nfrom __future__ import print_function, division\n\nfrom collections import defaultdict, Counter\n\nfrom nltk.tag import TaggerI\nfrom nltk.tbl import Feature, Template\nfrom nltk import jsontags\n\n\n\n@jsontags.register_tag\nclass Word(Feature):\n\n    json_tag = 'nltk.tag.brill.Word'\n\n    @staticmethod\n    def extract_property(tokens, index):\n        return tokens[index][0]\n\n\n@jsontags.register_tag\nclass Pos(Feature):\n\n    json_tag = 'nltk.tag.brill.Pos'\n\n    @staticmethod\n    def extract_property(tokens, index):\n        return tokens[index][1]\n\n\ndef nltkdemo18():\n    return [\n        Template(Pos([-1])),\n        Template(Pos([1])),\n        Template(Pos([-2])),\n        Template(Pos([2])),\n        Template(Pos([-2, -1])),\n        Template(Pos([1, 2])),\n        Template(Pos([-3, -2, -1])),\n        Template(Pos([1, 2, 3])),\n        Template(Pos([-1]), Pos([1])),\n        Template(Word([-1])),\n        Template(Word([1])),\n        Template(Word([-2])),\n        Template(Word([2])),\n        Template(Word([-2, -1])),\n        Template(Word([1, 2])),\n        Template(Word([-3, -2, -1])),\n        Template(Word([1, 2, 3])),\n        Template(Word([-1]), Word([1])),\n    ]\n\n\ndef nltkdemo18plus():\n    return nltkdemo18() + [\n        Template(Word([-1]), Pos([1])),\n        Template(Pos([-1]), Word([1])),\n        Template(Word([-1]), Word([0]), Pos([1])),\n        Template(Pos([-1]), Word([0]), Word([1])),\n        Template(Pos([-1]), Word([0]), Pos([1])),\n    ]\n\n\ndef fntbl37():\n    return [\n        Template(Word([0]), Word([1]), Word([2])),\n        Template(Word([-1]), Word([0]), Word([1])),\n        Template(Word([0]), Word([-1])),\n        Template(Word([0]), Word([1])),\n        Template(Word([0]), Word([2])),\n        Template(Word([0]), Word([-2])),\n        Template(Word([1, 2])),\n        Template(Word([-2, -1])),\n        Template(Word([1, 2, 3])),\n        Template(Word([-3, -2, -1])),\n        Template(Word([0]), Pos([2])),\n        Template(Word([0]), Pos([-2])),\n        Template(Word([0]), Pos([1])),\n        Template(Word([0]), Pos([-1])),\n        Template(Word([0])),\n        Template(Word([-2])),\n        Template(Word([2])),\n        Template(Word([1])),\n        Template(Word([-1])),\n        Template(Pos([-1]), Pos([1])),\n        Template(Pos([1]), Pos([2])),\n        Template(Pos([-1]), Pos([-2])),\n        Template(Pos([1])),\n        Template(Pos([-1])),\n        Template(Pos([-2])),\n        Template(Pos([2])),\n        Template(Pos([1, 2, 3])),\n        Template(Pos([1, 2])),\n        Template(Pos([-3, -2, -1])),\n        Template(Pos([-2, -1])),\n        Template(Pos([1]), Word([0]), Word([1])),\n        Template(Pos([1]), Word([0]), Word([-1])),\n        Template(Pos([-1]), Word([-1]), Word([0])),\n        Template(Pos([-1]), Word([0]), Word([1])),\n        Template(Pos([-2]), Pos([-1])),\n        Template(Pos([1]), Pos([2])),\n        Template(Pos([1]), Pos([2]), Word([1]))\n    ]\n\n\ndef brill24():\n    return [\n        Template(Pos([-1])),\n        Template(Pos([1])),\n        Template(Pos([-2])),\n        Template(Pos([2])),\n        Template(Pos([-2, -1])),\n        Template(Pos([1, 2])),\n        Template(Pos([-3, -2, -1])),\n        Template(Pos([1, 2, 3])),\n        Template(Pos([-1]), Pos([1])),\n        Template(Pos([-2]), Pos([-1])),\n        Template(Pos([1]), Pos([2])),\n        Template(Word([-1])),\n        Template(Word([1])),\n        Template(Word([-2])),\n        Template(Word([2])),\n        Template(Word([-2, -1])),\n        Template(Word([1, 2])),\n        Template(Word([-1, 0])),\n        Template(Word([0, 1])),\n        Template(Word([0])),\n        Template(Word([-1]), Pos([-1])),\n        Template(Word([1]), Pos([1])),\n        Template(Word([0]), Word([-1]), Pos([-1])),\n        Template(Word([0]), Word([1]), Pos([1])),\n    ]\n\n\ndef describe_template_sets():\n    import inspect\n    import sys\n\n    templatesets = inspect.getmembers(sys.modules[__name__], inspect.isfunction)\n    for (name, obj) in templatesets:\n        if name == \"describe_template_sets\":\n            continue\n        print(name, obj.__doc__, \"\\n\")\n\n\n\n@jsontags.register_tag\nclass BrillTagger(TaggerI):\n\n    json_tag = 'nltk.tag.BrillTagger'\n\n    def __init__(self, initial_tagger, rules, training_stats=None):\n        self._initial_tagger = initial_tagger\n        self._rules = tuple(rules)\n        self._training_stats = training_stats\n\n    def encode_json_obj(self):\n        return self._initial_tagger, self._rules, self._training_stats\n\n    @classmethod\n    def decode_json_obj(cls, obj):\n        _initial_tagger, _rules, _training_stats = obj\n        return cls(_initial_tagger, _rules, _training_stats)\n\n    def rules(self):\n        return self._rules\n\n    def train_stats(self, statistic=None):\n        if statistic is None:\n            return self._training_stats\n        else:\n            return self._training_stats.get(statistic)\n\n    def tag(self, tokens):\n\n        tagged_tokens = self._initial_tagger.tag(tokens)\n\n        tag_to_positions = defaultdict(set)\n        for i, (token, tag) in enumerate(tagged_tokens):\n            tag_to_positions[tag].add(i)\n\n        for rule in self._rules:\n            positions = tag_to_positions.get(rule.original_tag, [])\n            changed = rule.apply(tagged_tokens, positions)\n            for i in changed:\n                tag_to_positions[rule.original_tag].remove(i)\n                tag_to_positions[rule.replacement_tag].add(i)\n\n        return tagged_tokens\n\n    def print_template_statistics(self, test_stats=None, printunused=True):\n        tids = [r.templateid for r in self._rules]\n        train_stats = self.train_stats()\n\n        trainscores = train_stats['rulescores']\n        assert len(trainscores) == len(tids), \"corrupt statistics: \" \\\n            \"{0} train scores for {1} rules\".format(trainscores, tids)\n        template_counts = Counter(tids)\n        weighted_traincounts = Counter()\n        for (tid, score) in zip(tids, trainscores):\n            weighted_traincounts[tid] += score\n        tottrainscores = sum(trainscores)\n\n        def det_tplsort(tpl_value):\n            return (tpl_value[1], repr(tpl_value[0]))\n\n        def print_train_stats():\n            print(\"TEMPLATE STATISTICS (TRAIN)  {0} templates, {1} rules)\".format(\n                len(template_counts),\n                len(tids))\n            )\n            print(\"TRAIN ({tokencount:7d} tokens) initial {initialerrors:5d} {initialacc:.4f} \"\n                  \"final: {finalerrors:5d} {finalacc:.4f} \".format(**train_stats))\n            head = \"#ID | Score (train) |  #Rules     | Template\"\n            print(head, \"\\n\", \"-\" * len(head), sep=\"\")\n            train_tplscores = sorted(weighted_traincounts.items(), key=det_tplsort, reverse=True)\n            for (tid, trainscore) in train_tplscores:\n                s = \"{0} | {1:5d}   {2:5.3f} |{3:4d}   {4:.3f} | {5}\".format(\n                    tid,\n                    trainscore,\n                    trainscore/tottrainscores,\n                    template_counts[tid],\n                    template_counts[tid]/len(tids),\n                    Template.ALLTEMPLATES[int(tid)],\n                )\n                print(s)\n\n        def print_testtrain_stats():\n            testscores = test_stats['rulescores']\n            print(\"TEMPLATE STATISTICS (TEST AND TRAIN) ({0} templates, {1} rules)\".format(\n                len(template_counts),\n                len(tids)),\n            )\n            print(\"TEST  ({tokencount:7d} tokens) initial {initialerrors:5d} {initialacc:.4f} \"\n                  \"final: {finalerrors:5d} {finalacc:.4f} \".format(**test_stats))\n            print(\"TRAIN ({tokencount:7d} tokens) initial {initialerrors:5d} {initialacc:.4f} \"\n                  \"final: {finalerrors:5d} {finalacc:.4f} \".format(**train_stats))\n            weighted_testcounts = Counter()\n            for (tid, score) in zip(tids, testscores):\n                weighted_testcounts[tid] += score\n            tottestscores = sum(testscores)\n            head = \"#ID | Score (test) | Score (train) |  #Rules     | Template\"\n            print(head, \"\\n\", \"-\" * len(head), sep=\"\")\n            test_tplscores = sorted(weighted_testcounts.items(), key=det_tplsort, reverse=True)\n            for (tid, testscore) in test_tplscores:\n                s = \"{0:s} |{1:5d}  {2:6.3f} |  {3:4d}   {4:.3f} |{5:4d}   {6:.3f} | {7:s}\".format(\n                    tid,\n                    testscore,\n                    testscore/tottestscores,\n                    weighted_traincounts[tid],\n                    weighted_traincounts[tid]/tottrainscores,\n                    template_counts[tid],\n                    template_counts[tid]/len(tids),\n                    Template.ALLTEMPLATES[int(tid)],\n                )\n                print(s)\n\n        def print_unused_templates():\n            usedtpls = set([int(tid) for tid in tids])\n            unused = [(tid, tpl) for (tid, tpl) in enumerate(Template.ALLTEMPLATES) if tid not in usedtpls]\n            print(\"UNUSED TEMPLATES ({0})\".format(len(unused)))\n\n            for (tid, tpl) in unused:\n                print(\"{0:03d} {1:s}\".format(tid, str(tpl)))\n\n        if test_stats is None:\n            print_train_stats()\n        else:\n            print_testtrain_stats()\n        print()\n        if printunused:\n            print_unused_templates()\n        print()\n\n    def batch_tag_incremental(self, sequences, gold):\n        def counterrors(xs):\n            return sum(t[1] != g[1] for pair in zip(xs, gold) for (t, g) in zip(*pair))\n        testing_stats = {}\n        testing_stats['tokencount'] = sum(len(t) for t in sequences)\n        testing_stats['sequencecount'] = len(sequences)\n        tagged_tokenses = [self._initial_tagger.tag(tokens) for tokens in sequences]\n        testing_stats['initialerrors'] = counterrors(tagged_tokenses)\n        testing_stats['initialacc'] = 1 - testing_stats['initialerrors']/testing_stats['tokencount']\n        errors = [testing_stats['initialerrors']]\n        for rule in self._rules:\n            for tagged_tokens in tagged_tokenses:\n                rule.apply(tagged_tokens)\n            errors.append(counterrors(tagged_tokenses))\n        testing_stats['rulescores'] = [err0 - err1 for (err0, err1) in zip(errors, errors[1:])]\n        testing_stats['finalerrors'] = errors[-1]\n        testing_stats['finalacc'] = 1 - testing_stats['finalerrors']/testing_stats['tokencount']\n        return (tagged_tokenses, testing_stats)\n"], "nltk\\tag\\brill_trainer": [".py", "\nfrom __future__ import print_function, division\n\nimport bisect\nimport textwrap\nfrom collections import defaultdict\n\nfrom nltk.tag import untag, BrillTagger\n\n\n\nclass BrillTaggerTrainer(object):\n    def __init__(self, initial_tagger, templates, trace=0,\n                 deterministic=None, ruleformat=\"str\"):\n\n        if deterministic is None:\n            deterministic = (trace > 0)\n        self._initial_tagger = initial_tagger\n        self._templates = templates\n        self._trace = trace\n        self._deterministic = deterministic\n        self._ruleformat = ruleformat\n\n        self._tag_positions = None\n\n        self._rules_by_position = None\n        Trains the Brill tagger on the corpus *train_sents*,\n        producing at most *max_rules* transformations, each of which\n        reduces the net number of errors in the corpus by at least\n        *min_score*, and each of which has accuracy not lower than\n        *min_acc*.\n\n        >>> from nltk.tbl.template import Template\n        >>> from nltk.tag.brill import Pos, Word\n        >>> from nltk.tag import untag, RegexpTagger, BrillTaggerTrainer\n\n        >>> from nltk.corpus import treebank\n        >>> training_data = treebank.tagged_sents()[:100]\n        >>> baseline_data = treebank.tagged_sents()[100:200]\n        >>> gold_data = treebank.tagged_sents()[200:300]\n        >>> testing_data = [untag(s) for s in gold_data]\n\n        >>> backoff = RegexpTagger([\n        ... (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers\n        ... (r'(The|the|A|a|An|an)$', 'AT'),   # articles\n        ... (r'.*able$', 'JJ'),                # adjectives\n        ... (r'.*ness$', 'NN'),                # nouns formed from adjectives\n        ... (r'.*ly$', 'RB'),                  # adverbs\n        ... (r'.*s$', 'NNS'),                  # plural nouns\n        ... (r'.*ing$', 'VBG'),                # gerunds\n        ... (r'.*ed$', 'VBD'),                 # past tense verbs\n        ... (r'.*', 'NN')                      # nouns (default)\n        ... ])\n\n        >>> baseline = backoff #see NOTE1\n\n        >>> baseline.evaluate(gold_data) #doctest: +ELLIPSIS\n        0.2450142...\n\n        >>> Template._cleartemplates() #clear any templates created in earlier tests\n        >>> templates = [Template(Pos([-1])), Template(Pos([-1]), Word([0]))]\n\n        >>> tt = BrillTaggerTrainer(baseline, templates, trace=3)\n\n        >>> tagger1 = tt.train(training_data, max_rules=10)\n        TBL train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: None)\n        Finding initial useful rules...\n            Found 845 useful rules.\n        <BLANKLINE>\n                   B      |\n           S   F   r   O  |        Score = Fixed - Broken\n           c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n           o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n           r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n           e   d   n   r  |  e\n        ------------------+-------------------------------------------------------\n         132 132   0   0  | AT->DT if Pos:NN@[-1]\n          85  85   0   0  | NN->, if Pos:NN@[-1] & Word:,@[0]\n          69  69   0   0  | NN->. if Pos:NN@[-1] & Word:.@[0]\n          51  51   0   0  | NN->IN if Pos:NN@[-1] & Word:of@[0]\n          47  63  16 161  | NN->IN if Pos:NNS@[-1]\n          33  33   0   0  | NN->TO if Pos:NN@[-1] & Word:to@[0]\n          26  26   0   0  | IN->. if Pos:NNS@[-1] & Word:.@[0]\n          24  24   0   0  | IN->, if Pos:NNS@[-1] & Word:,@[0]\n          22  27   5  24  | NN->-NONE- if Pos:VBD@[-1]\n          17  17   0   0  | NN->CC if Pos:NN@[-1] & Word:and@[0]\n\n        >>> tagger1.rules()[1:3]\n        (Rule('001', 'NN', ',', [(Pos([-1]),'NN'), (Word([0]),',')]), Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]))\n\n        >>> train_stats = tagger1.train_stats()\n        >>> [train_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]\n        [1775, 1269, [132, 85, 69, 51, 47, 33, 26, 24, 22, 17]]\n\n        >>> tagger1.print_template_statistics(printunused=False)\n        TEMPLATE STATISTICS (TRAIN)  2 templates, 10 rules)\n        TRAIN (   2417 tokens) initial  1775 0.2656 final:  1269 0.4750\n        --------------------------------------------\n        001 |   305   0.603 |   7   0.700 | Template(Pos([-1]),Word([0]))\n        000 |   201   0.397 |   3   0.300 | Template(Pos([-1]))\n        <BLANKLINE>\n        <BLANKLINE>\n\n        >>> tagger1.evaluate(gold_data) # doctest: +ELLIPSIS\n        0.43996...\n\n        >>> tagged, test_stats = tagger1.batch_tag_incremental(testing_data, gold_data)\n\n        >>> tagged[33][12:] == [('foreign', 'IN'), ('debt', 'NN'), ('of', 'IN'), ('$', 'NN'), ('64', 'CD'),\n        ... ('billion', 'NN'), ('*U*', 'NN'), ('--', 'NN'), ('the', 'DT'), ('third-highest', 'NN'), ('in', 'NN'),\n        ... ('the', 'DT'), ('developing', 'VBG'), ('world', 'NN'), ('.', '.')]\n        True\n\n        >>> [test_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]\n        [1855, 1376, [100, 85, 67, 58, 27, 36, 27, 16, 31, 32]]\n\n        >>> tagger2 = tt.train(training_data, max_rules=10, min_acc=0.99)\n        TBL train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: 0.99)\n        Finding initial useful rules...\n            Found 845 useful rules.\n        <BLANKLINE>\n                   B      |\n           S   F   r   O  |        Score = Fixed - Broken\n           c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n           o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n           r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n           e   d   n   r  |  e\n        ------------------+-------------------------------------------------------\n         132 132   0   0  | AT->DT if Pos:NN@[-1]\n          85  85   0   0  | NN->, if Pos:NN@[-1] & Word:,@[0]\n          69  69   0   0  | NN->. if Pos:NN@[-1] & Word:.@[0]\n          51  51   0   0  | NN->IN if Pos:NN@[-1] & Word:of@[0]\n          36  36   0   0  | NN->TO if Pos:NN@[-1] & Word:to@[0]\n          26  26   0   0  | NN->. if Pos:NNS@[-1] & Word:.@[0]\n          24  24   0   0  | NN->, if Pos:NNS@[-1] & Word:,@[0]\n          19  19   0   6  | NN->VB if Pos:TO@[-1]\n          18  18   0   0  | CD->-NONE- if Pos:NN@[-1] & Word:0@[0]\n          18  18   0   0  | NN->CC if Pos:NN@[-1] & Word:and@[0]\n\n        >>> tagger2.evaluate(gold_data)  # doctest: +ELLIPSIS\n        0.44159544...\n        >>> tagger2.rules()[2:4]\n        (Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]), Rule('001', 'NN', 'IN', [(Pos([-1]),'NN'), (Word([0]),'of')]))\n\n\n        :param train_sents: training data\n        :type train_sents: list(list(tuple))\n        :param max_rules: output at most max_rules rules\n        :type max_rules: int\n        :param min_score: stop training when no rules better than min_score can be found\n        :type min_score: int\n        :param min_acc: discard any rule with lower accuracy than min_acc\n        :type min_acc: float or None\n        :return: the learned tagger\n        :rtype: BrillTagger\n\n        \"\"\"\n\n\n        test_sents = [list(self._initial_tagger.tag(untag(sent)))\n                      for sent in train_sents]\n\n        trainstats = {}\n        trainstats['min_acc'] = min_acc\n        trainstats['min_score'] = min_score\n        trainstats['tokencount'] = sum(len(t) for t in test_sents)\n        trainstats['sequencecount'] = len(test_sents)\n        trainstats['templatecount'] = len(self._templates)\n        trainstats['rulescores'] = []\n        trainstats['initialerrors'] = sum(\n            tag[1] != truth[1]\n            for paired in zip(test_sents, train_sents)\n            for (tag, truth) in zip(*paired)\n        )\n        trainstats['initialacc'] = 1 - trainstats['initialerrors']/trainstats['tokencount']\n        if self._trace > 0:\n            print(\"TBL train (fast) (seqs: {sequencecount}; tokens: {tokencount}; \"\n                  \"tpls: {templatecount}; min score: {min_score}; min acc: {min_acc})\".format(**trainstats))\n\n        if self._trace:\n            print(\"Finding initial useful rules...\")\n        self._init_mappings(test_sents, train_sents)\n        if self._trace:\n            print((\"    Found %d useful rules.\" % len(self._rule_scores)))\n\n        if self._trace > 2:\n            self._trace_header()\n        elif self._trace == 1:\n            print(\"Selecting rules...\")\n\n        rules = []\n        try:\n            while (len(rules) < max_rules):\n                rule = self._best_rule(train_sents, test_sents, min_score, min_acc)\n                if rule:\n                    rules.append(rule)\n                    score = self._rule_scores[rule]\n                    trainstats['rulescores'].append(score)\n                else:\n                    break  # No more good rules left!\n\n                if self._trace > 1:\n                    self._trace_rule(rule)\n\n                self._apply_rule(rule, test_sents)\n\n                self._update_tag_positions(rule)\n\n                self._update_rules(rule, train_sents, test_sents)\n\n        except KeyboardInterrupt:\n            print(\"Training stopped manually -- %d rules found\" % len(rules))\n\n        self._clean()\n        trainstats['finalerrors'] = trainstats['initialerrors'] - sum(trainstats['rulescores'])\n        trainstats['finalacc'] = 1 - trainstats['finalerrors']/trainstats['tokencount']\n        return BrillTagger(self._initial_tagger, rules, trainstats)\n\n    def _init_mappings(self, test_sents, train_sents):\n        \"\"\"\n        Initialize the tag position mapping & the rule related\n        mappings.  For each error in test_sents, find new rules that\n        would correct them, and add them to the rule mappings.\n        \"\"\"\n        self._tag_positions = defaultdict(list)\n        self._rules_by_position = defaultdict(set)\n        self._positions_by_rule = defaultdict(dict)\n        self._rules_by_score = defaultdict(set)\n        self._rule_scores = defaultdict(int)\n        self._first_unknown_position = defaultdict(int)\n        for sentnum, sent in enumerate(test_sents):\n            for wordnum, (word, tag) in enumerate(sent):\n\n                self._tag_positions[tag].append((sentnum, wordnum))\n\n                correct_tag = train_sents[sentnum][wordnum][1]\n                if tag != correct_tag:\n                    for rule in self._find_rules(sent, wordnum, correct_tag):\n                        self._update_rule_applies(rule, sentnum, wordnum,\n                                                  train_sents)\n\n    def _clean(self):\n        self._tag_positions = None\n        self._rules_by_position = None\n        self._positions_by_rule = None\n        self._rules_by_score = None\n        self._rule_scores = None\n        self._first_unknown_position = None\n\n    def _find_rules(self, sent, wordnum, new_tag):\n        \"\"\"\n        Use the templates to find rules that apply at index *wordnum*\n        in the sentence *sent* and generate the tag *new_tag*.\n        \"\"\"\n        for template in self._templates:\n            for rule in template.applicable_rules(sent, wordnum, new_tag):\n                yield rule\n\n    def _update_rule_applies(self, rule, sentnum, wordnum, train_sents):\n        \"\"\"\n        Update the rule data tables to reflect the fact that\n        *rule* applies at the position *(sentnum, wordnum)*.\n        \"\"\"\n        pos = sentnum, wordnum\n\n        if pos in self._positions_by_rule[rule]:\n            return\n\n        correct_tag = train_sents[sentnum][wordnum][1]\n        if rule.replacement_tag == correct_tag:\n            self._positions_by_rule[rule][pos] = 1\n        elif rule.original_tag == correct_tag:\n            self._positions_by_rule[rule][pos] = -1\n        else:  # was wrong, remains wrong\n            self._positions_by_rule[rule][pos] = 0\n\n        self._rules_by_position[pos].add(rule)\n\n        old_score = self._rule_scores[rule]\n        self._rule_scores[rule] += self._positions_by_rule[rule][pos]\n\n        self._rules_by_score[old_score].discard(rule)\n        self._rules_by_score[self._rule_scores[rule]].add(rule)\n\n    def _update_rule_not_applies(self, rule, sentnum, wordnum):\n        \"\"\"\n        Update the rule data tables to reflect the fact that *rule*\n        does not apply at the position *(sentnum, wordnum)*.\n        \"\"\"\n        pos = sentnum, wordnum\n\n        old_score = self._rule_scores[rule]\n        self._rule_scores[rule] -= self._positions_by_rule[rule][pos]\n\n        self._rules_by_score[old_score].discard(rule)\n        self._rules_by_score[self._rule_scores[rule]].add(rule)\n\n        del self._positions_by_rule[rule][pos]\n        self._rules_by_position[pos].remove(rule)\n\n\n    def _best_rule(self, train_sents, test_sents, min_score, min_acc):\n        \"\"\"\n        Find the next best rule.  This is done by repeatedly taking a\n        rule with the highest score and stepping through the corpus to\n        see where it applies.  When it makes an error (decreasing its\n        score) it's bumped down, and we try a new rule with the\n        highest score.  When we find a rule which has the highest\n        score *and* which has been tested against the entire corpus, we\n        can conclude that it's the next best rule.\n        \"\"\"\n        for max_score in sorted(self._rules_by_score.keys(), reverse=True):\n            if len(self._rules_by_score) == 0:\n                return None\n            if max_score < min_score or max_score <= 0:\n                return None\n            best_rules = list(self._rules_by_score[max_score])\n            if self._deterministic:\n                best_rules.sort(key=repr)\n            for rule in best_rules:\n                positions = self._tag_positions[rule.original_tag]\n\n                unk = self._first_unknown_position.get(rule, (0, -1))\n                start = bisect.bisect_left(positions, unk)\n\n                for i in range(start, len(positions)):\n                    sentnum, wordnum = positions[i]\n                    if rule.applies(test_sents[sentnum], wordnum):\n                        self._update_rule_applies(rule, sentnum, wordnum,\n                                                  train_sents)\n                        if self._rule_scores[rule] < max_score:\n                            self._first_unknown_position[rule] = (sentnum,\n                                                                  wordnum+1)\n                            break  # The update demoted the rule.\n\n                if self._rule_scores[rule] == max_score:\n                    self._first_unknown_position[rule] = (len(train_sents) + 1, 0)\n                    if min_acc is None:\n                        return rule\n                    else:\n                        changes = self._positions_by_rule[rule].values()\n                        num_fixed = len([c for c in changes if c == 1])\n                        num_broken = len([c for c in changes if c == -1])\n                        acc = num_fixed/(num_fixed+num_broken)\n                        if acc >= min_acc:\n                            return rule\n\n\n            assert min_acc is not None or not self._rules_by_score[max_score]\n            if not self._rules_by_score[max_score]:\n                del self._rules_by_score[max_score]\n\n    def _apply_rule(self, rule, test_sents):\n        \"\"\"\n        Update *test_sents* by applying *rule* everywhere where its\n        conditions are met.\n        \"\"\"\n        update_positions = set(self._positions_by_rule[rule])\n        new_tag = rule.replacement_tag\n\n        if self._trace > 3:\n            self._trace_apply(len(update_positions))\n\n        for (sentnum, wordnum) in update_positions:\n            text = test_sents[sentnum][wordnum][0]\n            test_sents[sentnum][wordnum] = (text, new_tag)\n\n    def _update_tag_positions(self, rule):\n        \"\"\"\n        Update _tag_positions to reflect the changes to tags that are\n        made by *rule*.\n        \"\"\"\n        for pos in self._positions_by_rule[rule]:\n            old_tag_positions = self._tag_positions[rule.original_tag]\n            old_index = bisect.bisect_left(old_tag_positions, pos)\n            del old_tag_positions[old_index]\n            new_tag_positions = self._tag_positions[rule.replacement_tag]\n            bisect.insort_left(new_tag_positions, pos)\n\n    def _update_rules(self, rule, train_sents, test_sents):\n        \"\"\"\n        Check if we should add or remove any rules from consideration,\n        given the changes made by *rule*.\n        \"\"\"\n        neighbors = set()\n        for sentnum, wordnum in self._positions_by_rule[rule]:\n            for template in self._templates:\n                n = template.get_neighborhood(test_sents[sentnum], wordnum)\n                neighbors.update([(sentnum, i) for i in n])\n\n        num_obsolete = num_new = num_unseen = 0\n        for sentnum, wordnum in neighbors:\n            test_sent = test_sents[sentnum]\n            correct_tag = train_sents[sentnum][wordnum][1]\n\n            old_rules = set(self._rules_by_position[sentnum, wordnum])\n            for old_rule in old_rules:\n                if not old_rule.applies(test_sent, wordnum):\n                    num_obsolete += 1\n                    self._update_rule_not_applies(old_rule, sentnum, wordnum)\n\n            for template in self._templates:\n                for new_rule in template.applicable_rules(test_sent, wordnum,\n                                                          correct_tag):\n                    if new_rule not in old_rules:\n                        num_new += 1\n                        if new_rule not in self._rule_scores:\n                            num_unseen += 1\n                        old_rules.add(new_rule)\n                        self._update_rule_applies(new_rule, sentnum,\n                                                  wordnum, train_sents)\n\n            for new_rule, pos in self._first_unknown_position.items():\n                if pos > (sentnum, wordnum):\n                    if new_rule not in old_rules:\n                        num_new += 1\n                        if new_rule.applies(test_sent, wordnum):\n                            self._update_rule_applies(new_rule, sentnum,\n                                                      wordnum, train_sents)\n\n        if self._trace > 3:\n            self._trace_update_rules(num_obsolete, num_new, num_unseen)\n\n\n    def _trace_header(self):\n        print(\"\"\"\n           B      |\n   S   F   r   O  |        Score = Fixed - Broken\n   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n   e   d   n   r  |  e\n------------------+-------------------------------------------------------\n        \"\"\".rstrip())\n\n    def _trace_rule(self, rule):\n        assert self._rule_scores[rule] == sum(self._positions_by_rule[rule].values())\n\n        changes = self._positions_by_rule[rule].values()\n        num_fixed = len([c for c in changes if c == 1])\n        num_broken = len([c for c in changes if c == -1])\n        num_other = len([c for c in changes if c == 0])\n        score = self._rule_scores[rule]\n\n        rulestr = rule.format(self._ruleformat)\n        if self._trace > 2:\n            print('%4d%4d%4d%4d  |' % (score, num_fixed, num_broken, num_other), end=' ')\n            print(textwrap.fill(rulestr, initial_indent=' '*20, width=79,\n                                subsequent_indent=' '*18+'|   ').strip())\n        else:\n            print(rulestr)\n\n    def _trace_apply(self, num_updates):\n        prefix = ' '*18+'|'\n        print(prefix)\n        print(prefix, 'Applying rule to %d positions.' % num_updates)\n\n    def _trace_update_rules(self, num_obsolete, num_new, num_unseen):\n        prefix = ' '*18+'|'\n        print(prefix, 'Updated rule tables:')\n        print(prefix, ('  - %d rule applications removed' % num_obsolete))\n        print(prefix, ('  - %d rule applications added (%d novel)' %\n                       (num_new, num_unseen)))\n        print(prefix)\n\n\n"], "nltk\\tag\\crf": [".py", "\nfrom __future__ import absolute_import\nfrom __future__ import unicode_literals\nimport unicodedata\nimport re \nfrom nltk.tag.api import TaggerI\n\ntry:\n    import pycrfsuite\nexcept ImportError:\n    pass\n\nclass CRFTagger(TaggerI):\n    \n    \n    def __init__(self,  feature_func = None, verbose = False, training_opt = {}):\n                   \n        self._model_file = ''\n        self._tagger = pycrfsuite.Tagger()\n        \n        if feature_func is None:\n            self._feature_func =  self._get_features\n        else:\n            self._feature_func =  feature_func\n        \n        self._verbose = verbose \n        self._training_options = training_opt\n        self._pattern = re.compile(r'\\d')\n        \n    def set_model_file(self, model_file):\n        self._model_file = model_file\n        self._tagger.open(self._model_file)\n            \n    def _get_features(self, tokens, idx):\n        token = tokens[idx]\n        \n        feature_list = []\n        \n        if not token:\n            return feature_list\n            \n        if token[0].isupper():\n            feature_list.append('CAPITALIZATION')\n        \n        if re.search(self._pattern, token) is not None:\n            feature_list.append('HAS_NUM') \n        \n        punc_cat = set([\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"])\n        if all (unicodedata.category(x) in punc_cat for x in token):\n            feature_list.append('PUNCTUATION')\n        \n        if len(token) > 1:\n            feature_list.append('SUF_' + token[-1:]) \n        if len(token) > 2: \n            feature_list.append('SUF_' + token[-2:])    \n        if len(token) > 3: \n            feature_list.append('SUF_' + token[-3:])\n            \n        feature_list.append('WORD_' + token )\n        \n        return feature_list\n        \n    def tag_sents(self, sents):\n        '''\n        Tag a list of sentences. NB before using this function, user should specify the mode_file either by \n                       - Train a new model using ``train'' function \n                       - Use the pre-trained model which is set via ``set_model_file'' function  \n        :params sentences : list of sentences needed to tag. \n        :type sentences : list(list(str))\n        :return : list of tagged sentences. \n        :rtype : list (list (tuple(str,str))) \n        '''\n        if self._model_file == '':\n            raise Exception(' No model file is found !! Please use train or set_model_file function')\n        \n        result = []  \n        for tokens in sents:\n            features = [self._feature_func(tokens,i) for i in range(len(tokens))]\n            labels = self._tagger.tag(features)\n                \n            if len(labels) != len(tokens):\n                raise Exception(' Predicted Length Not Matched, Expect Errors !')\n            \n            tagged_sent = list(zip(tokens,labels))\n            result.append(tagged_sent)\n            \n        return result \n    \n    def train(self, train_data, model_file):\n        '''\n        Train the CRF tagger using CRFSuite  \n        :params train_data : is the list of annotated sentences.        \n        :type train_data : list (list(tuple(str,str)))\n        :params model_file : the model will be saved to this file.     \n         \n        '''\n        trainer = pycrfsuite.Trainer(verbose=self._verbose)\n        trainer.set_params(self._training_options)\n        \n        for sent in train_data:\n            tokens,labels = zip(*sent)\n            features = [self._feature_func(tokens,i) for i in range(len(tokens))]\n            trainer.append(features,labels)\n                        \n        trainer.train(model_file)\n        self.set_model_file(model_file) \n\n    def tag(self, tokens):\n        '''\n        Tag a sentence using Python CRFSuite Tagger. NB before using this function, user should specify the mode_file either by \n                       - Train a new model using ``train'' function \n                       - Use the pre-trained model which is set via ``set_model_file'' function  \n        :params tokens : list of tokens needed to tag. \n        :type tokens : list(str)\n        :return : list of tagged tokens. \n        :rtype : list (tuple(str,str)) \n        '''\n        \n        return self.tag_sents([tokens])[0]\n\n"], "nltk\\tag\\hmm": [".py", "\nfrom __future__ import print_function, unicode_literals, division\n\nimport re\nimport itertools\n\nfrom six.moves import map, zip\n\ntry:\n    import numpy as np\nexcept ImportError:\n    pass\n\nfrom nltk.probability import (FreqDist, ConditionalFreqDist,\n                              ConditionalProbDist, DictionaryProbDist,\n                              DictionaryConditionalProbDist,\n                              LidstoneProbDist, MutableProbDist,\n                              MLEProbDist, RandomProbDist)\nfrom nltk.metrics import accuracy\nfrom nltk.util import LazyMap, unique_list\nfrom nltk.compat import python_2_unicode_compatible\nfrom nltk.tag.api import TaggerI\n\n\n_TEXT = 0  # index of text in a tuple\n_TAG = 1   # index of tag in a tuple\n\ndef _identity(labeled_symbols):\n    return labeled_symbols\n\n@python_2_unicode_compatible\nclass HiddenMarkovModelTagger(TaggerI):\n    def __init__(self, symbols, states, transitions, outputs, priors,\n                 transform=_identity):\n        self._symbols = unique_list(symbols)\n        self._states = unique_list(states)\n        self._transitions = transitions\n        self._outputs = outputs\n        self._priors = priors\n        self._cache = None\n        self._transform = transform\n\n    @classmethod\n    def _train(cls, labeled_sequence, test_sequence=None,\n                    unlabeled_sequence=None, transform=_identity,\n                    estimator=None, **kwargs):\n\n        if estimator is None:\n            def estimator(fd, bins):\n                return LidstoneProbDist(fd, 0.1, bins)\n\n        labeled_sequence = LazyMap(transform, labeled_sequence)\n        symbols = unique_list(word for sent in labeled_sequence\n            for word, tag in sent)\n        tag_set = unique_list(tag for sent in labeled_sequence\n            for word, tag in sent)\n\n        trainer = HiddenMarkovModelTrainer(tag_set, symbols)\n        hmm = trainer.train_supervised(labeled_sequence, estimator=estimator)\n        hmm = cls(hmm._symbols, hmm._states, hmm._transitions, hmm._outputs,\n                  hmm._priors, transform=transform)\n\n        if test_sequence:\n            hmm.test(test_sequence, verbose=kwargs.get('verbose', False))\n\n        if unlabeled_sequence:\n            max_iterations = kwargs.get('max_iterations', 5)\n            hmm = trainer.train_unsupervised(unlabeled_sequence, model=hmm,\n                max_iterations=max_iterations)\n            if test_sequence:\n                hmm.test(test_sequence, verbose=kwargs.get('verbose', False))\n\n        return hmm\n\n    @classmethod\n    def train(cls, labeled_sequence, test_sequence=None,\n                   unlabeled_sequence=None, **kwargs):\n        return cls._train(labeled_sequence, test_sequence,\n                          unlabeled_sequence, **kwargs)\n\n    def probability(self, sequence):\n        return 2**(self.log_probability(self._transform(sequence)))\n\n    def log_probability(self, sequence):\n        sequence = self._transform(sequence)\n\n        T = len(sequence)\n\n        if T > 0 and sequence[0][_TAG]:\n            last_state = sequence[0][_TAG]\n            p = self._priors.logprob(last_state) + \\\n                self._output_logprob(last_state, sequence[0][_TEXT])\n            for t in range(1, T):\n                state = sequence[t][_TAG]\n                p += self._transitions[last_state].logprob(state) + \\\n                     self._output_logprob(state, sequence[t][_TEXT])\n                last_state = state\n            return p\n        else:\n            alpha = self._forward_probability(sequence)\n            p = logsumexp2(alpha[T-1])\n            return p\n\n    def tag(self, unlabeled_sequence):\n        unlabeled_sequence = self._transform(unlabeled_sequence)\n        return self._tag(unlabeled_sequence)\n\n    def _tag(self, unlabeled_sequence):\n        path = self._best_path(unlabeled_sequence)\n        return list(zip(unlabeled_sequence, path))\n\n    def _output_logprob(self, state, symbol):\n        return self._outputs[state].logprob(symbol)\n\n    def _create_cache(self):\n        if not self._cache:\n            N = len(self._states)\n            M = len(self._symbols)\n            P = np.zeros(N, np.float32)\n            X = np.zeros((N, N), np.float32)\n            O = np.zeros((N, M), np.float32)\n            for i in range(N):\n                si = self._states[i]\n                P[i] = self._priors.logprob(si)\n                for j in range(N):\n                    X[i, j] = self._transitions[si].logprob(self._states[j])\n                for k in range(M):\n                    O[i, k] = self._output_logprob(si, self._symbols[k])\n            S = {}\n            for k in range(M):\n                S[self._symbols[k]] = k\n            self._cache = (P, O, X, S)\n\n    def _update_cache(self, symbols):\n        if symbols:\n            self._create_cache()\n            P, O, X, S = self._cache\n            for symbol in symbols:\n                if symbol not in self._symbols:\n                    self._cache = None\n                    self._symbols.append(symbol)\n            if not self._cache:\n                N = len(self._states)\n                M = len(self._symbols)\n                Q = O.shape[1]\n                O = np.hstack([O, np.zeros((N, M - Q), np.float32)])\n                for i in range(N):\n                    si = self._states[i]\n                    for k in range(Q, M):\n                        O[i, k] = self._output_logprob(si, self._symbols[k])\n                for k in range(Q, M):\n                    S[self._symbols[k]] = k\n                self._cache = (P, O, X, S)\n\n    def reset_cache(self):\n        self._cache = None\n\n    def best_path(self, unlabeled_sequence):\n        unlabeled_sequence = self._transform(unlabeled_sequence)\n        return self._best_path(unlabeled_sequence)\n\n    def _best_path(self, unlabeled_sequence):\n        T = len(unlabeled_sequence)\n        N = len(self._states)\n        self._create_cache()\n        self._update_cache(unlabeled_sequence)\n        P, O, X, S = self._cache\n\n        V = np.zeros((T, N), np.float32)\n        B = -np.ones((T, N), np.int)\n\n        V[0] = P + O[:, S[unlabeled_sequence[0]]]\n        for t in range(1, T):\n            for j in range(N):\n                vs = V[t-1, :] + X[:, j]\n                best = np.argmax(vs)\n                V[t, j] = vs[best] + O[j, S[unlabeled_sequence[t]]]\n                B[t, j] = best\n\n        current = np.argmax(V[T-1,:])\n        sequence = [current]\n        for t in range(T-1, 0, -1):\n            last = B[t, current]\n            sequence.append(last)\n            current = last\n\n        sequence.reverse()\n        return list(map(self._states.__getitem__, sequence))\n\n    def best_path_simple(self, unlabeled_sequence):\n        unlabeled_sequence = self._transform(unlabeled_sequence)\n        return self._best_path_simple(unlabeled_sequence)\n\n    def _best_path_simple(self, unlabeled_sequence):\n        T = len(unlabeled_sequence)\n        N = len(self._states)\n        V = np.zeros((T, N), np.float64)\n        B = {}\n\n        symbol = unlabeled_sequence[0]\n        for i, state in enumerate(self._states):\n            V[0, i] = self._priors.logprob(state) + \\\n                      self._output_logprob(state, symbol)\n            B[0, state] = None\n\n        for t in range(1, T):\n            symbol = unlabeled_sequence[t]\n            for j in range(N):\n                sj = self._states[j]\n                best = None\n                for i in range(N):\n                    si = self._states[i]\n                    va = V[t-1, i] + self._transitions[si].logprob(sj)\n                    if not best or va > best[0]:\n                        best = (va, si)\n                V[t, j] = best[0] + self._output_logprob(sj, symbol)\n                B[t, sj] = best[1]\n\n        best = None\n        for i in range(N):\n            val = V[T-1, i]\n            if not best or val > best[0]:\n                best = (val, self._states[i])\n\n        current = best[1]\n        sequence = [current]\n        for t in range(T-1, 0, -1):\n            last = B[t, current]\n            sequence.append(last)\n            current = last\n\n        sequence.reverse()\n        return sequence\n\n    def random_sample(self, rng, length):\n\n        tokens = []\n        state = self._sample_probdist(self._priors, rng.random(), self._states)\n        symbol = self._sample_probdist(self._outputs[state],\n                                  rng.random(), self._symbols)\n        tokens.append((symbol, state))\n\n        for i in range(1, length):\n            state = self._sample_probdist(self._transitions[state],\n                                     rng.random(), self._states)\n            symbol = self._sample_probdist(self._outputs[state],\n                                      rng.random(), self._symbols)\n            tokens.append((symbol, state))\n\n        return tokens\n\n    def _sample_probdist(self, probdist, p, samples):\n        cum_p = 0\n        for sample in samples:\n            add_p = probdist.prob(sample)\n            if cum_p <= p <= cum_p + add_p:\n                return sample\n            cum_p += add_p\n        raise Exception('Invalid probability distribution - '\n                        'does not sum to one')\n\n    def entropy(self, unlabeled_sequence):\n        unlabeled_sequence = self._transform(unlabeled_sequence)\n\n        T = len(unlabeled_sequence)\n        N = len(self._states)\n\n        alpha = self._forward_probability(unlabeled_sequence)\n        beta = self._backward_probability(unlabeled_sequence)\n        normalisation = logsumexp2(alpha[T-1])\n\n        entropy = normalisation\n\n        for i, state in enumerate(self._states):\n            p = 2**(alpha[0, i] + beta[0, i] - normalisation)\n            entropy -= p * self._priors.logprob(state)\n\n        for t0 in range(T - 1):\n            t1 = t0 + 1\n            for i0, s0 in enumerate(self._states):\n                for i1, s1 in enumerate(self._states):\n                    p = 2**(alpha[t0, i0] + self._transitions[s0].logprob(s1) +\n                            self._outputs[s1].logprob(\n                                unlabeled_sequence[t1][_TEXT]) +\n                            beta[t1, i1] - normalisation)\n                    entropy -= p * self._transitions[s0].logprob(s1)\n\n        for t in range(T):\n            for i, state in enumerate(self._states):\n                p = 2**(alpha[t, i] + beta[t, i] - normalisation)\n                entropy -= p * self._outputs[state].logprob(\n                    unlabeled_sequence[t][_TEXT])\n\n        return entropy\n\n    def point_entropy(self, unlabeled_sequence):\n        unlabeled_sequence = self._transform(unlabeled_sequence)\n\n        T = len(unlabeled_sequence)\n        N = len(self._states)\n\n        alpha = self._forward_probability(unlabeled_sequence)\n        beta = self._backward_probability(unlabeled_sequence)\n        normalisation = logsumexp2(alpha[T-1])\n\n        entropies = np.zeros(T, np.float64)\n        probs = np.zeros(N, np.float64)\n        for t in range(T):\n            for s in range(N):\n                probs[s] = alpha[t, s] + beta[t, s] - normalisation\n\n            for s in range(N):\n                entropies[t] -= 2**(probs[s]) * probs[s]\n\n        return entropies\n\n    def _exhaustive_entropy(self, unlabeled_sequence):\n        unlabeled_sequence = self._transform(unlabeled_sequence)\n\n        T = len(unlabeled_sequence)\n        N = len(self._states)\n\n        labellings = [[state] for state in self._states]\n        for t in range(T - 1):\n            current = labellings\n            labellings = []\n            for labelling in current:\n                for state in self._states:\n                    labellings.append(labelling + [state])\n\n        log_probs = []\n        for labelling in labellings:\n            labeled_sequence = unlabeled_sequence[:]\n            for t, label in enumerate(labelling):\n                labeled_sequence[t] = (labeled_sequence[t][_TEXT], label)\n            lp = self.log_probability(labeled_sequence)\n            log_probs.append(lp)\n        normalisation = _log_add(*log_probs)\n\n\n\n        entropy = 0\n        for lp in log_probs:\n            lp -= normalisation\n            entropy -= 2**(lp) * lp\n\n        return entropy\n\n    def _exhaustive_point_entropy(self, unlabeled_sequence):\n        unlabeled_sequence = self._transform(unlabeled_sequence)\n\n        T = len(unlabeled_sequence)\n        N = len(self._states)\n\n        labellings = [[state] for state in self._states]\n        for t in range(T - 1):\n            current = labellings\n            labellings = []\n            for labelling in current:\n                for state in self._states:\n                    labellings.append(labelling + [state])\n\n        log_probs = []\n        for labelling in labellings:\n            labelled_sequence = unlabeled_sequence[:]\n            for t, label in enumerate(labelling):\n                labelled_sequence[t] = (labelled_sequence[t][_TEXT], label)\n            lp = self.log_probability(labelled_sequence)\n            log_probs.append(lp)\n\n        normalisation = _log_add(*log_probs)\n\n        probabilities = _ninf_array((T,N))\n\n        for labelling, lp in zip(labellings, log_probs):\n            lp -= normalisation\n            for t, label in enumerate(labelling):\n                index = self._states.index(label)\n                probabilities[t, index] = _log_add(probabilities[t, index], lp)\n\n        entropies = np.zeros(T, np.float64)\n        for t in range(T):\n            for s in range(N):\n                entropies[t] -= 2**(probabilities[t, s]) * probabilities[t, s]\n\n        return entropies\n\n    def _transitions_matrix(self):\n        trans_iter = (self._transitions[sj].logprob(si)\n                      for sj in self._states\n                      for si in self._states)\n\n        transitions_logprob = np.fromiter(trans_iter, dtype=np.float64)\n        N = len(self._states)\n        return transitions_logprob.reshape((N, N)).T\n\n    def _outputs_vector(self, symbol):\n        out_iter = (self._output_logprob(sj, symbol) for sj in self._states)\n        return np.fromiter(out_iter, dtype=np.float64)\n\n    def _forward_probability(self, unlabeled_sequence):\n        T = len(unlabeled_sequence)\n        N = len(self._states)\n        alpha = _ninf_array((T, N))\n\n        transitions_logprob = self._transitions_matrix()\n\n        symbol = unlabeled_sequence[0][_TEXT]\n        for i, state in enumerate(self._states):\n            alpha[0, i] = self._priors.logprob(state) + \\\n                          self._output_logprob(state, symbol)\n\n        for t in range(1, T):\n            symbol = unlabeled_sequence[t][_TEXT]\n            output_logprob = self._outputs_vector(symbol)\n\n            for i in range(N):\n                summand = alpha[t-1] + transitions_logprob[i]\n                alpha[t, i] = logsumexp2(summand) + output_logprob[i]\n\n        return alpha\n\n    def _backward_probability(self, unlabeled_sequence):\n        T = len(unlabeled_sequence)\n        N = len(self._states)\n        beta = _ninf_array((T, N))\n\n        transitions_logprob = self._transitions_matrix().T\n\n        beta[T-1, :] = np.log2(1)\n\n        for t in range(T-2, -1, -1):\n            symbol = unlabeled_sequence[t+1][_TEXT]\n            outputs = self._outputs_vector(symbol)\n\n            for i in range(N):\n                summand = transitions_logprob[i] + beta[t+1] + outputs\n                beta[t, i] = logsumexp2(summand)\n\n        return beta\n\n    def test(self, test_sequence, verbose=False, **kwargs):\n\n        def words(sent):\n            return [word for (word, tag) in sent]\n\n        def tags(sent):\n            return [tag for (word, tag) in sent]\n\n        def flatten(seq):\n            return list(itertools.chain(*seq))\n\n        test_sequence = self._transform(test_sequence)\n        predicted_sequence = list(map(self._tag, map(words, test_sequence)))\n\n        if verbose:\n            for test_sent, predicted_sent in zip(test_sequence, predicted_sequence):\n                print('Test:',\n                    ' '.join('%s/%s' % (token, tag)\n                             for (token, tag) in test_sent))\n                print()\n                print('Untagged:',\n                    ' '.join(\"%s\" % token for (token, tag) in test_sent))\n                print()\n                print('HMM-tagged:',\n                    ' '.join('%s/%s' % (token, tag)\n                              for (token, tag) in predicted_sent))\n                print()\n                print('Entropy:',\n                    self.entropy([(token, None) for\n                                  (token, tag) in predicted_sent]))\n                print()\n                print('-' * 60)\n\n        test_tags = flatten(map(tags, test_sequence))\n        predicted_tags = flatten(map(tags, predicted_sequence))\n\n        acc = accuracy(test_tags, predicted_tags)\n        count = sum(len(sent) for sent in test_sequence)\n        print('accuracy over %d tokens: %.2f' % (count, acc * 100))\n\n    def __repr__(self):\n        return ('<HiddenMarkovModelTagger %d states and %d output symbols>'\n                % (len(self._states), len(self._symbols)))\n\n\nclass HiddenMarkovModelTrainer(object):\n    def __init__(self, states=None, symbols=None):\n        self._states = (states if states else [])\n        self._symbols = (symbols if symbols else [])\n\n    def train(self, labeled_sequences=None, unlabeled_sequences=None,\n              **kwargs):\n        assert labeled_sequences or unlabeled_sequences\n        model = None\n        if labeled_sequences:\n            model = self.train_supervised(labeled_sequences, **kwargs)\n        if unlabeled_sequences:\n            if model: kwargs['model'] = model\n            model = self.train_unsupervised(unlabeled_sequences, **kwargs)\n        return model\n\n\n    def _baum_welch_step(self, sequence, model, symbol_to_number):\n\n        N = len(model._states)\n        M = len(model._symbols)\n        T = len(sequence)\n\n        alpha = model._forward_probability(sequence)\n        beta = model._backward_probability(sequence)\n\n        lpk = logsumexp2(alpha[T-1])\n\n        A_numer = _ninf_array((N, N))\n        B_numer = _ninf_array((N, M))\n        A_denom = _ninf_array(N)\n        B_denom = _ninf_array(N)\n\n        transitions_logprob = model._transitions_matrix().T\n\n        for t in range(T):\n            symbol = sequence[t][_TEXT]  # not found? FIXME\n            next_symbol = None\n            if t < T - 1:\n                next_symbol = sequence[t+1][_TEXT]  # not found? FIXME\n            xi = symbol_to_number[symbol]\n\n            next_outputs_logprob = model._outputs_vector(next_symbol)\n            alpha_plus_beta = alpha[t] + beta[t]\n\n            if t < T - 1:\n                numer_add = transitions_logprob + next_outputs_logprob + \\\n                            beta[t+1] + alpha[t].reshape(N, 1)\n                A_numer = np.logaddexp2(A_numer, numer_add)\n                A_denom = np.logaddexp2(A_denom, alpha_plus_beta)\n            else:\n                B_denom = np.logaddexp2(A_denom, alpha_plus_beta)\n\n            B_numer[:,xi] = np.logaddexp2(B_numer[:,xi], alpha_plus_beta)\n\n        return lpk, A_numer, A_denom, B_numer, B_denom\n\n    def train_unsupervised(self, unlabeled_sequences, update_outputs=True,\n                           **kwargs):\n\n        model = kwargs.get('model')\n        if not model:\n            priors = RandomProbDist(self._states)\n            transitions = DictionaryConditionalProbDist(\n                            dict((state, RandomProbDist(self._states))\n                                  for state in self._states))\n            outputs = DictionaryConditionalProbDist(\n                            dict((state, RandomProbDist(self._symbols))\n                                  for state in self._states))\n            model = HiddenMarkovModelTagger(self._symbols, self._states,\n                            transitions, outputs, priors)\n\n        self._states = model._states\n        self._symbols = model._symbols\n\n        N = len(self._states)\n        M = len(self._symbols)\n        symbol_numbers = dict((sym, i) for i, sym in enumerate(self._symbols))\n\n\n        model._transitions = DictionaryConditionalProbDist(\n            dict((s, MutableProbDist(model._transitions[s], self._states))\n                 for s in self._states))\n\n        if update_outputs:\n            model._outputs = DictionaryConditionalProbDist(\n                dict((s, MutableProbDist(model._outputs[s], self._symbols))\n                     for s in self._states))\n\n        model.reset_cache()\n\n        converged = False\n        last_logprob = None\n        iteration = 0\n        max_iterations = kwargs.get('max_iterations', 1000)\n        epsilon = kwargs.get('convergence_logprob', 1e-6)\n\n        while not converged and iteration < max_iterations:\n            A_numer = _ninf_array((N, N))\n            B_numer = _ninf_array((N, M))\n            A_denom = _ninf_array(N)\n            B_denom = _ninf_array(N)\n\n            logprob = 0\n            for sequence in unlabeled_sequences:\n                sequence = list(sequence)\n                if not sequence:\n                    continue\n\n                (lpk, seq_A_numer, seq_A_denom,\n                seq_B_numer, seq_B_denom) = self._baum_welch_step(sequence, model, symbol_numbers)\n\n                for i in range(N):\n                    A_numer[i] = np.logaddexp2(A_numer[i], seq_A_numer[i]-lpk)\n                    B_numer[i] = np.logaddexp2(B_numer[i], seq_B_numer[i]-lpk)\n\n                A_denom = np.logaddexp2(A_denom, seq_A_denom-lpk)\n                B_denom = np.logaddexp2(B_denom, seq_B_denom-lpk)\n\n                logprob += lpk\n\n            for i in range(N):\n                logprob_Ai = A_numer[i] - A_denom[i]\n                logprob_Bi = B_numer[i] - B_denom[i]\n\n                logprob_Ai -= logsumexp2(logprob_Ai)\n                logprob_Bi -= logsumexp2(logprob_Bi)\n\n                si = self._states[i]\n\n                for j in range(N):\n                    sj = self._states[j]\n                    model._transitions[si].update(sj, logprob_Ai[j])\n\n                if update_outputs:\n                    for k in range(M):\n                        ok = self._symbols[k]\n                        model._outputs[si].update(ok, logprob_Bi[k])\n\n\n            if iteration > 0 and abs(logprob - last_logprob) < epsilon:\n                converged = True\n\n            print('iteration', iteration, 'logprob', logprob)\n            iteration += 1\n            last_logprob = logprob\n\n        return model\n\n    def train_supervised(self, labelled_sequences, estimator=None):\n\n        if estimator is None:\n            estimator = lambda fdist, bins: MLEProbDist(fdist)\n\n        known_symbols = set(self._symbols)\n        known_states = set(self._states)\n\n        starting = FreqDist()\n        transitions = ConditionalFreqDist()\n        outputs = ConditionalFreqDist()\n        for sequence in labelled_sequences:\n            lasts = None\n            for token in sequence:\n                state = token[_TAG]\n                symbol = token[_TEXT]\n                if lasts is None:\n                    starting[state] += 1\n                else:\n                    transitions[lasts][state] += 1\n                outputs[state][symbol] += 1\n                lasts = state\n\n                if state not in known_states:\n                    self._states.append(state)\n                    known_states.add(state)\n\n                if symbol not in known_symbols:\n                    self._symbols.append(symbol)\n                    known_symbols.add(symbol)\n\n        N = len(self._states)\n        pi = estimator(starting, N)\n        A = ConditionalProbDist(transitions, estimator, N)\n        B = ConditionalProbDist(outputs, estimator, len(self._symbols))\n\n        return HiddenMarkovModelTagger(self._symbols, self._states, A, B, pi)\n\n\ndef _ninf_array(shape):\n    res = np.empty(shape, np.float64)\n    res.fill(-np.inf)\n    return res\n\n\ndef logsumexp2(arr):\n    max_ = arr.max()\n    return np.log2(np.sum(2**(arr - max_))) + max_\n\n\ndef _log_add(*values):\n    x = max(values)\n    if x > -np.inf:\n        sum_diffs = 0\n        for value in values:\n            sum_diffs += 2**(value - x)\n        return x + np.log2(sum_diffs)\n    else:\n        return x\n\n\ndef _create_hmm_tagger(states, symbols, A, B, pi):\n    def pd(values, samples):\n        d = dict(zip(samples, values))\n        return DictionaryProbDist(d)\n\n    def cpd(array, conditions, samples):\n        d = {}\n        for values, condition in zip(array, conditions):\n            d[condition] = pd(values, samples)\n        return DictionaryConditionalProbDist(d)\n\n    A = cpd(A, states, states)\n    B = cpd(B, states, symbols)\n    pi = pd(pi, states)\n    return HiddenMarkovModelTagger(symbols=symbols, states=states,\n                                   transitions=A, outputs=B, priors=pi)\n\n\ndef _market_hmm_example():\n    states = ['bull', 'bear', 'static']\n    symbols = ['up', 'down', 'unchanged']\n    A = np.array([[0.6, 0.2, 0.2], [0.5, 0.3, 0.2], [0.4, 0.1, 0.5]], np.float64)\n    B = np.array([[0.7, 0.1, 0.2], [0.1, 0.6, 0.3], [0.3, 0.3, 0.4]], np.float64)\n    pi = np.array([0.5, 0.2, 0.3], np.float64)\n\n    model = _create_hmm_tagger(states, symbols, A, B, pi)\n    return model, states, symbols\n\n\ndef demo():\n\n    print()\n    print(\"HMM probability calculation demo\")\n    print()\n\n    model, states, symbols = _market_hmm_example()\n\n    print('Testing', model)\n\n    for test in [['up', 'up'], ['up', 'down', 'up'],\n                 ['down'] * 5, ['unchanged'] * 5 + ['up']]:\n\n        sequence = [(t, None) for t in test]\n\n        print('Testing with state sequence', test)\n        print('probability =', model.probability(sequence))\n        print('tagging =    ', model.tag([word for (word,tag) in sequence]))\n        print('p(tagged) =  ', model.probability(sequence))\n        print('H =          ', model.entropy(sequence))\n        print('H_exh =      ', model._exhaustive_entropy(sequence))\n        print('H(point) =   ', model.point_entropy(sequence))\n        print('H_exh(point)=', model._exhaustive_point_entropy(sequence))\n        print()\n\ndef load_pos(num_sents):\n    from nltk.corpus import brown\n\n    sentences = brown.tagged_sents(categories='news')[:num_sents]\n\n    tag_re = re.compile(r'[*]|--|[^+*-]+')\n    tag_set = set()\n    symbols = set()\n\n    cleaned_sentences = []\n    for sentence in sentences:\n        for i in range(len(sentence)):\n            word, tag = sentence[i]\n            word = word.lower()  # normalize\n            symbols.add(word)    # log this word\n            tag = tag_re.match(tag).group()\n            tag_set.add(tag)\n            sentence[i] = (word, tag)  # store cleaned-up tagged token\n        cleaned_sentences += [sentence]\n\n    return cleaned_sentences, list(tag_set), list(symbols)\n\ndef demo_pos():\n\n    print()\n    print(\"HMM POS tagging demo\")\n    print()\n\n    print('Training HMM...')\n    labelled_sequences, tag_set, symbols = load_pos(20000)\n    trainer = HiddenMarkovModelTrainer(tag_set, symbols)\n    hmm = trainer.train_supervised(labelled_sequences[10:],\n                    estimator=lambda fd, bins: LidstoneProbDist(fd, 0.1, bins))\n\n    print('Testing...')\n    hmm.test(labelled_sequences[:10], verbose=True)\n\ndef _untag(sentences):\n    unlabeled = []\n    for sentence in sentences:\n        unlabeled.append([(token[_TEXT], None) for token in sentence])\n    return unlabeled\n\ndef demo_pos_bw(test=10, supervised=20, unsupervised=10, verbose=True,\n                max_iterations=5):\n\n    print()\n    print(\"Baum-Welch demo for POS tagging\")\n    print()\n\n    print('Training HMM (supervised, %d sentences)...' % supervised)\n\n    sentences, tag_set, symbols = load_pos(test + supervised + unsupervised)\n\n    symbols = set()\n    for sentence in sentences:\n        for token in sentence:\n            symbols.add(token[_TEXT])\n\n    trainer = HiddenMarkovModelTrainer(tag_set, list(symbols))\n    hmm = trainer.train_supervised(sentences[test:test+supervised],\n                    estimator=lambda fd, bins: LidstoneProbDist(fd, 0.1, bins))\n\n    hmm.test(sentences[:test], verbose=verbose)\n\n    print('Training (unsupervised, %d sentences)...' % unsupervised)\n    unlabeled = _untag(sentences[test+supervised:])\n    hmm = trainer.train_unsupervised(unlabeled, model=hmm,\n                                     max_iterations=max_iterations)\n    hmm.test(sentences[:test], verbose=verbose)\n\ndef demo_bw():\n\n    print()\n    print(\"Baum-Welch demo for market example\")\n    print()\n\n    model, states, symbols = _market_hmm_example()\n\n    training = []\n    import random\n    rng = random.Random()\n    rng.seed(0)\n    for i in range(10):\n        item = model.random_sample(rng, 5)\n        training.append([(i[0], None) for i in item])\n\n    trainer = HiddenMarkovModelTrainer(states, symbols)\n    hmm = trainer.train_unsupervised(training, model=model,\n                                     max_iterations=1000)\n"], "nltk\\tag\\hunpos": [".py", "\n\nimport os\nfrom subprocess import Popen, PIPE\n\nfrom six import text_type\n\nfrom nltk.internals import find_binary, find_file\nfrom nltk.tag.api import TaggerI\n\n_hunpos_url = 'http://code.google.com/p/hunpos/'\n\n_hunpos_charset = 'ISO-8859-1'\n\nclass HunposTagger(TaggerI):\n\n    def __init__(self, path_to_model, path_to_bin=None,\n                 encoding=_hunpos_charset, verbose=False):\n        self._closed = True\n        hunpos_paths = ['.', '/usr/bin', '/usr/local/bin', '/opt/local/bin',\n                        '/Applications/bin', '~/bin', '~/Applications/bin']\n        hunpos_paths = list(map(os.path.expanduser, hunpos_paths))\n\n        self._hunpos_bin = find_binary(\n            'hunpos-tag', path_to_bin,\n            env_vars=('HUNPOS_TAGGER',),\n            searchpath=hunpos_paths,\n            url=_hunpos_url,\n            verbose=verbose\n        )\n\n        self._hunpos_model = find_file(\n            path_to_model, env_vars=('HUNPOS_TAGGER',), verbose=verbose)\n        self._encoding = encoding\n        self._hunpos = Popen([self._hunpos_bin, self._hunpos_model],\n                             shell=False, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n        self._closed = False\n\n    def __del__(self):\n        self.close()\n\n    def close(self):\n        if not self._closed:\n            self._hunpos.communicate()\n            self._closed = True\n\n    def __enter__(self):\n        return self\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n    def tag(self, tokens):\n        for token in tokens:\n            assert \"\\n\" not in token, \"Tokens should not contain newlines\"\n            if isinstance(token, text_type):\n                token = token.encode(self._encoding)\n            self._hunpos.stdin.write(token + b\"\\n\")\n        self._hunpos.stdin.write(b\"\\n\")\n        self._hunpos.stdin.flush()\n\n        tagged_tokens = []\n        for token in tokens:\n            tagged = self._hunpos.stdout.readline().strip().split(b\"\\t\")\n            tag = (tagged[1] if len(tagged) > 1 else None)\n            tagged_tokens.append((token, tag))\n        self._hunpos.stdout.readline()\n\n        return tagged_tokens\n\ndef setup_module(module):\n    from nose import SkipTest\n    try:\n        HunposTagger('en_wsj.model')\n    except LookupError:\n        raise SkipTest(\"HunposTagger is not available\")\n"], "nltk\\tag\\mapping": [".py", "\n\nfrom __future__ import print_function, unicode_literals, division\nfrom collections import defaultdict\nfrom os.path import join\n\nfrom nltk.data import load\n\n_UNIVERSAL_DATA = \"taggers/universal_tagset\"\n_UNIVERSAL_TAGS = ('VERB','NOUN','PRON','ADJ','ADV','ADP','CONJ','DET','NUM','PRT','X','.')\n\n_MAPPINGS = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 'UNK')))\n\n\ndef _load_universal_map(fileid):\n    contents = load(join(_UNIVERSAL_DATA, fileid+'.map'), format=\"text\")\n\n    _MAPPINGS[fileid]['universal'].default_factory = lambda: 'X'\n\n    for line in contents.splitlines():\n        line = line.strip()\n        if line == '':\n            continue\n        fine, coarse = line.split('\\t')\n\n        assert coarse in _UNIVERSAL_TAGS, 'Unexpected coarse tag: {}'.format(coarse)\n        assert fine not in _MAPPINGS[fileid]['universal'], 'Multiple entries for original tag: {}'.format(fine)\n\n        _MAPPINGS[fileid]['universal'][fine] = coarse\n\n\ndef tagset_mapping(source, target):\n\n    if source not in _MAPPINGS or target not in _MAPPINGS[source]:\n        if target == 'universal':\n            _load_universal_map(source)\n    return _MAPPINGS[source][target]\n\ndef map_tag(source, target, source_tag):\n\n    if target == 'universal':\n        if source == 'wsj':\n            source = 'en-ptb'\n        if source == 'brown':\n            source = 'en-brown'\n\n    return tagset_mapping(source, target)[source_tag]\n\n\n"], "nltk\\tag\\perceptron": [".py", "\nfrom __future__ import absolute_import\nfrom __future__ import print_function, division\n\nimport random\nfrom collections import defaultdict\nimport pickle\nimport logging\n\nfrom nltk.tag.api import TaggerI\nfrom nltk.data import find, load\nfrom nltk.compat import python_2_unicode_compatible\n\nPICKLE = \"averaged_perceptron_tagger.pickle\"\n\nclass AveragedPerceptron(object):\n\n    '''An averaged perceptron, as implemented by Matthew Honnibal.\n\n    See more implementation details here:\n        https://explosion.ai/blog/part-of-speech-pos-tagger-in-python\n    '''\n\n    def __init__(self):\n        self.weights = {}\n        self.classes = set()\n        self._totals = defaultdict(int)\n        self._tstamps = defaultdict(int)\n        self.i = 0\n\n    def predict(self, features):\n        '''Dot-product the features and current weights and return the best label.'''\n        scores = defaultdict(float)\n        for feat, value in features.items():\n            if feat not in self.weights or value == 0:\n                continue\n            weights = self.weights[feat]\n            for label, weight in weights.items():\n                scores[label] += value * weight\n        return max(self.classes, key=lambda label: (scores[label], label))\n\n    def update(self, truth, guess, features):\n        '''Update the feature weights.'''\n        def upd_feat(c, f, w, v):\n            param = (f, c)\n            self._totals[param] += (self.i - self._tstamps[param]) * w\n            self._tstamps[param] = self.i\n            self.weights[f][c] = w + v\n\n        self.i += 1\n        if truth == guess:\n            return None\n        for f in features:\n            weights = self.weights.setdefault(f, {})\n            upd_feat(truth, f, weights.get(truth, 0.0), 1.0)\n            upd_feat(guess, f, weights.get(guess, 0.0), -1.0)\n\n    def average_weights(self):\n        '''Average weights from all iterations.'''\n        for feat, weights in self.weights.items():\n            new_feat_weights = {}\n            for clas, weight in weights.items():\n                param = (feat, clas)\n                total = self._totals[param]\n                total += (self.i - self._tstamps[param]) * weight\n                averaged = round(total / self.i, 3)\n                if averaged:\n                    new_feat_weights[clas] = averaged\n            self.weights[feat] = new_feat_weights\n\n    def save(self, path):\n        '''Save the pickled model weights.'''\n        with open(path, 'wb') as fout:\n            return pickle.dump(dict(self.weights), fout)\n\n    def load(self, path):\n        '''Load the pickled model weights.'''\n        self.weights = load(path)\n\n@python_2_unicode_compatible\nclass PerceptronTagger(TaggerI):\n\n    '''\n    Greedy Averaged Perceptron tagger, as implemented by Matthew Honnibal.\n    See more implementation details here:\n        https://explosion.ai/blog/part-of-speech-pos-tagger-in-python\n    \n    >>> from nltk.tag.perceptron import PerceptronTagger\n\n    Train the model \n    \n    >>> tagger = PerceptronTagger(load=False)\n    \n    >>> tagger.train([[('today','NN'),('is','VBZ'),('good','JJ'),('day','NN')],\n    ... [('yes','NNS'),('it','PRP'),('beautiful','JJ')]])\n    \n    >>> tagger.tag(['today','is','a','beautiful','day'])\n    [('today', 'NN'), ('is', 'PRP'), ('a', 'PRP'), ('beautiful', 'JJ'), ('day', 'NN')]\n    \n    Use the pretrain model (the default constructor) \n    \n    >>> pretrain = PerceptronTagger()\n    \n    >>> pretrain.tag('The quick brown fox jumps over the lazy dog'.split())\n    [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n    \n    >>> pretrain.tag(\"The red cat\".split())\n    [('The', 'DT'), ('red', 'JJ'), ('cat', 'NN')]\n    '''\n\n    START = ['-START-', '-START2-']\n    END = ['-END-', '-END2-']\n    \n    def __init__(self, load=True):\n        '''\n        :param load: Load the pickled model upon instantiation.\n        '''\n        self.model = AveragedPerceptron()\n        self.tagdict = {}\n        self.classes = set()\n        if load:\n            AP_MODEL_LOC = 'file:'+str(find('taggers/averaged_perceptron_tagger/'+PICKLE))\n            self.load(AP_MODEL_LOC)\n\n    def tag(self, tokens):\n        '''\n        Tag tokenized sentences.\n        :params tokens: list of word\n        :type tokens: list(str)\n        '''\n        prev, prev2 = self.START\n        output = []\n        \n        context = self.START + [self.normalize(w) for w in tokens] + self.END\n        for i, word in enumerate(tokens):\n            tag = self.tagdict.get(word)\n            if not tag:\n                features = self._get_features(i, word, context, prev, prev2)\n                tag = self.model.predict(features)\n            output.append((word, tag))\n            prev2 = prev\n            prev = tag\n\n        return output\n\n    def train(self, sentences, save_loc=None, nr_iter=5):\n        '''Train a model from sentences, and save it at ``save_loc``. ``nr_iter``\n        controls the number of Perceptron training iterations.\n\n        :param sentences: A list or iterator of sentences, where each sentence\n            is a list of (words, tags) tuples.\n        :param save_loc: If not ``None``, saves a pickled model in this location.\n        :param nr_iter: Number of training iterations.\n        '''\n\n        self._sentences = list()  # to be populated by self._make_tagdict...\n        self._make_tagdict(sentences)\n        self.model.classes = self.classes\n        for iter_ in range(nr_iter):\n            c = 0\n            n = 0\n            for sentence in self._sentences:\n                words, tags = zip(*sentence)\n                \n                prev, prev2 = self.START\n                context = self.START + [self.normalize(w) for w in words] \\\n                                                                    + self.END\n                for i, word in enumerate(words):\n                    guess = self.tagdict.get(word)\n                    if not guess:\n                        feats = self._get_features(i, word, context, prev, prev2)\n                        guess = self.model.predict(feats)\n                        self.model.update(tags[i], guess, feats)\n                    prev2 = prev\n                    prev = guess\n                    c += guess == tags[i]\n                    n += 1\n            random.shuffle(self._sentences)\n            logging.info(\"Iter {0}: {1}/{2}={3}\".format(iter_, c, n, _pc(c, n)))\n\n        self._sentences = None\n\n        self.model.average_weights()\n        if save_loc is not None:\n            with open(save_loc, 'wb') as fout:\n                pickle.dump((self.model.weights, self.tagdict, self.classes), fout, 2)\n        \n\n    def load(self, loc):\n        '''\n        :param loc: Load a pickled model at location.\n        :type loc: str \n        '''\n\n        self.model.weights, self.tagdict, self.classes = load(loc)\n        self.model.classes = self.classes\n        \n\n    def normalize(self, word):\n        '''\n        Normalization used in pre-processing.\n        - All words are lower cased\n        - Groups of digits of length 4 are represented as !YEAR;\n        - Other digits are represented as !DIGITS\n\n        :rtype: str\n        '''\n        if '-' in word and word[0] != '-':\n            return '!HYPHEN'\n        elif word.isdigit() and len(word) == 4:\n            return '!YEAR'\n        elif word[0].isdigit():\n            return '!DIGITS'\n        else:\n            return word.lower()\n\n    def _get_features(self, i, word, context, prev, prev2):\n        '''Map tokens into a feature representation, implemented as a\n        {hashable: int} dict. If the features change, a new model must be\n        trained.\n        '''\n        def add(name, *args):\n            features[' '.join((name,) + tuple(args))] += 1\n\n        i += len(self.START)\n        features = defaultdict(int)\n        add('bias')\n        add('i suffix', word[-3:])\n        add('i pref1', word[0])\n        add('i-1 tag', prev)\n        add('i-2 tag', prev2)\n        add('i tag+i-2 tag', prev, prev2)\n        add('i word', context[i])\n        add('i-1 tag+i word', prev, context[i])\n        add('i-1 word', context[i-1])\n        add('i-1 suffix', context[i-1][-3:])\n        add('i-2 word', context[i-2])\n        add('i+1 word', context[i+1])\n        add('i+1 suffix', context[i+1][-3:])\n        add('i+2 word', context[i+2])\n        return features\n\n    def _make_tagdict(self, sentences):\n        '''\n        Make a tag dictionary for single-tag words.\n        :param sentences: A list of list of (word, tag) tuples.\n        '''\n        counts = defaultdict(lambda: defaultdict(int))\n        for sentence in sentences:\n            self._sentences.append(sentence)\n            for word, tag in sentence:\n                counts[word][tag] += 1\n                self.classes.add(tag)\n        freq_thresh = 20\n        ambiguity_thresh = 0.97\n        for word, tag_freqs in counts.items():\n            tag, mode = max(tag_freqs.items(), key=lambda item: item[1])\n            n = sum(tag_freqs.values())\n            if n >= freq_thresh and (mode / n) >= ambiguity_thresh:\n                self.tagdict[word] = tag\n\n\ndef _pc(n, d):\n    return (n / d) * 100\n\ndef _load_data_conll_format(filename):\n    print ('Read from file: ', filename)\n    with open(filename,'rb') as fin:\n        sentences = []\n        sentence = []\n        for line in fin.readlines():\n            line = line.strip()\n            if len(line) ==0:\n                sentences.append(sentence)\n                sentence = []\n                continue\n            tokens = line.split('\\t')\n            word = tokens[1]\n            tag = tokens[4]\n            sentence.append((word,tag)) \n        return sentences\n\ndef _get_pretrain_model():\n    tagger = PerceptronTagger()\n    training = _load_data_conll_format('english_ptb_train.conll')\n    testing = _load_data_conll_format('english_ptb_test.conll')\n    print ('Size of training and testing (sentence)', len(training), len(testing))\n    tagger.train(training, PICKLE) \n    print ('Accuracy : ',tagger.evaluate(testing))\n    \nif __name__ == '__main__':\n    pass\n"], "nltk\\tag\\senna": [".py", "\n\nfrom nltk.compat import python_2_unicode_compatible\nfrom nltk.classify import Senna\n\n@python_2_unicode_compatible\nclass SennaTagger(Senna):\n    def __init__(self, path, encoding='utf-8'):\n        super(SennaTagger, self).__init__(path, ['pos'], encoding)\n\n    def tag_sents(self, sentences):\n        tagged_sents = super(SennaTagger, self).tag_sents(sentences)\n        for i in range(len(tagged_sents)):\n            for j in range(len(tagged_sents[i])):\n                annotations = tagged_sents[i][j]\n                tagged_sents[i][j] = (annotations['word'], annotations['pos'])\n        return tagged_sents\n\n@python_2_unicode_compatible\nclass SennaChunkTagger(Senna):\n    def __init__(self, path, encoding='utf-8'):\n        super(SennaChunkTagger, self).__init__(path, ['chk'], encoding)\n\n    def tag_sents(self, sentences):\n        tagged_sents = super(SennaChunkTagger, self).tag_sents(sentences)\n        for i in range(len(tagged_sents)):\n            for j in range(len(tagged_sents[i])):\n                annotations = tagged_sents[i][j]\n                tagged_sents[i][j] = (annotations['word'], annotations['chk'])\n        return tagged_sents\n\n    def bio_to_chunks(self, tagged_sent, chunk_type):\n        current_chunk = []\n        current_chunk_position = []\n        for idx, word_pos in enumerate(tagged_sent):\n            word, pos = word_pos\n            if '-'+chunk_type in pos: # Append the word to the current_chunk.\n                current_chunk.append((word))\n                current_chunk_position.append((idx))\n            else:\n                if current_chunk: # Flush the full chunk when out of an NP.\n                    _chunk_str = ' '.join(current_chunk)\n                    _chunk_pos_str = '-'.join(map(str, current_chunk_position))\n                    yield _chunk_str, _chunk_pos_str\n                    current_chunk = []\n                    current_chunk_position = []\n        if current_chunk: # Flush the last chunk.\n            yield ' '.join(current_chunk), '-'.join(map(str, current_chunk_position))\n\n\n@python_2_unicode_compatible\nclass SennaNERTagger(Senna):\n    def __init__(self, path, encoding='utf-8'):\n        super(SennaNERTagger, self).__init__(path, ['ner'], encoding)\n\n    def tag_sents(self, sentences):\n        tagged_sents = super(SennaNERTagger, self).tag_sents(sentences)\n        for i in range(len(tagged_sents)):\n            for j in range(len(tagged_sents[i])):\n                annotations = tagged_sents[i][j]\n                tagged_sents[i][j] = (annotations['word'], annotations['ner'])\n        return tagged_sents\n\n\n\ndef setup_module(module):\n    from nose import SkipTest\n    try:\n        tagger = Senna('/usr/share/senna-v3.0', ['pos', 'chk', 'ner'])\n    except OSError:\n        raise SkipTest(\"Senna executable not found\")\n\n"], "nltk\\tag\\sequential": [".py", "\nfrom __future__ import print_function, unicode_literals\nfrom abc import abstractmethod\n\nimport re\n\nfrom nltk.probability import ConditionalFreqDist\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.compat import python_2_unicode_compatible\n\nfrom nltk.tag.api import TaggerI, FeaturesetTaggerI\n\nfrom nltk import jsontags\n\n\nclass SequentialBackoffTagger(TaggerI):\n    def __init__(self, backoff=None):\n        if backoff is None:\n            self._taggers = [self]\n        else:\n            self._taggers = [self] + backoff._taggers\n\n    @property\n    def backoff(self):\n        return self._taggers[1] if len(self._taggers) > 1 else None\n\n    def tag(self, tokens):\n        tags = []\n        for i in range(len(tokens)):\n            tags.append(self.tag_one(tokens, i, tags))\n        return list(zip(tokens, tags))\n\n    def tag_one(self, tokens, index, history):\n        tag = None\n        for tagger in self._taggers:\n            tag = tagger.choose_tag(tokens, index, history)\n            if tag is not None:\n                break\n        return tag\n\n    @abstractmethod\n    def choose_tag(self, tokens, index, history):\n\n\n@python_2_unicode_compatible\nclass ContextTagger(SequentialBackoffTagger):\n    def __init__(self, context_to_tag, backoff=None):\n        SequentialBackoffTagger.__init__(self, backoff)\n        self._context_to_tag = (context_to_tag if context_to_tag else {})\n\n    @abstractmethod\n    def context(self, tokens, index, history):\n\n    def choose_tag(self, tokens, index, history):\n        context = self.context(tokens, index, history)\n        return self._context_to_tag.get(context)\n\n    def size(self):\n        return len(self._context_to_tag)\n\n    def __repr__(self):\n        return '<%s: size=%d>' % (self.__class__.__name__, self.size())\n\n    def _train(self, tagged_corpus, cutoff=0, verbose=False):\n\n        token_count = hit_count = 0\n\n        useful_contexts = set()\n\n        fd = ConditionalFreqDist()\n        for sentence in tagged_corpus:\n            tokens, tags = zip(*sentence)\n            for index, (token, tag) in enumerate(sentence):\n                token_count += 1\n                context = self.context(tokens, index, tags[:index])\n                if context is None:\n                    continue\n                fd[context][tag] += 1\n                if (self.backoff is None or\n                        tag != self.backoff.tag_one(\n                        tokens, index, tags[:index])):\n                    useful_contexts.add(context)\n\n        for context in useful_contexts:\n            best_tag = fd[context].max()\n            hits = fd[context][best_tag]\n            if hits > cutoff:\n                self._context_to_tag[context] = best_tag\n                hit_count += hits\n\n        if verbose:\n            size = len(self._context_to_tag)\n            backoff = 100 - (hit_count * 100.0) / token_count\n            pruning = 100 - (size * 100.0) / len(fd.conditions())\n            print(\"[Trained Unigram tagger:\", end=' ')\n            print(\"size=%d, backoff=%.2f%%, pruning=%.2f%%]\" % (\n                size, backoff, pruning))\n\n\n@python_2_unicode_compatible\n@jsontags.register_tag\nclass DefaultTagger(SequentialBackoffTagger):\n\n    json_tag = 'nltk.tag.sequential.DefaultTagger'\n\n    def __init__(self, tag):\n        self._tag = tag\n        SequentialBackoffTagger.__init__(self, None)\n\n    def encode_json_obj(self):\n        return self._tag\n\n    @classmethod\n    def decode_json_obj(cls, obj):\n        tag = obj\n        return cls(tag)\n\n    def choose_tag(self, tokens, index, history):\n        return self._tag  # ignore token and history\n\n    def __repr__(self):\n        return '<DefaultTagger: tag=%s>' % self._tag\n\n\n@jsontags.register_tag\nclass NgramTagger(ContextTagger):\n    json_tag = 'nltk.tag.sequential.NgramTagger'\n\n    def __init__(self, n, train=None, model=None,\n                 backoff=None, cutoff=0, verbose=False):\n        self._n = n\n        self._check_params(train, model)\n\n        ContextTagger.__init__(self, model, backoff)\n\n        if train:\n            self._train(train, cutoff, verbose)\n\n    def encode_json_obj(self):\n        return self._n, self._context_to_tag, self.backoff\n\n    @classmethod\n    def decode_json_obj(cls, obj):\n        _n, _context_to_tag, backoff = obj\n        return cls(_n, model=_context_to_tag, backoff=backoff)\n\n    def context(self, tokens, index, history):\n        tag_context = tuple(history[max(0, index-self._n+1):index])\n        return tag_context, tokens[index]\n\n\n@jsontags.register_tag\nclass UnigramTagger(NgramTagger):\n\n    json_tag = 'nltk.tag.sequential.UnigramTagger'\n\n    def __init__(self, train=None, model=None,\n                 backoff=None, cutoff=0, verbose=False):\n        NgramTagger.__init__(self, 1, train, model,\n                             backoff, cutoff, verbose)\n\n    def encode_json_obj(self):\n        return self._context_to_tag, self.backoff\n\n    @classmethod\n    def decode_json_obj(cls, obj):\n        _context_to_tag, backoff = obj\n        return cls(model=_context_to_tag, backoff=backoff)\n\n    def context(self, tokens, index, history):\n        return tokens[index]\n\n\n@jsontags.register_tag\nclass BigramTagger(NgramTagger):\n    json_tag = 'nltk.tag.sequential.BigramTagger'\n\n    def __init__(self, train=None, model=None,\n                 backoff=None, cutoff=0, verbose=False):\n        NgramTagger.__init__(self, 2, train, model,\n                             backoff, cutoff, verbose)\n\n    def encode_json_obj(self):\n        return self._context_to_tag, self.backoff\n\n    @classmethod\n    def decode_json_obj(cls, obj):\n        _context_to_tag, backoff = obj\n        return cls(model=_context_to_tag, backoff=backoff)\n\n\n@jsontags.register_tag\nclass TrigramTagger(NgramTagger):\n    json_tag = 'nltk.tag.sequential.TrigramTagger'\n\n    def __init__(self, train=None, model=None,\n                 backoff=None, cutoff=0, verbose=False):\n        NgramTagger.__init__(self, 3, train, model,\n                             backoff, cutoff, verbose)\n\n    def encode_json_obj(self):\n        return self._context_to_tag, self.backoff\n\n    @classmethod\n    def decode_json_obj(cls, obj):\n        _context_to_tag, backoff = obj\n        return cls(model=_context_to_tag, backoff=backoff)\n\n\n@jsontags.register_tag\nclass AffixTagger(ContextTagger):\n\n    json_tag = 'nltk.tag.sequential.AffixTagger'\n\n    def __init__(self, train=None, model=None, affix_length=-3,\n                 min_stem_length=2, backoff=None, cutoff=0, verbose=False):\n\n        self._check_params(train, model)\n\n        ContextTagger.__init__(self, model, backoff)\n\n        self._affix_length = affix_length\n        self._min_word_length = min_stem_length + abs(affix_length)\n\n        if train:\n            self._train(train, cutoff, verbose)\n\n    def encode_json_obj(self):\n        return self._affix_length, self._min_word_length, self._context_to_tag, self.backoff\n\n    @classmethod\n    def decode_json_obj(cls, obj):\n        _affix_length, _min_word_length, _context_to_tag, backoff = obj\n        return cls(\n            affix_length=_affix_length,\n            min_stem_length=_min_word_length - abs(_affix_length),\n            model=_context_to_tag,\n            backoff=backoff\n        )\n\n    def context(self, tokens, index, history):\n        token = tokens[index]\n        if len(token) < self._min_word_length:\n            return None\n        elif self._affix_length > 0:\n            return token[:self._affix_length]\n        else:\n            return token[self._affix_length:]\n\n\n@python_2_unicode_compatible\n@jsontags.register_tag\nclass RegexpTagger(SequentialBackoffTagger):\n\n    json_tag = 'nltk.tag.sequential.RegexpTagger'\n\n    def __init__(self, regexps, backoff=None):\n        SequentialBackoffTagger.__init__(self, backoff)\n        self._regexs = [(re.compile(regexp), tag,) for regexp, tag in regexps]\n\n    def encode_json_obj(self):\n        return [(regexp.patten, tag,) for regexp, tag in self._regexs], self.backoff\n\n    @classmethod\n    def decode_json_obj(cls, obj):\n        regexps, backoff = obj\n        self = cls(())\n        self._regexs = [(re.compile(regexp), tag,) for regexp, tag in regexps]\n        SequentialBackoffTagger.__init__(self, backoff)\n        return self\n\n    def choose_tag(self, tokens, index, history):\n        for regexp, tag in self._regexs:\n            if re.match(regexp, tokens[index]):\n                return tag\n        return None\n\n    def __repr__(self):\n        return '<Regexp Tagger: size=%d>' % len(self._regexs)\n\n\n@python_2_unicode_compatible\nclass ClassifierBasedTagger(SequentialBackoffTagger, FeaturesetTaggerI):\n    def __init__(self, feature_detector=None, train=None,\n                 classifier_builder=NaiveBayesClassifier.train,\n                 classifier=None, backoff=None,\n                 cutoff_prob=None, verbose=False):\n        self._check_params(train, classifier)\n\n        SequentialBackoffTagger.__init__(self, backoff)\n\n        if (train and classifier) or (not train and not classifier):\n            raise ValueError('Must specify either training data or '\n                             'trained classifier.')\n\n        if feature_detector is not None:\n            self._feature_detector = feature_detector\n\n        self._cutoff_prob = cutoff_prob\n        Build a new classifier, based on the given training data\n        *tagged_corpus*.\n        \"\"\"\n\n        classifier_corpus = []\n        if verbose:\n            print('Constructing training corpus for classifier.')\n\n        for sentence in tagged_corpus:\n            history = []\n            untagged_sentence, tags = zip(*sentence)\n            for index in range(len(sentence)):\n                featureset = self.feature_detector(untagged_sentence,\n                                                   index, history)\n                classifier_corpus.append((featureset, tags[index]))\n                history.append(tags[index])\n\n        if verbose:\n            print('Training classifier (%d instances)' % len(classifier_corpus))\n        self._classifier = classifier_builder(classifier_corpus)\n\n    def __repr__(self):\n        return '<ClassifierBasedTagger: %r>' % self._classifier\n\n    def feature_detector(self, tokens, index, history):\n        \"\"\"\n        Return the feature detector that this tagger uses to generate\n        featuresets for its classifier.  The feature detector is a\n        function with the signature::\n\n          feature_detector(tokens, index, history) -> featureset\n\n        See ``classifier()``\n        \"\"\"\n        return self._feature_detector(tokens, index, history)\n\n    def classifier(self):\n        \"\"\"\n        Return the classifier that this tagger uses to choose a tag\n        for each word in a sentence.  The input for this classifier is\n        generated using this tagger's feature detector.\n        See ``feature_detector()``\n        \"\"\"\n        return self._classifier\n\n\nclass ClassifierBasedPOSTagger(ClassifierBasedTagger):\n    \"\"\"\n    A classifier based part of speech tagger.\n    \"\"\"\n    def feature_detector(self, tokens, index, history):\n        word = tokens[index]\n        if index == 0:\n            prevword = prevprevword = None\n            prevtag = prevprevtag = None\n        elif index == 1:\n            prevword = tokens[index-1].lower()\n            prevprevword = None\n            prevtag = history[index-1]\n            prevprevtag = None\n        else:\n            prevword = tokens[index-1].lower()\n            prevprevword = tokens[index-2].lower()\n            prevtag = history[index-1]\n            prevprevtag = history[index-2]\n\n        if re.match('[0-9]+(\\.[0-9]*)?|[0-9]*\\.[0-9]+$', word):\n            shape = 'number'\n        elif re.match('\\W+$', word):\n            shape = 'punct'\n        elif re.match('[A-Z][a-z]+$', word):\n            shape = 'upcase'\n        elif re.match('[a-z]+$', word):\n            shape = 'downcase'\n        elif re.match('\\w+$', word):\n            shape = 'mixedcase'\n        else:\n            shape = 'other'\n\n        features = {\n            'prevtag': prevtag,\n            'prevprevtag': prevprevtag,\n            'word': word,\n            'word.lower': word.lower(),\n            'suffix3': word.lower()[-3:],\n            'suffix2': word.lower()[-2:],\n            'suffix1': word.lower()[-1:],\n            'prevprevword': prevprevword,\n            'prevword': prevword,\n            'prevtag+word': '%s+%s' % (prevtag, word.lower()),\n            'prevprevtag+word': '%s+%s' % (prevprevtag, word.lower()),\n            'prevword+word': '%s+%s' % (prevword, word.lower()),\n            'shape': shape,\n            }\n        return features\n"], "nltk\\tag\\stanford": [".py", "\n\nfrom abc import abstractmethod\nimport os\nimport tempfile\nfrom subprocess import PIPE\nimport warnings\n\nfrom six import text_type\n\nfrom nltk.internals import find_file, find_jar, config_java, java, _java_options\nfrom nltk.tag.api import TaggerI\n\n_stanford_url = 'https://nlp.stanford.edu/software'\n\n\nclass StanfordTagger(TaggerI):\n\n    _SEPARATOR = ''\n    _JAR = ''\n\n    def __init__(self, model_filename, path_to_jar=None, encoding='utf8',\n                 verbose=False, java_options='-mx1000m'):\n        warnings.warn(str(\"\\nThe StanfordTokenizer will \"\n                          \"be deprecated in version 3.2.6.\\n\"\n                          \"Please use \\033[91mnltk.parse.corenlp.CoreNLPParser\\033[0m instead.\"),\n                      DeprecationWarning, stacklevel=2)\n\n        if not self._JAR:\n            warnings.warn('The StanfordTagger class is not meant to be '\n                          'instantiated directly. Did you mean '\n                          'StanfordPOSTagger or StanfordNERTagger?')\n        self._stanford_jar = find_jar(\n                self._JAR, path_to_jar,\n                searchpath=(), url=_stanford_url,\n                verbose=verbose)\n\n        self._stanford_model = find_file(model_filename,\n                                         env_vars=('STANFORD_MODELS',),\n                                         verbose=verbose)\n\n        self._encoding = encoding\n        self.java_options = java_options\n\n    @property\n    @abstractmethod\n    def _cmd(self):\n\n    def tag(self, tokens):\n        return sum(self.tag_sents([tokens]), [])\n\n    def tag_sents(self, sentences):\n        encoding = self._encoding\n        default_options = ' '.join(_java_options)\n        config_java(options=self.java_options, verbose=False)\n\n        _input_fh, self._input_file_path = tempfile.mkstemp(text=True)\n\n        cmd = list(self._cmd)\n        cmd.extend(['-encoding', encoding])\n\n        _input_fh = os.fdopen(_input_fh, 'wb')\n        _input = '\\n'.join((' '.join(x) for x in sentences))\n        if isinstance(_input, text_type) and encoding:\n            _input = _input.encode(encoding)\n        _input_fh.write(_input)\n        _input_fh.close()\n\n        stanpos_output, _stderr = java(cmd, classpath=self._stanford_jar,\n                                       stdout=PIPE, stderr=PIPE)\n        stanpos_output = stanpos_output.decode(encoding)\n\n        os.unlink(self._input_file_path)\n\n        config_java(options=default_options, verbose=False)\n\n        return self.parse_output(stanpos_output, sentences)\n\n    def parse_output(self, text, sentences=None):\n        tagged_sentences = []\n        for tagged_sentence in text.strip().split(\"\\n\"):\n            sentence = []\n            for tagged_word in tagged_sentence.strip().split():\n                word_tags = tagged_word.strip().split(self._SEPARATOR)\n                sentence.append((''.join(word_tags[:-1]), word_tags[-1]))\n            tagged_sentences.append(sentence)\n        return tagged_sentences\n\n\nclass StanfordPOSTagger(StanfordTagger):\n    _SEPARATOR = '_'\n    _JAR = 'stanford-postagger.jar'\n\n    def __init__(self, *args, **kwargs):\n        super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n\n    @property\n    def _cmd(self):\n        return ['edu.stanford.nlp.tagger.maxent.MaxentTagger',\n                '-model', self._stanford_model, '-textFile',\n                self._input_file_path, '-tokenize', 'false',\n                '-outputFormatOptions', 'keepEmptySentences']\n\n\nclass StanfordNERTagger(StanfordTagger):\n\n    _SEPARATOR = '/'\n    _JAR = 'stanford-ner.jar'\n    _FORMAT = 'slashTags'\n\n    def __init__(self, *args, **kwargs):\n        super(StanfordNERTagger, self).__init__(*args, **kwargs)\n\n    @property\n    def _cmd(self):\n        return ['edu.stanford.nlp.ie.crf.CRFClassifier',\n                '-loadClassifier', self._stanford_model, '-textFile',\n                self._input_file_path, '-outputFormat', self._FORMAT,\n                '-tokenizerFactory',\n                'edu.stanford.nlp.process.WhitespaceTokenizer',\n                '-tokenizerOptions', '\\\"tokenizeNLs=false\\\"']\n\n    def parse_output(self, text, sentences):\n        if self._FORMAT == 'slashTags':\n            tagged_sentences = []\n            for tagged_sentence in text.strip().split(\"\\n\"):\n                for tagged_word in tagged_sentence.strip().split():\n                    word_tags = tagged_word.strip().split(self._SEPARATOR)\n                    tagged_sentences.append((''.join(word_tags[:-1]),\n                                             word_tags[-1]))\n\n            result = []\n            start = 0\n            for sent in sentences:\n                result.append(tagged_sentences[start:start + len(sent)])\n                start += len(sent)\n            return result\n\n        raise NotImplementedError\n\ndef setup_module(module):\n    from nose import SkipTest\n\n    try:\n        StanfordPOSTagger('english-bidirectional-distsim.tagger')\n    except LookupError:\n        raise SkipTest('Doctests from nltk.tag.stanford are skipped because one \\\n                       of the stanford jars cannot be found.')\n"], "nltk\\tag\\tnt": [".py", "\n'''\nImplementation of 'TnT - A Statisical Part of Speech Tagger'\nby Thorsten Brants\n\nhttp://acl.ldc.upenn.edu/A/A00/A00-1031.pdf\n'''\nfrom __future__ import print_function, division\nfrom math import log\n\nfrom operator import itemgetter\n\nfrom nltk.probability import FreqDist, ConditionalFreqDist\nfrom nltk.tag.api import TaggerI\n\nclass TnT(TaggerI):\n    '''\n    TnT - Statistical POS tagger\n\n    IMPORTANT NOTES:\n\n    * DOES NOT AUTOMATICALLY DEAL WITH UNSEEN WORDS\n\n      - It is possible to provide an untrained POS tagger to\n        create tags for unknown words, see __init__ function\n\n    * SHOULD BE USED WITH SENTENCE-DELIMITED INPUT\n\n      - Due to the nature of this tagger, it works best when\n        trained over sentence delimited input.\n      - However it still produces good results if the training\n        data and testing data are separated on all punctuation eg: [,.?!]\n      - Input for training is expected to be a list of sentences\n        where each sentence is a list of (word, tag) tuples\n      - Input for tag function is a single sentence\n        Input for tagdata function is a list of sentences\n        Output is of a similar form\n\n    * Function provided to process text that is unsegmented\n\n      - Please see basic_sent_chop()\n\n\n    TnT uses a second order Markov model to produce tags for\n    a sequence of input, specifically:\n\n      argmax [Proj(P(t_i|t_i-1,t_i-2)P(w_i|t_i))] P(t_T+1 | t_T)\n\n    IE: the maximum projection of a set of probabilities\n\n    The set of possible tags for a given word is derived\n    from the training data. It is the set of all tags\n    that exact word has been assigned.\n\n    To speed up and get more precision, we can use log addition\n    to instead multiplication, specifically:\n\n      argmax [Sigma(log(P(t_i|t_i-1,t_i-2))+log(P(w_i|t_i)))] +\n             log(P(t_T+1|t_T))\n\n    The probability of a tag for a given word is the linear\n    interpolation of 3 markov models; a zero-order, first-order,\n    and a second order model.\n\n      P(t_i| t_i-1, t_i-2) = l1*P(t_i) + l2*P(t_i| t_i-1) +\n                             l3*P(t_i| t_i-1, t_i-2)\n\n    A beam search is used to limit the memory usage of the algorithm.\n    The degree of the beam can be changed using N in the initialization.\n    N represents the maximum number of possible solutions to maintain\n    while tagging.\n\n    It is possible to differentiate the tags which are assigned to\n    capitalized words. However this does not result in a significant\n    gain in the accuracy of the results.\n    '''\n\n    def __init__(self, unk=None, Trained=False, N=1000, C=False):\n        '''\n        Construct a TnT statistical tagger. Tagger must be trained\n        before being used to tag input.\n\n        :param unk: instance of a POS tagger, conforms to TaggerI\n        :type  unk:(TaggerI)\n        :param Trained: Indication that the POS tagger is trained or not\n        :type  Trained: boolean\n        :param N: Beam search degree (see above)\n        :type  N:(int)\n        :param C: Capitalization flag\n        :type  C: boolean\n\n        Initializer, creates frequency distributions to be used\n        for tagging\n\n        _lx values represent the portion of the tri/bi/uni taggers\n        to be used to calculate the probability\n\n        N value is the number of possible solutions to maintain\n        while tagging. A good value for this is 1000\n\n        C is a boolean value which specifies to use or\n        not use the Capitalization of the word as additional\n        information for tagging.\n        NOTE: using capitalization may not increase the accuracy\n        of the tagger\n        '''\n\n        self._uni  = FreqDist()\n        self._bi   = ConditionalFreqDist()\n        self._tri  = ConditionalFreqDist()\n        self._wd   = ConditionalFreqDist()\n        self._eos  = ConditionalFreqDist()\n        self._l1   = 0.0\n        self._l2   = 0.0\n        self._l3   = 0.0\n        self._N    = N\n        self._C    = C\n        self._T    = Trained\n\n        self._unk = unk\n\n        self.unknown = 0\n        self.known = 0\n\n    def train(self, data):\n        '''\n        Uses a set of tagged data to train the tagger.\n        If an unknown word tagger is specified,\n        it is trained on the same data.\n\n        :param data: List of lists of (word, tag) tuples\n        :type data: tuple(str)\n        '''\n\n        C = False\n\n        if self._unk is not None and self._T == False:\n            self._unk.train(data)\n\n        for sent in data:\n            history = [('BOS',False), ('BOS',False)]\n            for w, t in sent:\n\n                if self._C and w[0].isupper(): C=True\n\n                self._wd[w][t] += 1\n                self._uni[(t,C)] += 1\n                self._bi[history[1]][(t,C)] += 1\n                self._tri[tuple(history)][(t,C)] += 1\n\n                history.append((t,C))\n                history.pop(0)\n\n                C = False\n\n            self._eos[t]['EOS'] += 1\n\n\n        self._compute_lambda()\n\n\n\n    def _compute_lambda(self):\n        '''\n        creates lambda values based upon training data\n\n        NOTE: no need to explicitly reference C,\n        it is contained within the tag variable :: tag == (tag,C)\n\n        for each tag trigram (t1, t2, t3)\n        depending on the maximum value of\n        - f(t1,t2,t3)-1 / f(t1,t2)-1\n        - f(t2,t3)-1 / f(t2)-1\n        - f(t3)-1 / N-1\n\n        increment l3,l2, or l1 by f(t1,t2,t3)\n\n        ISSUES -- Resolutions:\n        if 2 values are equal, increment both lambda values\n        by (f(t1,t2,t3) / 2)\n        '''\n\n        tl1 = 0.0\n        tl2 = 0.0\n        tl3 = 0.0\n\n        for history in self._tri.conditions():\n            (h1, h2) = history\n\n            for tag in self._tri[history].keys():\n\n                if self._uni[tag] == 1:\n                    continue\n\n                c3 = self._safe_div((self._tri[history][tag]-1), (self._tri[history].N()-1))\n                c2 = self._safe_div((self._bi[h2][tag]-1), (self._bi[h2].N()-1))\n                c1 = self._safe_div((self._uni[tag]-1), (self._uni.N()-1))\n\n\n                if (c1 > c3) and (c1 > c2):\n                    tl1 += self._tri[history][tag]\n\n                elif (c2 > c3) and (c2 > c1):\n                    tl2 += self._tri[history][tag]\n\n                elif (c3 > c2) and (c3 > c1):\n                    tl3 += self._tri[history][tag]\n\n                elif (c3 == c2) and (c3 > c1):\n                    tl2 += self._tri[history][tag] / 2.0\n                    tl3 += self._tri[history][tag] / 2.0\n\n                elif (c2 == c1) and (c1 > c3):\n                    tl1 += self._tri[history][tag] / 2.0\n                    tl2 += self._tri[history][tag] / 2.0\n\n                else:\n                    pass\n\n        self._l1 = tl1 / (tl1+tl2+tl3)\n        self._l2 = tl2 / (tl1+tl2+tl3)\n        self._l3 = tl3 / (tl1+tl2+tl3)\n\n\n\n    def _safe_div(self, v1, v2):\n        '''\n        Safe floating point division function, does not allow division by 0\n        returns -1 if the denominator is 0\n        '''\n        if v2 == 0:\n            return -1\n        else:\n            return v1 / v2\n\n    def tagdata(self, data):\n        '''\n        Tags each sentence in a list of sentences\n\n        :param data:list of list of words\n        :type data: [[string,],]\n        :return: list of list of (word, tag) tuples\n\n        Invokes tag(sent) function for each sentence\n        compiles the results into a list of tagged sentences\n        each tagged sentence is a list of (word, tag) tuples\n        '''\n        res = []\n        for sent in data:\n            res1 = self.tag(sent)\n            res.append(res1)\n        return res\n\n\n    def tag(self, data):\n        '''\n        Tags a single sentence\n\n        :param data: list of words\n        :type data: [string,]\n\n        :return: [(word, tag),]\n\n        Calls recursive function '_tagword'\n        to produce a list of tags\n\n        Associates the sequence of returned tags\n        with the correct words in the input sequence\n\n        returns a list of (word, tag) tuples\n        '''\n\n        current_state = [(['BOS', 'BOS'], 0.0)]\n\n        sent = list(data)\n\n        tags = self._tagword(sent, current_state)\n\n        res = []\n        for i in range(len(sent)):\n            (t,C) = tags[i+2]\n            res.append((sent[i], t))\n\n        return res\n\n\n    def _tagword(self, sent, current_states):\n        '''\n        :param sent : List of words remaining in the sentence\n        :type sent  : [word,]\n        :param current_states : List of possible tag combinations for\n                                the sentence so far, and the log probability\n                                associated with each tag combination\n        :type current_states  : [([tag, ], logprob), ]\n\n        Tags the first word in the sentence and\n        recursively tags the reminder of sentence\n\n        Uses formula specified above to calculate the probability\n        of a particular tag\n        '''\n\n        if sent == []:\n            (h, logp) = current_states[0]\n            return h\n\n        word = sent[0]\n        sent = sent[1:]\n        new_states = []\n\n        C = False\n        if self._C and word[0].isupper(): C=True\n\n        if word in self._wd:\n            self.known += 1\n\n            for (history, curr_sent_logprob) in current_states:\n                logprobs = []\n\n                for t in self._wd[word].keys():\n                    tC = (t,C)\n                    p_uni = self._uni.freq(tC)\n                    p_bi = self._bi[history[-1]].freq(tC)\n                    p_tri = self._tri[tuple(history[-2:])].freq(tC)\n                    p_wd = self._wd[word][t] / self._uni[tC]\n                    p = self._l1 *p_uni + self._l2 *p_bi + self._l3 *p_tri\n                    p2 = log(p, 2) + log(p_wd, 2)\n\n                    new_states.append((history + [tC],\n                                       curr_sent_logprob + p2))\n\n        else:\n            self.unknown += 1\n\n            p = 1\n\n            if self._unk is None:\n                tag = ('Unk',C)\n\n            else:\n                [(_w, t)] = list(self._unk.tag([word]))\n                tag = (t,C)\n\n            for (history, logprob) in current_states:\n                history.append(tag)\n\n            new_states = current_states\n\n\n        new_states.sort(reverse=True, key=itemgetter(1))\n\n        if len(new_states) > self._N:\n            new_states = new_states[:self._N]\n\n        return self._tagword(sent, new_states)\n\n\n\ndef basic_sent_chop(data, raw=True):\n    '''\n    Basic method for tokenizing input into sentences\n    for this tagger:\n\n    :param data: list of tokens (words or (word, tag) tuples)\n    :type data: str or tuple(str, str)\n    :param raw: boolean flag marking the input data\n                as a list of words or a list of tagged words\n    :type raw: bool\n    :return: list of sentences\n             sentences are a list of tokens\n             tokens are the same as the input\n\n    Function takes a list of tokens and separates the tokens into lists\n    where each list represents a sentence fragment\n    This function can separate both tagged and raw sequences into\n    basic sentences.\n\n    Sentence markers are the set of [,.!?]\n\n    This is a simple method which enhances the performance of the TnT\n    tagger. Better sentence tokenization will further enhance the results.\n    '''\n\n    new_data = []\n    curr_sent = []\n    sent_mark = [',','.','?','!']\n\n\n    if raw:\n        for word in data:\n            if word in sent_mark:\n                curr_sent.append(word)\n                new_data.append(curr_sent)\n                curr_sent = []\n            else:\n                curr_sent.append(word)\n\n    else:\n        for (word,tag) in data:\n            if word in sent_mark:\n                curr_sent.append((word,tag))\n                new_data.append(curr_sent)\n                curr_sent = []\n            else:\n                curr_sent.append((word,tag))\n    return new_data\n\n\n\ndef demo():\n    from nltk.corpus import brown\n    sents = list(brown.tagged_sents())\n    test = list(brown.sents())\n\n    tagger = TnT()\n    tagger.train(sents[200:1000])\n\n    tagged_data = tagger.tagdata(test[100:120])\n\n    for j in range(len(tagged_data)):\n        s = tagged_data[j]\n        t = sents[j+100]\n        for i in range(len(s)):\n            print(s[i],'--', t[i])\n        print()\n\n\ndef demo2():\n    from nltk.corpus import treebank\n\n    d = list(treebank.tagged_sents())\n\n    t = TnT(N=1000, C=False)\n    s = TnT(N=1000, C=True)\n    t.train(d[(11)*100:])\n    s.train(d[(11)*100:])\n\n    for i in range(10):\n        tacc = t.evaluate(d[i*100:((i+1)*100)])\n        tp_un = t.unknown / (t.known + t.unknown)\n        tp_kn = t.known / (t.known + t.unknown)\n        t.unknown = 0\n        t.known = 0\n\n        print('Capitalization off:')\n        print('Accuracy:', tacc)\n        print('Percentage known:', tp_kn)\n        print('Percentage unknown:', tp_un)\n        print('Accuracy over known words:', (tacc / tp_kn))\n\n        sacc = s.evaluate(d[i*100:((i+1)*100)])\n        sp_un = s.unknown / (s.known + s.unknown)\n        sp_kn = s.known / (s.known + s.unknown)\n        s.unknown = 0\n        s.known = 0\n\n        print('Capitalization on:')\n        print('Accuracy:', sacc)\n        print('Percentage known:', sp_kn)\n        print('Percentage unknown:', sp_un)\n        print('Accuracy over known words:', (sacc / sp_kn))\n\ndef demo3():\n    from nltk.corpus import treebank, brown\n\n    d = list(treebank.tagged_sents())\n    e = list(brown.tagged_sents())\n\n    d = d[:1000]\n    e = e[:1000]\n\n    d10 = int(len(d)*0.1)\n    e10 = int(len(e)*0.1)\n\n    tknacc = 0\n    sknacc = 0\n    tallacc = 0\n    sallacc = 0\n    tknown = 0\n    sknown = 0\n\n    for i in range(10):\n\n        t = TnT(N=1000, C=False)\n        s = TnT(N=1000, C=False)\n\n        dtest = d[(i*d10):((i+1)*d10)]\n        etest = e[(i*e10):((i+1)*e10)]\n\n        dtrain = d[:(i*d10)] + d[((i+1)*d10):]\n        etrain = e[:(i*e10)] + e[((i+1)*e10):]\n\n        t.train(dtrain)\n        s.train(etrain)\n\n        tacc = t.evaluate(dtest)\n        tp_un = t.unknown / (t.known + t.unknown)\n        tp_kn = t.known / (t.known + t.unknown)\n        tknown += tp_kn\n        t.unknown = 0\n        t.known = 0\n\n        sacc = s.evaluate(etest)\n        sp_un = s.unknown / (s.known + s.unknown)\n        sp_kn = s.known / (s.known + s.unknown)\n        sknown += sp_kn\n        s.unknown = 0\n        s.known = 0\n\n        tknacc += (tacc / tp_kn)\n        sknacc += (sacc / tp_kn)\n        tallacc += tacc\n        sallacc += sacc\n\n\n\n    print(\"brown: acc over words known:\", 10 * tknacc)\n    print(\"     : overall accuracy:\", 10 * tallacc)\n    print(\"     : words known:\", 10 * tknown)\n    print(\"treebank: acc over words known:\", 10 * sknacc)\n    print(\"        : overall accuracy:\", 10 * sallacc)\n    print(\"        : words known:\", 10 * sknown)\n\n\n\n\n"], "nltk\\tag\\util": [".py", "\ndef str2tuple(s, sep='/'):\n    loc = s.rfind(sep)\n    if loc >= 0:\n        return (s[:loc], s[loc+len(sep):].upper())\n    else:\n        return (s, None)\n\ndef tuple2str(tagged_token, sep='/'):\n    word, tag = tagged_token\n    if tag is None:\n        return word\n    else:\n        assert sep not in tag, 'tag may not contain sep!'\n        return '%s%s%s' % (word, sep, tag)\n\ndef untag(tagged_sentence):\n    return [w for (w, t) in tagged_sentence]\n\n\n\n"], "nltk\\tag\\__init__": [".py", "from __future__ import print_function\n\nfrom nltk.tag.api           import TaggerI\nfrom nltk.tag.util          import str2tuple, tuple2str, untag\nfrom nltk.tag.sequential    import (SequentialBackoffTagger, ContextTagger,\n                                    DefaultTagger, NgramTagger, UnigramTagger,\n                                    BigramTagger, TrigramTagger, AffixTagger,\n                                    RegexpTagger, ClassifierBasedTagger,\n                                    ClassifierBasedPOSTagger)\nfrom nltk.tag.brill         import BrillTagger\nfrom nltk.tag.brill_trainer import BrillTaggerTrainer\nfrom nltk.tag.tnt           import TnT\nfrom nltk.tag.hunpos        import HunposTagger\nfrom nltk.tag.stanford      import StanfordTagger, StanfordPOSTagger, StanfordNERTagger\nfrom nltk.tag.hmm           import HiddenMarkovModelTagger, HiddenMarkovModelTrainer\nfrom nltk.tag.senna         import SennaTagger, SennaChunkTagger, SennaNERTagger\nfrom nltk.tag.mapping       import tagset_mapping, map_tag\nfrom nltk.tag.crf           import CRFTagger\nfrom nltk.tag.perceptron    import PerceptronTagger\n\nfrom nltk.data import load, find\n\nRUS_PICKLE = 'taggers/averaged_perceptron_tagger_ru/averaged_perceptron_tagger_ru.pickle'\n\n\ndef _get_tagger(lang=None):\n    if lang == 'rus':\n        tagger = PerceptronTagger(False)\n        ap_russian_model_loc = 'file:' + str(find(RUS_PICKLE))\n        tagger.load(ap_russian_model_loc)\n    else:\n        tagger = PerceptronTagger()\n    return tagger\n\n\ndef _pos_tag(tokens, tagset, tagger):\n    tagged_tokens = tagger.tag(tokens)\n    if tagset:\n        tagged_tokens = [(token, map_tag('en-ptb', tagset, tag)) for (token, tag) in tagged_tokens]\n    return tagged_tokens\n\n\ndef pos_tag(tokens, tagset=None, lang='eng'):\n    tagger = _get_tagger(lang)\n    return _pos_tag(tokens, tagset, tagger)    \n\n\ndef pos_tag_sents(sentences, tagset=None, lang='eng'):\n    tagger = _get_tagger(lang)\n    return [_pos_tag(sent, tagset, tagger) for sent in sentences]\n", 1], "nltk\\tbl\\api": [".py", "\n"], "nltk\\tbl\\demo": [".py", "\nfrom __future__ import print_function, absolute_import, division\nimport os\nimport pickle\n\nimport random\nimport time\n\nfrom nltk.corpus import treebank\n\nfrom nltk.tbl import error_list, Template\nfrom nltk.tag.brill import Word, Pos\nfrom nltk.tag import BrillTaggerTrainer, RegexpTagger, UnigramTagger\n\ndef demo():\n    postag()\n\ndef demo_repr_rule_format():\n    postag(ruleformat=\"repr\")\n\ndef demo_str_rule_format():\n    postag(ruleformat=\"str\")\n\ndef demo_verbose_rule_format():\n    postag(ruleformat=\"verbose\")\n\ndef demo_multiposition_feature():\n    postag(templates=[Template(Pos([-3,-2,-1]))])\n\ndef demo_multifeature_template():\n    postag(templates=[Template(Word([0]), Pos([-2,-1]))])\n\ndef demo_template_statistics():\n    postag(incremental_stats=True, template_stats=True)\n\ndef demo_generated_templates():\n    wordtpls = Word.expand([-1,0,1], [1,2], excludezero=False)\n    tagtpls = Pos.expand([-2,-1,0,1], [1,2], excludezero=True)\n    templates = list(Template.expand([wordtpls, tagtpls], combinations=(1,3)))\n    print(\"Generated {0} templates for transformation-based learning\".format(len(templates)))\n    postag(templates=templates, incremental_stats=True, template_stats=True)\n\ndef demo_learning_curve():\n    postag(incremental_stats=True, separate_baseline_data=True, learning_curve_output=\"learningcurve.png\")\n\ndef demo_error_analysis():\n    postag(error_output=\"errors.txt\")\n\ndef demo_serialize_tagger():\n    postag(serialize_output=\"tagger.pcl\")\n\ndef demo_high_accuracy_rules():\n    postag(num_sents=3000, min_acc=0.96, min_score=10)\n\ndef postag(\n    templates=None,\n    tagged_data=None,\n    num_sents=1000,\n    max_rules=300,\n    min_score=3,\n    min_acc=None,\n    train=0.8,\n    trace=3,\n    randomize=False,\n    ruleformat=\"str\",\n    incremental_stats=False,\n    template_stats=False,\n    error_output=None,\n    serialize_output=None,\n    learning_curve_output=None,\n    learning_curve_take=300,\n    baseline_backoff_tagger=None,\n    separate_baseline_data=False,\n    cache_baseline_tagger=None):\n\n    baseline_backoff_tagger = baseline_backoff_tagger or REGEXP_TAGGER\n    if templates is None:\n        from nltk.tag.brill import describe_template_sets, brill24\n        templates = brill24()\n    (training_data, baseline_data, gold_data, testing_data) = \\\n       _demo_prepare_data(tagged_data, train, num_sents, randomize, separate_baseline_data)\n\n    if cache_baseline_tagger:\n        if not os.path.exists(cache_baseline_tagger):\n            baseline_tagger = UnigramTagger(baseline_data, backoff=baseline_backoff_tagger)\n            with open(cache_baseline_tagger, 'w') as print_rules:\n                pickle.dump(baseline_tagger, print_rules)\n            print(\"Trained baseline tagger, pickled it to {0}\".format(cache_baseline_tagger))\n        with open(cache_baseline_tagger, \"r\") as print_rules:\n            baseline_tagger= pickle.load(print_rules)\n            print(\"Reloaded pickled tagger from {0}\".format(cache_baseline_tagger))\n    else:\n        baseline_tagger = UnigramTagger(baseline_data, backoff=baseline_backoff_tagger)\n        print(\"Trained baseline tagger\")\n    if gold_data:\n        print(\"    Accuracy on test set: {0:0.4f}\".format(baseline_tagger.evaluate(gold_data)))\n\n    tbrill = time.time()\n    trainer = BrillTaggerTrainer(baseline_tagger, templates, trace, ruleformat=ruleformat)\n    print(\"Training tbl tagger...\")\n    brill_tagger = trainer.train(training_data, max_rules, min_score, min_acc)\n    print(\"Trained tbl tagger in {0:0.2f} seconds\".format(time.time() - tbrill))\n    if gold_data:\n        print(\"    Accuracy on test set: %.4f\" % brill_tagger.evaluate(gold_data))\n\n    if trace == 1:\n        print(\"\\nLearned rules: \")\n        for (ruleno, rule) in enumerate(brill_tagger.rules(),1):\n            print(\"{0:4d} {1:s}\".format(ruleno, rule.format(ruleformat)))\n\n\n    if  incremental_stats:\n        print(\"Incrementally tagging the test data, collecting individual rule statistics\")\n        (taggedtest, teststats) = brill_tagger.batch_tag_incremental(testing_data, gold_data)\n        print(\"    Rule statistics collected\")\n        if not separate_baseline_data:\n            print(\"WARNING: train_stats asked for separate_baseline_data=True; the baseline \"\n                  \"will be artificially high\")\n        trainstats = brill_tagger.train_stats()\n        if template_stats:\n            brill_tagger.print_template_statistics(teststats)\n        if learning_curve_output:\n            _demo_plot(learning_curve_output, teststats, trainstats, take=learning_curve_take)\n            print(\"Wrote plot of learning curve to {0}\".format(learning_curve_output))\n    else:\n        print(\"Tagging the test data\")\n        taggedtest = brill_tagger.tag_sents(testing_data)\n        if template_stats:\n            brill_tagger.print_template_statistics()\n\n    if error_output is not None:\n        with open(error_output, 'w') as f:\n            f.write('Errors for Brill Tagger %r\\n\\n' % serialize_output)\n            f.write(u'\\n'.join(error_list(gold_data, taggedtest)).encode('utf-8') + '\\n')\n        print(\"Wrote tagger errors including context to {0}\".format(error_output))\n\n    if serialize_output is not None:\n        taggedtest = brill_tagger.tag_sents(testing_data)\n        with open(serialize_output, 'w') as print_rules:\n            pickle.dump(brill_tagger, print_rules)\n        print(\"Wrote pickled tagger to {0}\".format(serialize_output))\n        with open(serialize_output, \"r\") as print_rules:\n            brill_tagger_reloaded = pickle.load(print_rules)\n        print(\"Reloaded pickled tagger from {0}\".format(serialize_output))\n        taggedtest_reloaded = brill_tagger.tag_sents(testing_data)\n        if taggedtest == taggedtest_reloaded:\n            print(\"Reloaded tagger tried on test set, results identical\")\n        else:\n            print(\"PROBLEM: Reloaded tagger gave different results on test set\")\n\ndef _demo_prepare_data(tagged_data, train, num_sents, randomize, separate_baseline_data):\n    if tagged_data is None:\n        print(\"Loading tagged data from treebank... \")\n        tagged_data = treebank.tagged_sents()\n    if num_sents is None or len(tagged_data) <= num_sents:\n        num_sents = len(tagged_data)\n    if randomize:\n        random.seed(len(tagged_data))\n        random.shuffle(tagged_data)\n    cutoff = int(num_sents * train)\n    training_data = tagged_data[:cutoff]\n    gold_data = tagged_data[cutoff:num_sents]\n    testing_data = [[t[0] for t in sent] for sent in gold_data]\n    if not separate_baseline_data:\n        baseline_data = training_data\n    else:\n        bl_cutoff = len(training_data) // 3\n        (baseline_data, training_data) = (training_data[:bl_cutoff], training_data[bl_cutoff:])\n    (trainseqs, traintokens) = corpus_size(training_data)\n    (testseqs, testtokens) = corpus_size(testing_data)\n    (bltrainseqs, bltraintokens) = corpus_size(baseline_data)\n    print(\"Read testing data ({0:d} sents/{1:d} wds)\".format(testseqs, testtokens))\n    print(\"Read training data ({0:d} sents/{1:d} wds)\".format(trainseqs, traintokens))\n    print(\"Read baseline data ({0:d} sents/{1:d} wds) {2:s}\".format(\n        bltrainseqs, bltraintokens, \"\" if separate_baseline_data else \"[reused the training set]\"))\n    return (training_data, baseline_data, gold_data, testing_data)\n\n\ndef _demo_plot(learning_curve_output, teststats, trainstats=None, take=None):\n   testcurve = [teststats['initialerrors']]\n   for rulescore in teststats['rulescores']:\n       testcurve.append(testcurve[-1] - rulescore)\n   testcurve = [1 - x/teststats['tokencount'] for x in testcurve[:take]]\n\n   traincurve = [trainstats['initialerrors']]\n   for rulescore in trainstats['rulescores']:\n       traincurve.append(traincurve[-1] - rulescore)\n   traincurve = [1 - x/trainstats['tokencount'] for x in traincurve[:take]]\n\n   import matplotlib.pyplot as plt\n   r = list(range(len(testcurve)))\n   plt.plot(r, testcurve, r, traincurve)\n   plt.axis([None, None, None, 1.0])\n   plt.savefig(learning_curve_output)\n\n\nNN_CD_TAGGER = RegexpTagger(\n    [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),\n     (r'.*', 'NN')])\n\nREGEXP_TAGGER = RegexpTagger(\n    [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers\n     (r'(The|the|A|a|An|an)$', 'AT'),   # articles\n     (r'.*able$', 'JJ'),                # adjectives\n     (r'.*ness$', 'NN'),                # nouns formed from adjectives\n     (r'.*ly$', 'RB'),                  # adverbs\n     (r'.*s$', 'NNS'),                  # plural nouns\n     (r'.*ing$', 'VBG'),                # gerunds\n     (r'.*ed$', 'VBD'),                 # past tense verbs\n     (r'.*', 'NN')                      # nouns (default)\n])\n\n\ndef corpus_size(seqs):\n    return (len(seqs), sum(len(x) for x in seqs))\n\nif __name__ == '__main__':\n    demo_learning_curve()\n"], "nltk\\tbl\\erroranalysis": [".py", "\nfrom __future__ import print_function\n\n\n\ndef error_list(train_sents, test_sents):\n    hdr = (('%25s | %s | %s\\n' + '-'*26+'+'+'-'*24+'+'+'-'*26) %\n           ('left context', 'word/test->gold'.center(22), 'right context'))\n    errors = [hdr]\n    for (train_sent, test_sent) in zip(train_sents, test_sents):\n        for wordnum, (word, train_pos) in enumerate(train_sent):\n            test_pos = test_sent[wordnum][1]\n            if train_pos != test_pos:\n                left = ' '.join('%s/%s' % w for w in train_sent[:wordnum])\n                right = ' '.join('%s/%s' % w for w in train_sent[wordnum+1:])\n                mid = '%s/%s->%s' % (word, test_pos, train_pos)\n                errors.append('%25s | %s | %s' %\n                              (left[-25:], mid.center(22), right[:25]))\n\n    return errors\n"], "nltk\\tbl\\feature": [".py", "\nfrom __future__ import division, print_function, unicode_literals\nfrom abc import ABCMeta, abstractmethod\nfrom six import add_metaclass\n\n\n@add_metaclass(ABCMeta)\nclass Feature(object):\n\n    json_tag = 'nltk.tbl.Feature'\n    PROPERTY_NAME = None\n\n    def __init__(self, positions, end=None):\n        self.positions = None  # to avoid warnings\n        if end is None:\n            self.positions = tuple(sorted(set([int(i) for i in positions])))\n        else:                # positions was actually not a list, but only the start index\n            try:\n                if positions > end:\n                    raise TypeError\n                self.positions = tuple(range(positions, end+1))\n            except TypeError:\n                raise ValueError(\"illegal interval specification: (start={0}, end={1})\".format(positions, end))\n\n        self.PROPERTY_NAME = self.__class__.PROPERTY_NAME or self.__class__.__name__\n\n    def encode_json_obj(self):\n        return self.positions\n\n    @classmethod\n    def decode_json_obj(cls, obj):\n        positions = obj\n        return cls(positions)\n\n    def __repr__(self):\n        return \"%s(%r)\" % (\n            self.__class__.__name__, list(self.positions))\n\n    @classmethod\n    def expand(cls, starts, winlens, excludezero=False):\n        if not all(x > 0 for x in winlens):\n            raise ValueError(\"non-positive window length in {0}\".format(winlens))\n        xs = (starts[i:i+w] for w in winlens for i in range(len(starts)-w+1))\n        return [cls(x) for x in xs if not (excludezero and 0 in x)]\n\n    def issuperset(self, other):\n        return self.__class__ is other.__class__ and set(self.positions) >= set(other.positions)\n\n    def intersects(self, other):\n\n        return bool((self.__class__ is other.__class__ and set(self.positions) & set(other.positions)))\n\n    def __eq__(self, other):\n        return (self.__class__ is other.__class__ and self.positions == other.positions)\n\n    def __lt__(self, other):\n        return (\n            self.__class__.__name__ < other.__class__.__name__ or\n            self.positions < other.positions\n        )\n\n    def __ne__(self, other):\n        return not (self == other)\n\n    def __gt__(self, other):\n        return other < self\n\n    def __ge__(self, other):\n        return not self < other\n\n    def __le__(self, other):\n        return self < other or self == other\n\n    @staticmethod\n    @abstractmethod\n    def extract_property(tokens, index):\n"], "nltk\\tbl\\rule": [".py", "\nfrom __future__ import print_function\nfrom abc import ABCMeta, abstractmethod\nfrom six import add_metaclass\n\nfrom nltk.compat import python_2_unicode_compatible, unicode_repr\nfrom nltk import jsontags\n\n\n@add_metaclass(ABCMeta)\nclass TagRule(object):\n\n    def __init__(self, original_tag, replacement_tag):\n\n        self.original_tag = original_tag\n\n        self.replacement_tag = replacement_tag\n\n    def apply(self, tokens, positions=None):\n        if positions is None:\n            positions = list(range(len(tokens)))\n\n        change = [i for i in positions if self.applies(tokens, i)]\n\n        for i in change:\n            tokens[i] = (tokens[i][0], self.replacement_tag)\n\n        return change\n\n    @abstractmethod\n    def applies(self, tokens, index):\n\n    def __eq__(self, other):\n        raise TypeError(\"Rules must implement __eq__()\")\n\n    def __ne__(self, other):\n        raise TypeError(\"Rules must implement __ne__()\")\n\n    def __hash__(self):\n        raise TypeError(\"Rules must implement __hash__()\")\n\n\n@python_2_unicode_compatible\n@jsontags.register_tag\nclass Rule(TagRule):\n\n    json_tag = 'nltk.tbl.Rule'\n\n    def __init__(self, templateid, original_tag, replacement_tag, conditions):\n        TagRule.__init__(self, original_tag, replacement_tag)\n        self._conditions = conditions\n        self.templateid = templateid\n\n    def encode_json_obj(self):\n        return {\n            'templateid':   self.templateid,\n            'original':     self.original_tag,\n            'replacement':  self.replacement_tag,\n            'conditions':   self._conditions,\n        }\n\n    @classmethod\n    def decode_json_obj(cls, obj):\n        return cls(obj['templateid'], obj['original'], obj['replacement'], obj['conditions'])\n\n    def applies(self, tokens, index):\n\n        if tokens[index][1] != self.original_tag:\n            return False\n\n        for (feature, val) in self._conditions:\n\n            for pos in feature.positions:\n                if not (0 <= index + pos < len(tokens)):\n                    continue\n                if feature.extract_property(tokens, index+pos) == val:\n                    break\n            else:\n                return False\n\n        return True\n\n    def __eq__(self, other):\n        return (self is other or\n                (other is not None and\n                 other.__class__ == self.__class__ and\n                 self.original_tag == other.original_tag and\n                 self.replacement_tag == other.replacement_tag and\n                 self._conditions == other._conditions))\n\n    def __ne__(self, other):\n        return not (self == other)\n\n    def __hash__(self):\n\n        try:\n            return self.__hash\n        except AttributeError:\n            self.__hash = hash(repr(self))\n            return self.__hash\n\n    def __repr__(self):\n        try:\n            return self.__repr\n        except AttributeError:\n            self.__repr = (\n                \"{0}('{1}', {2}, {3}, [{4}])\".format(\n                    self.__class__.__name__,\n                    self.templateid,\n                    unicode_repr(self.original_tag),\n                    unicode_repr(self.replacement_tag),\n\n                    ', '.join(\"({0},{1})\".format(f, unicode_repr(v)) for (f, v) in self._conditions)\n                )\n            )\n\n            return self.__repr\n\n    def __str__(self):\n        def _condition_to_logic(feature, value):\n            return '{0}:{1}@[{2}]'.format(\n                feature.PROPERTY_NAME,\n                value,\n                \",\".join(str(w) for w in feature.positions)\n            )\n\n        conditions = ' & '.join([_condition_to_logic(f, v) for (f, v) in self._conditions])\n        s = '{0}->{1} if {2}'.format(\n            self.original_tag,\n            self.replacement_tag,\n            conditions\n        )\n\n        return s\n\n    def format(self, fmt):\n        if fmt == \"str\":\n            return self.__str__()\n        elif fmt == \"repr\":\n            return self.__repr__()\n        elif fmt == \"verbose\":\n            return self._verbose_format()\n        else:\n            raise ValueError(\"unknown rule format spec: {0}\".format(fmt))\n\n    def _verbose_format(self):\n        def condition_to_str(feature, value):\n            return ('the %s of %s is \"%s\"' %\n                    (feature.PROPERTY_NAME, range_to_str(feature.positions), value))\n\n        def range_to_str(positions):\n            if len(positions) == 1:\n                p = positions[0]\n                if p == 0:\n                    return 'this word'\n                if p == -1:\n                    return 'the preceding word'\n                elif p == 1:\n                    return 'the following word'\n                elif p < 0:\n                    return 'word i-%d' % -p\n                elif p > 0:\n                    return 'word i+%d' % p\n            else:\n                mx = max(positions)\n                mn = min(positions)\n                if mx - mn == len(positions) - 1:\n                    return 'words i%+d...i%+d' % (mn, mx)\n                else:\n                    return 'words {%s}' % (\",\".join(\"i%+d\" % d for d in positions),)\n\n        replacement = '%s -> %s' % (self.original_tag, self.replacement_tag)\n        conditions = (' if ' if self._conditions else \"\") + ', and '.join(\n            condition_to_str(f, v) for (f, v) in self._conditions\n        )\n        return replacement + conditions\n"], "nltk\\tbl\\template": [".py", "\nfrom __future__ import print_function\nfrom abc import ABCMeta, abstractmethod\nfrom six import add_metaclass\nimport itertools as it\nfrom nltk.tbl.feature import Feature\nfrom nltk.tbl.rule import Rule\n\n\n@add_metaclass(ABCMeta)\nclass BrillTemplateI(object):\n    @abstractmethod\n    def applicable_rules(self, tokens, i, correctTag):\n\n    @abstractmethod\n    def get_neighborhood(self, token, index):\n\n\nclass Template(BrillTemplateI):\n    ALLTEMPLATES = []\n\n    def __init__(self, *features):\n\n        if all(isinstance(f, Feature) for f in features):\n            self._features = features\n        elif issubclass(features[0], Feature) and all(isinstance(a, tuple) for a in features[1:]):\n            self._features = [features[0](*tp) for tp in features[1:]]\n        else:\n            raise TypeError(\n                \"expected either Feature1(args), Feature2(args), ... or Feature, (start1, end1), (start2, end2), ...\")\n        self.id = \"{0:03d}\".format(len(self.ALLTEMPLATES))\n        self.ALLTEMPLATES.append(self)\n\n    def __repr__(self):\n        return \"%s(%s)\" % (self.__class__.__name__, \",\".join([str(f) for f in self._features]))\n\n    def applicable_rules(self, tokens, index, correct_tag):\n        if tokens[index][1] == correct_tag:\n            return []\n\n\n        applicable_conditions = self._applicable_conditions(tokens, index)\n        xs = list(it.product(*applicable_conditions))\n        return [Rule(self.id, tokens[index][1], correct_tag, tuple(x)) for x in xs]\n\n    def _applicable_conditions(self, tokens, index):\n        conditions = []\n\n        for feature in self._features:\n            conditions.append([])\n            for pos in feature.positions:\n                if not (0 <= index+pos < len(tokens)):\n                    continue\n                value = feature.extract_property(tokens, index+pos)\n                conditions[-1].append( (feature, value) )\n        return conditions\n\n    def get_neighborhood(self, tokens, index):\n\n        neighborhood = set([index])  #set literal for python 2.7+\n\n\n        allpositions = [0] + [p for feat in self._features for p in feat.positions]\n        start, end = min(allpositions), max(allpositions)\n        s = max(0, index+(-end))\n        e = min(index+(-start)+1, len(tokens))\n        for i in range(s, e):\n            neighborhood.add(i)\n        return neighborhood\n\n    @classmethod\n    def expand(cls, featurelists, combinations=None, skipintersecting=True):\n\n        def nonempty_powerset(xs): #xs is a list\n\n            k = combinations #for brevity\n            combrange = ((1, len(xs)+1) if k is None else     # n over 1 .. n over n (all non-empty combinations)\n                         (k, k+1) if isinstance(k, int) else  # n over k (only\n                         (k[0], k[1]+1))                      # n over k1, n over k1+1... n over k2\n            return it.chain.from_iterable(it.combinations(xs, r)\n                                          for r in range(*combrange))\n        seentemplates = set()\n        for picks in nonempty_powerset(featurelists):\n            for pick in it.product(*picks):\n                if any(i != j and x.issuperset(y)\n                       for (i, x) in enumerate(pick)\n                       for (j, y) in enumerate(pick)):\n                    continue\n                if skipintersecting and any(i != j and x.intersects(y)\n                                            for (i, x) in enumerate(pick)\n                                            for (j, y) in enumerate(pick)):\n                    continue\n                thistemplate = cls(*sorted(pick))\n                strpick = str(thistemplate)\n                if strpick in seentemplates: #already added\n                    cls._poptemplate()\n                    continue\n                seentemplates.add(strpick)\n                yield thistemplate\n\n    @classmethod\n    def _cleartemplates(cls):\n        cls.ALLTEMPLATES = []\n\n    @classmethod\n    def _poptemplate(cls):\n        return cls.ALLTEMPLATES.pop() if cls.ALLTEMPLATES else None\n"], "nltk\\tbl\\__init__": [".py", "\n\nfrom nltk.tbl.template import Template\n\nfrom nltk.tbl.feature import Feature\n\nfrom nltk.tbl.rule import Rule\n\nfrom nltk.tbl.erroranalysis import error_list\n\n", 1], "nltk\\test\\all": [".py", "import doctest, unittest\nfrom glob import glob\nimport os.path\n\n\ndef additional_tests():\n    dir = os.path.dirname(__file__)\n    paths = glob(os.path.join(dir, '*.doctest'))\n    files = [os.path.basename(path) for path in paths]\n    return unittest.TestSuite(\n        [doctest.DocFileSuite(file) for file in files]\n    )\n"], "nltk\\test\\childes_fixt": [".py", "from __future__ import absolute_import\n\n\ndef setup_module(module):\n    from nose import SkipTest\n    import nltk.data\n    try:\n        nltk.data.find('corpora/childes/data-xml/Eng-USA-MOR/')\n    except LookupError as e:\n        print(e)\n        raise SkipTest(\"The CHILDES corpus is not found. \"\n                       \"It should be manually downloaded and saved/unpacked \"\n                       \"to [NLTK_Data_Dir]/corpora/childes/\")\n"], "nltk\\test\\classify_fixt": [".py", "from __future__ import absolute_import\n\n\ndef setup_module(module):\n    from nose import SkipTest\n    try:\n        import numpy\n    except ImportError:\n        raise SkipTest(\"classify.doctest requires numpy\")\n"], "nltk\\test\\compat_fixt": [".py", "from __future__ import absolute_import\nfrom nltk.compat import PY3\n\n\ndef setup_module(module):\n    from nose import SkipTest\n    if PY3:\n        raise SkipTest(\"compat.doctest is for Python 2.x\")\n"], "nltk\\test\\corpus_fixt": [".py", "from __future__ import absolute_import\n\nfrom nltk.corpus import teardown_module"], "nltk\\test\\discourse_fixt": [".py", "from __future__ import absolute_import\n\n\ndef setup_module(module):\n    from nose import SkipTest\n    from nltk.inference.mace import Mace\n    try:\n        m = Mace()\n        m._find_binary('mace4')\n    except LookupError:\n        raise SkipTest(\"Mace4/Prover9 is not available so discourse.doctest is skipped\")\n"], "nltk\\test\\doctest_nose_plugin": [".py", "from __future__ import print_function\nfrom nose.suite import ContextList\nimport re\nimport sys\nimport os\nimport codecs\nimport doctest\nfrom nose.plugins.base import Plugin\nfrom nose.util import tolist, anyp\nfrom nose.plugins.doctests import Doctest, log, DocFileCase\n\nALLOW_UNICODE = doctest.register_optionflag('ALLOW_UNICODE')\n\n\nclass _UnicodeOutputChecker(doctest.OutputChecker):\n    _literal_re = re.compile(r\"(\\W|^)[uU]([rR]?[\\'\\\"])\", re.UNICODE)\n\n    def _remove_u_prefixes(self, txt):\n        return re.sub(self._literal_re, r'\\1\\2', txt)\n\n    def check_output(self, want, got, optionflags):\n        res = doctest.OutputChecker.check_output(self, want, got, optionflags)\n        if res:\n            return True\n        if not (optionflags & ALLOW_UNICODE):\n            return False\n\n        cleaned_want = self._remove_u_prefixes(want)\n        cleaned_got = self._remove_u_prefixes(got)\n        res = doctest.OutputChecker.check_output(self, cleaned_want, cleaned_got, optionflags)\n        return res\n\n\n_checker = _UnicodeOutputChecker()\n\n\nclass DoctestPluginHelper(object):\n    OPTION_BY_NAME = ('doctestencoding',)\n\n    def loadTestsFromFileUnicode(self, filename):\n        if self.extension and anyp(filename.endswith, self.extension):\n            name = os.path.basename(filename)\n            dh = codecs.open(filename, 'r', self.options.get('doctestencoding'))\n            try:\n                doc = dh.read()\n            finally:\n                dh.close()\n\n            fixture_context = None\n            globs = {'__file__': filename}\n            if self.fixtures:\n                base, ext = os.path.splitext(name)\n                dirname = os.path.dirname(filename)\n                sys.path.append(dirname)\n                fixt_mod = base + self.fixtures\n                try:\n                    fixture_context = __import__(\n                        fixt_mod, globals(), locals(), [\"nop\"])\n                except ImportError as e:\n                    log.debug(\n                        \"Could not import %s: %s (%s)\", fixt_mod, e, sys.path)\n                log.debug(\"Fixture module %s resolved to %s\",\n                          fixt_mod, fixture_context)\n                if hasattr(fixture_context, 'globs'):\n                    globs = fixture_context.globs(globs)\n            parser = doctest.DocTestParser()\n            test = parser.get_doctest(\n                doc, globs=globs, name=name,\n                filename=filename, lineno=0)\n            if test.examples:\n                case = DocFileCase(\n                    test,\n                    optionflags=self.optionflags,\n                    setUp=getattr(fixture_context, 'setup_test', None),\n                    tearDown=getattr(fixture_context, 'teardown_test', None),\n                    result_var=self.doctest_result_var)\n                if fixture_context:\n                    yield ContextList((case,), context=fixture_context)\n                else:\n                    yield case\n            else:\n                yield False  # no tests to load\n\n    def loadTestsFromFile(self, filename):\n\n        cases = self.loadTestsFromFileUnicode(filename)\n\n        for case in cases:\n            if isinstance(case, ContextList):\n                yield ContextList([self._patchTestCase(c) for c in case], case.context)\n            else:\n                yield self._patchTestCase(case)\n\n    def loadTestsFromModule(self, module):\n        for suite in super(DoctestPluginHelper, self).loadTestsFromModule(module):\n            cases = [self._patchTestCase(case) for case in suite._get_tests()]\n            yield self.suiteClass(cases, context=module, can_split=False)\n\n    def _patchTestCase(self, case):\n        if case:\n            case._dt_test.globs['print_function'] = print_function\n            case._dt_checker = _checker\n        return case\n\n    def configure(self, options, config):\n\n        Plugin.configure(self, options, config)\n        self.doctest_result_var = options.doctest_result_var\n        self.doctest_tests = options.doctest_tests\n        self.extension = tolist(options.doctestExtension)\n        self.fixtures = options.doctestFixtures\n        self.finder = doctest.DocTestFinder()\n\n        self.optionflags = 0\n        self.options = {}\n\n        if options.doctestOptions:\n            stroptions = \",\".join(options.doctestOptions).split(',')\n            for stroption in stroptions:\n                try:\n                    if stroption.startswith('+'):\n                        self.optionflags |= doctest.OPTIONFLAGS_BY_NAME[stroption[1:]]\n                        continue\n                    elif stroption.startswith('-'):\n                        self.optionflags &= ~doctest.OPTIONFLAGS_BY_NAME[stroption[1:]]\n                        continue\n                    try:\n                        key, value = stroption.split('=')\n                    except ValueError:\n                        pass\n                    else:\n                        if not key in self.OPTION_BY_NAME:\n                            raise ValueError()\n                        self.options[key] = value\n                        continue\n                except (AttributeError, ValueError, KeyError):\n                    raise ValueError(\"Unknown doctest option {}\".format(stroption))\n                else:\n                    raise ValueError(\"Doctest option is not a flag or a key/value pair: {} \".format(stroption))\n\n\nclass DoctestFix(DoctestPluginHelper, Doctest):\n    pass\n"], "nltk\\test\\gensim_fixt": [".py", "from __future__ import absolute_import\n\n\ndef setup_module(module):\n    from nose import SkipTest\n    try:\n        import gensim\n    except ImportError:\n        raise SkipTest(\"Gensim doctest requires gensim\")\n"], "nltk\\test\\gluesemantics_malt_fixt": [".py", "from __future__ import absolute_import\n\n\ndef setup_module(module):\n    from nose import SkipTest\n    from nltk.parse.malt import MaltParser\n\n    try:\n        depparser = MaltParser('maltparser-1.7.2')\n    except LookupError:\n        raise SkipTest(\"MaltParser is not available\")\n"], "nltk\\test\\inference_fixt": [".py", "from __future__ import absolute_import\n\n\ndef setup_module(module):\n    from nose import SkipTest\n    from nltk.inference.mace import Mace\n    try:\n        m = Mace()\n        m._find_binary('mace4')\n    except LookupError:\n        raise SkipTest(\"Mace4/Prover9 is not available so inference.doctest was skipped\")\n"], "nltk\\test\\nonmonotonic_fixt": [".py", "from __future__ import absolute_import\n\n\ndef setup_module(module):\n    from nose import SkipTest\n    from nltk.inference.mace import Mace\n    try:\n        m = Mace()\n        m._find_binary('mace4')\n    except LookupError:\n        raise SkipTest(\"Mace4/Prover9 is not available so nonmonotonic.doctest was skipped\")\n"], "nltk\\test\\portuguese_en_fixt": [".py", "from __future__ import absolute_import\nfrom nltk.compat import PY3\n\nfrom nltk.corpus import teardown_module\n\n\ndef setup_module(module):\n    from nose import SkipTest\n\n    raise SkipTest(\"portuguese_en.doctest imports nltk.examples.pt which doesn't exist!\")\n\n    if not PY3:\n        raise SkipTest(\n            \"portuguese_en.doctest was skipped because non-ascii doctests are not supported under Python 2.x\"\n        )\n"], "nltk\\test\\probability_fixt": [".py", "from __future__ import absolute_import\n\n\n\ndef setup_module(module):\n    from nose import SkipTest\n    try:\n        import numpy\n    except ImportError:\n        raise SkipTest(\"probability.doctest requires numpy\")\n"], "nltk\\test\\runtests": [".py", "from __future__ import absolute_import, print_function\nimport sys\nimport os\nimport nose\nfrom nose.plugins.manager import PluginManager\nfrom nose.plugins.doctests import Doctest\nfrom nose.plugins import builtin\n\nNLTK_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))\nsys.path.insert(0, NLTK_ROOT)\n\nNLTK_TEST_DIR = os.path.join(NLTK_ROOT, 'nltk')\n\nif __name__ == '__main__':\n    from doctest_nose_plugin import DoctestFix\n\n    try:\n        from rednose import RedNose\n\n        rednose_available = True\n    except ImportError:\n        rednose_available = False\n\n\n    class NltkPluginManager(PluginManager):\n\n        def loadPlugins(self):\n            for plug in builtin.plugins:\n                if plug != Doctest:\n                    self.addPlugin(plug())\n            self.addPlugin(DoctestFix())\n            if rednose_available:\n                self.addPlugin(RedNose())\n\n            super(NltkPluginManager, self).loadPlugins()\n\n\n    manager = NltkPluginManager()\n    manager.loadPlugins()\n\n\n    args = sys.argv[1:]\n    if not args:\n        args = [NLTK_TEST_DIR]\n\n    if all(arg.startswith('-') for arg in args):\n        args += [NLTK_TEST_DIR]\n\n    if rednose_available:\n        args += [\n            '--rednose',\n            '--hide-skips'\n        ]\n\n    arguments = [\n                    '--exclude=',  # why is this needed?\n                    '--with-doctest',\n                    '--doctest-extension=.doctest',\n                    '--doctest-fixtures=_fixt',\n                    '--doctest-options=+ELLIPSIS,+NORMALIZE_WHITESPACE,+IGNORE_EXCEPTION_DETAIL,+ALLOW_UNICODE,'\n                    'doctestencoding=utf-8',\n                ] + args\n\n    nose.main(argv=arguments, plugins=manager.plugins)\n"], "nltk\\test\\segmentation_fixt": [".py", "from __future__ import absolute_import\n\n\ndef setup_module(module):\n    from nose import SkipTest\n    try:\n        import numpy\n    except ImportError:\n        raise SkipTest(\"segmentation.doctest requires numpy\")"], "nltk\\test\\semantics_fixt": [".py", "from __future__ import absolute_import\n\ndef setup_module(module):\n    from nltk.sem import logic\n    logic._counter._value = 0\n"], "nltk\\test\\translate_fixt": [".py", "from __future__ import absolute_import\n\nfrom nltk.corpus import teardown_module"], "nltk\\test\\unit\\test_2x_compat": [".py", "from __future__ import absolute_import, unicode_literals\nimport unittest\n\nfrom nltk.text import Text\nfrom nltk.compat import PY3, python_2_unicode_compatible\n\ndef setup_module(module):\n    from nose import SkipTest\n    if PY3:\n        raise SkipTest(\"test_2x_compat is for testing nltk.compat under Python 2.x\")\n\n\nclass TestTextTransliteration(unittest.TestCase):\n    txt = Text([\"S\u00e3o\", \"Tom\u00e9\", \"and\", \"Pr\u00edncipe\"])\n\n    def test_repr(self):\n        self.assertEqual(repr(self.txt), br\"<Text: S\\xe3o Tom\\xe9 and Pr\\xedncipe...>\")\n\n    def test_str(self):\n        self.assertEqual(str(self.txt), b\"<Text: Sao Tome and Principe...>\")\n\n\nclass TestFraction(unittest.TestCase):\n    def test_unnoramlize_fraction(self):\n        from fractions import Fraction as NativePythonFraction\n        from nltk.compat import Fraction as NLTKFraction\n        \n        with self.assertRaises(TypeError):\n            NativePythonFraction(0, 1000, _normalize=False)\n        \n        compat_frac = NLTKFraction(0, 1000, _normalize=False)\n        assert compat_frac.numerator == 0\n        assert compat_frac.denominator == 1000\n        assert float(compat_frac) == 0.0\n        \n        six_twelve = NLTKFraction(6, 12, _normalize=False)\n        assert six_twelve.numerator == 6\n        assert six_twelve.denominator == 12\n        \n        one_two = NLTKFraction(1, 2, _normalize=False)\n        assert one_two.numerator == 1\n        assert one_two.denominator == 2\n        \n        six_twelve_original = NativePythonFraction(6, 12)\n        assert float(one_two) == float(six_twelve) == float(six_twelve_original)\n        \n        assert NLTKFraction(3.142, _normalize=False) == NativePythonFraction(3.142)\n"], "nltk\\test\\unit\\test_aline": [".py", "\nfrom __future__ import unicode_literals\n\nimport unittest\n\nfrom nltk.metrics import aline\n\n\nclass TestAline(unittest.TestCase):\n\n    def test_aline(self):\n        result = aline.align('\u03b8in', 'tenwis')\n        expected = [[('\u03b8', 't'), ('i', 'e'), ('n', 'n'), ('-', 'w'), ('-', 'i'), ('-', 's')]]\n\n        self.assertEqual(result, expected)\n\n        result = aline.align('jo', '\u0292\u0259')\n        expected = [[('j', '\u0292'), ('o', '\u0259')]]\n\n        self.assertEqual(result, expected)\n\n        result = aline.align('pematesiweni', 'pematesewen')\n        expected = [[('p', 'p'), ('e', 'e'), ('m', 'm'), ('a', 'a'), ('t', 't'), ('e', 'e'),\n                     ('s', 's'), ('i', 'e'), ('w', 'w'), ('e', 'e'), ('n', 'n'), ('i', '-')]]\n\n        self.assertEqual(result, expected)\n\n        result = aline.align('tuw\u03b8', 'dentis')\n        expected = [[('t', 'd'), ('u', 'e'), ('w', '-'), ('-', 'n'), ('-', 't'), ('-', 'i'), ('\u03b8', 's')]]\n\n        self.assertEqual(result, expected)\n\n    def test_aline_delta(self):\n        result = aline.delta('p', 'q')\n        expected = 20.0\n\n        self.assertEqual(result, expected)\n\n        result = aline.delta('a', 'A')\n        expected = 0.0\n\n        self.assertEqual(result, expected)\n"], "nltk\\test\\unit\\test_chunk": [".py", "from __future__ import absolute_import, unicode_literals\nimport unittest\n\nfrom nltk import RegexpParser\n\n\nclass TestChunkRule(unittest.TestCase):\n\n    def test_tag_pattern2re_pattern_quantifier(self):\n        sent = [('The', 'AT'), ('September-October', 'NP'), ('term', 'NN'), ('jury', 'NN'), ('had', 'HVD'),\n                ('been', 'BEN'), ('charged', 'VBN'), ('by', 'IN'), ('Fulton', 'NP-TL'), ('Superior', 'JJ-TL'),\n                ('Court', 'NN-TL'), ('Judge', 'NN-TL'), ('Durwood', 'NP'), ('Pye', 'NP'), ('to', 'TO'),\n                ('investigate', 'VB'), ('reports', 'NNS'), ('of', 'IN'), ('possible', 'JJ'), ('``', '``'),\n                ('irregularities', 'NNS'), (\"''\", \"''\"), ('in', 'IN'), ('the', 'AT'), ('hard-fought', 'JJ'),\n                ('primary', 'NN'), ('which', 'WDT'), ('was', 'BEDZ'), ('won', 'VBN'), ('by', 'IN'),\n                ('Mayor-nominate', 'NN-TL'), ('Ivan', 'NP'), ('Allen', 'NP'), ('Jr.', 'NP'),\n                ('.', '.')]  # source: brown corpus\n        cp = RegexpParser('CHUNK: {<N.*>{4,}}')\n        tree = cp.parse(sent)\n        assert tree.pformat() == \"\"\"(S\n  The/AT\n  September-October/NP\n  term/NN\n  jury/NN\n  had/HVD\n  been/BEN\n  charged/VBN\n  by/IN\n  Fulton/NP-TL\n  Superior/JJ-TL\n  (CHUNK Court/NN-TL Judge/NN-TL Durwood/NP Pye/NP)\n  to/TO\n  investigate/VB\n  reports/NNS\n  of/IN\n  possible/JJ\n  ``/``\n  irregularities/NNS\n  ''/''\n  in/IN\n  the/AT\n  hard-fought/JJ\n  primary/NN\n  which/WDT\n  was/BEDZ\n  won/VBN\n  by/IN\n  (CHUNK Mayor-nominate/NN-TL Ivan/NP Allen/NP Jr./NP)\n  ./.)\"\"\"\n"], "nltk\\test\\unit\\test_classify": [".py", "from __future__ import absolute_import\nfrom nose import SkipTest\nfrom nltk import classify\n\nTRAIN = [\n    (dict(a=1, b=1, c=1), 'y'),\n    (dict(a=1, b=1, c=1), 'x'),\n    (dict(a=1, b=1, c=0), 'y'),\n    (dict(a=0, b=1, c=1), 'x'),\n    (dict(a=0, b=1, c=1), 'y'),\n    (dict(a=0, b=0, c=1), 'y'),\n    (dict(a=0, b=1, c=0), 'x'),\n    (dict(a=0, b=0, c=0), 'x'),\n    (dict(a=0, b=1, c=1), 'y'),\n]\n\nTEST = [\n    (dict(a=1, b=0, c=1)),  # unseen\n    (dict(a=1, b=0, c=0)),  # unseen\n    (dict(a=0, b=1, c=1)),  # seen 3 times, labels=y,y,x\n    (dict(a=0, b=1, c=0)),  # seen 1 time, label=x\n]\n\nRESULTS = [\n    (0.16, 0.84),\n    (0.46, 0.54),\n    (0.41, 0.59),\n    (0.76, 0.24),\n]\n\n\ndef assert_classifier_correct(algorithm):\n    try:\n        classifier = classify.MaxentClassifier.train(\n            TRAIN, algorithm, trace=0, max_iter=1000\n        )\n    except (LookupError, AttributeError) as e:\n        raise SkipTest(str(e))\n\n    for (px, py), featureset in zip(RESULTS, TEST):\n        pdist = classifier.prob_classify(featureset)\n        assert abs(pdist.prob('x') - px) < 1e-2, (pdist.prob('x'), px)\n        assert abs(pdist.prob('y') - py) < 1e-2, (pdist.prob('y'), py)\n\n\ndef test_megam():\n    assert_classifier_correct('MEGAM')\n\n\ndef test_tadm():\n    assert_classifier_correct('TADM')\n"], "nltk\\test\\unit\\test_collocations": [".py", "from __future__ import absolute_import, unicode_literals\nimport unittest\n\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\n\n\n_EPSILON = 1e-8\n\n\ndef close_enough(x, y):\n\n    for (x1, y1) in zip(x, y):\n        if x1[0] != y1[0] or abs(x1[1] - y1[1]) > _EPSILON:\n            return False\n    return True\n\n\nclass TestBigram(unittest.TestCase):\n    def test_bigram2(self):\n        sent = 'this this is is a a test test'.split()\n\n        b = BigramCollocationFinder.from_words(sent)\n\n        self.assertEqual(\n            sorted(b.ngram_fd.items()),\n            sorted([(('a', 'a'), 1), (('a', 'test'), 1), (('is', 'a'), 1), (('is', 'is'), 1), (('test', 'test'), 1),\n                    (('this', 'is'), 1), (('this', 'this'), 1)])\n        )\n        self.assertEqual(\n            sorted(b.word_fd.items()),\n            sorted([('a', 2), ('is', 2), ('test', 2), ('this', 2)])\n        )\n        self.assertTrue(len(sent) == sum(b.word_fd.values()) == sum(b.ngram_fd.values()) + 1)\n        self.assertTrue(close_enough(\n            sorted(b.score_ngrams(BigramAssocMeasures.pmi)),\n            sorted([(('a', 'a'), 1.0), (('a', 'test'), 1.0), (('is', 'a'), 1.0), (('is', 'is'), 1.0),\n                    (('test', 'test'), 1.0), (('this', 'is'), 1.0), (('this', 'this'), 1.0)])\n        ))\n\n    def test_bigram3(self):\n        sent = 'this this is is a a test test'.split()\n\n        b = BigramCollocationFinder.from_words(sent, window_size=3)\n        self.assertEqual(\n            sorted(b.ngram_fd.items()),\n            sorted([(('a', 'test'), 3), (('is', 'a'), 3), (('this', 'is'), 3), (('a', 'a'), 1), (('is', 'is'), 1),\n                    (('test', 'test'), 1), (('this', 'this'), 1)])\n        )\n        self.assertEqual(\n            sorted(b.word_fd.items()),\n            sorted([('a', 2), ('is', 2), ('test', 2), ('this', 2)])\n        )\n        self.assertTrue(len(sent) == sum(b.word_fd.values()) == (sum(b.ngram_fd.values()) + 2 + 1) / 2.0)\n        self.assertTrue(close_enough(\n            sorted(b.score_ngrams(BigramAssocMeasures.pmi)),\n            sorted([(('a', 'test'), 1.584962500721156), (('is', 'a'), 1.584962500721156),\n                    (('this', 'is'), 1.584962500721156), (('a', 'a'), 0.0), (('is', 'is'), 0.0),\n                    (('test', 'test'), 0.0), (('this', 'this'), 0.0)])\n        ))\n\n    def test_bigram5(self):\n        sent = 'this this is is a a test test'.split()\n\n        b = BigramCollocationFinder.from_words(sent, window_size=5)\n        self.assertEqual(\n            sorted(b.ngram_fd.items()),\n            sorted([(('a', 'test'), 4), (('is', 'a'), 4), (('this', 'is'), 4), (('is', 'test'), 3), (('this', 'a'), 3),\n                    (('a', 'a'), 1), (('is', 'is'), 1), (('test', 'test'), 1), (('this', 'this'), 1)])\n        )\n        self.assertEqual(\n            sorted(b.word_fd.items()),\n            sorted([('a', 2), ('is', 2), ('test', 2), ('this', 2)])\n        )\n        self.assertTrue(len(sent) == sum(b.word_fd.values()) == (sum(b.ngram_fd.values()) + 4 + 3 + 2 + 1) / 4.0)\n        self.assertTrue(close_enough(\n            sorted(b.score_ngrams(BigramAssocMeasures.pmi)),\n            sorted(\n                [(('a', 'test'), 1.0), (('is', 'a'), 1.0), (('this', 'is'), 1.0), (('is', 'test'), 0.5849625007211562),\n                 (('this', 'a'), 0.5849625007211562), (('a', 'a'), -1.0), (('is', 'is'), -1.0),\n                 (('test', 'test'), -1.0), (('this', 'this'), -1.0)])\n        ))\n"], "nltk\\test\\unit\\test_concordance": [".py", "from __future__ import absolute_import, unicode_literals\n\nimport unittest\nfrom nose import with_setup\n\nfrom nltk.corpus import gutenberg\nfrom nltk.text import Text\n\nimport contextlib\nimport sys\n\ntry:\n    from StringIO import StringIO\nexcept ImportError as e:\n    from io import StringIO\n\n\n@contextlib.contextmanager\ndef stdout_redirect(where):\n    sys.stdout = where\n    try:\n        yield where\n    finally:\n        sys.stdout = sys.__stdout__\n\n\nclass TestConcordance(unittest.TestCase):\n\n    @classmethod\n    def setup_class(cls):\n        cls.corpus = gutenberg.words('melville-moby_dick.txt')\n\n    @classmethod\n    def teardown_class(cls):\n        pass\n\n    def setUp(self):\n        self.text = Text(TestConcordance.corpus)\n        self.query = \"monstrous\"\n        self.maxDiff = None\n        self.list_out = [\n            'ong the former , one was of a most monstrous size . ... This came towards us , ',\n            'ON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r',\n            'll over with a heathenish array of monstrous clubs and spears . Some were thick',\n            'd as you gazed , and wondered what monstrous cannibal and savage could ever hav',\n            'that has survived the flood ; most monstrous and most mountainous ! That Himmal',\n            'they might scout at Moby Dick as a monstrous fable , or still worse and more de',\n            'th of Radney .\\'\" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l',\n            'ing Scenes . In connexion with the monstrous pictures of whales , I am strongly',\n            'ere to enter upon those still more monstrous stories of them which are to be fo',\n            'ght have been rummaged out of this monstrous cabinet there is no telling . But ',\n            'of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u']\n\n    def tearDown(self):\n        pass\n\n    def test_concordance_list(self):\n        concordance_out = self.text.concordance_list(self.query)\n        self.assertEqual(self.list_out, [c.line for c in concordance_out])\n\n    def test_concordance_width(self):\n        list_out = [\"monstrous\", \"monstrous\", \"monstrous\",\n                    \"monstrous\", \"monstrous\", \"monstrous\",\n                    \"Monstrous\", \"monstrous\", \"monstrous\",\n                    \"monstrous\", \"monstrous\"]\n\n        concordance_out = self.text.concordance_list(self.query, width=0)\n        self.assertEqual(list_out, [c.query for c in concordance_out])\n\n    def test_concordance_lines(self):\n        concordance_out = self.text.concordance_list(self.query, lines=3)\n        self.assertEqual(self.list_out[:3], [c.line for c in concordance_out])\n\n    def test_concordance_print(self):\n        print_out = \"\"\"Displaying 11 of 11 matches:\n        ong the former , one was of a most monstrous size . ... This came towards us ,\n        ON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r\n        ll over with a heathenish array of monstrous clubs and spears . Some were thick\n        d as you gazed , and wondered what monstrous cannibal and savage could ever hav\n        that has survived the flood ; most monstrous and most mountainous ! That Himmal\n        they might scout at Moby Dick as a monstrous fable , or still worse and more de\n        th of Radney .'\" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l\n        ing Scenes . In connexion with the monstrous pictures of whales , I am strongly\n        ere to enter upon those still more monstrous stories of them which are to be fo\n        ght have been rummaged out of this monstrous cabinet there is no telling . But\n        of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\n        \"\"\"\n\n        with stdout_redirect(StringIO()) as stdout:\n            self.text.concordance(self.query)\n\n        def strip_space(raw_str):\n            return raw_str.replace(\" \", \"\")\n\n        self.assertEqual(strip_space(print_out),\n                         strip_space(stdout.getvalue()))\n"], "nltk\\test\\unit\\test_corenlp": [".py", "\n\nimport sys\nfrom itertools import chain\nfrom unittest import TestCase, SkipTest\n\ntry:\n    from unittest.mock import MagicMock\nexcept ImportError:\n    raise SkipTest('unittest.mock no supported in Python2')\nfrom nltk.tree import Tree\nfrom nltk.parse import corenlp\n\n\nclass TestTokenizerAPI(TestCase):\n\n    def test_tokenize(self):\n        corenlp_tokenizer = corenlp.CoreNLPParser()\n\n        api_return_value = {\n            u'sentences': [{u'index': 0,\n                            u'tokens': [{u'after': u' ',\n                                         u'before': u'',\n                                         u'characterOffsetBegin': 0,\n                                         u'characterOffsetEnd': 4,\n                                         u'index': 1,\n                                         u'originalText': u'Good',\n                                         u'word': u'Good'},\n                                        {u'after': u' ',\n                                         u'before': u' ',\n                                         u'characterOffsetBegin': 5,\n                                         u'characterOffsetEnd': 12,\n                                         u'index': 2,\n                                         u'originalText': u'muffins',\n                                         u'word': u'muffins'},\n                                        {u'after': u' ',\n                                         u'before': u' ',\n                                         u'characterOffsetBegin': 13,\n                                         u'characterOffsetEnd': 17,\n                                         u'index': 3,\n                                         u'originalText': u'cost',\n                                         u'word': u'cost'},\n                                        {u'after': u'',\n                                         u'before': u' ',\n                                         u'characterOffsetBegin': 18,\n                                         u'characterOffsetEnd': 19,\n                                         u'index': 4,\n                                         u'originalText': u'$',\n                                         u'word': u'$'},\n                                        {u'after': u'\\n',\n                                         u'before': u'',\n                                         u'characterOffsetBegin': 19,\n                                         u'characterOffsetEnd': 23,\n                                         u'index': 5,\n                                         u'originalText': u'3.88',\n                                         u'word': u'3.88'},\n                                        {u'after': u' ',\n                                         u'before': u'\\n',\n                                         u'characterOffsetBegin': 24,\n                                         u'characterOffsetEnd': 26,\n                                         u'index': 6,\n                                         u'originalText': u'in',\n                                         u'word': u'in'},\n                                        {u'after': u' ',\n                                         u'before': u' ',\n                                         u'characterOffsetBegin': 27,\n                                         u'characterOffsetEnd': 30,\n                                         u'index': 7,\n                                         u'originalText': u'New',\n                                         u'word': u'New'},\n                                        {u'after': u'',\n                                         u'before': u' ',\n                                         u'characterOffsetBegin': 31,\n                                         u'characterOffsetEnd': 35,\n                                         u'index': 8,\n                                         u'originalText': u'York',\n                                         u'word': u'York'},\n                                        {u'after': u'  ',\n                                         u'before': u'',\n                                         u'characterOffsetBegin': 35,\n                                         u'characterOffsetEnd': 36,\n                                         u'index': 9,\n                                         u'originalText': u'.',\n                                         u'word': u'.'}]},\n                           {u'index': 1,\n                            u'tokens': [{u'after': u' ',\n                                         u'before': u'  ',\n                                         u'characterOffsetBegin': 38,\n                                         u'characterOffsetEnd': 44,\n                                         u'index': 1,\n                                         u'originalText': u'Please',\n                                         u'word': u'Please'},\n                                        {u'after': u' ',\n                                         u'before': u' ',\n                                         u'characterOffsetBegin': 45,\n                                         u'characterOffsetEnd': 48,\n                                         u'index': 2,\n                                         u'originalText': u'buy',\n                                         u'word': u'buy'},\n                                        {u'after': u'\\n',\n                                         u'before': u' ',\n                                         u'characterOffsetBegin': 49,\n                                         u'characterOffsetEnd': 51,\n                                         u'index': 3,\n                                         u'originalText': u'me',\n                                         u'word': u'me'},\n                                        {u'after': u' ',\n                                         u'before': u'\\n',\n                                         u'characterOffsetBegin': 52,\n                                         u'characterOffsetEnd': 55,\n                                         u'index': 4,\n                                         u'originalText': u'two',\n                                         u'word': u'two'},\n                                        {u'after': u' ',\n                                         u'before': u' ',\n                                         u'characterOffsetBegin': 56,\n                                         u'characterOffsetEnd': 58,\n                                         u'index': 5,\n                                         u'originalText': u'of',\n                                         u'word': u'of'},\n                                        {u'after': u'',\n                                         u'before': u' ',\n                                         u'characterOffsetBegin': 59,\n                                         u'characterOffsetEnd': 63,\n                                         u'index': 6,\n                                         u'originalText': u'them',\n                                         u'word': u'them'},\n                                        {u'after': u'\\n',\n                                         u'before': u'',\n                                         u'characterOffsetBegin': 63,\n                                         u'characterOffsetEnd': 64,\n                                         u'index': 7,\n                                         u'originalText': u'.',\n                                         u'word': u'.'}]},\n                           {u'index': 2,\n                            u'tokens': [{u'after': u'',\n                                         u'before': u'\\n',\n                                         u'characterOffsetBegin': 65,\n                                         u'characterOffsetEnd': 71,\n                                         u'index': 1,\n                                         u'originalText': u'Thanks',\n                                         u'word': u'Thanks'},\n                                        {u'after': u'',\n                                         u'before': u'',\n                                         u'characterOffsetBegin': 71,\n                                         u'characterOffsetEnd': 72,\n                                         u'index': 2,\n                                         u'originalText': u'.',\n                                         u'word': u'.'}]}]\n        }\n        corenlp_tokenizer.api_call = MagicMock(return_value=api_return_value)\n\n        input_string = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\nThanks.\"\n\n        expected_output = [u'Good', u'muffins', u'cost', u'$', u'3.88', u'in',\n                           u'New', u'York', u'.', u'Please', u'buy', u'me',\n                           u'two', u'of', u'them', u'.', u'Thanks', u'.']\n\n        tokenized_output = list(corenlp_tokenizer.tokenize(input_string))\n\n        corenlp_tokenizer.api_call.assert_called_once_with(\n            'Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\nThanks.', properties={'annotators': 'tokenize,ssplit'})\n        self.assertEqual(expected_output, tokenized_output)\n\n\nclass TestTaggerAPI(TestCase):\n\n    def test_pos_tagger(self):\n        corenlp_tagger = corenlp.CoreNLPParser(tagtype='pos')\n\n        api_return_value = {\n            u'sentences': [{u'basicDependencies': [{u'dep': u'ROOT',\n                                                    u'dependent': 1,\n                                                    u'dependentGloss': u'What',\n                                                    u'governor': 0,\n                                                    u'governorGloss': u'ROOT'},\n                                                   {u'dep': u'cop',\n                                                    u'dependent': 2,\n                                                    u'dependentGloss': u'is',\n                                                    u'governor': 1,\n                                                    u'governorGloss': u'What'},\n                                                   {u'dep': u'det',\n                                                    u'dependent': 3,\n                                                    u'dependentGloss': u'the',\n                                                    u'governor': 4,\n                                                    u'governorGloss': u'airspeed'},\n                                                   {u'dep': u'nsubj',\n                                                    u'dependent': 4,\n                                                    u'dependentGloss': u'airspeed',\n                                                    u'governor': 1,\n                                                    u'governorGloss': u'What'},\n                                                   {u'dep': u'case',\n                                                    u'dependent': 5,\n                                                    u'dependentGloss': u'of',\n                                                    u'governor': 8,\n                                                    u'governorGloss': u'swallow'},\n                                                   {u'dep': u'det',\n                                                    u'dependent': 6,\n                                                    u'dependentGloss': u'an',\n                                                    u'governor': 8,\n                                                    u'governorGloss': u'swallow'},\n                                                   {u'dep': u'compound',\n                                                    u'dependent': 7,\n                                                    u'dependentGloss': u'unladen',\n                                                    u'governor': 8,\n                                                    u'governorGloss': u'swallow'},\n                                                   {u'dep': u'nmod',\n                                                    u'dependent': 8,\n                                                    u'dependentGloss': u'swallow',\n                                                    u'governor': 4,\n                                                    u'governorGloss': u'airspeed'},\n                                                   {u'dep': u'punct',\n                                                    u'dependent': 9,\n                                                    u'dependentGloss': u'?',\n                                                    u'governor': 1,\n                                                    u'governorGloss': u'What'}],\n                            u'enhancedDependencies': [{u'dep': u'ROOT',\n                                                       u'dependent': 1,\n                                                       u'dependentGloss': u'What',\n                                                       u'governor': 0,\n                                                       u'governorGloss': u'ROOT'},\n                                                      {u'dep': u'cop',\n                                                       u'dependent': 2,\n                                                       u'dependentGloss': u'is',\n                                                       u'governor': 1,\n                                                       u'governorGloss': u'What'},\n                                                      {u'dep': u'det',\n                                                       u'dependent': 3,\n                                                       u'dependentGloss': u'the',\n                                                       u'governor': 4,\n                                                       u'governorGloss': u'airspeed'},\n                                                      {u'dep': u'nsubj',\n                                                       u'dependent': 4,\n                                                       u'dependentGloss': u'airspeed',\n                                                       u'governor': 1,\n                                                       u'governorGloss': u'What'},\n                                                      {u'dep': u'case',\n                                                       u'dependent': 5,\n                                                       u'dependentGloss': u'of',\n                                                       u'governor': 8,\n                                                       u'governorGloss': u'swallow'},\n                                                      {u'dep': u'det',\n                                                       u'dependent': 6,\n                                                       u'dependentGloss': u'an',\n                                                       u'governor': 8,\n                                                       u'governorGloss': u'swallow'},\n                                                      {u'dep': u'compound',\n                                                       u'dependent': 7,\n                                                       u'dependentGloss': u'unladen',\n                                                       u'governor': 8,\n                                                       u'governorGloss': u'swallow'},\n                                                      {u'dep': u'nmod:of',\n                                                       u'dependent': 8,\n                                                       u'dependentGloss': u'swallow',\n                                                       u'governor': 4,\n                                                       u'governorGloss': u'airspeed'},\n                                                      {u'dep': u'punct',\n                                                       u'dependent': 9,\n                                                       u'dependentGloss': u'?',\n                                                       u'governor': 1,\n                                                       u'governorGloss': u'What'}],\n                            u'enhancedPlusPlusDependencies': [{u'dep': u'ROOT',\n                                                               u'dependent': 1,\n                                                               u'dependentGloss': u'What',\n                                                               u'governor': 0,\n                                                               u'governorGloss': u'ROOT'},\n                                                              {u'dep': u'cop',\n                                                               u'dependent': 2,\n                                                               u'dependentGloss': u'is',\n                                                               u'governor': 1,\n                                                               u'governorGloss': u'What'},\n                                                              {u'dep': u'det',\n                                                               u'dependent': 3,\n                                                               u'dependentGloss': u'the',\n                                                               u'governor': 4,\n                                                               u'governorGloss': u'airspeed'},\n                                                              {u'dep': u'nsubj',\n                                                               u'dependent': 4,\n                                                               u'dependentGloss': u'airspeed',\n                                                               u'governor': 1,\n                                                               u'governorGloss': u'What'},\n                                                              {u'dep': u'case',\n                                                               u'dependent': 5,\n                                                               u'dependentGloss': u'of',\n                                                               u'governor': 8,\n                                                               u'governorGloss': u'swallow'},\n                                                              {u'dep': u'det',\n                                                               u'dependent': 6,\n                                                               u'dependentGloss': u'an',\n                                                               u'governor': 8,\n                                                               u'governorGloss': u'swallow'},\n                                                              {u'dep': u'compound',\n                                                               u'dependent': 7,\n                                                               u'dependentGloss': u'unladen',\n                                                               u'governor': 8,\n                                                               u'governorGloss': u'swallow'},\n                                                              {u'dep': u'nmod:of',\n                                                               u'dependent': 8,\n                                                               u'dependentGloss': u'swallow',\n                                                               u'governor': 4,\n                                                               u'governorGloss': u'airspeed'},\n                                                              {u'dep': u'punct',\n                                                               u'dependent': 9,\n                                                               u'dependentGloss': u'?',\n                                                               u'governor': 1,\n                                                               u'governorGloss': u'What'}],\n                            u'index': 0,\n                            u'parse': u'(ROOT\\n  (SBARQ\\n    (WHNP (WP What))\\n    (SQ (VBZ is)\\n      (NP\\n        (NP (DT the) (NN airspeed))\\n        (PP (IN of)\\n          (NP (DT an) (NN unladen) (NN swallow)))))\\n    (. ?)))',\n                            u'tokens': [{u'after': u' ',\n                                         u'before': u'',\n                                         u'characterOffsetBegin': 0,\n                                         u'characterOffsetEnd': 4,\n                                         u'index': 1,\n                                         u'lemma': u'what',\n                                         u'originalText': u'What',\n                                         u'pos': u'WP',\n                                         u'word': u'What'},\n                                        {u'after': u' ',\n                                         u'before': u' ',\n                                         u'characterOffsetBegin': 5,\n                                         u'characterOffsetEnd': 7,\n                                         u'index': 2,\n                                         u'lemma': u'be',\n                                         u'originalText': u'is',\n                                         u'pos': u'VBZ',\n                                         u'word': u'is'},\n                                        {u'after': u' ',\n                                         u'before': u' ',\n                                         u'characterOffsetBegin': 8,\n                                         u'characterOffsetEnd': 11,\n                                         u'index': 3,\n                                         u'lemma': u'the',\n                                         u'originalText': u'the',\n                                         u'pos': u'DT',\n                                         u'word': u'the'},\n                                        {u'after': u' ',\n                                         u'before': u' ',\n                                         u'characterOffsetBegin': 12,\n                                         u'characterOffsetEnd': 20,\n                                         u'index': 4,\n                                         u'lemma': u'airspeed',\n                                         u'originalText': u'airspeed',\n                                         u'pos': u'NN',\n                                         u'word': u'airspeed'},\n                                        {u'after': u' ',\n                                         u'before': u' ',\n                                         u'characterOffsetBegin': 21,\n                                         u'characterOffsetEnd': 23,\n                                         u'index': 5,\n                                         u'lemma': u'of',\n                                         u'originalText': u'of',\n                                         u'pos': u'IN',\n                                         u'word': u'of'},\n                                        {u'after': u' ',\n                                         u'before': u' ',\n                                         u'characterOffsetBegin': 24,\n                                         u'characterOffsetEnd': 26,\n                                         u'index': 6,\n                                         u'lemma': u'a',\n                                         u'originalText': u'an',\n                                         u'pos': u'DT',\n                                         u'word': u'an'},\n                                        {u'after': u' ',\n                                         u'before': u' ',\n                                         u'characterOffsetBegin': 27,\n                                         u'characterOffsetEnd': 34,\n                                         u'index': 7,\n                                         u'lemma': u'unladen',\n                                         u'originalText': u'unladen',\n                                         u'pos': u'JJ',\n                                         u'word': u'unladen'},\n                                        {u'after': u' ',\n                                         u'before': u' ',\n                                         u'characterOffsetBegin': 35,\n                                         u'characterOffsetEnd': 42,\n                                         u'index': 8,\n                                         u'lemma': u'swallow',\n                                         u'originalText': u'swallow',\n                                         u'pos': u'VB',\n                                         u'word': u'swallow'},\n                                        {u'after': u'',\n                                         u'before': u' ',\n                                         u'characterOffsetBegin': 43,\n                                         u'characterOffsetEnd': 44,\n                                         u'index': 9,\n                                         u'lemma': u'?',\n                                         u'originalText': u'?',\n                                         u'pos': u'.',\n                                         u'word': u'?'}]}]\n        }\n        corenlp_tagger.api_call = MagicMock(return_value=api_return_value)\n\n        input_tokens = 'What is the airspeed of an unladen swallow ?'.split()\n        expected_output = [('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'),\n                           ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'),\n                           ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]\n        tagged_output = corenlp_tagger.tag(input_tokens)\n\n        corenlp_tagger.api_call.assert_called_once_with('What is the airspeed of an unladen swallow ?', properties={'ssplit.isOneSentence': 'true',\n                                                                                                                    'annotators': 'tokenize,ssplit,pos'})\n        self.assertEqual(expected_output, tagged_output)\n\n    def test_ner_tagger(self):\n        corenlp_tagger = corenlp.CoreNLPParser(tagtype='ner')\n\n        api_return_value = {\n            'sentences': [{'index': 0,\n                           'tokens': [{'after': ' ',\n                                       'before': '',\n                                       'characterOffsetBegin': 0,\n                                       'characterOffsetEnd': 4,\n                                       'index': 1,\n                                       'lemma': 'Rami',\n                                       'ner': 'PERSON',\n                                       'originalText': 'Rami',\n                                                       'pos': 'NNP',\n                                                       'word': 'Rami'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 5,\n                                       'characterOffsetEnd': 8,\n                                       'index': 2,\n                                       'lemma': 'Eid',\n                                       'ner': 'PERSON',\n                                       'originalText': 'Eid',\n                                                       'pos': 'NNP',\n                                                       'word': 'Eid'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 9,\n                                       'characterOffsetEnd': 11,\n                                       'index': 3,\n                                       'lemma': 'be',\n                                       'ner': 'O',\n                                       'originalText': 'is',\n                                                       'pos': 'VBZ',\n                                                       'word': 'is'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 12,\n                                       'characterOffsetEnd': 20,\n                                       'index': 4,\n                                       'lemma': 'study',\n                                       'ner': 'O',\n                                       'originalText': 'studying',\n                                                       'pos': 'VBG',\n                                                       'word': 'studying'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 21,\n                                       'characterOffsetEnd': 23,\n                                       'index': 5,\n                                       'lemma': 'at',\n                                       'ner': 'O',\n                                       'originalText': 'at',\n                                                       'pos': 'IN',\n                                                       'word': 'at'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 24,\n                                       'characterOffsetEnd': 29,\n                                       'index': 6,\n                                       'lemma': 'Stony',\n                                       'ner': 'ORGANIZATION',\n                                       'originalText': 'Stony',\n                                                       'pos': 'NNP',\n                                                       'word': 'Stony'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 30,\n                                       'characterOffsetEnd': 35,\n                                       'index': 7,\n                                       'lemma': 'Brook',\n                                       'ner': 'ORGANIZATION',\n                                       'originalText': 'Brook',\n                                                       'pos': 'NNP',\n                                                       'word': 'Brook'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 36,\n                                       'characterOffsetEnd': 46,\n                                       'index': 8,\n                                       'lemma': 'University',\n                                       'ner': 'ORGANIZATION',\n                                       'originalText': 'University',\n                                       'pos': 'NNP',\n                                                       'word': 'University'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 47,\n                                       'characterOffsetEnd': 49,\n                                       'index': 9,\n                                       'lemma': 'in',\n                                       'ner': 'O',\n                                       'originalText': 'in',\n                                       'pos': 'IN',\n                                       'word': 'in'},\n                                      {'after': '',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 50,\n                                       'characterOffsetEnd': 52,\n                                       'index': 10,\n                                       'lemma': 'NY',\n                                       'ner': 'O',\n                                       'originalText': 'NY',\n                                       'pos': 'NNP',\n                                       'word': 'NY'}]}]}\n\n        corenlp_tagger.api_call = MagicMock(return_value=api_return_value)\n\n        input_tokens = 'Rami Eid is studying at Stony Brook University in NY'.split()\n        expected_output = [('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'),\n                           ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'O')]\n        tagged_output = corenlp_tagger.tag(input_tokens)\n\n        corenlp_tagger.api_call.assert_called_once_with('Rami Eid is studying at Stony Brook University in NY', properties={'ssplit.isOneSentence': 'true',\n                                                                                                                            'annotators': 'tokenize,ssplit,ner'})\n        self.assertEqual(expected_output, tagged_output)\n\n    def test_unexpected_tagtype(self):\n        with self.assertRaises(ValueError):\n            corenlp_tagger = corenlp.CoreNLPParser(tagtype='test')\n\nclass TestParserAPI(TestCase):\n\n    def test_parse(self):\n        corenlp_parser = corenlp.CoreNLPParser()\n\n        api_return_value = {\n            'sentences': [{'basicDependencies': [{'dep': 'ROOT',\n                                                  'dependent': 4,\n                                                  'dependentGloss': 'fox',\n                                                  'governor': 0,\n                                                  'governorGloss': 'ROOT'},\n                                                 {'dep': 'det',\n                                                  'dependent': 1,\n                                                  'dependentGloss': 'The',\n                                                  'governor': 4,\n                                                  'governorGloss': 'fox'},\n                                                 {'dep': 'amod',\n                                                  'dependent': 2,\n                                                  'dependentGloss': 'quick',\n                                                  'governor': 4,\n                                                  'governorGloss': 'fox'},\n                                                 {'dep': 'amod',\n                                                  'dependent': 3,\n                                                  'dependentGloss': 'brown',\n                                                  'governor': 4,\n                                                  'governorGloss': 'fox'},\n                                                 {'dep': 'dep',\n                                                  'dependent': 5,\n                                                  'dependentGloss': 'jumps',\n                                                  'governor': 4,\n                                                  'governorGloss': 'fox'},\n                                                 {'dep': 'case',\n                                                  'dependent': 6,\n                                                  'dependentGloss': 'over',\n                                                  'governor': 9,\n                                                  'governorGloss': 'dog'},\n                                                 {'dep': 'det',\n                                                  'dependent': 7,\n                                                  'dependentGloss': 'the',\n                                                  'governor': 9,\n                                                  'governorGloss': 'dog'},\n                                                 {'dep': 'amod',\n                                                  'dependent': 8,\n                                                  'dependentGloss': 'lazy',\n                                                  'governor': 9,\n                                                  'governorGloss': 'dog'},\n                                                 {'dep': 'nmod',\n                                                  'dependent': 9,\n                                                  'dependentGloss': 'dog',\n                                                  'governor': 5,\n                                                  'governorGloss': 'jumps'}],\n                           'enhancedDependencies': [{'dep': 'ROOT',\n                                                     'dependent': 4,\n                                                     'dependentGloss': 'fox',\n                                                     'governor': 0,\n                                                     'governorGloss': 'ROOT'},\n                                                    {'dep': 'det',\n                                                     'dependent': 1,\n                                                     'dependentGloss': 'The',\n                                                     'governor': 4,\n                                                     'governorGloss': 'fox'},\n                                                    {'dep': 'amod',\n                                                     'dependent': 2,\n                                                     'dependentGloss': 'quick',\n                                                     'governor': 4,\n                                                     'governorGloss': 'fox'},\n                                                    {'dep': 'amod',\n                                                     'dependent': 3,\n                                                     'dependentGloss': 'brown',\n                                                     'governor': 4,\n                                                     'governorGloss': 'fox'},\n                                                    {'dep': 'dep',\n                                                     'dependent': 5,\n                                                     'dependentGloss': 'jumps',\n                                                     'governor': 4,\n                                                     'governorGloss': 'fox'},\n                                                    {'dep': 'case',\n                                                     'dependent': 6,\n                                                     'dependentGloss': 'over',\n                                                     'governor': 9,\n                                                     'governorGloss': 'dog'},\n                                                    {'dep': 'det',\n                                                     'dependent': 7,\n                                                     'dependentGloss': 'the',\n                                                     'governor': 9,\n                                                     'governorGloss': 'dog'},\n                                                    {'dep': 'amod',\n                                                     'dependent': 8,\n                                                     'dependentGloss': 'lazy',\n                                                     'governor': 9,\n                                                     'governorGloss': 'dog'},\n                                                    {'dep': 'nmod:over',\n                                                     'dependent': 9,\n                                                     'dependentGloss': 'dog',\n                                                     'governor': 5,\n                                                     'governorGloss': 'jumps'}],\n                           'enhancedPlusPlusDependencies': [{'dep': 'ROOT',\n                                                             'dependent': 4,\n                                                             'dependentGloss': 'fox',\n                                                             'governor': 0,\n                                                             'governorGloss': 'ROOT'},\n                                                            {'dep': 'det',\n                                                             'dependent': 1,\n                                                             'dependentGloss': 'The',\n                                                             'governor': 4,\n                                                             'governorGloss': 'fox'},\n                                                            {'dep': 'amod',\n                                                             'dependent': 2,\n                                                             'dependentGloss': 'quick',\n                                                             'governor': 4,\n                                                             'governorGloss': 'fox'},\n                                                            {'dep': 'amod',\n                                                             'dependent': 3,\n                                                             'dependentGloss': 'brown',\n                                                             'governor': 4,\n                                                             'governorGloss': 'fox'},\n                                                            {'dep': 'dep',\n                                                             'dependent': 5,\n                                                             'dependentGloss': 'jumps',\n                                                             'governor': 4,\n                                                             'governorGloss': 'fox'},\n                                                            {'dep': 'case',\n                                                             'dependent': 6,\n                                                             'dependentGloss': 'over',\n                                                             'governor': 9,\n                                                             'governorGloss': 'dog'},\n                                                            {'dep': 'det',\n                                                             'dependent': 7,\n                                                             'dependentGloss': 'the',\n                                                             'governor': 9,\n                                                             'governorGloss': 'dog'},\n                                                            {'dep': 'amod',\n                                                             'dependent': 8,\n                                                             'dependentGloss': 'lazy',\n                                                             'governor': 9,\n                                                             'governorGloss': 'dog'},\n                                                            {'dep': 'nmod:over',\n                                                             'dependent': 9,\n                                                             'dependentGloss': 'dog',\n                                                             'governor': 5,\n                                                             'governorGloss': 'jumps'}],\n                           'index': 0,\n                           'parse': '(ROOT\\n  (NP\\n    (NP (DT The) (JJ quick) (JJ brown) (NN fox))\\n    (NP\\n      (NP (NNS jumps))\\n      (PP (IN over)\\n        (NP (DT the) (JJ lazy) (NN dog))))))',\n                           'tokens': [{'after': ' ',\n                                       'before': '',\n                                       'characterOffsetBegin': 0,\n                                       'characterOffsetEnd': 3,\n                                       'index': 1,\n                                       'lemma': 'the',\n                                       'originalText': 'The',\n                                       'pos': 'DT',\n                                       'word': 'The'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 4,\n                                       'characterOffsetEnd': 9,\n                                       'index': 2,\n                                       'lemma': 'quick',\n                                       'originalText': 'quick',\n                                       'pos': 'JJ',\n                                       'word': 'quick'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 10,\n                                       'characterOffsetEnd': 15,\n                                       'index': 3,\n                                       'lemma': 'brown',\n                                       'originalText': 'brown',\n                                       'pos': 'JJ',\n                                       'word': 'brown'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 16,\n                                       'characterOffsetEnd': 19,\n                                       'index': 4,\n                                       'lemma': 'fox',\n                                       'originalText': 'fox',\n                                       'pos': 'NN',\n                                       'word': 'fox'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 20,\n                                       'characterOffsetEnd': 25,\n                                       'index': 5,\n                                       'lemma': 'jump',\n                                       'originalText': 'jumps',\n                                       'pos': 'VBZ',\n                                       'word': 'jumps'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 26,\n                                       'characterOffsetEnd': 30,\n                                       'index': 6,\n                                       'lemma': 'over',\n                                       'originalText': 'over',\n                                       'pos': 'IN',\n                                       'word': 'over'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 31,\n                                       'characterOffsetEnd': 34,\n                                       'index': 7,\n                                       'lemma': 'the',\n                                       'originalText': 'the',\n                                       'pos': 'DT',\n                                       'word': 'the'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 35,\n                                       'characterOffsetEnd': 39,\n                                       'index': 8,\n                                       'lemma': 'lazy',\n                                       'originalText': 'lazy',\n                                       'pos': 'JJ',\n                                       'word': 'lazy'},\n                                      {'after': '',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 40,\n                                       'characterOffsetEnd': 43,\n                                       'index': 9,\n                                       'lemma': 'dog',\n                                       'originalText': 'dog',\n                                       'pos': 'NN',\n                                       'word': 'dog'}]}]\n        }\n\n        corenlp_parser.api_call = MagicMock(return_value=api_return_value)\n\n        input_string = \"The quick brown fox jumps over the lazy dog\".split()\n        expected_output = Tree('ROOT', [Tree('NP', [Tree('NP', [Tree('DT', ['The']), Tree('JJ', ['quick']),\n                          Tree('JJ', ['brown']), Tree('NN', ['fox'])]), Tree('NP', [Tree('NP', [Tree('NNS', ['jumps'])]),\n                          Tree('PP', [Tree('IN', ['over']), Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])])])])\n\n        parsed_data = next(corenlp_parser.parse(input_string))\n\n        corenlp_parser.api_call.assert_called_once_with(\"The quick brown fox jumps over the lazy dog\", properties={\n                                                        'ssplit.ssplit.eolonly': 'true'})\n        self.assertEqual(expected_output, parsed_data)\n\n    def test_dependency_parser(self):\n        corenlp_parser = corenlp.CoreNLPDependencyParser()\n\n        api_return_value = {\n            'sentences': [{'basicDependencies': [{'dep': 'ROOT',\n                                                  'dependent': 5,\n                                                  'dependentGloss': 'jumps',\n                                                  'governor': 0,\n                                                  'governorGloss': 'ROOT'},\n                                                 {'dep': 'det',\n                                                  'dependent': 1,\n                                                  'dependentGloss': 'The',\n                                                  'governor': 4,\n                                                  'governorGloss': 'fox'},\n                                                 {'dep': 'amod',\n                                                  'dependent': 2,\n                                                  'dependentGloss': 'quick',\n                                                  'governor': 4,\n                                                  'governorGloss': 'fox'},\n                                                 {'dep': 'amod',\n                                                  'dependent': 3,\n                                                  'dependentGloss': 'brown',\n                                                  'governor': 4,\n                                                  'governorGloss': 'fox'},\n                                                 {'dep': 'nsubj',\n                                                  'dependent': 4,\n                                                  'dependentGloss': 'fox',\n                                                  'governor': 5,\n                                                  'governorGloss': 'jumps'},\n                                                 {'dep': 'case',\n                                                  'dependent': 6,\n                                                  'dependentGloss': 'over',\n                                                  'governor': 9,\n                                                  'governorGloss': 'dog'},\n                                                 {'dep': 'det',\n                                                  'dependent': 7,\n                                                  'dependentGloss': 'the',\n                                                  'governor': 9,\n                                                  'governorGloss': 'dog'},\n                                                 {'dep': 'amod',\n                                                  'dependent': 8,\n                                                  'dependentGloss': 'lazy',\n                                                  'governor': 9,\n                                                  'governorGloss': 'dog'},\n                                                 {'dep': 'nmod',\n                                                  'dependent': 9,\n                                                  'dependentGloss': 'dog',\n                                                  'governor': 5,\n                                                  'governorGloss': 'jumps'}],\n                           'enhancedDependencies': [{'dep': 'ROOT',\n                                                     'dependent': 5,\n                                                     'dependentGloss': 'jumps',\n                                                     'governor': 0,\n                                                     'governorGloss': 'ROOT'},\n                                                    {'dep': 'det',\n                                                     'dependent': 1,\n                                                     'dependentGloss': 'The',\n                                                     'governor': 4,\n                                                     'governorGloss': 'fox'},\n                                                    {'dep': 'amod',\n                                                     'dependent': 2,\n                                                     'dependentGloss': 'quick',\n                                                     'governor': 4,\n                                                     'governorGloss': 'fox'},\n                                                    {'dep': 'amod',\n                                                     'dependent': 3,\n                                                     'dependentGloss': 'brown',\n                                                     'governor': 4,\n                                                     'governorGloss': 'fox'},\n                                                    {'dep': 'nsubj',\n                                                     'dependent': 4,\n                                                     'dependentGloss': 'fox',\n                                                     'governor': 5,\n                                                     'governorGloss': 'jumps'},\n                                                    {'dep': 'case',\n                                                     'dependent': 6,\n                                                     'dependentGloss': 'over',\n                                                     'governor': 9,\n                                                     'governorGloss': 'dog'},\n                                                    {'dep': 'det',\n                                                     'dependent': 7,\n                                                     'dependentGloss': 'the',\n                                                     'governor': 9,\n                                                     'governorGloss': 'dog'},\n                                                    {'dep': 'amod',\n                                                     'dependent': 8,\n                                                     'dependentGloss': 'lazy',\n                                                     'governor': 9,\n                                                     'governorGloss': 'dog'},\n                                                    {'dep': 'nmod:over',\n                                                     'dependent': 9,\n                                                     'dependentGloss': 'dog',\n                                                     'governor': 5,\n                                                     'governorGloss': 'jumps'}],\n                           'enhancedPlusPlusDependencies': [{'dep': 'ROOT',\n                                                             'dependent': 5,\n                                                             'dependentGloss': 'jumps',\n                                                             'governor': 0,\n                                                             'governorGloss': 'ROOT'},\n                                                            {'dep': 'det',\n                                                             'dependent': 1,\n                                                             'dependentGloss': 'The',\n                                                             'governor': 4,\n                                                             'governorGloss': 'fox'},\n                                                            {'dep': 'amod',\n                                                             'dependent': 2,\n                                                             'dependentGloss': 'quick',\n                                                             'governor': 4,\n                                                             'governorGloss': 'fox'},\n                                                            {'dep': 'amod',\n                                                             'dependent': 3,\n                                                             'dependentGloss': 'brown',\n                                                             'governor': 4,\n                                                             'governorGloss': 'fox'},\n                                                            {'dep': 'nsubj',\n                                                             'dependent': 4,\n                                                             'dependentGloss': 'fox',\n                                                             'governor': 5,\n                                                             'governorGloss': 'jumps'},\n                                                            {'dep': 'case',\n                                                             'dependent': 6,\n                                                             'dependentGloss': 'over',\n                                                             'governor': 9,\n                                                             'governorGloss': 'dog'},\n                                                            {'dep': 'det',\n                                                             'dependent': 7,\n                                                             'dependentGloss': 'the',\n                                                             'governor': 9,\n                                                             'governorGloss': 'dog'},\n                                                            {'dep': 'amod',\n                                                             'dependent': 8,\n                                                             'dependentGloss': 'lazy',\n                                                             'governor': 9,\n                                                             'governorGloss': 'dog'},\n                                                            {'dep': 'nmod:over',\n                                                             'dependent': 9,\n                                                             'dependentGloss': 'dog',\n                                                             'governor': 5,\n                                                             'governorGloss': 'jumps'}],\n                           'index': 0,\n                           'tokens': [{'after': ' ',\n                                       'before': '',\n                                       'characterOffsetBegin': 0,\n                                       'characterOffsetEnd': 3,\n                                       'index': 1,\n                                       'lemma': 'the',\n                                       'originalText': 'The',\n                                       'pos': 'DT',\n                                       'word': 'The'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 4,\n                                       'characterOffsetEnd': 9,\n                                       'index': 2,\n                                       'lemma': 'quick',\n                                       'originalText': 'quick',\n                                       'pos': 'JJ',\n                                       'word': 'quick'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 10,\n                                       'characterOffsetEnd': 15,\n                                       'index': 3,\n                                       'lemma': 'brown',\n                                       'originalText': 'brown',\n                                       'pos': 'JJ',\n                                       'word': 'brown'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 16,\n                                       'characterOffsetEnd': 19,\n                                       'index': 4,\n                                       'lemma': 'fox',\n                                       'originalText': 'fox',\n                                       'pos': 'NN',\n                                       'word': 'fox'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 20,\n                                       'characterOffsetEnd': 25,\n                                       'index': 5,\n                                       'lemma': 'jump',\n                                       'originalText': 'jumps',\n                                       'pos': 'VBZ',\n                                       'word': 'jumps'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 26,\n                                       'characterOffsetEnd': 30,\n                                       'index': 6,\n                                       'lemma': 'over',\n                                       'originalText': 'over',\n                                       'pos': 'IN',\n                                       'word': 'over'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 31,\n                                       'characterOffsetEnd': 34,\n                                       'index': 7,\n                                       'lemma': 'the',\n                                       'originalText': 'the',\n                                       'pos': 'DT',\n                                       'word': 'the'},\n                                      {'after': ' ',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 35,\n                                       'characterOffsetEnd': 39,\n                                       'index': 8,\n                                       'lemma': 'lazy',\n                                       'originalText': 'lazy',\n                                       'pos': 'JJ',\n                                       'word': 'lazy'},\n                                      {'after': '',\n                                       'before': ' ',\n                                       'characterOffsetBegin': 40,\n                                       'characterOffsetEnd': 43,\n                                       'index': 9,\n                                       'lemma': 'dog',\n                                       'originalText': 'dog',\n                                       'pos': 'NN',\n                                       'word': 'dog'}]}]\n        }\n\n        corenlp_parser.api_call = MagicMock(return_value=api_return_value)\n\n        input_string = \"The quick brown fox jumps over the lazy dog\".split()\n        expected_output = Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree(\n            'dog', ['over', 'the', 'lazy'])])\n\n        parsed_data = next(corenlp_parser.parse(input_string))\n\n        corenlp_parser.api_call.assert_called_once_with(\"The quick brown fox jumps over the lazy dog\", properties={\n                                                        'ssplit.ssplit.eolonly': 'true'})\n        self.assertEqual(expected_output, parsed_data.tree())\n"], "nltk\\test\\unit\\test_corpora": [".py", "from __future__ import absolute_import, unicode_literals\nimport unittest\n\nfrom nltk.corpus import (sinica_treebank, conll2007, indian, cess_cat, cess_esp,\n                         floresta, ptb, udhr) # mwa_ppdb\n\nfrom nltk.compat import python_2_unicode_compatible\nfrom nltk.tree import Tree\nfrom nltk.test.unit.utils import skipIf\n\n\nclass TestUdhr(unittest.TestCase):\n\n    def test_words(self):\n        for name in udhr.fileids():\n            try:\n                words = list(udhr.words(name))\n            except AssertionError:\n                print(name)\n                raise\n            self.assertTrue(words)\n\n    def test_raw_unicode(self):\n        for name in udhr.fileids():\n            txt = udhr.raw(name)\n            assert not isinstance(txt, bytes), name\n\n\nclass TestIndian(unittest.TestCase):\n\n    def test_words(self):\n        words = indian.words()[:3]\n        self.assertEqual(words, ['\u09ae\u09b9\u09bf\u09b7\u09c7\u09b0', '\u09b8\u09a8\u09cd\u09a4\u09be\u09a8', ':'])\n\n    def test_tagged_words(self):\n        tagged_words = indian.tagged_words()[:3]\n        self.assertEqual(tagged_words, [('\u09ae\u09b9\u09bf\u09b7\u09c7\u09b0', 'NN'), ('\u09b8\u09a8\u09cd\u09a4\u09be\u09a8', 'NN'), (':', 'SYM')])\n\n\nclass TestCess(unittest.TestCase):\n    def test_catalan(self):\n        words = cess_cat.words()[:15]\n        txt = \"El Tribunal_Suprem -Fpa- TS -Fpt- ha confirmat la condemna a quatre anys d' inhabilitaci\u00f3 especial\"\n        self.assertEqual(words, txt.split())\n        self.assertEqual(cess_cat.tagged_sents()[0][34][0], \"c\u00e0rrecs\")\n\n    def test_esp(self):\n        words = cess_esp.words()[:15]\n        txt = \"El grupo estatal Electricit\u00e9_de_France -Fpa- EDF -Fpt- anunci\u00f3 hoy , jueves , la compra del\"\n        self.assertEqual(words, txt.split())\n        self.assertEqual(cess_esp.words()[115], \"a\u00f1os\")\n\n\nclass TestFloresta(unittest.TestCase):\n    def test_words(self):\n        words = floresta.words()[:10]\n        txt = \"Um revivalismo refrescante O 7_e_Meio \u00e9 um ex-libris de a\"\n        self.assertEqual(words, txt.split())\n\nclass TestSinicaTreebank(unittest.TestCase):\n\n    def test_sents(self):\n        first_3_sents = sinica_treebank.sents()[:3]\n        self.assertEqual(\n            first_3_sents,\n            [['\u4e00'], ['\u53cb\u60c5'], ['\u5609\u73cd', '\u548c', '\u6211', '\u4f4f\u5728', '\u540c\u4e00\u689d', '\u5df7\u5b50']]\n        )\n\n    def test_parsed_sents(self):\n        parsed_sents = sinica_treebank.parsed_sents()[25]\n        self.assertEqual(parsed_sents,\n            Tree('S', [\n                Tree('NP', [\n                    Tree('Nba', ['\u5609\u73cd'])\n                ]),\n                Tree('V\u2027\u5730', [\n                    Tree('VA11', ['\u4e0d\u505c']),\n                    Tree('DE', ['\u7684'])\n                ]),\n                Tree('VA4', ['\u54ed\u6ce3'])\n            ]))\n\n\nclass TestCoNLL2007(unittest.TestCase):\n\n    def test_sents(self):\n        sents = conll2007.sents('esp.train')[0]\n        self.assertEqual(\n            sents[:6],\n            ['El', 'aumento', 'del', '\u00edndice', 'de', 'desempleo']\n        )\n\n    def test_parsed_sents(self):\n\n        parsed_sents = conll2007.parsed_sents('esp.train')[0]\n\n        self.assertEqual(parsed_sents.tree(),\n            Tree('fortaleci\u00f3', [\n                Tree('aumento', [\n                    'El',\n                    Tree('del', [\n                        Tree('\u00edndice', [\n                            Tree('de', [\n                                Tree('desempleo', ['estadounidense'])\n                            ])\n                        ])\n                    ])\n                ]),\n                'hoy',\n                'considerablemente',\n                Tree('al', [\n                    Tree('euro', [\n                        Tree('cotizaba', [\n                            ',',\n                            'que',\n                            Tree('a', [\n                                Tree('15.35', ['las', 'GMT'])\n                            ]),\n                            'se',\n                            Tree('en', [\n                                Tree('mercado', [\n                                    'el',\n                                    Tree('de', ['divisas']),\n                                    Tree('de', ['Fr\u00e1ncfort'])\n                                ])\n                            ]),\n                            Tree('a', ['0,9452_d\u00f3lares']),\n                            Tree('frente_a', [\n                                ',',\n                                Tree('0,9349_d\u00f3lares', [\n                                    'los',\n                                    Tree('de', [\n                                        Tree('ma\u00f1ana', ['esta'])\n                                    ])\n                                ])\n                            ])\n                        ])\n                    ])\n                ]),\n                '.'\n            ])\n        )\n\n\n@skipIf(not ptb.fileids(), \"A full installation of the Penn Treebank is not available\")\nclass TestPTB(unittest.TestCase):\n    def test_fileids(self):\n        self.assertEqual(\n            ptb.fileids()[:4],\n            ['BROWN/CF/CF01.MRG', 'BROWN/CF/CF02.MRG', 'BROWN/CF/CF03.MRG', 'BROWN/CF/CF04.MRG']\n        )\n\n    def test_words(self):\n        self.assertEqual(\n            ptb.words('WSJ/00/WSJ_0003.MRG')[:7],\n            ['A', 'form', 'of', 'asbestos', 'once', 'used', '*']\n        )\n\n    def test_tagged_words(self):\n        self.assertEqual(\n            ptb.tagged_words('WSJ/00/WSJ_0003.MRG')[:3],\n            [('A', 'DT'), ('form', 'NN'), ('of', 'IN')]\n        )\n\n    def test_categories(self):\n        self.assertEqual(\n            ptb.categories(),\n            ['adventure', 'belles_lettres', 'fiction', 'humor', 'lore', 'mystery', 'news', 'romance', 'science_fiction']\n        )\n\n    def test_news_fileids(self):\n        self.assertEqual(\n            ptb.fileids('news')[:3],\n            ['WSJ/00/WSJ_0001.MRG', 'WSJ/00/WSJ_0002.MRG', 'WSJ/00/WSJ_0003.MRG']\n        )\n\n    def test_category_words(self):\n        self.assertEqual(\n            ptb.words(categories=['humor','fiction'])[:6],\n            ['Thirty-three', 'Scotty', 'did', 'not', 'go', 'back']\n        )\n\n@unittest.skip(\"Skipping test for mwa_ppdb.\")\nclass TestMWAPPDB(unittest.TestCase):\n    def test_fileids(self):\n        self.assertEqual(mwa_ppdb.fileids(),\n            ['ppdb-1.0-xxxl-lexical.extended.synonyms.uniquepairs'])\n\n    def test_entries(self):\n        self.assertEqual(mwa_ppdb.entries()[:10],\n            [('10/17/01', '17/10/2001'), ('102,70', '102.70'),\n            ('13,53', '13.53'), ('3.2.5.3.2.1', '3.2.5.3.2.1.'),\n            ('53,76', '53.76'), ('6.9.5', '6.9.5.'),\n            ('7.7.6.3', '7.7.6.3.'), ('76,20', '76.20'),\n            ('79,85', '79.85'), ('93,65', '93.65')] )\n\nfrom nltk.corpus import teardown_module\n"], "nltk\\test\\unit\\test_corpus_views": [".py", "from __future__ import absolute_import, unicode_literals\nimport unittest\nimport nltk.data\nfrom nltk.corpus.reader.util import (StreamBackedCorpusView,\n                                     read_whitespace_block, read_line_block)\n\nclass TestCorpusViews(unittest.TestCase):\n\n    linetok = nltk.LineTokenizer(blanklines='keep')\n    names = [\n        'corpora/inaugural/README', # A very short file (160 chars)\n        'corpora/inaugural/1793-Washington.txt', # A relatively short file (791 chars)\n        'corpora/inaugural/1909-Taft.txt', # A longer file (32k chars)\n    ]\n\n    def data(self):\n        for name in self.names:\n            f = nltk.data.find(name)\n            with f.open() as fp:\n                file_data = fp.read().decode('utf8')\n            yield f, file_data\n\n    def test_correct_values(self):\n\n        for f, file_data in self.data():\n            v = StreamBackedCorpusView(f, read_whitespace_block)\n            self.assertEqual(list(v), file_data.split())\n\n            v = StreamBackedCorpusView(f, read_line_block)\n            self.assertEqual(list(v), self.linetok.tokenize(file_data))\n\n    def test_correct_length(self):\n\n        for f, file_data in self.data():\n            v = StreamBackedCorpusView(f, read_whitespace_block)\n            self.assertEqual(len(v), len(file_data.split()))\n\n            v = StreamBackedCorpusView(f, read_line_block)\n            self.assertEqual(len(v), len(self.linetok.tokenize(file_data)))\n"], "nltk\\test\\unit\\test_hmm": [".py", "from __future__ import absolute_import, unicode_literals\nfrom nltk.tag import hmm\n\ndef _wikipedia_example_hmm():\n\n    states = ['rain', 'no rain']\n    symbols = ['umbrella', 'no umbrella']\n\n    A = [[0.7, 0.3], [0.3, 0.7]]  # transition probabilities\n    B = [[0.9, 0.1], [0.2, 0.8]]  # emission probabilities\n    pi = [0.5, 0.5]  # initial probabilities\n\n    seq = ['umbrella', 'umbrella', 'no umbrella', 'umbrella', 'umbrella']\n    seq = list(zip(seq, [None]*len(seq)))\n\n    model = hmm._create_hmm_tagger(states, symbols, A, B, pi)\n    return model, states, symbols, seq\n\n\ndef test_forward_probability():\n    from numpy.testing import assert_array_almost_equal\n\n    model, states, symbols = hmm._market_hmm_example()\n    seq = [('up', None), ('up', None)]\n    expected = [\n        [0.35, 0.02, 0.09],\n        [0.1792, 0.0085, 0.0357]\n    ]\n\n    fp = 2**model._forward_probability(seq)\n\n    assert_array_almost_equal(fp, expected)\n\n\ndef test_forward_probability2():\n    from numpy.testing import assert_array_almost_equal\n\n    model, states, symbols, seq = _wikipedia_example_hmm()\n    fp = 2**model._forward_probability(seq)\n\n    fp = (fp.T / fp.sum(axis=1)).T\n\n    wikipedia_results = [\n        [0.8182, 0.1818],\n        [0.8834, 0.1166],\n        [0.1907, 0.8093],\n        [0.7308, 0.2692],\n        [0.8673, 0.1327],\n    ]\n\n    assert_array_almost_equal(wikipedia_results, fp, 4)\n\n\ndef test_backward_probability():\n    from numpy.testing import assert_array_almost_equal\n\n    model, states, symbols, seq = _wikipedia_example_hmm()\n\n    bp = 2**model._backward_probability(seq)\n\n    bp = (bp.T / bp.sum(axis=1)).T\n\n    wikipedia_results = [\n        [0.5923, 0.4077],\n        [0.3763, 0.6237],\n        [0.6533, 0.3467],\n        [0.6273, 0.3727],\n        [0.5, 0.5],\n    ]\n\n    assert_array_almost_equal(wikipedia_results, bp, 4)\n\n\ndef setup_module(module):\n    from nose import SkipTest\n    try:\n        import numpy\n    except ImportError:\n        raise SkipTest(\"numpy is required for nltk.test.test_hmm\")\n"], "nltk\\test\\unit\\test_json2csv_corpus": [".py", "\n\nimport os\nfrom nltk.compat import TemporaryDirectory\nimport unittest\n\nfrom six.moves import zip\n\nfrom nltk.corpus import twitter_samples\nfrom nltk.twitter.common import json2csv, json2csv_entities\n\n\ndef are_files_identical(filename1, filename2, debug=False):\n    with open(filename1, \"rb\") as fileA:\n        with open(filename2, \"rb\") as fileB:\n            result = True\n            for lineA, lineB in zip(sorted(fileA.readlines()),\n                                     sorted(fileB.readlines())):\n                if lineA.strip() != lineB.strip():\n                    if debug:\n                        print(\"Error while comparing files. \" +\n                              \"First difference at line below.\")\n                        print(\"=> Output file line: {0}\".format(lineA))\n                        print(\"=> Refer. file line: {0}\".format(lineB))\n                    result = False\n                    break\n            return result\n\n\nclass TestJSON2CSV(unittest.TestCase):\n\n    def setUp(self):\n        with open(twitter_samples.abspath(\"tweets.20150430-223406.json\")) as infile:\n            self.infile = [next(infile) for x in range(100)]\n        infile.close()\n        self.msg = \"Test and reference files are not the same\"\n        self.subdir = os.path.join(os.path.dirname(__file__), 'files')\n\n    def tearDown(self):\n        return\n\n    def test_textoutput(self):\n        ref_fn = os.path.join(self.subdir, 'tweets.20150430-223406.text.csv.ref')\n        with TemporaryDirectory() as tempdir:\n            outfn = os.path.join(tempdir, 'tweets.20150430-223406.text.csv')\n            json2csv(self.infile, outfn, ['text'], gzip_compress=False)\n            self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)\n\n\n    def test_tweet_metadata(self):\n        ref_fn = os.path.join(self.subdir, 'tweets.20150430-223406.tweet.csv.ref')\n        fields = ['created_at', 'favorite_count', 'id',\n                  'in_reply_to_status_id', 'in_reply_to_user_id', 'retweet_count',\n                  'retweeted', 'text', 'truncated', 'user.id']\n\n        with TemporaryDirectory() as tempdir:\n            outfn = os.path.join(tempdir, 'tweets.20150430-223406.tweet.csv')\n            json2csv(self.infile, outfn, fields, gzip_compress=False)\n            self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)\n\n\n    def test_user_metadata(self):\n        ref_fn = os.path.join(self.subdir, 'tweets.20150430-223406.user.csv.ref')\n        fields = ['id', 'text', 'user.id', 'user.followers_count', 'user.friends_count']\n\n        with TemporaryDirectory() as tempdir:\n            outfn = os.path.join(tempdir, 'tweets.20150430-223406.user.csv')\n            json2csv(self.infile, outfn, fields, gzip_compress=False)\n            self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)\n\n\n    def test_tweet_hashtag(self):\n        ref_fn = os.path.join(self.subdir, 'tweets.20150430-223406.hashtag.csv.ref')\n        with TemporaryDirectory() as tempdir:\n            outfn = os.path.join(tempdir, 'tweets.20150430-223406.hashtag.csv')\n            json2csv_entities(self.infile, outfn,\n                              ['id', 'text'], 'hashtags', ['text'],\n                              gzip_compress=False)\n            self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)\n\n\n    def test_tweet_usermention(self):\n        ref_fn = os.path.join(self.subdir, 'tweets.20150430-223406.usermention.csv.ref')\n        with TemporaryDirectory() as tempdir:\n            outfn = os.path.join(tempdir, 'tweets.20150430-223406.usermention.csv')\n            json2csv_entities(self.infile, outfn,\n                              ['id', 'text'], 'user_mentions', ['id', 'screen_name'],\n                              gzip_compress=False)\n            self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)\n\n\n    def test_tweet_media(self):\n        ref_fn = os.path.join(self.subdir, 'tweets.20150430-223406.media.csv.ref')\n        with TemporaryDirectory() as tempdir:\n            outfn = os.path.join(tempdir, 'tweets.20150430-223406.media.csv')\n            json2csv_entities(self.infile, outfn,\n                              ['id'], 'media', ['media_url', 'url'],\n                              gzip_compress=False)\n\n            self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)\n\n\n    def test_tweet_url(self):\n        ref_fn = os.path.join(self.subdir, 'tweets.20150430-223406.url.csv.ref')\n        with TemporaryDirectory() as tempdir:\n            outfn = os.path.join(tempdir, 'tweets.20150430-223406.url.csv')\n            json2csv_entities(self.infile, outfn,\n                              ['id'], 'urls', ['url', 'expanded_url'],\n                              gzip_compress=False)\n\n            self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)\n\n\n    def test_userurl(self):\n        ref_fn = os.path.join(self.subdir, 'tweets.20150430-223406.userurl.csv.ref')\n        with TemporaryDirectory() as tempdir:\n            outfn = os.path.join(tempdir, 'tweets.20150430-223406.userurl.csv')\n            json2csv_entities(self.infile, outfn, ['id', 'screen_name'],\n                              'user.urls', ['url', 'expanded_url'],\n                              gzip_compress=False)\n\n            self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)\n\n\n    def test_tweet_place(self):\n        ref_fn = os.path.join(self.subdir, 'tweets.20150430-223406.place.csv.ref')\n        with TemporaryDirectory() as tempdir:\n            outfn = os.path.join(tempdir, 'tweets.20150430-223406.place.csv')\n            json2csv_entities(self.infile, outfn,\n                              ['id', 'text'], 'place', ['name', 'country'],\n                              gzip_compress=False)\n\n            self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)\n\n\n    def test_tweet_place_boundingbox(self):\n        ref_fn = os.path.join(self.subdir, 'tweets.20150430-223406.placeboundingbox.csv.ref')\n        with TemporaryDirectory() as tempdir:\n            outfn = os.path.join(tempdir, 'tweets.20150430-223406.placeboundingbox.csv')\n            json2csv_entities(self.infile, outfn,\n                              ['id', 'name'], 'place.bounding_box', ['coordinates'],\n                              gzip_compress=False)\n\n            self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)\n\n\n    def test_retweet_original_tweet(self):\n        ref_fn = os.path.join(self.subdir, 'tweets.20150430-223406.retweet.csv.ref')\n        with TemporaryDirectory() as tempdir:\n            outfn = os.path.join(tempdir, 'tweets.20150430-223406.retweet.csv')\n            json2csv_entities(self.infile, outfn, ['id'], 'retweeted_status',\n                              ['created_at', 'favorite_count', 'id', 'in_reply_to_status_id',\n                               'in_reply_to_user_id', 'retweet_count', 'text', 'truncated',\n                               'user.id'],\n                              gzip_compress=False)\n\n            self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)\n\n\n    def test_file_is_wrong(self):\n        ref_fn = os.path.join(self.subdir, 'tweets.20150430-223406.retweet.csv.ref')\n        with TemporaryDirectory() as tempdir:\n            outfn = os.path.join(tempdir, 'tweets.20150430-223406.text.csv')\n            json2csv(self.infile, outfn, ['text'], gzip_compress=False)\n            self.assertFalse(are_files_identical(outfn, ref_fn), msg=self.msg)\n\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"], "nltk\\test\\unit\\test_naivebayes": [".py", "from __future__ import print_function, unicode_literals\n\n\nimport unittest\nfrom nltk.classify.naivebayes import NaiveBayesClassifier\n\n\nclass NaiveBayesClassifierTest(unittest.TestCase):\n\n    def test_simple(self):\n        training_features = [\n            ({'nice': True, 'good': True}, 'positive'),\n            ({'bad': True, 'mean': True}, 'negative')\n        ]\n\n        classifier = NaiveBayesClassifier.train(training_features)\n\n        result = classifier.prob_classify({'nice': True})\n        self.assertTrue(result.prob('positive') >\n                        result.prob('negative'))\n        self.assertEqual(result.max(), 'positive')\n\n        result = classifier.prob_classify({'bad': True})\n        self.assertTrue(result.prob('positive') < result.prob('negative'))\n        self.assertEqual(result.max(), 'negative')\n"], "nltk\\test\\unit\\test_rte_classify": [".py", "from __future__ import print_function, unicode_literals\n\nimport unittest\n\nfrom nltk.corpus import rte as rte_corpus\nfrom nltk.classify.rte_classify import RTEFeatureExtractor, rte_features, rte_classifier\n\nexpected_from_rte_feature_extration = \"\"\"\nalwayson        => True\nne_hyp_extra    => 0\nne_overlap      => 1\nneg_hyp         => 0\nneg_txt         => 0\nword_hyp_extra  => 3\nword_overlap    => 3\n\nalwayson        => True\nne_hyp_extra    => 0\nne_overlap      => 1\nneg_hyp         => 0\nneg_txt         => 0\nword_hyp_extra  => 2\nword_overlap    => 1\n\nalwayson        => True\nne_hyp_extra    => 1\nne_overlap      => 1\nneg_hyp         => 0\nneg_txt         => 0\nword_hyp_extra  => 1\nword_overlap    => 2\n\nalwayson        => True\nne_hyp_extra    => 1\nne_overlap      => 0\nneg_hyp         => 0\nneg_txt         => 0\nword_hyp_extra  => 6\nword_overlap    => 2\n\nalwayson        => True\nne_hyp_extra    => 1\nne_overlap      => 0\nneg_hyp         => 0\nneg_txt         => 0\nword_hyp_extra  => 4\nword_overlap    => 0\n\nalwayson        => True\nne_hyp_extra    => 1\nne_overlap      => 0\nneg_hyp         => 0\nneg_txt         => 0\nword_hyp_extra  => 3\nword_overlap    => 1\n\"\"\"\n\n\nclass RTEClassifierTest(unittest.TestCase):\n    def test_rte_feature_extraction(self):\n        pairs = rte_corpus.pairs(['rte1_dev.xml'])[:6]\n        test_output = [\"%-15s => %s\" % (key, rte_features(pair)[key])\n                       for pair in pairs for key in sorted(rte_features(pair))]\n        expected_output = expected_from_rte_feature_extration.strip().split('\\n')\n        expected_output = list(filter(None, expected_output))\n        self.assertEqual(test_output, expected_output)\n    def test_feature_extractor_object(self):\n        rtepair = rte_corpus.pairs(['rte3_dev.xml'])[33]\n        extractor = RTEFeatureExtractor(rtepair)\n        self.assertEqual(extractor.hyp_words, {'member', 'China', 'SCO.'})\n        self.assertEqual(extractor.overlap('word'), set())\n        self.assertEqual(extractor.overlap('ne'), {'China'})\n        self.assertEqual(extractor.hyp_extra('word'), {'member'})\n    def test_rte_classification_without_megam(self):\n        clf = rte_classifier('IIS')\n        clf = rte_classifier('GIS')\n    @unittest.skip(\"Skipping tests with dependencies on MEGAM\")\n    def test_rte_classification_with_megam(self):\n        nltk.config_megam('/usr/local/bin/megam')\n        clf = rte_classifier('megam')\n        clf = rte_classifier('BFGS')\n"], "nltk\\test\\unit\\test_seekable_unicode_stream_reader": [".py", "from __future__ import absolute_import, unicode_literals\nimport random\nimport functools\nfrom io import BytesIO\nfrom nltk.corpus.reader import SeekableUnicodeStreamReader\n\ndef check_reader(unicode_string, encoding, n=1000):\n    bytestr = unicode_string.encode(encoding)\n    strlen = len(unicode_string)\n    stream = BytesIO(bytestr)\n    reader = SeekableUnicodeStreamReader(stream, encoding)\n    chars = []\n    while True:\n        pos = reader.tell()\n        chars.append( (pos, reader.read(1)) )\n        if chars[-1][1] == '': break\n    strings = dict( (pos,'') for (pos,c) in chars )\n    for pos1, char in chars:\n        for pos2, _ in chars:\n            if pos2 <= pos1:\n                strings[pos2] += char\n    while True:\n        op = random.choice('tsrr')\n        if op == 't': # tell\n            reader.tell()\n        if op == 's': # seek\n            new_pos = random.choice([p for (p,c) in chars])\n            reader.seek(new_pos)\n        if op == 'r': # read\n            if random.random() < .3: pos = reader.tell()\n            else: pos = None\n            if random.random() < .2: size = None\n            elif random.random() < .8:\n                size = random.randint(0, int(strlen/6))\n            else: size = random.randint(0, strlen+20)\n            if random.random() < .8:\n                s = reader.read(size)\n            else:\n                s = reader.readline(size)\n            if pos is not None:\n                assert pos in strings\n                assert strings[pos].startswith(s)\n                n -= 1\n                if n == 0:\n                    return 'passed'\n\n\n\nENCODINGS = ['ascii', 'latin1', 'greek', 'hebrew', 'utf-16', 'utf-8']\n\nSTRINGS = ["], "nltk\\test\\unit\\test_senna": [".py", "\nfrom __future__ import unicode_literals\nfrom os import environ, path, sep\n\nimport logging\nimport unittest\n\nfrom nltk.classify import Senna\nfrom nltk.tag import SennaTagger, SennaChunkTagger, SennaNERTagger\n\nif 'SENNA' in environ:\n    SENNA_EXECUTABLE_PATH = path.normpath(environ['SENNA']) + sep\nelse:\n    SENNA_EXECUTABLE_PATH = '/usr/share/senna-v3.0'\n\nsenna_is_installed = path.exists(SENNA_EXECUTABLE_PATH)\n\n@unittest.skipUnless(senna_is_installed, \"Requires Senna executable\")\nclass TestSennaPipeline(unittest.TestCase):\n\n    def test_senna_pipeline(self):\n\n        pipeline = Senna(SENNA_EXECUTABLE_PATH, ['pos', 'chk', 'ner'])\n        sent = 'Dusseldorf is an international business center'.split()\n        result = [(token['word'], token['chk'], token['ner'], token['pos']) for token in pipeline.tag(sent)]\n        expected = [('Dusseldorf', 'B-NP', 'B-LOC', 'NNP'), ('is', 'B-VP',\n            'O', 'VBZ'), ('an', 'B-NP', 'O', 'DT'), ('international', 'I-NP',\n            'O', 'JJ'), ('business', 'I-NP', 'O', 'NN'), ('center', 'I-NP',\n            'O', 'NN')]\n        self.assertEqual(result, expected)\n\n@unittest.skipUnless(senna_is_installed, \"Requires Senna executable\")\nclass TestSennaTagger(unittest.TestCase):\n\n    def test_senna_tagger(self):\n        tagger = SennaTagger(SENNA_EXECUTABLE_PATH)\n        result = tagger.tag('What is the airspeed of an unladen swallow ?'.split())\n        expected = [('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed',\n            'NN'),('of', 'IN'), ('an', 'DT'), ('unladen', 'NN'), ('swallow',\n            'NN'), ('?', '.')]\n        self.assertEqual(result, expected)\n\n    def test_senna_chunk_tagger(self):\n        chktagger = SennaChunkTagger(SENNA_EXECUTABLE_PATH)\n        result_1 = chktagger.tag('What is the airspeed of an unladen swallow ?'.split())\n        expected_1 = [('What', 'B-NP'), ('is', 'B-VP'), ('the', 'B-NP'), ('airspeed',\n            'I-NP'), ('of', 'B-PP'), ('an', 'B-NP'), ('unladen', 'I-NP'), ('swallow',\n            'I-NP'), ('?', 'O')]\n\n        result_2 = list(chktagger.bio_to_chunks(result_1, chunk_type='NP'))\n        expected_2 = [('What', '0'), ('the airspeed', '2-3'), ('an unladen swallow',\n            '5-6-7')]\n        self.assertEqual(result_1, expected_1)\n        self.assertEqual(result_2, expected_2)\n\n    def test_senna_ner_tagger(self):\n        nertagger = SennaNERTagger(SENNA_EXECUTABLE_PATH)\n        result_1 = nertagger.tag('Shakespeare theatre was in London .'.split())\n        expected_1 = [('Shakespeare', 'B-PER'), ('theatre', 'O'), ('was', 'O'),\n                ('in', 'O'), ('London', 'B-LOC'), ('.', 'O')]\n\n        result_2 = nertagger.tag('UN headquarters are in NY , USA .'.split())\n        expected_2 = [('UN', 'B-ORG'), ('headquarters', 'O'), ('are', 'O'),\n        ('in', 'O'), ('NY', 'B-LOC'), (',', 'O'), ('USA', 'B-LOC'), ('.', 'O')]\n        self.assertEqual(result_1, expected_1)\n        self.assertEqual(result_2, expected_2)\n"], "nltk\\test\\unit\\test_stem": [".py", "from __future__ import print_function, unicode_literals\nimport unittest\nfrom contextlib import closing\nfrom nltk import data\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.porter import PorterStemmer\nimport os\n\n\nclass SnowballTest(unittest.TestCase):\n\n    def test_arabic(self):\n        ar_stemmer = SnowballStemmer(\"arabic\")\n        assert ar_stemmer.stem('\u0627\u0644\u0652\u0639\u064e\u0631\u064e\u0628\u0650\u0640\u0640\u0640\u0640\u0640\u0640\u064a\u0651\u064e\u0629') == \"\u0639\u0631\u0628\"\n        assert ar_stemmer.stem(\"\u0627\u0644\u0639\u0631\u0628\u064a\u0629\") == \"\u0639\u0631\u0628\"\n        assert ar_stemmer.stem(\"\u0641\u0642\u0627\u0644\u0648\u0627\") == \"\u0642\u0627\u0644\"\n        assert ar_stemmer.stem(\"\u0627\u0644\u0637\u0627\u0644\u0628\u0627\u062a\") == \"\u0637\u0627\u0644\u0628\"\n        assert ar_stemmer.stem(\"\u0641\u0627\u0644\u0637\u0627\u0644\u0628\u0627\u062a\") == \"\u0637\u0627\u0644\u0628\"\n        assert ar_stemmer.stem(\"\u0648\u0627\u0644\u0637\u0627\u0644\u0628\u0627\u062a\") == \"\u0637\u0627\u0644\u0628\"\n        assert ar_stemmer.stem(\"\u0627\u0644\u0637\u0627\u0644\u0628\u0648\u0646\") == \"\u0637\u0627\u0644\u0628\"\n\n    def test_russian(self):\n        stemmer_russian = SnowballStemmer(\"russian\")\n        assert stemmer_russian.stem(\"\u0430\u0432\u0430\u043d\u0442\u043d\u0435\u043d\u044c\u043a\u0430\u044f\") == \"\u0430\u0432\u0430\u043d\u0442\u043d\u0435\u043d\u044c\u043a\"\n        assert stemmer_russian.stem(\"avenantnen'kai^a\") == \"avenantnen'k\"\n\n    def test_german(self):\n        stemmer_german = SnowballStemmer(\"german\")\n        stemmer_german2 = SnowballStemmer(\"german\", ignore_stopwords=True)\n\n        assert stemmer_german.stem(\"Schr\\xe4nke\") == 'schrank'\n        assert stemmer_german2.stem(\"Schr\\xe4nke\") == 'schrank'\n\n        assert stemmer_german.stem(\"keinen\") == 'kein'\n        assert stemmer_german2.stem(\"keinen\") == 'keinen'\n\n    def test_spanish(self):\n        stemmer = SnowballStemmer('spanish')\n\n        assert stemmer.stem(\"Visionado\") == 'vision'\n\n        assert stemmer.stem(\"algue\") == 'algu'\n\n    def test_short_strings_bug(self):\n        stemmer = SnowballStemmer('english')\n        assert stemmer.stem(\"y's\") == 'y'\n\nclass PorterTest(unittest.TestCase):\n    \n    def _vocabulary(self):\n        with closing(data.find('stemmers/porter_test/porter_vocabulary.txt').open(encoding='utf-8')) as fp:\n            return fp.read().splitlines()\n        \n    def _test_against_expected_output(self, stemmer_mode, expected_stems):\n        stemmer = PorterStemmer(mode=stemmer_mode)\n        for word, true_stem in zip(self._vocabulary(), expected_stems):\n            our_stem = stemmer.stem(word)\n            assert our_stem == true_stem, (\n                \"%s should stem to %s in %s mode but got %s\" % (\n                    word, true_stem, stemmer_mode, our_stem\n                )\n            )\n    \n    def test_vocabulary_martin_mode(self):\n        with closing(data.find('stemmers/porter_test/porter_martin_output.txt').open(encoding='utf-8')) as fp:\n            self._test_against_expected_output(\n                PorterStemmer.MARTIN_EXTENSIONS,\n                fp.read().splitlines()\n            )\n        \n    def test_vocabulary_nltk_mode(self):\n        with closing(data.find('stemmers/porter_test/porter_nltk_output.txt').open(encoding='utf-8')) as fp:\n            self._test_against_expected_output(\n                PorterStemmer.NLTK_EXTENSIONS,\n                fp.read().splitlines()\n            )\n        \n    def test_vocabulary_original_mode(self):\n\n        with closing(data.find('stemmers/porter_test/porter_original_output.txt').open(encoding='utf-8')) as fp:\n            self._test_against_expected_output(\n                PorterStemmer.ORIGINAL_ALGORITHM,\n                fp.read().splitlines()\n            )\n\n        self._test_against_expected_output(\n            PorterStemmer.ORIGINAL_ALGORITHM,\n            data.find('stemmers/porter_test/porter_original_output.txt')\n                .open(encoding='utf-8')\n                .read()\n                .splitlines()\n        )\n\n    def test_oed_bug(self):\n        assert PorterStemmer().stem('oed') == 'o'\n"], "nltk\\test\\unit\\test_tag": [".py", "from __future__ import absolute_import, unicode_literals\n\ndef test_basic():\n    from nltk.tag import pos_tag\n    from nltk.tokenize import word_tokenize\n\n    result = pos_tag(word_tokenize(\"John's big idea isn't all that bad.\"))\n    assert result == [('John', 'NNP'), (\"'s\", 'POS'), ('big', 'JJ'),\n                      ('idea', 'NN'), ('is', 'VBZ'), (\"n't\", 'RB'),\n                      ('all', 'PDT'), ('that', 'DT'), ('bad', 'JJ'),\n                      ('.', '.')]\n\n\ndef setup_module(module):\n    from nose import SkipTest\n    try:\n        import numpy\n    except ImportError:\n        raise SkipTest(\"numpy is required for nltk.test.test_tag\")\n"], "nltk\\test\\unit\\test_tgrep": [".py", "\n'''\nUnit tests for nltk.tgrep.\n'''\n\nfrom __future__ import absolute_import, print_function, unicode_literals\n\nfrom six import b\n\nfrom nltk.tree import ParentedTree\nfrom nltk import tgrep\nimport unittest\n\nclass TestSequenceFunctions(unittest.TestCase):\n\n    '''\n    Class containing unit tests for nltk.tgrep.\n    '''\n\n    def test_tokenize_simple(self):\n        '''\n        Simple test of tokenization.\n        '''\n        tokens = tgrep.tgrep_tokenize('A .. (B !< C . D) | ![<< (E , F) $ G]')\n        self.assertEqual(tokens,\n                         ['A', '..', '(', 'B', '!', '<', 'C', '.', 'D', ')',\n                          '|', '!', '[', '<<', '(', 'E', ',', 'F', ')', '$',\n                          'G', ']'])\n\n    def test_tokenize_encoding(self):\n        '''\n        Test that tokenization handles bytes and strs the same way.\n        '''\n        self.assertEqual(\n            tgrep.tgrep_tokenize(b('A .. (B !< C . D) | ![<< (E , F) $ G]')),\n            tgrep.tgrep_tokenize('A .. (B !< C . D) | ![<< (E , F) $ G]'))\n\n    def test_tokenize_link_types(self):\n        '''\n        Test tokenization of basic link types.\n        '''\n        self.assertEqual(tgrep.tgrep_tokenize('A<B'),     ['A', '<', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A>B'),     ['A', '>', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A<3B'),    ['A', '<3', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A>3B'),    ['A', '>3', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A<,B'),    ['A', '<,', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A>,B'),    ['A', '>,', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A<-3B'),   ['A', '<-3', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A>-3B'),   ['A', '>-3', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A<-B'),    ['A', '<-', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A>-B'),    ['A', '>-', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A<\\'B'),   ['A', '<\\'', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A>\\'B'),   ['A', '>\\'', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A<:B'),    ['A', '<:', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A>:B'),    ['A', '>:', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A<<B'),    ['A', '<<', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A>>B'),    ['A', '>>', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A<<,B'),   ['A', '<<,', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A>>,B'),   ['A', '>>,', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A<<\\'B'),  ['A', '<<\\'', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A>>\\'B'),  ['A', '>>\\'', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A<<:B'),   ['A', '<<:', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A>>:B'),   ['A', '>>:', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A.B'),     ['A', '.', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A,B'),     ['A', ',', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A..B'),    ['A', '..', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A,,B'),    ['A', ',,', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A$B'),     ['A', '$', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A$.B'),    ['A', '$.', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A$,B'),    ['A', '$,', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A$..B'),   ['A', '$..', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A$,,B'),   ['A', '$,,', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!<B'),    ['A', '!', '<', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!>B'),    ['A', '!', '>', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!<3B'),   ['A', '!', '<3', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!>3B'),   ['A', '!', '>3', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!<,B'),   ['A', '!', '<,', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!>,B'),   ['A', '!', '>,', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!<-3B'),\n                         ['A', '!', '<-3', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!>-3B'),\n                         ['A', '!', '>-3', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!<-B'),   ['A', '!', '<-', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!>-B'),   ['A', '!', '>-', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!<\\'B'),\n                         ['A', '!', '<\\'', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!>\\'B'),\n                         ['A', '!', '>\\'', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!<:B'),   ['A', '!', '<:', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!>:B'),   ['A', '!', '>:', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!<<B'),   ['A', '!', '<<', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!>>B'),   ['A', '!', '>>', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!<<,B'),\n                         ['A', '!', '<<,', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!>>,B'),\n                         ['A', '!', '>>,', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!<<\\'B'),\n                         ['A', '!', '<<\\'', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!>>\\'B'),\n                         ['A', '!', '>>\\'', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!<<:B'),\n                         ['A', '!', '<<:', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!>>:B'),\n                         ['A', '!', '>>:', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!.B'),    ['A', '!', '.', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!,B'),    ['A', '!', ',', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!..B'),   ['A', '!', '..', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!,,B'),   ['A', '!', ',,', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!$B'),    ['A', '!', '$', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!$.B'),   ['A', '!', '$.', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!$,B'),   ['A', '!', '$,', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!$..B'),\n                         ['A', '!', '$..', 'B'])\n        self.assertEqual(tgrep.tgrep_tokenize('A!$,,B'),\n                         ['A', '!', '$,,', 'B'])\n\n    def test_tokenize_examples(self):\n        '''\n        Test tokenization of the TGrep2 manual example patterns.\n        '''\n        self.assertEqual(tgrep.tgrep_tokenize('NP < PP'),\n                         ['NP', '<', 'PP'])\n        self.assertEqual(tgrep.tgrep_tokenize('/^NP/'),\n                         ['/^NP/'])\n        self.assertEqual(tgrep.tgrep_tokenize('NP << PP . VP'),\n                         ['NP', '<<', 'PP', '.', 'VP'])\n        self.assertEqual(tgrep.tgrep_tokenize('NP << PP | . VP'),\n                         ['NP', '<<', 'PP', '|', '.', 'VP'])\n        self.assertEqual(tgrep.tgrep_tokenize('NP !<< PP [> NP | >> VP]'),\n                         ['NP', '!', '<<', 'PP', '[', '>', 'NP', '|',\n                          '>>', 'VP', ']'])\n        self.assertEqual(tgrep.tgrep_tokenize('NP << (PP . VP)'),\n                         ['NP', '<<', '(', 'PP', '.', 'VP', ')'])\n        self.assertEqual(tgrep.tgrep_tokenize('NP <\\' (PP <, (IN < on))'),\n                         ['NP', '<\\'', '(', 'PP', '<,', '(', 'IN', '<',\n                          'on', ')', ')'])\n        self.assertEqual(tgrep.tgrep_tokenize('S < (A < B) < C'),\n                         ['S', '<', '(', 'A', '<', 'B', ')', '<', 'C'])\n        self.assertEqual(tgrep.tgrep_tokenize('S < ((A < B) < C)'),\n                         ['S', '<', '(', '(', 'A', '<', 'B', ')',\n                          '<', 'C', ')'])\n        self.assertEqual(tgrep.tgrep_tokenize('S < (A < B < C)'),\n                         ['S', '<', '(', 'A', '<', 'B', '<', 'C', ')'])\n        self.assertEqual(tgrep.tgrep_tokenize('A<B&.C'),\n                         ['A', '<', 'B', '&', '.', 'C'])\n\n    def test_tokenize_quoting(self):\n        '''\n        Test tokenization of quoting.\n        '''\n        self.assertEqual(tgrep.tgrep_tokenize('\"A<<:B\"<<:\"A $.. B\"<\"A>3B\"<C'),\n                         ['\"A<<:B\"', '<<:', '\"A $.. B\"', '<', '\"A>3B\"',\n                          '<', 'C'])\n\n    def test_tokenize_nodenames(self):\n        '''\n        Test tokenization of node names.\n        '''\n        self.assertEqual(tgrep.tgrep_tokenize('Robert'), ['Robert'])\n        self.assertEqual(tgrep.tgrep_tokenize('/^[Bb]ob/'), ['/^[Bb]ob/'])\n        self.assertEqual(tgrep.tgrep_tokenize('*'), ['*'])\n        self.assertEqual(tgrep.tgrep_tokenize('__'), ['__'])\n        self.assertEqual(tgrep.tgrep_tokenize('N()'),\n                         ['N(', ')'])\n        self.assertEqual(tgrep.tgrep_tokenize('N(0,)'),\n                         ['N(', '0', ',', ')'])\n        self.assertEqual(tgrep.tgrep_tokenize('N(0,0)'),\n                         ['N(', '0', ',', '0', ')'])\n        self.assertEqual(tgrep.tgrep_tokenize('N(0,0,)'),\n                         ['N(', '0', ',', '0', ',', ')'])\n\n    def test_tokenize_macros(self):\n        '''\n        Test tokenization of macro definitions.\n        '''\n        self.assertEqual(tgrep.tgrep_tokenize(\n            '@ NP /^NP/;\\n@ NN /^NN/;\\n@NP [!< NP | < @NN] !$.. @NN'),\n                         ['@', 'NP', '/^NP/', ';', '@', 'NN', '/^NN/', ';',\n                          '@NP', '[', '!', '<', 'NP', '|', '<', '@NN', ']',\n                          '!', '$..', '@NN'])\n\n    def test_node_simple(self):\n        '''\n        Test a simple use of tgrep for finding nodes matching a given\n        pattern.\n        '''\n        tree = ParentedTree.fromstring(\n            '(S (NP (DT the) (JJ big) (NN dog)) '\n            '(VP bit) (NP (DT a) (NN cat)))')\n        self.assertEqual(list(tgrep.tgrep_positions('NN', [tree])),\n                         [[(0,2), (2,1)]])\n        self.assertEqual(list(tgrep.tgrep_nodes('NN', [tree])),\n                         [[tree[0,2], tree[2,1]]])\n        self.assertEqual(list(tgrep.tgrep_positions('NN|JJ', [tree])),\n                         [[(0, 1), (0, 2), (2, 1)]])\n\n    def test_node_printing(self):\n        '''Test that the tgrep print operator ' is properly ignored.'''\n        tree = ParentedTree.fromstring('(S (n x) (N x))')\n        self.assertEqual(list(tgrep.tgrep_positions('N', [tree])),\n                         list(tgrep.tgrep_positions('\\'N', [tree])))\n        self.assertEqual(list(tgrep.tgrep_positions('/[Nn]/', [tree])),\n                         list(tgrep.tgrep_positions('\\'/[Nn]/', [tree])))\n\n    def test_node_encoding(self):\n        '''\n        Test that tgrep search strings handles bytes and strs the same\n        way.\n        '''\n        tree = ParentedTree.fromstring(\n            '(S (NP (DT the) (JJ big) (NN dog)) '\n            '(VP bit) (NP (DT a) (NN cat)))')\n        self.assertEqual(list(tgrep.tgrep_positions(b('NN'), [tree])),\n                         list(tgrep.tgrep_positions('NN', [tree])))\n        self.assertEqual(list(tgrep.tgrep_nodes(b('NN'), [tree])),\n                         list(tgrep.tgrep_nodes('NN', [tree])))\n        self.assertEqual(list(tgrep.tgrep_positions(b('NN|JJ'), [tree])),\n                         list(tgrep.tgrep_positions('NN|JJ', [tree])))\n\n    def test_node_nocase(self):\n        '''\n        Test selecting nodes using case insensitive node names.\n        '''\n        tree = ParentedTree.fromstring('(S (n x) (N x))')\n        self.assertEqual(list(tgrep.tgrep_positions('\"N\"', [tree])), [[(1,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('i@\"N\"', [tree])), [[(0,), (1,)]])\n\n    def test_node_quoted(self):\n        '''\n        Test selecting nodes using quoted node names.\n        '''\n        tree = ParentedTree.fromstring('(N (\"N\" x) (N\" x) (\"\\\\\" x))')\n        self.assertEqual(list(tgrep.tgrep_positions('\"N\"', [tree])), [[()]])\n        self.assertEqual(list(tgrep.tgrep_positions('\"\\\\\"N\\\\\"\"', [tree])), [[(0,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('\"N\\\\\"\"', [tree])), [[(1,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('\"\\\\\"\\\\\\\\\\\\\"\"', [tree])), [[(2,)]])\n\n    def test_node_regex(self):\n        '''\n        Test regex matching on nodes.\n        '''\n        tree = ParentedTree.fromstring('(S (NP-SBJ x) (NP x) (NNP x) (VP x))')\n        self.assertEqual(list(tgrep.tgrep_positions('/^NP/', [tree])),\n                         [[(0,), (1,)]])\n\n    def test_node_regex_2(self):\n        '''\n        Test regex matching on nodes.\n        '''\n        tree = ParentedTree.fromstring('(S (SBJ x) (SBJ1 x) (NP-SBJ x))')\n        self.assertEqual(list(tgrep.tgrep_positions('/^SBJ/', [tree])),\n                         [[(0,), (1,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('/SBJ/', [tree])),\n                         [[(0,), (1,), (2,)]])\n\n    def test_node_tree_position(self):\n        '''\n        Test matching on nodes based on NLTK tree position.\n        '''\n        tree = ParentedTree.fromstring('(S (NP-SBJ x) (NP x) (NNP x) (VP x))')\n        leaf_positions = set([tree.leaf_treeposition(x)\n                              for x in range(len(tree.leaves()))])\n        tree_positions = [x for x in tree.treepositions()\n                          if x not in leaf_positions]\n        for position in tree_positions:\n            node_id = 'N{0}'.format(position)\n            tgrep_positions = list(tgrep.tgrep_positions(node_id, [tree]))\n            self.assertEqual(len(tgrep_positions[0]), 1)\n            self.assertEqual(tgrep_positions[0][0], position)\n\n    def test_node_noleaves(self):\n        '''\n        Test node name matching with the search_leaves flag set to False.\n        '''\n        tree = ParentedTree.fromstring('(S (A (T x)) (B (N x)))')\n        self.assertEqual(list(tgrep.tgrep_positions('x', [tree])),\n                         [[(0, 0, 0), (1, 0, 0)]])\n        self.assertEqual(list(tgrep.tgrep_positions('x', [tree], False)),\n                         [[]])\n\n    def tests_rel_dominance(self):\n        '''\n        Test matching nodes based on dominance relations.\n        '''\n        tree = ParentedTree.fromstring('(S (A (T x)) (B (N x)))')\n        self.assertEqual(list(tgrep.tgrep_positions('* < T', [tree])),\n                         [[(0,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* < T > S', [tree])),\n                         [[(0,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* !< T', [tree])),\n                         [[(), (0, 0), (0, 0, 0), (1,), (1, 0), (1, 0, 0)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* !< T > S', [tree])),\n                         [[(1,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* > A', [tree])),\n                         [[(0, 0)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* > B', [tree])),\n                         [[(1, 0)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* !> B', [tree])),\n                         [[(), (0,), (0, 0), (0, 0, 0), (1,), (1, 0, 0)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* !> B >> S', [tree])),\n                         [[(0,), (0, 0), (1,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* >> S', [tree])),\n                         [[(0,), (0, 0), (1,), (1, 0)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* >>, S', [tree])),\n                         [[(0,), (0, 0)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* >>\\' S', [tree])),\n                         [[(1,), (1, 0)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* << T', [tree])),\n                         [[(), (0,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* <<\\' T', [tree])),\n                         [[(0,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* <<1 N', [tree])),\n                         [[(1,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* !<< T', [tree])),\n                         [[(0, 0), (0, 0, 0), (1,), (1, 0), (1, 0, 0)]])\n        tree = ParentedTree.fromstring('(S (A (T x)) (B (T x) (N x )))')\n        self.assertEqual(list(tgrep.tgrep_positions('* <: T', [tree])),\n                         [[(0,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* < T', [tree])),\n                         [[(0,), (1,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* !<: T', [tree])),\n                         [[(), (0, 0), (0, 0, 0), (1,), (1, 0), (1, 0, 0),\n                          (1, 1), (1, 1, 0)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* !<: T > S', [tree])),\n                         [[(1,)]])\n        tree = ParentedTree.fromstring('(S (T (A x) (B x)) (T (C x)))')\n        self.assertEqual(list(tgrep.tgrep_positions('* >: T', [tree])),\n                         [[(1, 0)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* !>: T', [tree])),\n                         [[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0),\n                          (1,), (1, 0, 0)]])\n        tree = ParentedTree.fromstring('(S (A (B (C (D (E (T x))))))'\n                                       ' (A (B (C (D (E (T x))) (N x)))))')\n        self.assertEqual(list(tgrep.tgrep_positions('* <<: T', [tree])),\n                         [[(0,), (0, 0), (0, 0, 0), (0, 0, 0, 0),\n                          (0, 0, 0, 0, 0), (1, 0, 0, 0), (1, 0, 0, 0, 0)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* >>: A', [tree])),\n                         [[(0, 0), (0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0, 0),\n                          (0, 0, 0, 0, 0, 0), (1, 0), (1, 0, 0)]])\n\n    def test_bad_operator(self):\n        '''\n        Test error handling of undefined tgrep operators.\n        '''\n        tree = ParentedTree.fromstring('(S (A (T x)) (B (N x)))')\n        self.assertRaises(\n            tgrep.TgrepException,\n            list,\n            tgrep.tgrep_positions('* >>> S', [tree]))\n\n    def test_comments(self):\n        '''\n        Test that comments are correctly filtered out of tgrep search\n        strings.\n        '''\n        tree = ParentedTree.fromstring('(S (NN x) (NP x) (NN x))')\n        search1 = '''\n        @ NP /^NP/;\n        @ NN /^NN/;\n        @NN\n        '''\n        self.assertEqual(list(tgrep.tgrep_positions(search1, [tree])),\n                         [[(0,), (2,)]])\n        search2 = '''\n        @ NP /^NP/;\n        @ NN /^NN/;\n\n        @NN\n        '''\n        self.assertEqual(list(tgrep.tgrep_positions(search2, [tree])),\n                         [[(0,), (2,)]])\n\n    def test_rel_sister_nodes(self):\n        '''\n        Test matching sister nodes in a tree.\n        '''\n        tree = ParentedTree.fromstring('(S (A x) (B x) (C x))')\n        self.assertEqual(list(tgrep.tgrep_positions('* $. B', [tree])),  [[(0,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* $.. B', [tree])), [[(0,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* $, B', [tree])),  [[(2,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* $,, B', [tree])), [[(2,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* $ B', [tree])),   [[(0,), (2,)]])\n\n    def tests_rel_indexed_children(self):\n        '''\n        Test matching nodes based on their index in their parent node.\n        '''\n        tree = ParentedTree.fromstring('(S (A x) (B x) (C x))')\n        self.assertEqual(list(tgrep.tgrep_positions('* >, S', [tree])),   [[(0,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* >1 S', [tree])),   [[(0,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* >2 S', [tree])),   [[(1,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* >3 S', [tree])),   [[(2,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* >\\' S', [tree])),  [[(2,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* >-1 S', [tree])),  [[(2,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* >-2 S', [tree])),  [[(1,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* >-3 S', [tree])),  [[(0,)]])\n        tree = ParentedTree.fromstring(\n            '(S (D (A x) (B x) (C x)) (E (B x) (C x) (A x)) '\n            '(F (C x) (A x) (B x)))')\n        self.assertEqual(list(tgrep.tgrep_positions('* <, A', [tree])),   [[(0,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* <1 A', [tree])),   [[(0,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* <2 A', [tree])),   [[(2,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* <3 A', [tree])),   [[(1,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* <\\' A', [tree])),  [[(1,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* <-1 A', [tree])),  [[(1,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* <-2 A', [tree])),  [[(2,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* <-3 A', [tree])),  [[(0,)]])\n\n    def test_rel_precedence(self):\n        '''\n        Test matching nodes based on precedence relations.\n        '''\n        tree = ParentedTree.fromstring('(S (NP (NP (PP x)) (NP (AP x)))'\n                                       ' (VP (AP (X (PP x)) (Y (AP x))))'\n                                       ' (NP (RC (NP (AP x)))))')\n        self.assertEqual(list(tgrep.tgrep_positions('* . X', [tree])),\n                         [[(0,), (0, 1), (0, 1, 0)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* . Y', [tree])),\n                         [[(1, 0, 0), (1, 0, 0, 0)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* .. X', [tree])),\n                         [[(0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* .. Y', [tree])),\n                         [[(0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0),\n                          (1, 0, 0), (1, 0, 0, 0)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* , X', [tree])),\n                         [[(1, 0, 1), (1, 0, 1, 0)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* , Y', [tree])),\n                         [[(2,), (2, 0), (2, 0, 0), (2, 0, 0, 0)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* ,, X', [tree])),\n                         [[(1, 0, 1), (1, 0, 1, 0), (2,), (2, 0), (2, 0, 0),\n                          (2, 0, 0, 0)]])\n        self.assertEqual(list(tgrep.tgrep_positions('* ,, Y', [tree])),\n                         [[(2,), (2, 0), (2, 0, 0), (2, 0, 0, 0)]])\n\n    def test_examples(self):\n        '''\n        Test the Basic Examples from the TGrep2 manual.\n        '''\n        tree = ParentedTree.fromstring('(S (NP (AP x)) (NP (PP x)))')\n        self.assertEqual(list(tgrep.tgrep_positions('NP < PP', [tree])),\n                         [[(1,)]])\n\n        tree = ParentedTree.fromstring('(S (NP x) (VP x) (NP (PP x)) (VP x))')\n        self.assertEqual(list(tgrep.tgrep_positions('NP << PP . VP', [tree])),\n                         [[(2,)]])\n\n        tree = ParentedTree.fromstring('(S (NP (AP x)) (NP (PP x)) '\n                                       '(NP (DET x) (NN x)) (VP x))')\n        self.assertEqual(list(tgrep.tgrep_positions('NP << PP | . VP', [tree])),\n                         [[(1,), (2,)]])\n\n        tree = ParentedTree.fromstring('(S (NP (NP (PP x)) (NP (AP x)))'\n                                       ' (VP (AP (NP (PP x)) (NP (AP x))))'\n                                       ' (NP (RC (NP (AP x)))))')\n        self.assertEqual(list(tgrep.tgrep_positions(\n                                               'NP !<< PP [> NP | >> VP]', [tree])),\n                         [[(0, 1), (1, 0, 1)]])\n\n        tree = ParentedTree.fromstring('(S (NP (AP (PP x) (VP x))) '\n                                       '(NP (AP (PP x) (NP x))) (NP x))')\n        self.assertEqual(list(tgrep.tgrep_positions('NP << (PP . VP)', [tree])),\n                         [[(0,)]])\n\n        tree = ParentedTree.fromstring(\n            '(S (NP (DET a) (NN cat) (PP (IN on) (NP x)))'\n            ' (NP (DET a) (NN cat) (PP (IN on) (NP x)) (PP x))'\n            ' (NP x))')\n        self.assertEqual(list(tgrep.tgrep_positions(\n                                               'NP <\\' (PP <, (IN < on))', [tree])),\n                         [[(0,)]])\n\n        tree = ParentedTree.fromstring(\n            '(S (S (C x) (A (B x))) (S (C x) (A x)) '\n            '(S (D x) (A (B x))))')\n        self.assertEqual(list(tgrep.tgrep_positions('S < (A < B) < C', [tree])),\n                         [[(0,)]])\n\n        tree = ParentedTree.fromstring(\n            '(S (S (A (B x) (C x))) (S (S (C x) (A (B x)))))')\n        self.assertEqual(list(tgrep.tgrep_positions('S < ((A < B) < C)', [tree])),\n                         [[(0,)]])\n\n        self.assertEqual(list(tgrep.tgrep_positions('S < (A < B < C)', [tree])),\n                         [[(0,)]])\n\n    def test_use_macros(self):\n        '''\n        Test defining and using tgrep2 macros.\n        '''\n        tree = ParentedTree.fromstring(\n            '(VP (VB sold) (NP (DET the) '\n            '(NN heiress)) (NP (NN deed) (PREP to) '\n            '(NP (DET the) (NN school) (NN house))))')\n        self.assertEqual(list(tgrep.tgrep_positions(\n            '@ NP /^NP/;\\n@ NN /^NN/;\\n@NP !< @NP !$.. @NN',\n            [tree])),\n                         [[(1,), (2, 2)]])\n        self.assertRaises(\n            tgrep.TgrepException,\n            list,\n            tgrep.tgrep_positions(\n                '@ NP /^NP/;\\n@ NN /^NN/;\\n@CNP !< @NP !$.. @NN', [tree]))\n\n    def test_tokenize_node_labels(self):\n        '''Test tokenization of labeled nodes.'''\n        self.assertEqual(tgrep.tgrep_tokenize(\n            'S < @SBJ < (@VP < (@VB $.. @OBJ))'),\n                         ['S', '<', '@SBJ', '<', '(', '@VP', '<', '(',\n                          '@VB', '$..', '@OBJ', ')', ')'])\n        self.assertEqual(tgrep.tgrep_tokenize(\n            'S < @SBJ=s < (@VP=v < (@VB $.. @OBJ))'),\n                         ['S', '<', '@SBJ', '=', 's', '<', '(', '@VP',\n                          '=', 'v', '<', '(', '@VB', '$..', '@OBJ', ')',\n                          ')'])\n\n    def test_tokenize_segmented_patterns(self):\n        '''Test tokenization of segmented patterns.'''\n        self.assertEqual(tgrep.tgrep_tokenize(\n            'S < @SBJ=s < (@VP=v < (@VB $.. @OBJ)) : =s .. =v'),\n                         ['S', '<', '@SBJ', '=', 's', '<', '(', '@VP',\n                          '=', 'v', '<', '(', '@VB', '$..', '@OBJ', ')',\n                          ')', ':', '=s', '..', '=v'])\n\n    def test_labeled_nodes(self):\n        '''\n        Test labeled nodes.\n\n        Test case from Emily M. Bender.\n        '''\n        search = '''\n            @ SBJ /SBJ/;\n            @ VP /VP/;\n            @ VB /VB/;\n            @ VPoB /V[PB]/;\n            @ OBJ /OBJ/;\n\n            S < @SBJ=s < (@VP=v < (@VB $.. @OBJ)) : =s .. =v'''\n        sent1 = ParentedTree.fromstring(\n            '(S (NP-SBJ I) (VP (VB eat) (NP-OBJ (NNS apples))))')\n        sent2 = ParentedTree.fromstring(\n            '(S (VP (VB eat) (NP-OBJ (NNS apples))) (NP-SBJ I))')\n        search_firsthalf = (search.split('\\n\\n')[0] +\n                            'S < @SBJ < (@VP < (@VB $.. @OBJ))')\n        search_rewrite = 'S < (/.*SBJ/ $.. (/VP/ < (/VB/ $.. /.*OBJ/)))'\n\n        self.assertTrue(list(tgrep.tgrep_positions(search_firsthalf, [sent1]))[0])\n        self.assertTrue(list(tgrep.tgrep_positions(search, [sent1]))[0])\n        self.assertTrue(list(tgrep.tgrep_positions(search_rewrite, [sent1]))[0])\n        self.assertEqual(list(tgrep.tgrep_positions(search, [sent1])),\n                         list(tgrep.tgrep_positions(search_rewrite, [sent1])))\n        self.assertTrue(list(tgrep.tgrep_positions(search_firsthalf, [sent2]))[0])\n        self.assertFalse(list(tgrep.tgrep_positions(search, [sent2]))[0])\n        self.assertFalse(list(tgrep.tgrep_positions(search_rewrite, [sent2]))[0])\n        self.assertEqual(list(tgrep.tgrep_positions(search, [sent2])),\n                         list(tgrep.tgrep_positions(search_rewrite, [sent2])))\n\n    def test_multiple_conjs(self):\n        '''\n        Test that multiple (3 or more) conjunctions of node relations are\n        handled properly.\n        '''\n        sent = ParentedTree.fromstring(\n            '((A (B b) (C c)) (A (B b) (C c) (D d)))')\n        self.assertEqual(list(tgrep.tgrep_positions('(A < B < C < D)', [sent])),\n                         [[(1,)]])\n        self.assertEqual(list(tgrep.tgrep_positions('(A < B < C)', [sent])),\n                         [[(0,), (1,)]])\n\n    def test_trailing_semicolon(self):\n        '''\n        Test that semicolons at the end of a tgrep2 search string won't\n        cause a parse failure.\n        '''\n        tree = ParentedTree.fromstring(\n            '(S (NP (DT the) (JJ big) (NN dog)) '\n            '(VP bit) (NP (DT a) (NN cat)))')\n        self.assertEqual(list(tgrep.tgrep_positions('NN', [tree])),\n                         [[(0,2), (2,1)]])\n        self.assertEqual(list(tgrep.tgrep_positions('NN;', [tree])),\n                         [[(0,2), (2,1)]])\n        self.assertEqual(list(tgrep.tgrep_positions('NN;;', [tree])),\n                         [[(0,2), (2,1)]])\n\nif __name__ == '__main__':\n    unittest.main()\n"], "nltk\\test\\unit\\test_tokenize": [".py", "\nfrom __future__ import unicode_literals\nfrom nltk.tokenize import TweetTokenizer, StanfordSegmenter, TreebankWordTokenizer\nfrom nose import SkipTest\nimport unittest\nimport os\n\n\nclass TestTokenize(unittest.TestCase):\n\n    def test_tweet_tokenizer(self):\n\n        tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n        s9 = \"@myke: Let's test these words: resum\u00e9 Espa\u00f1a M\u00fcnchen fran\u00e7ais\"\n        tokens = tokenizer.tokenize(s9)\n        expected = [':', \"Let's\", 'test', 'these', 'words', ':', 'resum\u00e9',\n                    'Espa\u00f1a', 'M\u00fcnchen', 'fran\u00e7ais']\n        self.assertEqual(tokens, expected)\n\n    def test_stanford_segmenter_arabic(self):\n        try:\n            seg = StanfordSegmenter()\n            seg.default_config('ar')\n            sent = u'\u064a\u0628\u062d\u062b \u0639\u0644\u0645 \u0627\u0644\u062d\u0627\u0633\u0648\u0628 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u062d\u0648\u0633\u0628\u0629 \u0628\u062c\u0645\u064a\u0639 \u0627\u0634\u0643\u0627\u0644\u0647\u0627 \u0644\u062d\u0644 \u0627\u0644\u0645\u0634\u0643\u0644\u0627\u062a'\n            segmented_sent = seg.segment(sent.split())\n            assert segmented_sent.split() == ['\u064a\u0628\u062d\u062b', '\u0639\u0644\u0645', '\u0627\u0644\u062d\u0627\u0633\u0648\u0628', '\u0627\u0633\u062a\u062e\u062f\u0627\u0645',\n                                              '\u0627\u0644\u062d\u0648\u0633\u0628\u0629', '\u0628', '\u062c\u0645\u064a\u0639', '\u0627\u0634\u0643\u0627\u0644',\n                                              '\u0647\u0627', '\u0644', '\u062d\u0644', '\u0627\u0644\u0645\u0634\u0643\u0644\u0627\u062a']\n        except LookupError as e:\n            raise SkipTest(str(e))\n\n    def test_stanford_segmenter_chinese(self):\n        try:\n            seg = StanfordSegmenter()\n            seg.default_config('zh')\n            sent = u\"\u8fd9\u662f\u65af\u5766\u798f\u4e2d\u6587\u5206\u8bcd\u5668\u6d4b\u8bd5\"\n            segmented_sent = seg.segment(sent.split())\n            assert segmented_sent.split() == ['\u8fd9', '\u662f', '\u65af\u5766\u798f',\n                                              '\u4e2d\u6587', '\u5206\u8bcd\u5668', '\u6d4b\u8bd5']\n        except LookupError as e:\n            raise SkipTest(str(e))\n\n    def test_phone_tokenizer(self):\n\n        tokenizer = TweetTokenizer()\n        test1 = \"(393)  928 -3010\"\n        expected = ['(393)  928 -3010']\n        result = tokenizer.tokenize(test1)\n        self.assertEqual(result, expected)\n\n        test2= \"(393)\\n928 -3010\"\n        expected = ['(', '393', ')', \"928 -3010\"]\n        result = tokenizer.tokenize(test2)\n        self.assertEqual(result, expected)\n\n    def test_remove_handle(self):\n\n        tokenizer = TweetTokenizer(strip_handles=True)\n\n        test1 = \"@twitter hello @twi_tter_. hi @12345 @123news\"\n        expected = ['hello', '.', 'hi']\n        result = tokenizer.tokenize(test1)\n        self.assertEqual(result, expected)\n\n        test2 = \"@n`@n~@n(@n)@n-@n=@n+@n\\\\@n|@n[@n]@n{@n}@n;@n:@n'@n\\\"@n/@n?@n.@n,@n<@n>@n @n\\n@n \u00f1@n.\u00fc@n.\u00e7@n.\"\n        expected = ['`', '~', '(', ')', '-', '=', '+', '\\\\', '|', '[', ']', '{', '}', ';', ':', \"'\", '\"', '/', '?', '.', ',', '<', '>', '\u00f1', '.', '\u00fc', '.', '\u00e7', '.']\n        result = tokenizer.tokenize(test2)\n        self.assertEqual(result, expected)\n\n\n        test3 = \"a@n j@n z@n A@n L@n Z@n 1@n 4@n 7@n 9@n 0@n _@n !@n @@n #@n $@n %@n &@n *@n\"\n        expected = ['a', '@n', 'j', '@n', 'z', '@n', 'A', '@n', 'L', '@n', 'Z', '@n', '1', '@n', '4', '@n', '7', '@n', '9', '@n', '0', '@n', '_', '@n', '!', '@n', '@', '@n', '#', '@n', '$', '@n', '%', '@n', '&', '@n', '*', '@n']\n        result = tokenizer.tokenize(test3)\n        self.assertEqual(result, expected)\n\n\n        test4 = \"@n!a @n#a @n$a @n%a @n&a @n*a\"\n        expected = ['!', 'a', '#', 'a', '$', 'a', '%', 'a', '&', 'a', '*', 'a']\n        result = tokenizer.tokenize(test4)\n        self.assertEqual(result, expected)\n\n\n        test5 = \"@n!@n @n#@n @n$@n @n%@n @n&@n @n*@n @n@n @@n @n@@n @n_@n @n7@n @nj@n\"\n        expected = ['!', '@n', '#', '@n', '$', '@n', '%', '@n', '&', '@n', '*', '@n', '@n', '@n', '@', '@n', '@n', '@', '@n', '@n_', '@n', '@n7', '@n', '@nj', '@n']\n        result = tokenizer.tokenize(test5)\n        self.assertEqual(result, expected)\n\n\n        test6 = \"@abcdefghijklmnopqrstuvwxyz @abcdefghijklmnopqrst1234 @abcdefghijklmnopqrst_ @abcdefghijklmnopqrstendofhandle\"\n        expected = ['uvwxyz', '1234', '_', 'endofhandle']\n        result = tokenizer.tokenize(test6)\n        self.assertEqual(result, expected)\n\n\n        test7 = \"@abcdefghijklmnopqrstu@abcde @abcdefghijklmnopqrst@abcde @abcdefghijklmnopqrst_@abcde @abcdefghijklmnopqrst5@abcde\"\n        expected = ['u', '@abcde', '@abcdefghijklmnopqrst', '@abcde', '_', '@abcde', '5', '@abcde']\n        result = tokenizer.tokenize(test7)\n        self.assertEqual(result, expected)\n\n    def test_treebank_span_tokenizer(self):\n\n        tokenizer = TreebankWordTokenizer()\n\n        test1 = \"Good muffins cost $3.88\\nin New (York).  Please (buy) me\\ntwo of them.\\n(Thanks).\"\n        expected = [\n            (0, 4), (5, 12), (13, 17), (18, 19), (19, 23),\n            (24, 26), (27, 30), (31, 32), (32, 36), (36, 37), (37, 38),\n            (40, 46), (47, 48), (48, 51), (51, 52), (53, 55), (56, 59),\n            (60, 62), (63, 68), (69, 70), (70, 76), (76, 77), (77, 78)\n        ]\n        result = list(tokenizer.span_tokenize(test1))\n        self.assertEqual(result, expected)\n\n        test2 = \"The DUP is similar to the \\\"religious right\\\" in the United States and takes a hardline stance on social issues\"\n        expected = [\n            (0, 3), (4, 7), (8, 10), (11, 18), (19, 21), (22, 25), (26, 27),\n            (27, 36), (37, 42), (42, 43), (44, 46), (47, 50), (51, 57), (58, 64),\n            (65, 68), (69, 74), (75, 76), (77, 85), (86, 92), (93, 95), (96, 102),\n            (103, 109)\n        ]\n        result = list(tokenizer.span_tokenize(test2))\n        self.assertEqual(result, expected)\n\n        test3 = \"The DUP is similar to the \\\"religious right\\\" in the United States and takes a ``hardline'' stance on social issues\"\n        expected = [\n            (0, 3), (4, 7), (8, 10), (11, 18), (19, 21), (22, 25), (26, 27),\n            (27, 36), (37, 42), (42, 43), (44, 46), (47, 50), (51, 57), (58, 64),\n            (65, 68), (69, 74), (75, 76), (77, 79), (79, 87), (87, 89), (90, 96),\n            (97, 99), (100, 106), (107, 113)\n        ]\n        result = list(tokenizer.span_tokenize(test3))\n        self.assertEqual(result, expected)\n"], "nltk\\test\\unit\\test_twitter_auth": [".py", "\nimport os\nimport unittest\nfrom nose import SkipTest\n\ntry:\n    import twython\nexcept ImportError as e:\n    raise SkipTest(\"The twython library has not been installed.\")\n\nfrom nltk.twitter import Authenticate\n\n\nclass TestCredentials(unittest.TestCase):\n\n    def setUp(self):\n        self.subdir = os.path.join(os.path.dirname(__file__), 'files')\n        self.auth = Authenticate()\n        os.environ['TWITTER'] = 'twitter-files'\n\n    def test_environment(self):\n        fn = os.path.basename(self.auth.creds_subdir)\n        self.assertEqual(fn, os.environ['TWITTER'])\n\n    def test_empty_subdir1(self):\n        try:\n            self.auth.load_creds(subdir='')\n        except OSError:\n            pass\n        except ValueError:\n            pass\n        except Exception as e:\n            self.fail('Unexpected exception thrown: %s' % e)\n        else:\n            self.fail('OSError exception not thrown.')\n\n    def test_empty_subdir2(self):\n        self.auth.creds_subdir = None\n        try:\n            self.auth.load_creds()\n        except ValueError:\n            pass\n        except Exception as e:\n            self.fail('Unexpected exception thrown: %s' % e)\n        else:\n            self.fail('ValueError exception not thrown.')\n\n    def test_missingdir(self):\n        try:\n            self.auth.load_creds(subdir='/nosuchdir')\n        except OSError:\n            pass\n        except ValueError:\n            pass\n        except Exception as e:\n            self.fail('Unexpected exception thrown: %s' % e)\n        else:\n            self.fail('OSError exception not thrown.')\n\n    def test_missingfile1(self):\n        try:\n            self.auth.load_creds()\n        except OSError:\n            pass\n        except ValueError:\n            pass\n        except Exception as e:\n            self.fail('Unexpected exception thrown: %s' % e)\n        else:\n            self.fail('OSError exception not thrown.')\n\n    def test_missingfile2(self):\n        try:\n            self.auth.load_creds(creds_file='foobar')\n        except OSError:\n            pass\n        except ValueError:\n            pass\n        except Exception as e:\n            self.fail('Unexpected exception thrown: %s' % e)\n        else:\n            self.fail('OSError exception not thrown.')\n\n    def test_incomplete_file(self):\n        try:\n            self.auth.load_creds(creds_file='bad_oauth1-1.txt',\n                                 subdir=self.subdir)\n        except ValueError:\n            pass\n        except Exception as e:\n            self.fail('Unexpected exception thrown: %s' % e)\n        else:\n            self.fail('ValueError exception not thrown.')\n\n    def test_malformed_file1(self):\n        try:\n            self.auth.load_creds(creds_file='bad_oauth1-2.txt',\n                                 subdir=self.subdir)\n        except ValueError:\n            pass\n        except Exception as e:\n            self.fail('Unexpected exception thrown: %s' % e)\n        else:\n            self.fail('ValueError exception not thrown.')\n\n    def test_malformed_file2(self):\n        try:\n            self.auth.load_creds(creds_file='bad_oauth1-3.txt',\n                                 subdir=self.subdir)\n        except ValueError:\n            pass\n        except Exception as e:\n            self.fail('Unexpected exception thrown: %s' % e)\n        else:\n            self.fail('ValueError exception not thrown.')\n\n    def test_correct_path(self):\n        self.auth.load_creds(subdir=self.subdir)\n        self.auth.creds_fullpath = os.path.join(self.subdir, self.auth.creds_file)\n\n    def test_correct_file1(self):\n        self.auth.load_creds(subdir=self.subdir)\n        self.assertEqual(self.auth.creds_file, 'credentials.txt')\n\n    def test_correct_file2(self):\n        oauth = self.auth.load_creds(subdir=self.subdir)\n        self.assertEqual(oauth['app_key'], 'a')\n\n\nif __name__ == '__main__':\n    unittest.main()\n\n"], "nltk\\test\\unit\\test_wordnet": [".py", "\nfrom __future__ import unicode_literals\nfrom nose import SkipTest\nimport unittest\nimport os\n\nfrom nltk.corpus.reader.wordnet import WordNetCorpusReader\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import wordnet_ic as wnic\nfrom nltk.data import find as find_data\n\n\nwn.ensure_loaded()\nS = wn.synset\nL = wn.lemma\n\nclass WordnNetDemo(unittest.TestCase):\n\n    def test_retrieve_synset(self):\n        move_synset = S('go.v.21')\n        self.assertEqual(move_synset.name(), \"move.v.15\")\n        self.assertEqual(move_synset.lemma_names(), ['move', 'go'])\n        self.assertEqual(move_synset.definition(), \"have a turn; make one's move in a game\")\n        self.assertEqual(move_synset.examples(), ['Can I go now?'])\n\n\n    def test_retrieve_synsets(self):\n        self.assertEqual(sorted(wn.synsets('zap', pos='n')),\n                        [S('zap.n.01')])\n        self.assertEqual(sorted(wn.synsets('zap', pos='v')),\n                        [S('microwave.v.01'), S('nuke.v.01'), S('zap.v.01'), S('zap.v.02')])\n\n    def test_hyperhyponyms(self):\n        self.assertEqual(S('travel.v.01').hypernyms(), [])\n        self.assertEqual(S('travel.v.02').hypernyms(),\n                        [S('travel.v.03')])\n        self.assertEqual(S('travel.v.03').hypernyms(), [])\n\n        self.assertEqual(S('breakfast.n.1').hypernyms(), [S('meal.n.01')])\n        first_five_meal_hypo = [S('banquet.n.02'), S('bite.n.04'), S('breakfast.n.01'), S('brunch.n.01'), S('buffet.n.02')]\n        self.assertEqual(sorted(S('meal.n.1').hyponyms()[:5]), first_five_meal_hypo)\n        self.assertEqual(S('Austen.n.1').instance_hypernyms(), [S('writer.n.01')])\n        first_five_composer_hypo = [S('ambrose.n.01'), S('bach.n.01'), S('barber.n.01'), S('bartok.n.01'), S('beethoven.n.01')]\n        self.assertEqual(S('composer.n.1').instance_hyponyms()[:5], first_five_composer_hypo)\n\n        self.assertEqual(S('person.n.01').root_hypernyms(), [S('entity.n.01')])\n        self.assertEqual(S('sail.v.01').root_hypernyms(), [S('travel.v.01')])\n        self.assertEqual(S('fall.v.12').root_hypernyms(), [S('act.v.01'), S('fall.v.17')])\n\n    def test_derivationally_related_forms(self):\n        self.assertEqual(L('zap.v.03.nuke').derivationally_related_forms(),\n                        [L('atomic_warhead.n.01.nuke')])\n        self.assertEqual(L('zap.v.03.atomize').derivationally_related_forms(),\n                        [L('atomization.n.02.atomization')])\n        self.assertEqual(L('zap.v.03.atomise').derivationally_related_forms(),\n                        [L('atomization.n.02.atomisation')])\n        self.assertEqual(L('zap.v.03.zap').derivationally_related_forms(),\n                        [])\n\n    def test_meronyms_holonyms(self):\n        self.assertEqual(S('dog.n.01').member_holonyms(), [S('canis.n.01'), S('pack.n.06')])\n        self.assertEqual(S('dog.n.01').part_meronyms(), [S('flag.n.07')])\n\n        self.assertEqual(S('faculty.n.2').member_meronyms(),\n                        [S('professor.n.01')])\n        self.assertEqual(S('copilot.n.1').member_holonyms(),\n                        [S('crew.n.01')])\n\n        self.assertEqual(S('table.n.2').part_meronyms(),\n                        [S('leg.n.03'), S('tabletop.n.01'), S('tableware.n.01')])\n        self.assertEqual(S('course.n.7').part_holonyms(),\n                        [S('meal.n.01')])\n\n        self.assertEqual(S('water.n.1').substance_meronyms(),\n                        [S('hydrogen.n.01'), S('oxygen.n.01')])\n        self.assertEqual(S('gin.n.1').substance_holonyms(),\n                        [S('gin_and_it.n.01'), S('gin_and_tonic.n.01'),\n                         S('martini.n.01'), S('pink_lady.n.01')])\n\n    def test_antonyms(self):\n        self.assertEqual(L('leader.n.1.leader').antonyms(), [L('follower.n.01.follower')])\n        self.assertEqual(L('increase.v.1.increase').antonyms(), [L('decrease.v.01.decrease')])\n\n\n    def test_misc_relations(self):\n        self.assertEqual(S('snore.v.1').entailments(), [S('sleep.v.01')])\n        self.assertEqual(S('heavy.a.1').similar_tos(),\n                        [S('dense.s.03'), S('doughy.s.01'),\n                         S('heavier-than-air.s.01'), S('hefty.s.02'),\n                         S('massive.s.04'), S('non-buoyant.s.01'),\n                         S('ponderous.s.02')])\n        self.assertEqual(S('light.a.1').attributes(), [S('weight.n.01')])\n        self.assertEqual(S('heavy.a.1').attributes(), [S('weight.n.01')])\n\n        self.assertEqual(L('English.a.1.English').pertainyms(),\n                         [L('england.n.01.England')])\n\n    def test_lch(self):\n        self.assertEqual(S('person.n.01').lowest_common_hypernyms(S('dog.n.01')),\n                         [S('organism.n.01')])\n        self.assertEqual(S('woman.n.01').lowest_common_hypernyms(S('girlfriend.n.02')),\n                         [S('woman.n.01')])\n\n    def test_domains(self):\n        self.assertEqual(S('code.n.03').topic_domains(), [S('computer_science.n.01')])\n        self.assertEqual(S('pukka.a.01').region_domains(), [S('india.n.01')])\n        self.assertEqual(S('freaky.a.01').usage_domains(), [S('slang.n.02')])\n\n    def test_wordnet_similarities(self):\n        self.assertAlmostEqual(S('cat.n.01').path_similarity(S('cat.n.01')), 1.0)\n        self.assertAlmostEqual(S('dog.n.01').path_similarity(S('cat.n.01')), 0.2)\n        self.assertAlmostEqual(S('dog.n.01').lch_similarity(S('cat.n.01')), 2.028, places=3)\n        self.assertAlmostEqual(S('dog.n.01').wup_similarity(S('cat.n.01')), 0.8571, places=3)\n        brown_ic = wnic.ic('ic-brown.dat')\n        self.assertAlmostEqual(S('dog.n.01').jcn_similarity(S('cat.n.01'), brown_ic), 0.4497, places=3)\n        semcor_ic = wnic.ic('ic-semcor.dat')\n        self.assertAlmostEqual(S('dog.n.01').lin_similarity(S('cat.n.01'), semcor_ic), 0.8863, places=3)\n"], "nltk\\test\\unit\\translate\\test_bleu": [".py", "\nimport functools\nimport io\nimport unittest\n\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import modified_precision, brevity_penalty, closest_ref_length\nfrom nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n\n\nclass TestBLEU(unittest.TestCase):\n    def test_modified_precision(self):\n        ref1 = 'the cat is on the mat'.split()\n        ref2 = 'there is a cat on the mat'.split()\n        hyp1 = 'the the the the the the the'.split()\n\n        references = [ref1, ref2]\n\n        hyp1_unigram_precision =  float(modified_precision(references, hyp1, n=1))\n        assert (round(hyp1_unigram_precision, 4) == 0.2857)\n        self.assertAlmostEqual(hyp1_unigram_precision, 0.28571428, places=4)\n\n        assert(float(modified_precision(references, hyp1, n=2)) == 0.0)\n\n\n        ref1 = str('It is a guide to action that ensures that the military '\n                   'will forever heed Party commands').split()\n        ref2 = str('It is the guiding principle which guarantees the military '\n                   'forces always being under the command of the Party').split()\n        ref3 = str('It is the practical guide for the army always to heed '\n                   'the directions of the party').split()\n        hyp1 = 'of the'.split()\n\n        references = [ref1, ref2, ref3]\n        assert (float(modified_precision(references, hyp1, n=1)) == 1.0)\n\n        assert(float(modified_precision(references, hyp1, n=2)) == 1.0)\n\n\n        hyp1 = str('It is a guide to action which ensures that the military '\n                   'always obeys the commands of the party').split()\n        hyp2 = str('It is to insure the troops forever hearing the activity '\n                   'guidebook that party direct').split()\n\n        references = [ref1, ref2, ref3]\n\n        hyp1_unigram_precision = float(modified_precision(references, hyp1, n=1))\n        hyp2_unigram_precision = float(modified_precision(references, hyp2, n=1))\n        self.assertAlmostEqual(hyp1_unigram_precision, 0.94444444, places=4)\n        self.assertAlmostEqual(hyp2_unigram_precision, 0.57142857, places=4)\n        assert (round(hyp1_unigram_precision, 4) == 0.9444)\n        assert (round(hyp2_unigram_precision, 4) == 0.5714)\n\n        hyp1_bigram_precision = float(modified_precision(references, hyp1, n=2))\n        hyp2_bigram_precision = float(modified_precision(references, hyp2, n=2))\n        self.assertAlmostEqual(hyp1_bigram_precision, 0.58823529, places=4)\n        self.assertAlmostEqual(hyp2_bigram_precision, 0.07692307, places=4)\n        assert (round(hyp1_bigram_precision, 4) == 0.5882)\n        assert (round(hyp2_bigram_precision, 4) == 0.0769)\n\n    def test_brevity_penalty(self):\n        references = [['a'] * 11, ['a'] * 8]\n        hypothesis = ['a'] * 7\n        hyp_len = len(hypothesis)\n        closest_ref_len =  closest_ref_length(references, hyp_len)\n        self.assertAlmostEqual(brevity_penalty(closest_ref_len, hyp_len), 0.8669, places=4)\n\n        references = [['a'] * 11, ['a'] * 8, ['a'] * 6, ['a'] * 7]\n        hypothesis = ['a'] * 7\n        hyp_len = len(hypothesis)\n        closest_ref_len =  closest_ref_length(references, hyp_len)\n        assert brevity_penalty(closest_ref_len, hyp_len) == 1.0\n\n    def test_zero_matches(self):\n        references = ['The candidate has no alignment to any of the references'.split()]\n        hypothesis = 'John loves Mary'.split()\n\n        for n in range(1,len(hypothesis)):\n            weights = [1.0/n] * n # Uniform weights.\n            assert(sentence_bleu(references, hypothesis, weights) == 0)\n\n    def test_full_matches(self):\n        references = ['John loves Mary'.split()]\n        hypothesis = 'John loves Mary'.split()\n\n        for n in range(1,len(hypothesis)):\n            weights = [1.0/n] * n # Uniform weights.\n            assert(sentence_bleu(references, hypothesis, weights) == 1.0)\n\n    def test_partial_matches_hypothesis_longer_than_reference(self):\n        references = ['John loves Mary'.split()]\n        hypothesis = 'John loves Mary who loves Mike'.split()\n        self.assertAlmostEqual(sentence_bleu(references, hypothesis), 0.0, places=4)\n        try:\n            self.assertWarns(UserWarning, sentence_bleu, references, hypothesis)\n        except AttributeError:\n            pass # unittest.TestCase.assertWarns is only supported in Python >= 3.2.\n\n\nclass TestBLEUFringeCases(unittest.TestCase):\n\n    def test_case_where_n_is_bigger_than_hypothesis_length(self):\n        references = ['John loves Mary ?'.split()]\n        hypothesis = 'John loves Mary'.split()\n        n = len(hypothesis) + 1 #\n        weights = [1.0/n] * n # Uniform weights.\n        self.assertAlmostEqual(sentence_bleu(references, hypothesis, weights), 0.0, places=4)\n        try:\n            self.assertWarns(UserWarning, sentence_bleu, references, hypothesis)\n        except AttributeError:\n            pass # unittest.TestCase.assertWarns is only supported in Python >= 3.2.\n\n        references = ['John loves Mary'.split()]\n        hypothesis = 'John loves Mary'.split()\n        self.assertAlmostEqual(sentence_bleu(references, hypothesis, weights), 0.0, places=4)\n\n    def test_empty_hypothesis(self):\n        references = ['The candidate has no alignment to any of the references'.split()]\n        hypothesis = []\n        assert(sentence_bleu(references, hypothesis) == 0)\n\n    def test_empty_references(self):\n        references = [[]]\n        hypothesis = 'John loves Mary'.split()\n        assert(sentence_bleu(references, hypothesis) == 0)\n\n    def test_empty_references_and_hypothesis(self):\n        references = [[]]\n        hypothesis = []\n        assert(sentence_bleu(references, hypothesis) == 0)\n\n    def test_reference_or_hypothesis_shorter_than_fourgrams(self):\n        references = ['let it go'.split()]\n        hypothesis = 'let go it'.split()\n        self.assertAlmostEqual(sentence_bleu(references, hypothesis), 0.0, places=4)\n        try:\n            self.assertWarns(UserWarning, sentence_bleu, references, hypothesis)\n        except AttributeError:\n            pass # unittest.TestCase.assertWarns is only supported in Python >= 3.2.\n\nclass TestBLEUvsMteval13a(unittest.TestCase):\n\n    def test_corpus_bleu(self):\n        ref_file = find('models/wmt15_eval/ref.ru')\n        hyp_file = find('models/wmt15_eval/google.ru')\n        mteval_output_file = find('models/wmt15_eval/mteval-13a.output')\n\n        with open(mteval_output_file, 'r') as mteval_fin:\n            mteval_bleu_scores = map(float, mteval_fin.readlines()[-2].split()[1:-1])\n\n        with io.open(ref_file, 'r', encoding='utf8') as ref_fin:\n            with io.open(hyp_file, 'r', encoding='utf8') as hyp_fin:\n                hypothesis = list(map(lambda x: x.split(), hyp_fin))\n                references = list(map(lambda x: [x.split()], ref_fin))\n                for i, mteval_bleu in zip(range(1,10), mteval_bleu_scores):\n                    nltk_bleu = corpus_bleu(references, hypothesis, weights=(1.0/i,)*i)\n                    assert abs(mteval_bleu - nltk_bleu) < 0.005\n\n                chencherry = SmoothingFunction()\n                for i, mteval_bleu in zip(range(1,10), mteval_bleu_scores):\n                    nltk_bleu = corpus_bleu(references, hypothesis,\n                                            weights=(1.0/i,)*i,\n                                            smoothing_function=chencherry.method3)\n                    assert abs(mteval_bleu - nltk_bleu) < 0.005\n\nclass TestBLEUWithBadSentence(unittest.TestCase):\n    def test_corpus_bleu_with_bad_sentence(self):\n        hyp = \"Teo S yb , oe uNb , R , T t , , t Tue Ar saln S , , 5istsi l , 5oe R ulO sae oR R\"\n        ref = str(\"Their tasks include changing a pump on the faulty stokehold .\"\n                  \"Likewise , two species that are very similar in morphology \"\n                  \"were distinguished using genetics .\")\n        references = [[ref.split()]]\n        hypotheses = [hyp.split()]\n        try: # Check that the warning is raised since no. of 2-grams < 0.\n            with self.assertWarns(UserWarning):\n                self.assertAlmostEqual(corpus_bleu(references, hypotheses), 0.0, places=4)\n        except AttributeError: # unittest.TestCase.assertWarns is only supported in Python >= 3.2.\n            self.assertAlmostEqual(corpus_bleu(references, hypotheses), 0.0, places=4)\n"], "nltk\\test\\unit\\translate\\test_gdfa": [".py", "\nimport functools\nimport io\nimport unittest\n\nfrom nltk.translate.gdfa import grow_diag_final_and\n\nclass TestGDFA(unittest.TestCase):\n    def test_from_eflomal_outputs(self):\n        forwards = ['0-0 1-2',\n                    '0-0 1-1',\n                    '0-0 2-1 3-2 4-3 5-4 6-5 7-6 8-7 7-8 9-9 10-10 9-11 11-12 12-13 13-14',\n                    '0-0 1-1 1-2 2-3 3-4 4-5 4-6 5-7 6-8 8-9 9-10',\n                    '0-0 14-1 15-2 16-3 20-5 21-6 22-7 5-8 6-9 7-10 8-11 9-12 10-13 11-14 12-15 13-16 14-17 17-18 18-19 19-20 20-21 23-22 24-23 25-24 26-25 27-27 28-28 29-29 30-30 31-31',\n                    '0-0 1-1 0-2 2-3',\n                    '0-0 2-2 4-4',\n                    '0-0 1-1 2-3 3-4 5-5 7-6 8-7 9-8 10-9 11-10 12-11 13-12 14-13 15-14 16-16 17-17 18-18 19-19 20-20',\n                    '3-0 4-1 6-2 5-3 6-4 7-5 8-6 9-7 10-8 11-9 16-10 9-12 10-13 12-14',\n                    '1-0']\n        backwards = ['0-0 1-2',\n                     '0-0 1-1',\n                     '0-0 2-1 3-2 4-3 5-4 6-5 7-6 8-7 9-8 10-10 11-12 12-11 13-13',\n                     '0-0 1-2 2-3 3-4 4-6 6-8 7-5 8-7 9-8',\n                     '0-0 1-8 2-9 3-10 4-11 5-12 6-11 8-13 9-14 10-15 11-16 12-17 13-18 14-19 15-20 16-21 17-22 18-23 19-24 20-29 21-30 22-31 23-2 24-3 25-4 26-5 27-5 28-6 29-7 30-28 31-31',\n                     '0-0 1-1 2-3',\n                     '0-0 1-1 2-3 4-4',\n                     '0-0 1-1 2-3 3-4 5-5 7-6 8-7 9-8 10-9 11-10 12-11 13-12 14-13 15-14 16-16 17-17 18-18 19-19 20-16 21-18',\n                     '0-0 1-1 3-2 4-1 5-3 6-4 7-5 8-6 9-7 10-8 11-9 12-8 13-9 14-8 15-9 16-10',\n                     '1-0']\n        source_lens = [2, 3, 3, 15, 11, 33, 4, 6, 23, 18]\n        target_lens = [2, 4, 3, 16, 12, 33, 5, 6, 22, 16]\n        expected = [ [(0, 0), (1, 2)],\n                     [(0, 0), (1, 1)],\n                     [(0, 0), (2, 1), (3, 2), (4, 3), (5, 4), (6, 5), (7, 6), (8, 7), (10, 10), (11, 12)],\n                     [(0, 0), (1, 1), (1, 2), (2, 3), (3, 4), (4, 5), (4, 6), (5, 7), (6, 8), (7, 5), (8, 7), (8, 9), (9, 8), (9, 10)],\n                     [(0, 0), (1, 8), (2, 9), (3, 10), (4, 11), (5, 8), (6, 9), (6, 11), (7, 10), (8, 11), (31, 31)],\n                     [(0, 0), (0, 2), (1, 1), (2, 3)],\n                     [(0, 0), (1, 1), (2, 2), (2, 3), (4, 4)],\n                     [(0, 0), (1, 1), (2, 3), (3, 4), (5, 5), (7, 6), (8, 7), (9, 8), (10, 9), (11, 10), (12, 11), (13, 12), (14, 13), (15, 14), (16, 16), (17, 17), (18, 18), (19, 19)],\n                     [(0, 0), (1, 1), (3, 0), (3, 2), (4, 1), (5, 3), (6, 2), (6, 4), (7, 5), (8, 6), (9, 7), (9, 12), (10, 8), (10, 13), (11, 9), (12, 8), (12, 14), (13, 9), (14, 8), (15, 9), (16, 10)],\n                     [(1, 0)],\n                     [(0, 0), (1, 1), (3, 2), (4, 3), (5, 4), (6, 5), (7, 6), (9, 10), (10, 12), (11, 13), (12, 14), (13, 15)],\n                   ]\n\n        for fw, bw, src_len, trg_len, expect in zip(forwards, backwards, source_lens, target_lens, expected):\n            self.assertListEqual(expect, grow_diag_final_and(src_len, trg_len, fw, bw))\n"], "nltk\\test\\unit\\translate\\test_ibm1": [".py", "\nimport unittest\n\nfrom collections import defaultdict\nfrom nltk.translate import AlignedSent\nfrom nltk.translate import IBMModel\nfrom nltk.translate import IBMModel1\nfrom nltk.translate.ibm_model import AlignmentInfo\n\n\nclass TestIBMModel1(unittest.TestCase):\n    def test_set_uniform_translation_probabilities(self):\n        corpus = [\n            AlignedSent(['ham', 'eggs'], ['schinken', 'schinken', 'eier']),\n            AlignedSent(['spam', 'spam', 'spam', 'spam'], ['spam', 'spam']),\n        ]\n        model1 = IBMModel1(corpus, 0)\n\n        model1.set_uniform_probabilities(corpus)\n\n        self.assertEqual(model1.translation_table['ham']['eier'], 1.0 / 3)\n        self.assertEqual(model1.translation_table['eggs'][None], 1.0 / 3)\n\n    def test_set_uniform_translation_probabilities_of_non_domain_values(self):\n        corpus = [\n            AlignedSent(['ham', 'eggs'], ['schinken', 'schinken', 'eier']),\n            AlignedSent(['spam', 'spam', 'spam', 'spam'], ['spam', 'spam']),\n        ]\n        model1 = IBMModel1(corpus, 0)\n\n        model1.set_uniform_probabilities(corpus)\n\n        self.assertEqual(model1.translation_table['parrot']['eier'],\n                         IBMModel.MIN_PROB)\n\n    def test_prob_t_a_given_s(self):\n        src_sentence = [\"ich\", 'esse', 'ja', 'gern', 'r\u00e4ucherschinken']\n        trg_sentence = ['i', 'love', 'to', 'eat', 'smoked', 'ham']\n        corpus = [AlignedSent(trg_sentence, src_sentence)]\n        alignment_info = AlignmentInfo((0, 1, 4, 0, 2, 5, 5),\n                                       [None] + src_sentence,\n                                       ['UNUSED'] + trg_sentence,\n                                       None)\n\n        translation_table = defaultdict(lambda: defaultdict(float))\n        translation_table['i']['ich'] = 0.98\n        translation_table['love']['gern'] = 0.98\n        translation_table['to'][None] = 0.98\n        translation_table['eat']['esse'] = 0.98\n        translation_table['smoked']['r\u00e4ucherschinken'] = 0.98\n        translation_table['ham']['r\u00e4ucherschinken'] = 0.98\n\n        model1 = IBMModel1(corpus, 0)\n        model1.translation_table = translation_table\n\n        probability = model1.prob_t_a_given_s(alignment_info)\n\n        lexical_translation = 0.98 * 0.98 * 0.98 * 0.98 * 0.98 * 0.98\n        expected_probability = lexical_translation\n        self.assertEqual(round(probability, 4), round(expected_probability, 4))\n"], "nltk\\test\\unit\\translate\\test_ibm2": [".py", "\nimport unittest\n\nfrom collections import defaultdict\nfrom nltk.translate import AlignedSent\nfrom nltk.translate import IBMModel\nfrom nltk.translate import IBMModel2\nfrom nltk.translate.ibm_model import AlignmentInfo\n\n\nclass TestIBMModel2(unittest.TestCase):\n    def test_set_uniform_alignment_probabilities(self):\n        corpus = [\n            AlignedSent(['ham', 'eggs'], ['schinken', 'schinken', 'eier']),\n            AlignedSent(['spam', 'spam', 'spam', 'spam'], ['spam', 'spam']),\n        ]\n        model2 = IBMModel2(corpus, 0)\n\n        model2.set_uniform_probabilities(corpus)\n\n        self.assertEqual(model2.alignment_table[0][1][3][2], 1.0 / 4)\n        self.assertEqual(model2.alignment_table[2][4][2][4], 1.0 / 3)\n\n    def test_set_uniform_alignment_probabilities_of_non_domain_values(self):\n        corpus = [\n            AlignedSent(['ham', 'eggs'], ['schinken', 'schinken', 'eier']),\n            AlignedSent(['spam', 'spam', 'spam', 'spam'], ['spam', 'spam']),\n        ]\n        model2 = IBMModel2(corpus, 0)\n\n        model2.set_uniform_probabilities(corpus)\n\n        self.assertEqual(model2.alignment_table[99][1][3][2], IBMModel.MIN_PROB)\n        self.assertEqual(model2.alignment_table[2][99][2][4], IBMModel.MIN_PROB)\n\n    def test_prob_t_a_given_s(self):\n        src_sentence = [\"ich\", 'esse', 'ja', 'gern', 'r\u00e4ucherschinken']\n        trg_sentence = ['i', 'love', 'to', 'eat', 'smoked', 'ham']\n        corpus = [AlignedSent(trg_sentence, src_sentence)]\n        alignment_info = AlignmentInfo((0, 1, 4, 0, 2, 5, 5),\n                                       [None] + src_sentence,\n                                       ['UNUSED'] + trg_sentence,\n                                       None)\n\n        translation_table = defaultdict(lambda: defaultdict(float))\n        translation_table['i']['ich'] = 0.98\n        translation_table['love']['gern'] = 0.98\n        translation_table['to'][None] = 0.98\n        translation_table['eat']['esse'] = 0.98\n        translation_table['smoked']['r\u00e4ucherschinken'] = 0.98\n        translation_table['ham']['r\u00e4ucherschinken'] = 0.98\n\n        alignment_table = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(\n                lambda: defaultdict(float))))\n        alignment_table[0][3][5][6] = 0.97  # None -> to\n        alignment_table[1][1][5][6] = 0.97  # ich -> i\n        alignment_table[2][4][5][6] = 0.97  # esse -> eat\n        alignment_table[4][2][5][6] = 0.97  # gern -> love\n        alignment_table[5][5][5][6] = 0.96  # r\u00e4ucherschinken -> smoked\n        alignment_table[5][6][5][6] = 0.96  # r\u00e4ucherschinken -> ham\n\n        model2 = IBMModel2(corpus, 0)\n        model2.translation_table = translation_table\n        model2.alignment_table = alignment_table\n\n        probability = model2.prob_t_a_given_s(alignment_info)\n\n        lexical_translation = 0.98 * 0.98 * 0.98 * 0.98 * 0.98 * 0.98\n        alignment = 0.97 * 0.97 * 0.97 * 0.97 * 0.96 * 0.96\n        expected_probability = lexical_translation * alignment\n        self.assertEqual(round(probability, 4), round(expected_probability, 4))\n"], "nltk\\test\\unit\\translate\\test_ibm3": [".py", "\nimport unittest\n\nfrom collections import defaultdict\nfrom nltk.translate import AlignedSent\nfrom nltk.translate import IBMModel\nfrom nltk.translate import IBMModel3\nfrom nltk.translate.ibm_model import AlignmentInfo\n\n\nclass TestIBMModel3(unittest.TestCase):\n    def test_set_uniform_distortion_probabilities(self):\n        corpus = [\n            AlignedSent(['ham', 'eggs'], ['schinken', 'schinken', 'eier']),\n            AlignedSent(['spam', 'spam', 'spam', 'spam'], ['spam', 'spam']),\n        ]\n        model3 = IBMModel3(corpus, 0)\n\n        model3.set_uniform_probabilities(corpus)\n\n        self.assertEqual(model3.distortion_table[1][0][3][2], 1.0 / 2)\n        self.assertEqual(model3.distortion_table[4][2][2][4], 1.0 / 4)\n\n    def test_set_uniform_distortion_probabilities_of_non_domain_values(self):\n        corpus = [\n            AlignedSent(['ham', 'eggs'], ['schinken', 'schinken', 'eier']),\n            AlignedSent(['spam', 'spam', 'spam', 'spam'], ['spam', 'spam']),\n        ]\n        model3 = IBMModel3(corpus, 0)\n\n        model3.set_uniform_probabilities(corpus)\n\n        self.assertEqual(model3.distortion_table[0][0][3][2], IBMModel.MIN_PROB)\n        self.assertEqual(model3.distortion_table[9][2][2][4], IBMModel.MIN_PROB)\n        self.assertEqual(model3.distortion_table[2][9][2][4], IBMModel.MIN_PROB)\n\n    def test_prob_t_a_given_s(self):\n        src_sentence = [\"ich\", 'esse', 'ja', 'gern', 'r\u00e4ucherschinken']\n        trg_sentence = ['i', 'love', 'to', 'eat', 'smoked', 'ham']\n        corpus = [AlignedSent(trg_sentence, src_sentence)]\n        alignment_info = AlignmentInfo((0, 1, 4, 0, 2, 5, 5),\n                                       [None] + src_sentence,\n                                       ['UNUSED'] + trg_sentence,\n                                       [[3], [1], [4], [], [2], [5, 6]])\n\n        distortion_table = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(\n                lambda: defaultdict(float))))\n        distortion_table[1][1][5][6] = 0.97  # i -> ich\n        distortion_table[2][4][5][6] = 0.97  # love -> gern\n        distortion_table[3][0][5][6] = 0.97  # to -> NULL\n        distortion_table[4][2][5][6] = 0.97  # eat -> esse\n        distortion_table[5][5][5][6] = 0.97  # smoked -> r\u00e4ucherschinken\n        distortion_table[6][5][5][6] = 0.97  # ham -> r\u00e4ucherschinken\n\n        translation_table = defaultdict(lambda: defaultdict(float))\n        translation_table['i']['ich'] = 0.98\n        translation_table['love']['gern'] = 0.98\n        translation_table['to'][None] = 0.98\n        translation_table['eat']['esse'] = 0.98\n        translation_table['smoked']['r\u00e4ucherschinken'] = 0.98\n        translation_table['ham']['r\u00e4ucherschinken'] = 0.98\n\n        fertility_table = defaultdict(lambda: defaultdict(float))\n        fertility_table[1]['ich'] = 0.99\n        fertility_table[1]['esse'] = 0.99\n        fertility_table[0]['ja'] = 0.99\n        fertility_table[1]['gern'] = 0.99\n        fertility_table[2]['r\u00e4ucherschinken'] = 0.999\n        fertility_table[1][None] = 0.99\n\n        probabilities = {\n            'p1': 0.167,\n            'translation_table': translation_table,\n            'distortion_table': distortion_table,\n            'fertility_table': fertility_table,\n            'alignment_table': None\n        }\n\n        model3 = IBMModel3(corpus, 0, probabilities)\n\n        probability = model3.prob_t_a_given_s(alignment_info)\n\n        null_generation = 5 * pow(0.167, 1) * pow(0.833, 4)\n        fertility = 1*0.99 * 1*0.99 * 1*0.99 * 1*0.99 * 2*0.999\n        lexical_translation = 0.98 * 0.98 * 0.98 * 0.98 * 0.98 * 0.98\n        distortion = 0.97 * 0.97 * 0.97 * 0.97 * 0.97 * 0.97\n        expected_probability = (null_generation * fertility *\n                                lexical_translation * distortion)\n        self.assertEqual(round(probability, 4), round(expected_probability, 4))\n"], "nltk\\test\\unit\\translate\\test_ibm4": [".py", "\nimport unittest\n\nfrom collections import defaultdict\nfrom nltk.translate import AlignedSent\nfrom nltk.translate import IBMModel\nfrom nltk.translate import IBMModel4\nfrom nltk.translate.ibm_model import AlignmentInfo\n\n\nclass TestIBMModel4(unittest.TestCase):\n    def test_set_uniform_distortion_probabilities_of_max_displacements(self):\n        src_classes = {'schinken': 0, 'eier': 0, 'spam': 1}\n        trg_classes = {'ham': 0, 'eggs': 1, 'spam': 2}\n        corpus = [\n            AlignedSent(['ham', 'eggs'], ['schinken', 'schinken', 'eier']),\n            AlignedSent(['spam', 'spam', 'spam', 'spam'], ['spam', 'spam']),\n        ]\n        model4 = IBMModel4(corpus, 0, src_classes, trg_classes)\n\n        model4.set_uniform_probabilities(corpus)\n\n        expected_prob = 1.0 / (2 * (4 - 1))\n\n        self.assertEqual(model4.head_distortion_table[3][0][0], expected_prob)\n        self.assertEqual(model4.head_distortion_table[-3][1][2], expected_prob)\n        self.assertEqual(model4.non_head_distortion_table[3][0], expected_prob)\n        self.assertEqual(model4.non_head_distortion_table[-3][2], expected_prob)\n\n    def test_set_uniform_distortion_probabilities_of_non_domain_values(self):\n        src_classes = {'schinken': 0, 'eier': 0, 'spam': 1}\n        trg_classes = {'ham': 0, 'eggs': 1, 'spam': 2}\n        corpus = [\n            AlignedSent(['ham', 'eggs'], ['schinken', 'schinken', 'eier']),\n            AlignedSent(['spam', 'spam', 'spam', 'spam'], ['spam', 'spam']),\n        ]\n        model4 = IBMModel4(corpus, 0, src_classes, trg_classes)\n\n        model4.set_uniform_probabilities(corpus)\n\n        self.assertEqual(model4.head_distortion_table[4][0][0],\n                         IBMModel.MIN_PROB)\n        self.assertEqual(model4.head_distortion_table[100][1][2],\n                         IBMModel.MIN_PROB)\n        self.assertEqual(model4.non_head_distortion_table[4][0],\n                         IBMModel.MIN_PROB)\n        self.assertEqual(model4.non_head_distortion_table[100][2],\n                         IBMModel.MIN_PROB)\n\n    def test_prob_t_a_given_s(self):\n        src_sentence = [\"ich\", 'esse', 'ja', 'gern', 'r\u00e4ucherschinken']\n        trg_sentence = ['i', 'love', 'to', 'eat', 'smoked', 'ham']\n        src_classes = {'r\u00e4ucherschinken': 0, 'ja': 1, 'ich': 2, 'esse': 3,\n                       'gern': 4}\n        trg_classes = {'ham': 0, 'smoked': 1, 'i': 3, 'love': 4, 'to': 2,\n                       'eat': 4}\n        corpus = [AlignedSent(trg_sentence, src_sentence)]\n        alignment_info = AlignmentInfo((0, 1, 4, 0, 2, 5, 5),\n                                       [None] + src_sentence,\n                                       ['UNUSED'] + trg_sentence,\n                                       [[3], [1], [4], [], [2], [5, 6]])\n\n        head_distortion_table = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(float)))\n        head_distortion_table[1][None][3] = 0.97  # None, i\n        head_distortion_table[3][2][4] = 0.97  # ich, eat\n        head_distortion_table[-2][3][4] = 0.97  # esse, love\n        head_distortion_table[3][4][1] = 0.97  # gern, smoked\n\n        non_head_distortion_table = defaultdict(lambda: defaultdict(float))\n        non_head_distortion_table[1][0] = 0.96  # ham\n\n        translation_table = defaultdict(lambda: defaultdict(float))\n        translation_table['i']['ich'] = 0.98\n        translation_table['love']['gern'] = 0.98\n        translation_table['to'][None] = 0.98\n        translation_table['eat']['esse'] = 0.98\n        translation_table['smoked']['r\u00e4ucherschinken'] = 0.98\n        translation_table['ham']['r\u00e4ucherschinken'] = 0.98\n\n        fertility_table = defaultdict(lambda: defaultdict(float))\n        fertility_table[1]['ich'] = 0.99\n        fertility_table[1]['esse'] = 0.99\n        fertility_table[0]['ja'] = 0.99\n        fertility_table[1]['gern'] = 0.99\n        fertility_table[2]['r\u00e4ucherschinken'] = 0.999\n        fertility_table[1][None] = 0.99\n\n        probabilities = {\n            'p1': 0.167,\n            'translation_table': translation_table,\n            'head_distortion_table': head_distortion_table,\n            'non_head_distortion_table': non_head_distortion_table,\n            'fertility_table': fertility_table,\n            'alignment_table': None\n        }\n\n        model4 = IBMModel4(corpus, 0, src_classes, trg_classes,\n                           probabilities)\n\n        probability = model4.prob_t_a_given_s(alignment_info)\n\n        null_generation = 5 * pow(0.167, 1) * pow(0.833, 4)\n        fertility = 1*0.99 * 1*0.99 * 1*0.99 * 1*0.99 * 2*0.999\n        lexical_translation = 0.98 * 0.98 * 0.98 * 0.98 * 0.98 * 0.98\n        distortion = 0.97 * 0.97 * 1 * 0.97 * 0.97 * 0.96\n        expected_probability = (null_generation * fertility *\n                                lexical_translation * distortion)\n        self.assertEqual(round(probability, 4), round(expected_probability, 4))\n"], "nltk\\test\\unit\\translate\\test_ibm5": [".py", "\nimport unittest\n\nfrom collections import defaultdict\nfrom nltk.translate import AlignedSent\nfrom nltk.translate import IBMModel\nfrom nltk.translate import IBMModel4\nfrom nltk.translate import IBMModel5\nfrom nltk.translate.ibm_model import AlignmentInfo\n\n\nclass TestIBMModel5(unittest.TestCase):\n    def test_set_uniform_vacancy_probabilities_of_max_displacements(self):\n        src_classes = {'schinken': 0, 'eier': 0, 'spam': 1}\n        trg_classes = {'ham': 0, 'eggs': 1, 'spam': 2}\n        corpus = [\n            AlignedSent(['ham', 'eggs'], ['schinken', 'schinken', 'eier']),\n            AlignedSent(['spam', 'spam', 'spam', 'spam'], ['spam', 'spam']),\n        ]\n        model5 = IBMModel5(corpus, 0, src_classes, trg_classes)\n\n        model5.set_uniform_probabilities(corpus)\n\n        expected_prob = 1.0 / (2 * 4)\n\n        self.assertEqual(model5.head_vacancy_table[4][4][0], expected_prob)\n        self.assertEqual(model5.head_vacancy_table[-3][1][2], expected_prob)\n        self.assertEqual(model5.non_head_vacancy_table[4][4][0], expected_prob)\n        self.assertEqual(model5.non_head_vacancy_table[-3][1][2], expected_prob)\n\n    def test_set_uniform_vacancy_probabilities_of_non_domain_values(self):\n        src_classes = {'schinken': 0, 'eier': 0, 'spam': 1}\n        trg_classes = {'ham': 0, 'eggs': 1, 'spam': 2}\n        corpus = [\n            AlignedSent(['ham', 'eggs'], ['schinken', 'schinken', 'eier']),\n            AlignedSent(['spam', 'spam', 'spam', 'spam'], ['spam', 'spam']),\n        ]\n        model5 = IBMModel5(corpus, 0, src_classes, trg_classes)\n\n        model5.set_uniform_probabilities(corpus)\n\n        self.assertEqual(model5.head_vacancy_table[5][4][0],\n                         IBMModel.MIN_PROB)\n        self.assertEqual(model5.head_vacancy_table[-4][1][2],\n                         IBMModel.MIN_PROB)\n        self.assertEqual(model5.head_vacancy_table[4][0][0],\n                         IBMModel.MIN_PROB)\n        self.assertEqual(model5.non_head_vacancy_table[5][4][0],\n                         IBMModel.MIN_PROB)\n        self.assertEqual(model5.non_head_vacancy_table[-4][1][2],\n                         IBMModel.MIN_PROB)\n\n    def test_prob_t_a_given_s(self):\n        src_sentence = [\"ich\", 'esse', 'ja', 'gern', 'r\u00e4ucherschinken']\n        trg_sentence = ['i', 'love', 'to', 'eat', 'smoked', 'ham']\n        src_classes = {'r\u00e4ucherschinken': 0, 'ja': 1, 'ich': 2, 'esse': 3,\n                       'gern': 4}\n        trg_classes = {'ham': 0, 'smoked': 1, 'i': 3, 'love': 4, 'to': 2,\n                       'eat': 4}\n        corpus = [AlignedSent(trg_sentence, src_sentence)]\n        alignment_info = AlignmentInfo((0, 1, 4, 0, 2, 5, 5),\n                                       [None] + src_sentence,\n                                       ['UNUSED'] + trg_sentence,\n                                       [[3], [1], [4], [], [2], [5, 6]])\n\n        head_vacancy_table = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(float)))\n        head_vacancy_table[1 - 0][6][3] = 0.97  # ich -> i\n        head_vacancy_table[3 - 0][5][4] = 0.97  # esse -> eat\n        head_vacancy_table[1 - 2][4][4] = 0.97  # gern -> love\n        head_vacancy_table[2 - 0][2][1] = 0.97  # r\u00e4ucherschinken -> smoked\n\n        non_head_vacancy_table = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(float)))\n        non_head_vacancy_table[1 - 0][1][0] = 0.96  # r\u00e4ucherschinken -> ham\n\n        translation_table = defaultdict(lambda: defaultdict(float))\n        translation_table['i']['ich'] = 0.98\n        translation_table['love']['gern'] = 0.98\n        translation_table['to'][None] = 0.98\n        translation_table['eat']['esse'] = 0.98\n        translation_table['smoked']['r\u00e4ucherschinken'] = 0.98\n        translation_table['ham']['r\u00e4ucherschinken'] = 0.98\n\n        fertility_table = defaultdict(lambda: defaultdict(float))\n        fertility_table[1]['ich'] = 0.99\n        fertility_table[1]['esse'] = 0.99\n        fertility_table[0]['ja'] = 0.99\n        fertility_table[1]['gern'] = 0.99\n        fertility_table[2]['r\u00e4ucherschinken'] = 0.999\n        fertility_table[1][None] = 0.99\n\n        probabilities = {\n            'p1': 0.167,\n            'translation_table': translation_table,\n            'fertility_table': fertility_table,\n            'head_vacancy_table': head_vacancy_table,\n            'non_head_vacancy_table': non_head_vacancy_table,\n            'head_distortion_table': None,\n            'non_head_distortion_table': None,\n            'alignment_table': None\n        }\n\n        model5 = IBMModel5(corpus, 0, src_classes, trg_classes,\n                           probabilities)\n\n        probability = model5.prob_t_a_given_s(alignment_info)\n\n        null_generation = 5 * pow(0.167, 1) * pow(0.833, 4)\n        fertility = 1*0.99 * 1*0.99 * 1*0.99 * 1*0.99 * 2*0.999\n        lexical_translation = 0.98 * 0.98 * 0.98 * 0.98 * 0.98 * 0.98\n        vacancy = 0.97 * 0.97 * 1 * 0.97 * 0.97 * 0.96\n        expected_probability = (null_generation * fertility *\n                                lexical_translation * vacancy)\n        self.assertEqual(round(probability, 4), round(expected_probability, 4))\n\n    def test_prune(self):\n        alignment_infos = [\n            AlignmentInfo((1, 1), None, None, None),\n            AlignmentInfo((1, 2), None, None, None),\n            AlignmentInfo((2, 1), None, None, None),\n            AlignmentInfo((2, 2), None, None, None),\n            AlignmentInfo((0, 0), None, None, None)\n        ]\n        min_factor = IBMModel5.MIN_SCORE_FACTOR\n        best_score = 0.9\n        scores = {\n            (1, 1): min(min_factor * 1.5, 1) * best_score,  # above threshold\n            (1, 2): best_score,\n            (2, 1): min_factor * best_score,        # at threshold\n            (2, 2): min_factor * best_score * 0.5,  # low score\n            (0, 0): min(min_factor * 1.1, 1) * 1.2  # above threshold\n        }\n        corpus = [AlignedSent(['a'], ['b'])]\n        original_prob_function = IBMModel4.model4_prob_t_a_given_s\n        IBMModel4.model4_prob_t_a_given_s = staticmethod(\n            lambda a, model: scores[a.alignment])\n        model5 = IBMModel5(corpus, 0, None, None)\n\n        pruned_alignments = model5.prune(alignment_infos)\n\n        self.assertEqual(len(pruned_alignments), 3)\n\n        IBMModel4.model4_prob_t_a_given_s = original_prob_function\n"], "nltk\\test\\unit\\translate\\test_ibm_model": [".py", "\nimport unittest\n\nfrom collections import defaultdict\nfrom nltk.translate import AlignedSent\nfrom nltk.translate import IBMModel\nfrom nltk.translate.ibm_model import AlignmentInfo\n\n\nclass TestIBMModel(unittest.TestCase):\n    __TEST_SRC_SENTENCE = [\"j'\", 'aime', 'bien', 'jambon']\n    __TEST_TRG_SENTENCE = ['i', 'love', 'ham']\n\n    def test_vocabularies_are_initialized(self):\n        parallel_corpora = [\n            AlignedSent(['one', 'two', 'three', 'four'],\n                        ['un', 'deux', 'trois']),\n            AlignedSent(['five', 'one', 'six'], ['quatre', 'cinq', 'six']),\n            AlignedSent([], ['sept'])\n        ]\n\n        ibm_model = IBMModel(parallel_corpora)\n        self.assertEqual(len(ibm_model.src_vocab), 8)\n        self.assertEqual(len(ibm_model.trg_vocab), 6)\n\n    def test_vocabularies_are_initialized_even_with_empty_corpora(self):\n        parallel_corpora = []\n\n        ibm_model = IBMModel(parallel_corpora)\n        self.assertEqual(len(ibm_model.src_vocab), 1)  # addition of NULL token\n        self.assertEqual(len(ibm_model.trg_vocab), 0)\n\n    def test_best_model2_alignment(self):\n        sentence_pair = AlignedSent(\n            TestIBMModel.__TEST_TRG_SENTENCE,\n            TestIBMModel.__TEST_SRC_SENTENCE)\n        translation_table = {\n            'i': {\"j'\": 0.9, 'aime': 0.05, 'bien': 0.02, 'jambon': 0.03,\n                  None: 0},\n            'love': {\"j'\": 0.05, 'aime': 0.9, 'bien': 0.01, 'jambon': 0.01,\n                     None: 0.03},\n            'ham': {\"j'\": 0, 'aime': 0.01, 'bien': 0, 'jambon': 0.99,\n                    None: 0}\n        }\n        alignment_table = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(\n                lambda: 0.2))))\n\n        ibm_model = IBMModel([])\n        ibm_model.translation_table = translation_table\n        ibm_model.alignment_table = alignment_table\n\n        a_info = ibm_model.best_model2_alignment(sentence_pair)\n\n        self.assertEqual(a_info.alignment[1:], (1, 2, 4))  # 0th element unused\n        self.assertEqual(a_info.cepts, [[], [1], [2], [], [3]])\n\n    def test_best_model2_alignment_does_not_change_pegged_alignment(self):\n        sentence_pair = AlignedSent(\n            TestIBMModel.__TEST_TRG_SENTENCE,\n            TestIBMModel.__TEST_SRC_SENTENCE)\n        translation_table = {\n            'i': {\"j'\": 0.9, 'aime': 0.05, 'bien': 0.02, 'jambon': 0.03,\n                  None: 0},\n            'love': {\"j'\": 0.05, 'aime': 0.9, 'bien': 0.01, 'jambon': 0.01,\n                     None: 0.03},\n            'ham': {\"j'\": 0, 'aime': 0.01, 'bien': 0, 'jambon': 0.99, None: 0}\n        }\n        alignment_table = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(\n                lambda: 0.2))))\n\n        ibm_model = IBMModel([])\n        ibm_model.translation_table = translation_table\n        ibm_model.alignment_table = alignment_table\n\n        a_info = ibm_model.best_model2_alignment(sentence_pair, 2, 4)\n        self.assertEqual(a_info.alignment[1:], (1, 4, 4))\n        self.assertEqual(a_info.cepts, [[], [1], [], [], [2, 3]])\n\n    def test_best_model2_alignment_handles_fertile_words(self):\n        sentence_pair = AlignedSent(\n            ['i', 'really', ',', 'really', 'love', 'ham'],\n            TestIBMModel.__TEST_SRC_SENTENCE)\n        translation_table = {\n            'i': {\"j'\": 0.9, 'aime': 0.05, 'bien': 0.02, 'jambon': 0.03, None: 0},\n            'really': {\"j'\": 0, 'aime': 0, 'bien': 0.9, 'jambon': 0.01, None: 0.09},\n            ',': {\"j'\": 0, 'aime': 0, 'bien': 0.3, 'jambon': 0, None: 0.7},\n            'love': {\"j'\": 0.05, 'aime': 0.9, 'bien': 0.01, 'jambon': 0.01, None: 0.03},\n            'ham': {\"j'\": 0, 'aime': 0.01, 'bien': 0, 'jambon': 0.99, None: 0}\n        }\n        alignment_table = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(\n                lambda: 0.2))))\n\n        ibm_model = IBMModel([])\n        ibm_model.translation_table = translation_table\n        ibm_model.alignment_table = alignment_table\n\n        a_info = ibm_model.best_model2_alignment(sentence_pair)\n\n        self.assertEqual(a_info.alignment[1:], (1, 3, 0, 3, 2, 4))\n        self.assertEqual(a_info.cepts, [[3], [1], [5], [2, 4], [6]])\n\n    def test_best_model2_alignment_handles_empty_src_sentence(self):\n        sentence_pair = AlignedSent(TestIBMModel.__TEST_TRG_SENTENCE, [])\n        ibm_model = IBMModel([])\n\n        a_info = ibm_model.best_model2_alignment(sentence_pair)\n\n        self.assertEqual(a_info.alignment[1:], (0, 0, 0))\n        self.assertEqual(a_info.cepts, [[1, 2, 3]])\n\n    def test_best_model2_alignment_handles_empty_trg_sentence(self):\n        sentence_pair = AlignedSent([], TestIBMModel.__TEST_SRC_SENTENCE)\n        ibm_model = IBMModel([])\n\n        a_info = ibm_model.best_model2_alignment(sentence_pair)\n\n        self.assertEqual(a_info.alignment[1:], ())\n        self.assertEqual(a_info.cepts, [[], [], [], [], []])\n\n    def test_neighboring_finds_neighbor_alignments(self):\n        a_info = AlignmentInfo(\n            (0, 3, 2),\n            (None, 'des', '\u0153ufs', 'verts'),\n            ('UNUSED', 'green', 'eggs'),\n            [[], [], [2], [1]]\n        )\n        ibm_model = IBMModel([])\n\n        neighbors = ibm_model.neighboring(a_info)\n\n        neighbor_alignments = set()\n        for neighbor in neighbors:\n            neighbor_alignments.add(neighbor.alignment)\n        expected_alignments = set([\n            (0, 0, 2), (0, 1, 2), (0, 2, 2),\n            (0, 3, 0), (0, 3, 1), (0, 3, 3),\n            (0, 2, 3),\n            (0, 3, 2)\n        ])\n        self.assertEqual(neighbor_alignments, expected_alignments)\n\n    def test_neighboring_sets_neighbor_alignment_info(self):\n        a_info = AlignmentInfo(\n            (0, 3, 2),\n            (None, 'des', '\u0153ufs', 'verts'),\n            ('UNUSED', 'green', 'eggs'),\n            [[], [], [2], [1]]\n        )\n        ibm_model = IBMModel([])\n\n        neighbors = ibm_model.neighboring(a_info)\n\n        for neighbor in neighbors:\n            if neighbor.alignment == (0, 2, 2):\n                moved_alignment = neighbor\n            elif neighbor.alignment == (0, 3, 2):\n                swapped_alignment = neighbor\n\n        self.assertEqual(moved_alignment.cepts, [[], [], [1, 2], []])\n        self.assertEqual(swapped_alignment.cepts, [[], [], [2], [1]])\n\n    def test_neighboring_returns_neighbors_with_pegged_alignment(self):\n        a_info = AlignmentInfo(\n            (0, 3, 2),\n            (None, 'des', '\u0153ufs', 'verts'),\n            ('UNUSED', 'green', 'eggs'),\n            [[], [], [2], [1]]\n        )\n        ibm_model = IBMModel([])\n\n        neighbors = ibm_model.neighboring(a_info, 2)\n\n        neighbor_alignments = set()\n        for neighbor in neighbors:\n            neighbor_alignments.add(neighbor.alignment)\n        expected_alignments = set([\n            (0, 0, 2), (0, 1, 2), (0, 2, 2),\n            (0, 3, 2)\n        ])\n        self.assertEqual(neighbor_alignments, expected_alignments)\n\n    def test_hillclimb(self):\n        initial_alignment = AlignmentInfo((0, 3, 2), None, None, None)\n\n        def neighboring_mock(a, j):\n            if a.alignment == (0, 3, 2):\n                return set([\n                    AlignmentInfo((0, 2, 2), None, None, None),\n                    AlignmentInfo((0, 1, 1), None, None, None)\n                ])\n            elif a.alignment == (0, 2, 2):\n                return set([\n                    AlignmentInfo((0, 3, 3), None, None, None),\n                    AlignmentInfo((0, 4, 4), None, None, None)\n                ])\n            return set()\n\n        def prob_t_a_given_s_mock(a):\n            prob_values = {\n                (0, 3, 2): 0.5,\n                (0, 2, 2): 0.6,\n                (0, 1, 1): 0.4,\n                (0, 3, 3): 0.6,\n                (0, 4, 4): 0.7\n            }\n            return prob_values.get(a.alignment, 0.01)\n\n        ibm_model = IBMModel([])\n        ibm_model.neighboring = neighboring_mock\n        ibm_model.prob_t_a_given_s = prob_t_a_given_s_mock\n\n        best_alignment = ibm_model.hillclimb(initial_alignment)\n\n        self.assertEqual(best_alignment.alignment, (0, 4, 4))\n\n    def test_sample(self):\n        sentence_pair = AlignedSent(\n            TestIBMModel.__TEST_TRG_SENTENCE,\n            TestIBMModel.__TEST_SRC_SENTENCE)\n        ibm_model = IBMModel([])\n        ibm_model.prob_t_a_given_s = lambda x: 0.001\n\n        samples, best_alignment = ibm_model.sample(sentence_pair)\n\n        self.assertEqual(len(samples), 61)\n"], "nltk\\test\\unit\\translate\\test_nist": [".py", "\nimport io\nimport unittest\n\nfrom nltk.data import find\nfrom nltk.translate.nist_score import sentence_nist, corpus_nist\n\n\nclass TestNIST(unittest.TestCase):\n    def test_sentence_nist(self):\n        ref_file = find('models/wmt15_eval/ref.ru')\n        hyp_file = find('models/wmt15_eval/google.ru')\n        mteval_output_file = find('models/wmt15_eval/mteval-13a.output')\n\n        with open(mteval_output_file, 'r') as mteval_fin:\n            mteval_nist_scores = map(float, mteval_fin.readlines()[-4].split()[1:-1])\n\n        with io.open(ref_file, 'r', encoding='utf8') as ref_fin:\n            with io.open(hyp_file, 'r', encoding='utf8') as hyp_fin:\n                hypotheses = list(map(lambda x: x.split(), hyp_fin))\n                references = list(map(lambda x: [x.split()], ref_fin))\n                for i, mteval_nist in zip(range(1,10), mteval_nist_scores):\n                    nltk_nist = corpus_nist(references, hypotheses, i)\n                    assert abs(mteval_nist - nltk_nist) < 0.05\n"], "nltk\\test\\unit\\translate\\test_stack_decoder": [".py", "\n\nimport unittest\nfrom collections import defaultdict\nfrom math import log\nfrom nltk.translate import PhraseTable\nfrom nltk.translate import StackDecoder\nfrom nltk.translate.stack_decoder import _Hypothesis, _Stack\n\n\nclass TestStackDecoder(unittest.TestCase):\n    def test_find_all_src_phrases(self):\n        phrase_table = TestStackDecoder.create_fake_phrase_table()\n        stack_decoder = StackDecoder(phrase_table, None)\n        sentence = ('my', 'hovercraft', 'is', 'full', 'of', 'eels')\n\n        src_phrase_spans = stack_decoder.find_all_src_phrases(sentence)\n\n        self.assertEqual(src_phrase_spans[0], [2])  # 'my hovercraft'\n        self.assertEqual(src_phrase_spans[1], [2])  # 'hovercraft'\n        self.assertEqual(src_phrase_spans[2], [3])  # 'is'\n        self.assertEqual(src_phrase_spans[3], [5, 6])  # 'full of', 'full of eels'\n        self.assertFalse(src_phrase_spans[4])  # no entry starting with 'of'\n        self.assertEqual(src_phrase_spans[5], [6])  # 'eels'\n\n    def test_distortion_score(self):\n        stack_decoder = StackDecoder(None, None)\n        stack_decoder.distortion_factor = 0.5\n        hypothesis = _Hypothesis()\n        hypothesis.src_phrase_span = (3, 5)\n\n        score = stack_decoder.distortion_score(hypothesis, (8, 10))\n\n        expected_score = log(stack_decoder.distortion_factor) * (8 - 5)\n        self.assertEqual(score, expected_score)\n\n    def test_distortion_score_of_first_expansion(self):\n        stack_decoder = StackDecoder(None, None)\n        stack_decoder.distortion_factor = 0.5\n        hypothesis = _Hypothesis()\n\n        score = stack_decoder.distortion_score(hypothesis, (8, 10))\n\n        self.assertEqual(score, 0.0)\n\n    def test_compute_future_costs(self):\n        phrase_table = TestStackDecoder.create_fake_phrase_table()\n        language_model = TestStackDecoder.create_fake_language_model()\n        stack_decoder = StackDecoder(phrase_table, language_model)\n        sentence = ('my', 'hovercraft', 'is', 'full', 'of', 'eels')\n\n        future_scores = stack_decoder.compute_future_scores(sentence)\n\n        self.assertEqual(\n            future_scores[1][2],\n            (phrase_table.translations_for(('hovercraft',))[0].log_prob +\n             language_model.probability(('hovercraft',))))\n        self.assertEqual(\n            future_scores[0][2],\n            (phrase_table.translations_for(('my', 'hovercraft'))[0].log_prob +\n             language_model.probability(('my', 'hovercraft'))))\n\n    def test_compute_future_costs_for_phrases_not_in_phrase_table(self):\n        phrase_table = TestStackDecoder.create_fake_phrase_table()\n        language_model = TestStackDecoder.create_fake_language_model()\n        stack_decoder = StackDecoder(phrase_table, language_model)\n        sentence = ('my', 'hovercraft', 'is', 'full', 'of', 'eels')\n\n        future_scores = stack_decoder.compute_future_scores(sentence)\n\n        self.assertEqual(\n            future_scores[1][3],  # 'hovercraft is' is not in phrase table\n            future_scores[1][2] + future_scores[2][3])  # backoff\n\n    def test_future_score(self):\n        hypothesis = _Hypothesis()\n        hypothesis.untranslated_spans = lambda _: [(0, 2), (5, 8)]  # mock\n        future_score_table = defaultdict(lambda: defaultdict(float))\n        future_score_table[0][2] = 0.4\n        future_score_table[5][8] = 0.5\n        stack_decoder = StackDecoder(None, None)\n\n        future_score = stack_decoder.future_score(\n            hypothesis, future_score_table, 8)\n\n        self.assertEqual(future_score, 0.4 + 0.5)\n\n    def test_valid_phrases(self):\n        hypothesis = _Hypothesis()\n        hypothesis.untranslated_spans = lambda _: [\n            (0, 2),\n            (3, 6)\n        ]\n        all_phrases_from = [\n            [1, 4],\n            [2],\n            [],\n            [5],\n            [5, 6, 7],\n            [],\n            [7]\n        ]\n\n        phrase_spans = StackDecoder.valid_phrases(all_phrases_from, hypothesis)\n\n        self.assertEqual(phrase_spans, [(0, 1), (1, 2), (3, 5), (4, 5), (4, 6)])\n\n    @staticmethod\n    def create_fake_phrase_table():\n        phrase_table = PhraseTable()\n        phrase_table.add(('hovercraft',), ('',), 0.8)\n        phrase_table.add(('my', 'hovercraft'), ('', ''), 0.7)\n        phrase_table.add(('my', 'cheese'), ('', ''), 0.7)\n        phrase_table.add(('is',), ('',), 0.8)\n        phrase_table.add(('is',), ('',), 0.5)\n        phrase_table.add(('full', 'of'), ('', ''), 0.01)\n        phrase_table.add(('full', 'of', 'eels'), ('', '', ''), 0.5)\n        phrase_table.add(('full', 'of', 'spam'), ('', ''), 0.5)\n        phrase_table.add(('eels',), ('',), 0.5)\n        phrase_table.add(('spam',), ('',), 0.5)\n        return phrase_table\n\n    @staticmethod\n    def create_fake_language_model():\n        language_prob = defaultdict(lambda: -999.0)\n        language_prob[('my',)] = log(0.1)\n        language_prob[('hovercraft',)] = log(0.1)\n        language_prob[('is',)] = log(0.1)\n        language_prob[('full',)] = log(0.1)\n        language_prob[('of',)] = log(0.1)\n        language_prob[('eels',)] = log(0.1)\n        language_prob[('my', 'hovercraft',)] = log(0.3)\n        language_model = type(\n            '', (object,),\n            {'probability': lambda _, phrase: language_prob[phrase]})()\n        return language_model\n\n\nclass TestHypothesis(unittest.TestCase):\n    def setUp(self):\n        root = _Hypothesis()\n        child = _Hypothesis(\n            raw_score=0.5,\n            src_phrase_span=(3, 7),\n            trg_phrase=('hello', 'world'),\n            previous=root\n        )\n        grandchild = _Hypothesis(\n            raw_score=0.4,\n            src_phrase_span=(1, 2),\n            trg_phrase=('and', 'goodbye'),\n            previous=child\n        )\n        self.hypothesis_chain = grandchild\n\n    def test_translation_so_far(self):\n        translation = self.hypothesis_chain.translation_so_far()\n\n        self.assertEqual(translation, ['hello', 'world', 'and', 'goodbye'])\n\n    def test_translation_so_far_for_empty_hypothesis(self):\n        hypothesis = _Hypothesis()\n\n        translation = hypothesis.translation_so_far()\n\n        self.assertEqual(translation, [])\n\n    def test_total_translated_words(self):\n        total_translated_words = self.hypothesis_chain.total_translated_words()\n\n        self.assertEqual(total_translated_words, 5)\n\n    def test_translated_positions(self):\n        translated_positions = self.hypothesis_chain.translated_positions()\n\n        translated_positions.sort()\n        self.assertEqual(translated_positions, [1, 3, 4, 5, 6])\n\n    def test_untranslated_spans(self):\n        untranslated_spans = self.hypothesis_chain.untranslated_spans(10)\n\n        self.assertEqual(untranslated_spans, [(0, 1), (2, 3), (7, 10)])\n\n    def test_untranslated_spans_for_empty_hypothesis(self):\n        hypothesis = _Hypothesis()\n\n        untranslated_spans = hypothesis.untranslated_spans(10)\n\n        self.assertEqual(untranslated_spans, [(0, 10)])\n\n\nclass TestStack(unittest.TestCase):\n    def test_push_bumps_off_worst_hypothesis_when_stack_is_full(self):\n        stack = _Stack(3)\n        poor_hypothesis = _Hypothesis(0.01)\n\n        stack.push(_Hypothesis(0.2))\n        stack.push(poor_hypothesis)\n        stack.push(_Hypothesis(0.1))\n        stack.push(_Hypothesis(0.3))\n\n        self.assertFalse(poor_hypothesis in stack)\n\n    def test_push_removes_hypotheses_that_fall_below_beam_threshold(self):\n        stack = _Stack(3, 0.5)\n        poor_hypothesis = _Hypothesis(0.01)\n        worse_hypothesis = _Hypothesis(0.009)\n\n        stack.push(poor_hypothesis)\n        stack.push(worse_hypothesis)\n        stack.push(_Hypothesis(0.9))  # greatly superior hypothesis\n\n        self.assertFalse(poor_hypothesis in stack)\n        self.assertFalse(worse_hypothesis in stack)\n\n    def test_push_does_not_add_hypothesis_that_falls_below_beam_threshold(self):\n        stack = _Stack(3, 0.5)\n        poor_hypothesis = _Hypothesis(0.01)\n\n        stack.push(_Hypothesis(0.9))  # greatly superior hypothesis\n        stack.push(poor_hypothesis)\n\n        self.assertFalse(poor_hypothesis in stack)\n\n    def test_best_returns_the_best_hypothesis(self):\n        stack = _Stack(3)\n        best_hypothesis = _Hypothesis(0.99)\n\n        stack.push(_Hypothesis(0.0))\n        stack.push(best_hypothesis)\n        stack.push(_Hypothesis(0.5))\n\n        self.assertEqual(stack.best(), best_hypothesis)\n\n    def test_best_returns_none_when_stack_is_empty(self):\n        stack = _Stack(3)\n\n        self.assertEqual(stack.best(), None)\n"], "nltk\\test\\unit\\translate\\__init__": [".py", "", 1], "nltk\\test\\unit\\utils": [".py", "from __future__ import absolute_import\nfrom unittest import TestCase\nfrom functools import wraps\nfrom nose.plugins.skip import SkipTest\nfrom nltk.util import py26\n\ndef skip(reason):\n    def decorator(test_item):\n        is_test_class = isinstance(test_item, type) and issubclass(test_item, TestCase)\n\n        if is_test_class and py26():\n            for meth_name in (m for m in dir(test_item) if m.startswith('test_')):\n                patched_method = skip(reason)(getattr(test_item, meth_name))\n                setattr(test_item, meth_name, patched_method)\n\n        if not is_test_class:\n            @wraps(test_item)\n            def skip_wrapper(*args, **kwargs):\n                raise SkipTest(reason)\n            skip_wrapper.__name__ = test_item.__name__\n            test_item = skip_wrapper\n\n        test_item.__unittest_skip__ = True\n        test_item.__unittest_skip_why__ = reason\n        return test_item\n    return decorator\n\n\ndef skipIf(condition, reason):\n    if condition:\n        return skip(reason)\n    return lambda obj: obj"], "nltk\\test\\unit\\__init__": [".py", "", 1], "nltk\\test\\wordnet_fixt": [".py", "from __future__ import absolute_import\n\n\ndef teardown_module(module=None):\n    from nltk.corpus import wordnet\n    wordnet._unload()\n"], "nltk\\test\\__init__": [".py", "\n", 1], "nltk\\text": [".py", "\nfrom __future__ import print_function, division, unicode_literals, absolute_import\n\nfrom math import log\nfrom collections import defaultdict, Counter, namedtuple\nfrom functools import reduce\nfrom itertools import islice\nimport re\n\nfrom six import text_type\n\nfrom nltk.probability import FreqDist, LidstoneProbDist\nfrom nltk.probability import ConditionalFreqDist as CFD\nfrom nltk.util import tokenwrap, LazyConcatenation\nfrom nltk.metrics import f_measure, BigramAssocMeasures\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.compat import python_2_unicode_compatible\n\nConcordanceLine = namedtuple('ConcordanceLine',\n                             ['left', 'query', 'right', 'offset',\n                              'left_print', 'right_print', 'line'])\n\nclass ContextIndex(object):\n    @staticmethod\n    def _default_context(tokens, i):\n        left = (tokens[i-1].lower() if i != 0 else '*START*')\n        right = (tokens[i+1].lower() if i != len(tokens) - 1 else '*END*')\n        return (left, right)\n\n    def __init__(self, tokens, context_func=None, filter=None, key=lambda x:x):\n        self._key = key\n        self._tokens = tokens\n        if context_func:\n            self._context_func = context_func\n        else:\n            self._context_func = self._default_context\n        if filter:\n            tokens = [t for t in tokens if filter(t)]\n        self._word_to_contexts = CFD((self._key(w), self._context_func(tokens, i))\n                                     for i, w in enumerate(tokens))\n        self._context_to_words = CFD((self._context_func(tokens, i), self._key(w))\n                                     for i, w in enumerate(tokens))\n\n    def tokens(self):\n        return self._tokens\n\n    def word_similarity_dict(self, word):\n        word = self._key(word)\n        word_contexts = set(self._word_to_contexts[word])\n\n        scores = {}\n        for w, w_contexts in self._word_to_contexts.items():\n            scores[w] = f_measure(word_contexts, set(w_contexts))\n\n        return scores\n\n    def similar_words(self, word, n=20):\n        scores = defaultdict(int)\n        for c in self._word_to_contexts[self._key(word)]:\n            for w in self._context_to_words[c]:\n                if w != word:\n                    scores[w] += self._context_to_words[c][word] * self._context_to_words[c][w]\n        return sorted(scores, key=scores.get, reverse=True)[:n]\n\n    def common_contexts(self, words, fail_on_unknown=False):\n        words = [self._key(w) for w in words]\n        contexts = [set(self._word_to_contexts[w]) for w in words]\n        empty = [words[i] for i in range(len(words)) if not contexts[i]]\n        common = reduce(set.intersection, contexts)\n        if empty and fail_on_unknown:\n            raise ValueError(\"The following word(s) were not found:\",\n                             \" \".join(words))\n        elif not common:\n            return FreqDist()\n        else:\n            fd = FreqDist(c for w in words\n                          for c in self._word_to_contexts[w]\n                          if c in common)\n            return fd\n\n\n@python_2_unicode_compatible\nclass ConcordanceIndex(object):\n    def __init__(self, tokens, key=lambda x:x):\n        self._tokens = tokens\n        :rtype: list(str)\n        :return: The document that this concordance index was\n            created from.\n        \"\"\"\n        return self._tokens\n\n    def offsets(self, word):\n        \"\"\"\n        :rtype: list(int)\n        :return: A list of the offset positions at which the given\n            word occurs.  If a key function was specified for the\n            index, then given word's key will be looked up.\n        \"\"\"\n        word = self._key(word)\n        return self._offsets[word]\n\n    def __repr__(self):\n        return '<ConcordanceIndex for %d tokens (%d types)>' % (\n            len(self._tokens), len(self._offsets))\n\n    def find_concordance(self, word, width=80, lines=25):\n        \"\"\"\n        Find the concordance lines given the query word.\n        \"\"\"\n        half_width = (width - len(word) - 2) // 2\n        context = width // 4  # approx number of words of context\n\n        concordance_list = []\n        offsets = self.offsets(word)\n        if offsets:\n            for i in offsets:\n                query_word = self._tokens[i]\n                left_context = self._tokens[i-context:i]\n                right_context = self._tokens[i+1:i+context]\n                left_print= ' '.join(left_context)[-half_width:]\n                right_print = ' '.join(right_context)[:half_width]\n                line_print = ' '.join([left_print, query_word, right_print])\n                concordance_line = ConcordanceLine(left_context, query_word,\n                                                    right_context, i,\n                                                    left_print, right_print, line_print)\n                concordance_list.append(concordance_line)\n        return concordance_list[:lines]\n\n    def print_concordance(self, word, width=80, lines=25):\n        \"\"\"\n        Print concordance lines given the query word.\n        :param word: The target word\n        :type word: str\n        :param lines: The number of lines to display (default=25)\n        :type lines: int\n        :param width: The width of each line, in characters (default=80)\n        :type width: int\n        :param save: The option to save the concordance.\n        :type save: bool\n        \"\"\"\n        concordance_list = self.find_concordance(word, width=80, lines=25)\n\n        if not concordance_list:\n            print(\"no matches\")\n        else:\n            lines = min(lines, len(concordance_list))\n            print(\"Displaying {} of {} matches:\".format(lines,len(concordance_list)))\n            for i, concordance_line in enumerate(concordance_list[:lines]):\n                print(concordance_line.line)\n\n\nclass TokenSearcher(object):\n    \"\"\"\n    A class that makes it easier to use regular expressions to search\n    over tokenized strings.  The tokenized string is converted to a\n    string where tokens are marked with angle brackets -- e.g.,\n    ``'<the><window><is><still><open>'``.  The regular expression\n    passed to the ``findall()`` method is modified to treat angle\n    brackets as non-capturing parentheses, in addition to matching the\n    token boundaries; and to have ``'.'`` not match the angle brackets.\n    \"\"\"\n    def __init__(self, tokens):\n        self._raw = ''.join('<'+w+'>' for w in tokens)\n\n    def findall(self, regexp):\n        \"\"\"\n        Find instances of the regular expression in the text.\n        The text is a list of tokens, and a regexp pattern to match\n        a single token must be surrounded by angle brackets.  E.g.\n\n        >>> from nltk.text import TokenSearcher\n        >>> print('hack'); from nltk.book import text1, text5, text9\n        hack...\n        >>> text5.findall(\"<.*><.*><bro>\")\n        you rule bro; telling you bro; u twizted bro\n        >>> text1.findall(\"<a>(<.*>)<man>\")\n        monied; nervous; dangerous; white; white; white; pious; queer; good;\n        mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n        pale; furious; better; certain; complete; dismasted; younger; brave;\n        brave; brave; brave\n        >>> text9.findall(\"<th.*>{3,}\")\n        thread through those; the thought that; that the thing; the thing\n        that; that that thing; through these than through; them that the;\n        through the thick; them that they; thought that the\n\n        :param regexp: A regular expression\n        :type regexp: str\n        \"\"\"\n        regexp = re.sub(r'\\s', '', regexp)\n        regexp = re.sub(r'<', '(?:<(?:', regexp)\n        regexp = re.sub(r'>', ')>)', regexp)\n        regexp = re.sub(r'(?<!\\\\)\\.', '[^>]', regexp)\n\n        hits = re.findall(regexp, self._raw)\n\n        for h in hits:\n            if not h.startswith('<') and h.endswith('>'):\n                raise ValueError('Bad regexp for TokenSearcher.findall')\n\n        hits = [h[1:-1].split('><') for h in hits]\n        return hits\n\n\n@python_2_unicode_compatible\nclass Text(object):\n    \"\"\"\n    A wrapper around a sequence of simple (string) tokens, which is\n    intended to support initial exploration of texts (via the\n    interactive console).  Its methods perform a variety of analyses\n    on the text's contexts (e.g., counting, concordancing, collocation\n    discovery), and display the results.  If you wish to write a\n    program which makes use of these analyses, then you should bypass\n    the ``Text`` class, and use the appropriate analysis function or\n    class directly instead.\n\n    A ``Text`` is typically initialized from a given document or\n    corpus.  E.g.:\n\n    >>> import nltk.corpus\n    >>> from nltk.text import Text\n    >>> moby = Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))\n\n    \"\"\"\n    _COPY_TOKENS = True\n\n    def __init__(self, tokens, name=None):\n        \"\"\"\n        Create a Text object.\n\n        :param tokens: The source text.\n        :type tokens: sequence of str\n        \"\"\"\n        if self._COPY_TOKENS:\n            tokens = list(tokens)\n        self.tokens = tokens\n\n        if name:\n            self.name = name\n        elif ']' in tokens[:20]:\n            end = tokens[:20].index(']')\n            self.name = \" \".join(text_type(tok) for tok in tokens[1:end])\n        else:\n            self.name = \" \".join(text_type(tok) for tok in tokens[:8]) + \"...\"\n\n\n    def __getitem__(self, i):\n        return self.tokens[i]\n\n    def __len__(self):\n        return len(self.tokens)\n\n\n    def concordance(self, word, width=79, lines=25):\n        \"\"\"\n        Prints a concordance for ``word`` with the specified context window.\n        Word matching is not case-sensitive.\n\n        :param word: The target word\n        :type word: str\n        :param width: The width of each line, in characters (default=80)\n        :type width: int\n        :param lines: The number of lines to display (default=25)\n        :type lines: int\n\n        :seealso: ``ConcordanceIndex``\n        \"\"\"\n        if '_concordance_index' not in self.__dict__:\n            self._concordance_index = ConcordanceIndex(self.tokens, key=lambda s:s.lower())\n\n        return self._concordance_index.print_concordance(word, width, lines)\n\n    def concordance_list(self,  word, width=79, lines=25):\n        \"\"\"\n        Generate a concordance for ``word`` with the specified context window.\n        Word matching is not case-sensitive.\n\n        :param word: The target word\n        :type word: str\n        :param width: The width of each line, in characters (default=80)\n        :type width: int\n        :param lines: The number of lines to display (default=25)\n        :type lines: int\n\n        :seealso: ``ConcordanceIndex``\n        \"\"\"\n        if '_concordance_index' not in self.__dict__:\n            self._concordance_index = ConcordanceIndex(self.tokens, key=lambda s:s.lower())\n        return self._concordance_index.find_concordance(word, width, lines)\n\n    def collocations(self, num=20, window_size=2):\n        \"\"\"\n        Print collocations derived from the text, ignoring stopwords.\n\n        :seealso: find_collocations\n        :param num: The maximum number of collocations to print.\n        :type num: int\n        :param window_size: The number of tokens spanned by a collocation (default=2)\n        :type window_size: int\n        \"\"\"\n        if not ('_collocations' in self.__dict__ and self._num == num and self._window_size == window_size):\n            self._num = num\n            self._window_size = window_size\n\n            from nltk.corpus import stopwords\n            ignored_words = stopwords.words('english')\n            finder = BigramCollocationFinder.from_words(self.tokens, window_size)\n            finder.apply_freq_filter(2)\n            finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n            bigram_measures = BigramAssocMeasures()\n            self._collocations = finder.nbest(bigram_measures.likelihood_ratio, num)\n        colloc_strings = [w1+' '+w2 for w1, w2 in self._collocations]\n        print(tokenwrap(colloc_strings, separator=\"; \"))\n\n    def count(self, word):\n        \"\"\"\n        Count the number of times this word appears in the text.\n        \"\"\"\n        return self.tokens.count(word)\n\n    def index(self, word):\n        \"\"\"\n        Find the index of the first occurrence of the word in the text.\n        \"\"\"\n        return self.tokens.index(word)\n\n    def readability(self, method):\n        raise NotImplementedError\n\n    def similar(self, word, num=20):\n        \"\"\"\n        Distributional similarity: find other words which appear in the\n        same contexts as the specified word; list most similar words first.\n\n        :param word: The word used to seed the similarity search\n        :type word: str\n        :param num: The number of words to generate (default=20)\n        :type num: int\n        :seealso: ContextIndex.similar_words()\n        \"\"\"\n        if '_word_context_index' not in self.__dict__:\n            self._word_context_index = ContextIndex(self.tokens,\n                                                    filter=lambda x:x.isalpha(),\n                                                    key=lambda s:s.lower())\n\n\n        word = word.lower()\n        wci = self._word_context_index._word_to_contexts\n        if word in wci.conditions():\n            contexts = set(wci[word])\n            fd = Counter(w for w in wci.conditions() for c in wci[w]\n                          if c in contexts and not w == word)\n            words = [w for w, _ in fd.most_common(num)]\n            print(tokenwrap(words))\n        else:\n            print(\"No matches\")\n\n\n    def common_contexts(self, words, num=20):\n        \"\"\"\n        Find contexts where the specified words appear; list\n        most frequent common contexts first.\n\n        :param word: The word used to seed the similarity search\n        :type word: str\n        :param num: The number of words to generate (default=20)\n        :type num: int\n        :seealso: ContextIndex.common_contexts()\n        \"\"\"\n        if '_word_context_index' not in self.__dict__:\n            self._word_context_index = ContextIndex(self.tokens,\n                                                    key=lambda s:s.lower())\n\n        try:\n            fd = self._word_context_index.common_contexts(words, True)\n            if not fd:\n                print(\"No common contexts were found\")\n            else:\n                ranked_contexts = [w for w, _ in fd.most_common(num)]\n                print(tokenwrap(w1+\"_\"+w2 for w1,w2 in ranked_contexts))\n\n        except ValueError as e:\n            print(e)\n\n    def dispersion_plot(self, words):\n        \"\"\"\n        Produce a plot showing the distribution of the words through the text.\n        Requires pylab to be installed.\n\n        :param words: The words to be plotted\n        :type words: list(str)\n        :seealso: nltk.draw.dispersion_plot()\n        \"\"\"\n        from nltk.draw import dispersion_plot\n        dispersion_plot(self, words)\n\n    def generate(self, words):\n        \"\"\"\n        Issues a reminder to users following the book online\n        \"\"\"\n        import warnings\n        warnings.warn('The generate() method is no longer available.', DeprecationWarning)\n\n    def plot(self, *args):\n        \"\"\"\n        See documentation for FreqDist.plot()\n        :seealso: nltk.prob.FreqDist.plot()\n        \"\"\"\n        self.vocab().plot(*args)\n\n    def vocab(self):\n        \"\"\"\n        :seealso: nltk.prob.FreqDist\n        \"\"\"\n        if \"_vocab\" not in self.__dict__:\n            self._vocab = FreqDist(self)\n        return self._vocab\n\n    def findall(self, regexp):\n        \"\"\"\n        Find instances of the regular expression in the text.\n        The text is a list of tokens, and a regexp pattern to match\n        a single token must be surrounded by angle brackets.  E.g.\n\n        >>> print('hack'); from nltk.book import text1, text5, text9\n        hack...\n        >>> text5.findall(\"<.*><.*><bro>\")\n        you rule bro; telling you bro; u twizted bro\n        >>> text1.findall(\"<a>(<.*>)<man>\")\n        monied; nervous; dangerous; white; white; white; pious; queer; good;\n        mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n        pale; furious; better; certain; complete; dismasted; younger; brave;\n        brave; brave; brave\n        >>> text9.findall(\"<th.*>{3,}\")\n        thread through those; the thought that; that the thing; the thing\n        that; that that thing; through these than through; them that the;\n        through the thick; them that they; thought that the\n\n        :param regexp: A regular expression\n        :type regexp: str\n        \"\"\"\n\n        if \"_token_searcher\" not in self.__dict__:\n            self._token_searcher = TokenSearcher(self)\n\n        hits = self._token_searcher.findall(regexp)\n        hits = [' '.join(h) for h in hits]\n        print(tokenwrap(hits, \"; \"))\n\n\n    _CONTEXT_RE = re.compile('\\w+|[\\.\\!\\?]')\n    def _context(self, tokens, i):\n        \"\"\"\n        One left & one right token, both case-normalized.  Skip over\n        non-sentence-final punctuation.  Used by the ``ContextIndex``\n        that is created for ``similar()`` and ``common_contexts()``.\n        \"\"\"\n        j = i-1\n        while j>=0 and not self._CONTEXT_RE.match(tokens[j]):\n            j -= 1\n        left = (tokens[j] if j != 0 else '*START*')\n\n        j = i+1\n        while j<len(tokens) and not self._CONTEXT_RE.match(tokens[j]):\n            j += 1\n        right = (tokens[j] if j != len(tokens) else '*END*')\n\n        return (left, right)\n\n\n    def __str__(self):\n        return '<Text: %s>' % self.name\n\n    def __repr__(self):\n        return '<Text: %s>' % self.name\n\n\nclass TextCollection(Text):\n    \"\"\"A collection of texts, which can be loaded with list of texts, or\n    with a corpus consisting of one or more texts, and which supports\n    counting, concordancing, collocation discovery, etc.  Initialize a\n    TextCollection as follows:\n\n    >>> import nltk.corpus\n    >>> from nltk.text import TextCollection\n    >>> print('hack'); from nltk.book import text1, text2, text3\n    hack...\n    >>> gutenberg = TextCollection(nltk.corpus.gutenberg)\n    >>> mytexts = TextCollection([text1, text2, text3])\n\n    Iterating over a TextCollection produces all the tokens of all the\n    texts in order.\n    \"\"\"\n    def __init__(self, source):\n        if hasattr(source, 'words'): # bridge to the text corpus reader\n            source = [source.words(f) for f in source.fileids()]\n\n        self._texts = source\n        Text.__init__(self, LazyConcatenation(source))\n        self._idf_cache = {}\n\n    def tf(self, term, text):\n        return text.count(term) / len(text)\n\n    def idf(self, term):\n        \"\"\" The number of texts in the corpus divided by the\n        number of texts that the term appears in.\n        If a term does not appear in the corpus, 0.0 is returned. \"\"\"\n        idf = self._idf_cache.get(term)\n        if idf is None:\n            matches = len([True for text in self._texts if term in text])\n            if len(self._texts) == 0:\n                raise ValueError('IDF undefined for empty document collection')\n            idf = (log(len(self._texts) / matches) if matches else 0.0)\n            self._idf_cache[term] = idf\n        return idf\n\n    def tf_idf(self, term, text):\n        return self.tf(term, text) * self.idf(term)\n\ndef demo():\n    from nltk.corpus import brown\n    text = Text(brown.words(categories='news'))\n    print(text)\n    print()\n    print(\"Concordance:\")\n    text.concordance('news')\n    print()\n    print(\"Distributionally similar words:\")\n    text.similar('news')\n    print()\n    print(\"Collocations:\")\n    text.collocations()\n    print()\n    print(\"Dispersion plot:\")\n    text.dispersion_plot(['news', 'report', 'said', 'announced'])\n    print()\n    print(\"Vocabulary plot:\")\n    text.plot(50)\n    print()\n    print(\"Indexing:\")\n    print(\"text[3]:\", text[3])\n    print(\"text[3:5]:\", text[3:5])\n    print(\"text.vocab()['news']:\", text.vocab()['news'])\n\nif __name__ == '__main__':\n    demo()\n\n__all__ = [\"ContextIndex\",\n           \"ConcordanceIndex\",\n           \"TokenSearcher\",\n           \"Text\",\n           \"TextCollection\"]\n"], "nltk\\tgrep": [".py", "\n'''\n============================================\n TGrep search implementation for NLTK trees\n============================================\n\nThis module supports TGrep2 syntax for matching parts of NLTK Trees.\nNote that many tgrep operators require the tree passed to be a\n``ParentedTree``.\n\nExternal links:\n\n- `Tgrep tutorial <http://www.stanford.edu/dept/linguistics/corpora/cas-tut-tgrep.html>`_\n- `Tgrep2 manual <http://tedlab.mit.edu/~dr/Tgrep2/tgrep2.pdf>`_\n- `Tgrep2 source <http://tedlab.mit.edu/~dr/Tgrep2/>`_\n\nUsage\n=====\n\n>>> from nltk.tree import ParentedTree\n>>> from nltk.tgrep import tgrep_nodes, tgrep_positions\n>>> tree = ParentedTree.fromstring('(S (NP (DT the) (JJ big) (NN dog)) (VP bit) (NP (DT a) (NN cat)))')\n>>> list(tgrep_nodes('NN', [tree]))\n[[ParentedTree('NN', ['dog']), ParentedTree('NN', ['cat'])]]\n>>> list(tgrep_positions('NN', [tree]))\n[[(0, 2), (2, 1)]]\n>>> list(tgrep_nodes('DT', [tree]))\n[[ParentedTree('DT', ['the']), ParentedTree('DT', ['a'])]]\n>>> list(tgrep_nodes('DT $ JJ', [tree]))\n[[ParentedTree('DT', ['the'])]]\n\nThis implementation adds syntax to select nodes based on their NLTK\ntree position.  This syntax is ``N`` plus a Python tuple representing\nthe tree position.  For instance, ``N()``, ``N(0,)``, ``N(0,0)`` are\nvalid node selectors.  Example:\n\n>>> tree = ParentedTree.fromstring('(S (NP (DT the) (JJ big) (NN dog)) (VP bit) (NP (DT a) (NN cat)))')\n>>> tree[0,0]\nParentedTree('DT', ['the'])\n>>> tree[0,0].treeposition()\n(0, 0)\n>>> list(tgrep_nodes('N(0,0)', [tree]))\n[[ParentedTree('DT', ['the'])]]\n\nCaveats:\n========\n\n- Link modifiers: \"?\" and \"=\" are not implemented.\n- Tgrep compatibility: Using \"@\" for \"!\", \"{\" for \"<\", \"}\" for \">\" are\n  not implemented.\n- The \"=\" and \"~\" links are not implemented.\n\nKnown Issues:\n=============\n\n- There are some issues with link relations involving leaf nodes\n  (which are represented as bare strings in NLTK trees).  For\n  instance, consider the tree::\n\n      (S (A x))\n\n  The search string ``* !>> S`` should select all nodes which are not\n  dominated in some way by an ``S`` node (i.e., all nodes which are\n  not descendants of an ``S``).  Clearly, in this tree, the only node\n  which fulfills this criterion is the top node (since it is not\n  dominated by anything).  However, the code here will find both the\n  top node and the leaf node ``x``.  This is because we cannot recover\n  the parent of the leaf, since it is stored as a bare string.\n\n  A possible workaround, when performing this kind of search, would be\n  to filter out all leaf nodes.\n\nImplementation notes\n====================\n\nThis implementation is (somewhat awkwardly) based on lambda functions\nwhich are predicates on a node.  A predicate is a function which is\neither True or False; using a predicate function, we can identify sets\nof nodes with particular properties.  A predicate function, could, for\ninstance, return True only if a particular node has a label matching a\nparticular regular expression, and has a daughter node which has no\nsisters.  Because tgrep2 search strings can do things statefully (such\nas substituting in macros, and binding nodes with node labels), the\nactual predicate function is declared with three arguments::\n\n    pred = lambda n, m, l: return True # some logic here\n\n``n``\n    is a node in a tree; this argument must always be given\n\n``m``\n    contains a dictionary, mapping macro names onto predicate functions\n\n``l``\n    is a dictionary to map node labels onto nodes in the tree\n\n``m`` and ``l`` are declared to default to ``None``, and so need not be\nspecified in a call to a predicate.  Predicates which call other\npredicates must always pass the value of these arguments on.  The\ntop-level predicate (constructed by ``_tgrep_exprs_action``) binds the\nmacro definitions to ``m`` and initialises ``l`` to an empty dictionary.\n'''\n\nfrom __future__ import absolute_import, print_function, unicode_literals\n\nimport functools\nimport re\n\nfrom six import binary_type, text_type\n\ntry:\n    import pyparsing\nexcept ImportError:\n    print('Warning: nltk.tgrep will not work without the `pyparsing` package')\n    print('installed.')\n\nimport nltk.tree\n\nclass TgrepException(Exception):\n    '''Tgrep exception type.'''\n    pass\n\ndef ancestors(node):\n    '''\n    Returns the list of all nodes dominating the given tree node.\n    This method will not work with leaf nodes, since there is no way\n    to recover the parent.\n    '''\n    results = []\n    try:\n        current = node.parent()\n    except AttributeError:\n        return results\n    while current:\n        results.append(current)\n        current = current.parent()\n    return results\n\ndef unique_ancestors(node):\n    '''\n    Returns the list of all nodes dominating the given node, where\n    there is only a single path of descent.\n    '''\n    results = []\n    try:\n        current = node.parent()\n    except AttributeError:\n        return results\n    while current and len(current) == 1:\n        results.append(current)\n        current = current.parent()\n    return results\n\ndef _descendants(node):\n    '''\n    Returns the list of all nodes which are descended from the given\n    tree node in some way.\n    '''\n    try:\n        treepos = node.treepositions()\n    except AttributeError:\n        return []\n    return [node[x] for x in treepos[1:]]\n\ndef _leftmost_descendants(node):\n    '''\n    Returns the set of all nodes descended in some way through\n    left branches from this node.\n    '''\n    try:\n        treepos = node.treepositions()\n    except AttributeError:\n        return []\n    return [node[x] for x in treepos[1:] if all(y == 0 for y in x)]\n\ndef _rightmost_descendants(node):\n    '''\n    Returns the set of all nodes descended in some way through\n    right branches from this node.\n    '''\n    try:\n        rightmost_leaf = max(node.treepositions())\n    except AttributeError:\n        return []\n    return [node[rightmost_leaf[:i]] for i in range(1, len(rightmost_leaf) + 1)]\n\ndef _istree(obj):\n    '''Predicate to check whether `obj` is a nltk.tree.Tree.'''\n    return isinstance(obj, nltk.tree.Tree)\n\ndef _unique_descendants(node):\n    '''\n    Returns the list of all nodes descended from the given node, where\n    there is only a single path of descent.\n    '''\n    results = []\n    current = node\n    while current and _istree(current) and len(current) == 1:\n        current = current[0]\n        results.append(current)\n    return results\n\ndef _before(node):\n    '''\n    Returns the set of all nodes that are before the given node.\n    '''\n    try:\n        pos = node.treeposition()\n        tree = node.root()\n    except AttributeError:\n        return []\n    return [tree[x] for x in tree.treepositions()\n            if x[:len(pos)] < pos[:len(x)]]\n\ndef _immediately_before(node):\n    '''\n    Returns the set of all nodes that are immediately before the given\n    node.\n\n    Tree node A immediately precedes node B if the last terminal\n    symbol (word) produced by A immediately precedes the first\n    terminal symbol produced by B.\n    '''\n    try:\n        pos = node.treeposition()\n        tree = node.root()\n    except AttributeError:\n        return []\n    idx = len(pos) - 1\n    while 0 <= idx and pos[idx] == 0:\n        idx -= 1\n    if idx < 0:\n        return []\n    pos = list(pos[:idx + 1])\n    pos[-1] -= 1\n    before = tree[pos]\n    return [before] + _rightmost_descendants(before)\n\ndef _after(node):\n    '''\n    Returns the set of all nodes that are after the given node.\n    '''\n    try:\n        pos = node.treeposition()\n        tree = node.root()\n    except AttributeError:\n        return []\n    return [tree[x] for x in tree.treepositions()\n            if x[:len(pos)] > pos[:len(x)]]\n\ndef _immediately_after(node):\n    '''\n    Returns the set of all nodes that are immediately after the given\n    node.\n\n    Tree node A immediately follows node B if the first terminal\n    symbol (word) produced by A immediately follows the last\n    terminal symbol produced by B.\n    '''\n    try:\n        pos = node.treeposition()\n        tree = node.root()\n        current = node.parent()\n    except AttributeError:\n        return []\n    idx = len(pos) - 1\n    while 0 <= idx and pos[idx] == len(current) - 1:\n        idx -= 1\n        current = current.parent()\n    if idx < 0:\n        return []\n    pos = list(pos[:idx + 1])\n    pos[-1] += 1\n    after = tree[pos]\n    return [after] + _leftmost_descendants(after)\n\ndef _tgrep_node_literal_value(node):\n    '''\n    Gets the string value of a given parse tree node, for comparison\n    using the tgrep node literal predicates.\n    '''\n    return (node.label() if _istree(node) else text_type(node))\n\ndef _tgrep_macro_use_action(_s, _l, tokens):\n    '''\n    Builds a lambda function which looks up the macro name used.\n    '''\n    assert len(tokens) == 1\n    assert tokens[0][0] == '@'\n    macro_name = tokens[0][1:]\n    def macro_use(n, m=None, l=None):\n        if m is None or macro_name not in m:\n            raise TgrepException('macro {0} not defined'.format(macro_name))\n        return m[macro_name](n, m, l)\n    return macro_use\n\ndef _tgrep_node_action(_s, _l, tokens):\n    '''\n    Builds a lambda function representing a predicate on a tree node\n    depending on the name of its node.\n    '''\n    if tokens[0] == \"'\":\n        tokens = tokens[1:]\n    if len(tokens) > 1:\n        assert list(set(tokens[1::2])) == ['|']\n        tokens = [_tgrep_node_action(None, None, [node])\n                  for node in tokens[::2]]\n        return (lambda t: lambda n, m=None, l=None: any(f(n, m, l) for f in t))(tokens)\n    else:\n        if hasattr(tokens[0], '__call__'):\n            return tokens[0]\n        elif tokens[0] == '*' or tokens[0] == '__':\n            return lambda n, m=None, l=None: True\n        elif tokens[0].startswith('\"'):\n            assert tokens[0].endswith('\"')\n            node_lit = tokens[0][1:-1].replace('\\\\\"', '\"').replace('\\\\\\\\', '\\\\')\n            return (lambda s: lambda n, m=None, l=None: _tgrep_node_literal_value(n) == s)(node_lit)\n        elif tokens[0].startswith('/'):\n            assert tokens[0].endswith('/')\n            node_lit = tokens[0][1:-1]\n            return (lambda r: lambda n, m=None, l=None:\n                    r.search(_tgrep_node_literal_value(n)))(re.compile(node_lit))\n        elif tokens[0].startswith('i@'):\n            node_func = _tgrep_node_action(_s, _l, [tokens[0][2:].lower()])\n            return (lambda f: lambda n, m=None, l=None:\n                    f(_tgrep_node_literal_value(n).lower()))(node_func)\n        else:\n            return (lambda s: lambda n, m=None, l=None:\n                    _tgrep_node_literal_value(n) == s)(tokens[0])\n\ndef _tgrep_parens_action(_s, _l, tokens):\n    '''\n    Builds a lambda function representing a predicate on a tree node\n    from a parenthetical notation.\n    '''\n    assert len(tokens) == 3\n    assert tokens[0] == '('\n    assert tokens[2] == ')'\n    return tokens[1]\n\ndef _tgrep_nltk_tree_pos_action(_s, _l, tokens):\n    '''\n    Builds a lambda function representing a predicate on a tree node\n    which returns true if the node is located at a specific tree\n    position.\n    '''\n    node_tree_position = tuple(int(x) for x in tokens if x.isdigit())\n    return (lambda i: lambda n, m=None, l=None: (hasattr(n, 'treeposition') and\n                                                 n.treeposition() == i))(node_tree_position)\n\ndef _tgrep_relation_action(_s, _l, tokens):\n    '''\n    Builds a lambda function representing a predicate on a tree node\n    depending on its relation to other nodes in the tree.\n    '''\n    negated = False\n    if tokens[0] == '!':\n        negated = True\n        tokens = tokens[1:]\n    if tokens[0] == '[':\n        assert len(tokens) == 3\n        assert tokens[2] == ']'\n        retval = tokens[1]\n    else:\n        assert len(tokens) == 2\n        operator, predicate = tokens\n        if operator == '<':\n            retval = lambda n, m=None, l=None: (_istree(n) and\n                                                any(predicate(x, m, l) for x in n))\n        elif operator == '>':\n            retval = lambda n, m=None, l=None: (hasattr(n, 'parent') and\n                                                bool(n.parent()) and\n                                                predicate(n.parent(), m, l))\n        elif operator == '<,' or operator == '<1':\n            retval = lambda n, m=None, l=None: (_istree(n) and\n                                                bool(list(n)) and\n                                                predicate(n[0], m, l))\n        elif operator == '>,' or operator == '>1':\n            retval = lambda n, m=None, l=None: (hasattr(n, 'parent') and\n                                                bool(n.parent()) and\n                                                (n is n.parent()[0]) and\n                                                predicate(n.parent(), m, l))\n        elif operator[0] == '<' and operator[1:].isdigit():\n            idx = int(operator[1:])\n            retval = (lambda i: lambda n, m=None, l=None: (_istree(n) and\n                                                           bool(list(n)) and\n                                                           0 <= i < len(n) and\n                                                           predicate(n[i], m, l)))(idx - 1)\n        elif operator[0] == '>' and operator[1:].isdigit():\n            idx = int(operator[1:])\n            retval = (lambda i: lambda n, m=None, l=None: (hasattr(n, 'parent') and\n                                                           bool(n.parent()) and\n                                                           0 <= i < len(n.parent()) and\n                                                           (n is n.parent()[i]) and\n                                                           predicate(n.parent(), m, l)))(idx - 1)\n        elif operator == '<\\'' or operator == '<-' or operator == '<-1':\n            retval = lambda n, m=None, l=None: (_istree(n) and bool(list(n))\n                                                and predicate(n[-1], m, l))\n        elif operator == '>\\'' or operator == '>-' or operator == '>-1':\n            retval = lambda n, m=None, l=None: (hasattr(n, 'parent') and\n                                                bool(n.parent()) and\n                                                (n is n.parent()[-1]) and\n                                                predicate(n.parent(), m, l))\n        elif operator[:2] == '<-' and operator[2:].isdigit():\n            idx = -int(operator[2:])\n            retval = (lambda i: lambda n, m=None, l=None: (_istree(n) and\n                                                           bool(list(n)) and\n                                                           0 <= (i + len(n)) < len(n) and\n                                                           predicate(n[i + len(n)], m, l)))(idx)\n        elif operator[:2] == '>-' and operator[2:].isdigit():\n            idx = -int(operator[2:])\n            retval = (lambda i: lambda n, m=None, l=None:\n                          (hasattr(n, 'parent') and\n                           bool(n.parent()) and\n                           0 <= (i + len(n.parent())) < len(n.parent()) and\n                           (n is n.parent()[i + len(n.parent())]) and\n                           predicate(n.parent(), m, l)))(idx)\n        elif operator == '<:':\n            retval = lambda n, m=None, l=None: (_istree(n) and\n                                                len(n) == 1 and\n                                                predicate(n[0], m, l))\n        elif operator == '>:':\n            retval = lambda n, m=None, l=None: (hasattr(n, 'parent') and\n                                                bool(n.parent()) and\n                                                len(n.parent()) == 1 and\n                                                predicate(n.parent(), m, l))\n        elif operator == '<<':\n            retval = lambda n, m=None, l=None: (_istree(n) and\n                                                any(predicate(x, m, l) for x in _descendants(n)))\n        elif operator == '>>':\n            retval = lambda n, m=None, l=None: any(predicate(x, m, l) for x in ancestors(n))\n        elif operator == '<<,' or operator == '<<1':\n            retval = lambda n, m=None, l=None: (_istree(n) and\n                                                any(predicate(x, m, l)\n                                                    for x in _leftmost_descendants(n)))\n        elif operator == '>>,':\n            retval = lambda n, m=None, l=None: any((predicate(x, m, l) and\n                                                    n in _leftmost_descendants(x))\n                                                   for x in ancestors(n))\n        elif operator == '<<\\'':\n            retval = lambda n, m=None, l=None: (_istree(n) and\n                                                any(predicate(x, m, l)\n                                                    for x in _rightmost_descendants(n)))\n        elif operator == '>>\\'':\n            retval = lambda n, m=None, l=None: any((predicate(x, m, l) and\n                                                    n in _rightmost_descendants(x))\n                                                   for x in ancestors(n))\n        elif operator == '<<:':\n            retval = lambda n, m=None, l=None: (_istree(n) and\n                                                any(predicate(x, m, l)\n                                                    for x in _unique_descendants(n)))\n        elif operator == '>>:':\n            retval = lambda n, m=None, l=None: any(predicate(x, m, l) for x in unique_ancestors(n))\n        elif operator == '.':\n            retval = lambda n, m=None, l=None: any(predicate(x, m, l)\n                                                   for x in _immediately_after(n))\n        elif operator == ',':\n            retval = lambda n, m=None, l=None: any(predicate(x, m, l)\n                                                   for x in _immediately_before(n))\n        elif operator == '..':\n            retval = lambda n, m=None, l=None: any(predicate(x, m, l) for x in _after(n))\n        elif operator == ',,':\n            retval = lambda n, m=None, l=None: any(predicate(x, m, l) for x in _before(n))\n        elif operator == '$' or operator == '%':\n            retval = lambda n, m=None, l=None: (hasattr(n, 'parent') and\n                                                bool(n.parent()) and\n                                                any(predicate(x, m, l)\n                                                    for x in n.parent() if x is not n))\n        elif operator == '$.' or operator == '%.':\n            retval = lambda n, m=None, l=None: (hasattr(n, 'right_sibling') and\n                                                bool(n.right_sibling()) and\n                                                predicate(n.right_sibling(), m, l))\n        elif operator == '$,' or operator == '%,':\n            retval = lambda n, m=None, l=None: (hasattr(n, 'left_sibling') and\n                                                bool(n.left_sibling()) and\n                                                predicate(n.left_sibling(), m, l))\n        elif operator == '$..' or operator == '%..':\n            retval = lambda n, m=None, l=None: (hasattr(n, 'parent') and\n                                                hasattr(n, 'parent_index') and\n                                                bool(n.parent()) and\n                                                any(predicate(x, m, l) for x in\n                                                    n.parent()[n.parent_index() + 1:]))\n        elif operator == '$,,' or operator == '%,,':\n            retval = lambda n, m=None, l=None: (hasattr(n, 'parent') and\n                                                hasattr(n, 'parent_index') and\n                                                bool(n.parent()) and\n                                                any(predicate(x, m, l) for x in\n                                                    n.parent()[:n.parent_index()]))\n        else:\n            raise TgrepException(\n                'cannot interpret tgrep operator \"{0}\"'.format(operator))\n    if negated:\n        return (lambda r: (lambda n, m=None, l=None: not r(n, m, l)))(retval)\n    else:\n        return retval\n\ndef _tgrep_conjunction_action(_s, _l, tokens, join_char = '&'):\n    '''\n    Builds a lambda function representing a predicate on a tree node\n    from the conjunction of several other such lambda functions.\n\n    This is prototypically called for expressions like\n    (`tgrep_rel_conjunction`)::\n\n        < NP & < AP < VP\n\n    where tokens is a list of predicates representing the relations\n    (`< NP`, `< AP`, and `< VP`), possibly with the character `&`\n    included (as in the example here).\n\n    This is also called for expressions like (`tgrep_node_expr2`)::\n\n        NP < NN\n        S=s < /NP/=n : s < /VP/=v : n .. v\n\n    tokens[0] is a tgrep_expr predicate; tokens[1:] are an (optional)\n    list of segmented patterns (`tgrep_expr_labeled`, processed by\n    `_tgrep_segmented_pattern_action`).\n    '''\n    tokens = [x for x in tokens if x != join_char]\n    if len(tokens) == 1:\n        return tokens[0]\n    else:\n        return (lambda ts: lambda n, m=None, l=None: all(predicate(n, m, l)\n                                                         for predicate in ts))(tokens)\n\ndef _tgrep_segmented_pattern_action(_s, _l, tokens):\n    '''\n    Builds a lambda function representing a segmented pattern.\n\n    Called for expressions like (`tgrep_expr_labeled`)::\n\n        =s .. =v < =n\n\n    This is a segmented pattern, a tgrep2 expression which begins with\n    a node label.\n\n    The problem is that for segemented_pattern_action (': =v < =s'),\n    the first element (in this case, =v) is specifically selected by\n    virtue of matching a particular node in the tree; to retrieve\n    the node, we need the label, not a lambda function.  For node\n    labels inside a tgrep_node_expr, we need a lambda function which\n    returns true if the node visited is the same as =v.\n\n    We solve this by creating two copies of a node_label_use in the\n    grammar; the label use inside a tgrep_expr_labeled has a separate\n    parse action to the pred use inside a node_expr.  See\n    `_tgrep_node_label_use_action` and\n    `_tgrep_node_label_pred_use_action`.\n    '''\n    node_label = tokens[0]\n    reln_preds = tokens[1:]\n    def pattern_segment_pred(n, m=None, l=None):\n        '''This predicate function ignores its node argument.'''\n        if l is None or node_label not in l:\n            raise TgrepException('node_label ={0} not bound in pattern'.format(\n                node_label))\n        node = l[node_label]\n        return all(pred(node, m, l) for pred in reln_preds)\n    return pattern_segment_pred\n\ndef _tgrep_node_label_use_action(_s, _l, tokens):\n    '''\n    Returns the node label used to begin a tgrep_expr_labeled.  See\n    `_tgrep_segmented_pattern_action`.\n\n    Called for expressions like (`tgrep_node_label_use`)::\n\n        =s\n\n    when they appear as the first element of a `tgrep_expr_labeled`\n    expression (see `_tgrep_segmented_pattern_action`).\n\n    It returns the node label.\n    '''\n    assert len(tokens) == 1\n    assert tokens[0].startswith('=')\n    return tokens[0][1:]\n\ndef _tgrep_node_label_pred_use_action(_s, _l, tokens):\n    '''\n    Builds a lambda function representing a predicate on a tree node\n    which describes the use of a previously bound node label.\n\n    Called for expressions like (`tgrep_node_label_use_pred`)::\n\n        =s\n\n    when they appear inside a tgrep_node_expr (for example, inside a\n    relation).  The predicate returns true if and only if its node\n    argument is identical the the node looked up in the node label\n    dictionary using the node's label.\n    '''\n    assert len(tokens) == 1\n    assert tokens[0].startswith('=')\n    node_label = tokens[0][1:]\n    def node_label_use_pred(n, m=None, l=None):\n        if l is None or node_label not in l:\n            raise TgrepException('node_label ={0} not bound in pattern'.format(\n                node_label))\n        node = l[node_label]\n        return n is node\n    return node_label_use_pred\n\ndef _tgrep_bind_node_label_action(_s, _l, tokens):\n    '''\n    Builds a lambda function representing a predicate on a tree node\n    which can optionally bind a matching node into the tgrep2 string's\n    label_dict.\n\n    Called for expressions like (`tgrep_node_expr2`)::\n\n        /NP/\n        @NP=n\n    '''\n    if len(tokens) == 1:\n        return tokens[0]\n    else:\n        assert len(tokens) == 3\n        assert tokens[1] == '='\n        node_pred = tokens[0]\n        node_label = tokens[2]\n        def node_label_bind_pred(n, m=None, l=None):\n            if node_pred(n, m, l):\n                if l is None:\n                    raise TgrepException(\n                        'cannot bind node_label {0}: label_dict is None'.format(\n                            node_label))\n                l[node_label] = n\n                return True\n            else:\n                return False\n        return node_label_bind_pred\n\ndef _tgrep_rel_disjunction_action(_s, _l, tokens):\n    '''\n    Builds a lambda function representing a predicate on a tree node\n    from the disjunction of several other such lambda functions.\n    '''\n    tokens = [x for x in tokens if x != '|']\n    if len(tokens) == 1:\n        return tokens[0]\n    elif len(tokens) == 2:\n        return (lambda a, b: lambda n, m=None, l=None:\n                a(n, m, l) or b(n, m, l))(tokens[0], tokens[1])\n\ndef _macro_defn_action(_s, _l, tokens):\n    '''\n    Builds a dictionary structure which defines the given macro.\n    '''\n    assert len(tokens) == 3\n    assert tokens[0] == '@'\n    return {tokens[1]: tokens[2]}\n\ndef _tgrep_exprs_action(_s, _l, tokens):\n    '''\n    This is the top-lebel node in a tgrep2 search string; the\n    predicate function it returns binds together all the state of a\n    tgrep2 search string.\n\n    Builds a lambda function representing a predicate on a tree node\n    from the disjunction of several tgrep expressions.  Also handles\n    macro definitions and macro name binding, and node label\n    definitions and node label binding.\n    '''\n    if len(tokens) == 1:\n        return lambda n, m=None, l=None: tokens[0](n, None, {})\n    tokens = [x for x in tokens if x != ';']\n    macro_dict = {}\n    macro_defs = [tok for tok in tokens if isinstance(tok, dict)]\n    for macro_def in macro_defs:\n        macro_dict.update(macro_def)\n    tgrep_exprs = [tok for tok in tokens if not isinstance(tok, dict)]\n    def top_level_pred(n, m=macro_dict, l=None):\n        label_dict = {}\n        return any(predicate(n, m, label_dict) for predicate in tgrep_exprs)\n    return top_level_pred\n\ndef _build_tgrep_parser(set_parse_actions = True):\n    '''\n    Builds a pyparsing-based parser object for tokenizing and\n    interpreting tgrep search strings.\n    '''\n    tgrep_op = (pyparsing.Optional('!') +\n                pyparsing.Regex('[$%,.<>][%,.<>0-9-\\':]*'))\n    tgrep_qstring = pyparsing.QuotedString(quoteChar='\"', escChar='\\\\',\n                                           unquoteResults=False)\n    tgrep_node_regex = pyparsing.QuotedString(quoteChar='/', escChar='\\\\',\n                                              unquoteResults=False)\n    tgrep_qstring_icase = pyparsing.Regex(\n        'i@\\\\\"(?:[^\"\\\\n\\\\r\\\\\\\\]|(?:\\\\\\\\.))*\\\\\"')\n    tgrep_node_regex_icase = pyparsing.Regex(\n        'i@\\\\/(?:[^/\\\\n\\\\r\\\\\\\\]|(?:\\\\\\\\.))*\\\\/')\n    tgrep_node_literal = pyparsing.Regex('[^][ \\r\\t\\n;:.,&|<>()$!@%\\'^=]+')\n    tgrep_expr = pyparsing.Forward()\n    tgrep_relations = pyparsing.Forward()\n    tgrep_parens = pyparsing.Literal('(') + tgrep_expr + ')'\n    tgrep_nltk_tree_pos = (\n        pyparsing.Literal('N(') +\n        pyparsing.Optional(pyparsing.Word(pyparsing.nums) + ',' +\n                           pyparsing.Optional(pyparsing.delimitedList(\n                    pyparsing.Word(pyparsing.nums), delim=',') +\n                                              pyparsing.Optional(','))) + ')')\n    tgrep_node_label = pyparsing.Regex('[A-Za-z0-9]+')\n    tgrep_node_label_use = pyparsing.Combine('=' + tgrep_node_label)\n    tgrep_node_label_use_pred = tgrep_node_label_use.copy()\n    macro_name = pyparsing.Regex('[^];:.,&|<>()[$!@%\\'^=\\r\\t\\n ]+')\n    macro_name.setWhitespaceChars('')\n    macro_use = pyparsing.Combine('@' + macro_name)\n    tgrep_node_expr = (tgrep_node_label_use_pred |\n                       macro_use |\n                       tgrep_nltk_tree_pos |\n                       tgrep_qstring_icase |\n                       tgrep_node_regex_icase |\n                       tgrep_qstring |\n                       tgrep_node_regex |\n                       '*' |\n                       tgrep_node_literal)\n    tgrep_node_expr2 = ((tgrep_node_expr +\n                         pyparsing.Literal('=').setWhitespaceChars('') +\n                         tgrep_node_label.copy().setWhitespaceChars('')) |\n                        tgrep_node_expr)\n    tgrep_node = (tgrep_parens |\n                  (pyparsing.Optional(\"'\") +\n                   tgrep_node_expr2 +\n                   pyparsing.ZeroOrMore(\"|\" + tgrep_node_expr)))\n    tgrep_brackets = pyparsing.Optional('!') + '[' + tgrep_relations + ']'\n    tgrep_relation = tgrep_brackets | (tgrep_op + tgrep_node)\n    tgrep_rel_conjunction = pyparsing.Forward()\n    tgrep_rel_conjunction << (tgrep_relation +\n                              pyparsing.ZeroOrMore(pyparsing.Optional('&') +\n                                                   tgrep_rel_conjunction))\n    tgrep_relations << tgrep_rel_conjunction + pyparsing.ZeroOrMore(\n        \"|\" + tgrep_relations)\n    tgrep_expr << tgrep_node + pyparsing.Optional(tgrep_relations)\n    tgrep_expr_labeled = tgrep_node_label_use + pyparsing.Optional(tgrep_relations)\n    tgrep_expr2 = tgrep_expr + pyparsing.ZeroOrMore(':' + tgrep_expr_labeled)\n    macro_defn = (pyparsing.Literal('@') +\n                  pyparsing.White().suppress() +\n                  macro_name +\n                  tgrep_expr2)\n    tgrep_exprs = (pyparsing.Optional(macro_defn + pyparsing.ZeroOrMore(';' + macro_defn) + ';') +\n                   tgrep_expr2 +\n                   pyparsing.ZeroOrMore(';' + (macro_defn | tgrep_expr2)) +\n                   pyparsing.ZeroOrMore(';').suppress())\n    if set_parse_actions:\n        tgrep_node_label_use.setParseAction(_tgrep_node_label_use_action)\n        tgrep_node_label_use_pred.setParseAction(_tgrep_node_label_pred_use_action)\n        macro_use.setParseAction(_tgrep_macro_use_action)\n        tgrep_node.setParseAction(_tgrep_node_action)\n        tgrep_node_expr2.setParseAction(_tgrep_bind_node_label_action)\n        tgrep_parens.setParseAction(_tgrep_parens_action)\n        tgrep_nltk_tree_pos.setParseAction(_tgrep_nltk_tree_pos_action)\n        tgrep_relation.setParseAction(_tgrep_relation_action)\n        tgrep_rel_conjunction.setParseAction(_tgrep_conjunction_action)\n        tgrep_relations.setParseAction(_tgrep_rel_disjunction_action)\n        macro_defn.setParseAction(_macro_defn_action)\n        tgrep_expr.setParseAction(_tgrep_conjunction_action)\n        tgrep_expr_labeled.setParseAction(_tgrep_segmented_pattern_action)\n        tgrep_expr2.setParseAction(functools.partial(_tgrep_conjunction_action,\n                                                     join_char = ':'))\n        tgrep_exprs.setParseAction(_tgrep_exprs_action)\n    return tgrep_exprs.ignore('#' + pyparsing.restOfLine)\n\ndef tgrep_tokenize(tgrep_string):\n    '''\n    Tokenizes a TGrep search string into separate tokens.\n    '''\n    parser = _build_tgrep_parser(False)\n    if isinstance(tgrep_string, binary_type):\n        tgrep_string = tgrep_string.decode()\n    return list(parser.parseString(tgrep_string))\n\ndef tgrep_compile(tgrep_string):\n    '''\n    Parses (and tokenizes, if necessary) a TGrep search string into a\n    lambda function.\n    '''\n    parser = _build_tgrep_parser(True)\n    if isinstance(tgrep_string, binary_type):\n        tgrep_string = tgrep_string.decode()\n    return list(parser.parseString(tgrep_string, parseAll=True))[0]\n\ndef treepositions_no_leaves(tree):\n    '''\n    Returns all the tree positions in the given tree which are not\n    leaf nodes.\n    '''\n    treepositions = tree.treepositions()\n    prefixes = set()\n    for pos in treepositions:\n        for length in range(len(pos)):\n            prefixes.add(pos[:length])\n    return [pos for pos in treepositions if pos in prefixes]\n\ndef tgrep_positions(pattern, trees, search_leaves=True):\n\n    if isinstance(pattern, (binary_type, text_type)):\n        pattern = tgrep_compile(pattern)\n\n    for tree in trees:\n        try:\n            if search_leaves:\n                positions = tree.treepositions()\n            else:\n                positions = treepositions_no_leaves(tree)\n            yield [position for position in positions\n                      if pattern(tree[position])]\n        except AttributeError:\n            yield []\n\ndef tgrep_nodes(pattern, trees, search_leaves=True):\n\n    if isinstance(pattern, (binary_type, text_type)):\n        pattern = tgrep_compile(pattern)\n\n    for tree in trees:\n        try:\n            if search_leaves:\n                positions = tree.treepositions()\n            else:\n                positions = treepositions_no_leaves(tree)\n            yield [tree[position] for position in positions\n                      if pattern(tree[position])]\n        except AttributeError:\n            yield []\n"], "nltk\\tokenize\\api": [".py", "\n\nfrom abc import ABCMeta, abstractmethod\nfrom six import add_metaclass\n\nfrom nltk.internals import overridden\nfrom nltk.tokenize.util import string_span_tokenize\n\n\n@add_metaclass(ABCMeta)\nclass TokenizerI(object):\n    @abstractmethod\n    def tokenize(self, s):\n        if overridden(self.tokenize_sents):\n            return self.tokenize_sents([s])[0]\n\n    def span_tokenize(self, s):\n        raise NotImplementedError()\n\n    def tokenize_sents(self, strings):\n        return [self.tokenize(s) for s in strings]\n\n    def span_tokenize_sents(self, strings):\n        for s in strings:\n            yield list(self.span_tokenize(s))\n\n\nclass StringTokenizer(TokenizerI):\n\n    def tokenize(self, s):\n        return s.split(self._string)\n\n    def span_tokenize(self, s):\n        for span in string_span_tokenize(s, self._string):\n            yield span\n"], "nltk\\tokenize\\casual": [".py", "\n\n\n\n\n\nfrom __future__ import unicode_literals\nimport re\n\nfrom six import int2byte, unichr\nfrom six.moves import html_entities\n\n\n\nEMOTICONS = r\"\"\"\n    (?:\n      [<>]?\n      [:;=8]                     # eyes\n      [\\-o\\*\\']?                 # optional nose\n      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n      |\n      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n      [\\-o\\*\\']?                 # optional nose\n      [:;=8]                     # eyes\n      [<>]?\n      |\n      <3                         # heart\n    )\"\"\"\n\n\nURLS = r\"\"\"\t\t\t# Capture 1: entire matched URL\n  (?:\n  https?:\t\t\t\t# URL protocol and colon\n    (?:\n      /{1,3}\t\t\t\t# 1-3 slashes\n      |\t\t\t\t\t#   or\n      [a-z0-9%]\t\t\t\t# Single letter or digit or '%'\n    )\n    |\t\t\t\t\t#   or\n    [a-z0-9.\\-]+[.]\n    (?:[a-z]{2,13})\n    /\n  )\n  (?:\t\t\t\t\t# One or more:\n    [^\\s()<>{}\\[\\]]+\t\t\t# Run of non-space, non-()<>{}[]\n    |\t\t\t\t\t#   or\n    \\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\) # balanced parens, one level deep: (...(...)...)\n    |\n    \\([^\\s]+?\\)\t\t\t\t# balanced parens, non-recursive: (...)\n  )+\n  (?:\t\t\t\t\t# End with:\n    \\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\) # balanced parens, one level deep: (...(...)...)\n    |\n    \\([^\\s]+?\\)\t\t\t\t# balanced parens, non-recursive: (...)\n    |\t\t\t\t\t#   or\n    [^\\s`!()\\[\\]{};:'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]\t# not a space or one of these punct chars\n  )\n  |\t\t\t\t\t# OR, the following to match naked domains:\n  (?:\n  \t(?<!@)\t\t\t        # not preceded by a @, avoid matching foo@_gmail.com_\n    [a-z0-9]+\n    (?:[.\\-][a-z0-9]+)*\n    [.]\n    (?:[a-z]{2,13})\n    \\b\n    /?\n    (?!@)\t\t\t        # not succeeded by a @,\n  )\n\"\"\"\n\nREGEXPS = (\n    URLS,\n    r\"\"\"\n    (?:\n      (?:            # (international)\n        \\+?[01]\n        [ *\\-.\\)]*\n      )?\n      (?:            # (area code)\n        [\\(]?\n        \\d{3}\n        [ *\\-.\\)]*\n      )?\n      \\d{3}          # exchange\n      [ *\\-.\\)]*\n      \\d{4}          # base\n    )\"\"\"\n    ,\n    EMOTICONS\n    ,\n    r\"\"\"<[^>\\s]+>\"\"\"\n    ,\n    r\"\"\"[\\-]+>|<[\\-]+\"\"\"\n    ,\n    r\"\"\"(?:@[\\w_]+)\"\"\"\n    ,\n    r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n    ,\n    r\"\"\"[\\w.+-]+@[\\w-]+\\.(?:[\\w-]\\.?)+[\\w-]\"\"\"\n    ,\n    r\"\"\"\n    (?:[^\\W\\d_](?:[^\\W\\d_]|['\\-_])+[^\\W\\d_]) # Words with apostrophes or dashes.\n    |\n    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n    |\n    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n    |\n    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots.\n    |\n    (?:\\S)                         # Everything else that isn't whitespace.\n    \"\"\"\n    )\n\n\nWORD_RE = re.compile(r\"\"\"(%s)\"\"\" % \"|\".join(REGEXPS), re.VERBOSE | re.I\n                     | re.UNICODE)\n\nHANG_RE = re.compile(r'([^a-zA-Z0-9])\\1{3,}')\n\nEMOTICON_RE = re.compile(EMOTICONS, re.VERBOSE | re.I | re.UNICODE)\n\nENT_RE = re.compile(r'&(#?(x?))([^&;\\s]+);')\n\n\n\ndef _str_to_unicode(text, encoding=None, errors='strict'):\n    if encoding is None:\n        encoding = 'utf-8'\n    if isinstance(text, bytes):\n        return text.decode(encoding, errors)\n    return text\n\ndef _replace_html_entities(text, keep=(), remove_illegal=True, encoding='utf-8'):\n    \"\"\"\n    Remove entities from text by converting them to their\n    corresponding unicode character.\n\n    :param text: a unicode string or a byte string encoded in the given\n    `encoding` (which defaults to 'utf-8').\n\n    :param list keep:  list of entity names which should not be replaced.\\\n    This supports both numeric entities (``&#nnnn;`` and ``&#hhhh;``)\n    and named entities (such as ``&nbsp;`` or ``&gt;``).\n\n    :param bool remove_illegal: If `True`, entities that can't be converted are\\\n    removed. Otherwise, entities that can't be converted are kept \"as\n    is\".\n\n    :returns: A unicode string with the entities removed.\n\n    See https://github.com/scrapy/w3lib/blob/master/w3lib/html.py\n\n        >>> from nltk.tokenize.casual import _replace_html_entities\n        >>> _replace_html_entities(b'Price: &pound;100')\n        'Price: \\\\xa3100'\n        >>> print(_replace_html_entities(b'Price: &pound;100'))\n        Price: \u00a3100\n        >>>\n    \"\"\"\n\n    def _convert_entity(match):\n        entity_body = match.group(3)\n        if match.group(1):\n            try:\n                if match.group(2):\n                    number = int(entity_body, 16)\n                else:\n                    number = int(entity_body, 10)\n                if 0x80 <= number <= 0x9f:\n                    return int2byte(number).decode('cp1252')\n            except ValueError:\n                number = None\n        else:\n            if entity_body in keep:\n                return match.group(0)\n            else:\n                number = html_entities.name2codepoint.get(entity_body)\n        if number is not None:\n            try:\n                return unichr(number)\n            except ValueError:\n                pass\n\n        return \"\" if remove_illegal else match.group(0)\n\n    return ENT_RE.sub(_convert_entity, _str_to_unicode(text, encoding))\n\n\n\nclass TweetTokenizer:\n    r\"\"\"\n    Tokenizer for tweets.\n\n        >>> from nltk.tokenize import TweetTokenizer\n        >>> tknzr = TweetTokenizer()\n        >>> s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n        >>> tknzr.tokenize(s0)\n        ['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--']\n\n    Examples using `strip_handles` and `reduce_len parameters`:\n\n        >>> tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n        >>> s1 = '@remy: This is waaaaayyyy too much for you!!!!!!'\n        >>> tknzr.tokenize(s1)\n        [':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n    \"\"\"\n\n    def __init__(self, preserve_case=True, reduce_len=False, strip_handles=False):\n        self.preserve_case = preserve_case\n        self.reduce_len = reduce_len\n        self.strip_handles = strip_handles\n\n    def tokenize(self, text):\n        \"\"\"\n        :param text: str\n        :rtype: list(str)\n        :return: a tokenized list of strings; concatenating this list returns\\\n        the original string if `preserve_case=False`\n        \"\"\"\n        text = _replace_html_entities(text)\n        if self.strip_handles:\n            text = remove_handles(text)\n        if self.reduce_len:\n            text = reduce_lengthening(text)\n        safe_text = HANG_RE.sub(r'\\1\\1\\1', text)\n        words = WORD_RE.findall(safe_text)\n        if not self.preserve_case:\n            words = list(map((lambda x : x if EMOTICON_RE.search(x) else\n                              x.lower()), words))\n        return words\n\n\ndef reduce_lengthening(text):\n    \"\"\"\n    Replace repeated character sequences of length 3 or greater with sequences\n    of length 3.\n    \"\"\"\n    pattern = re.compile(r\"(.)\\1{2,}\")\n    return pattern.sub(r\"\\1\\1\\1\", text)\n\ndef remove_handles(text):\n    \"\"\"\n    Remove Twitter username handles from text.\n    \"\"\"\n    pattern = re.compile(r\"(?<![A-Za-z0-9_!@#\\$%&*])@(([A-Za-z0-9_]){20}(?!@))|(?<![A-Za-z0-9_!@#\\$%&*])@(([A-Za-z0-9_]){1,19})(?![A-Za-z0-9_]*@)\")\n    return pattern.sub(' ', text)\n\n\ndef casual_tokenize(text, preserve_case=True, reduce_len=False, strip_handles=False):\n    \"\"\"\n    Convenience function for wrapping the tokenizer.\n    \"\"\"\n    return TweetTokenizer(preserve_case=preserve_case, reduce_len=reduce_len,\n                          strip_handles=strip_handles).tokenize(text)\n\n"], "nltk\\tokenize\\mwe": [".py", "\nfrom nltk.util import Trie\n\nfrom nltk.tokenize.api import TokenizerI\n\n\nclass MWETokenizer(TokenizerI):\n\n    def __init__(self, mwes=None, separator='_'):\n        if not mwes:\n            mwes = []\n        self._mwes = Trie(mwes)\n        self._separator = separator\n\n    def add_mwe(self, mwe):\n        self._mwes.insert(mwe)\n\n    def tokenize(self, text):\n        i = 0\n        n = len(text)\n        result = []\n\n        while i < n:\n            if text[i] in self._mwes:\n                j = i\n                trie = self._mwes\n                while j < n and text[j] in trie:\n                    trie = trie[text[j]]\n                    j = j + 1\n                else:\n                    if Trie.LEAF in trie:\n                        result.append(self._separator.join(text[i:j]))\n                        i = j\n                    else:\n                        result.append(text[i])\n                        i += 1\n            else:\n                result.append(text[i])\n                i += 1\n\n        return result\n"], "nltk\\tokenize\\nist": [".py", "\n\nfrom __future__ import unicode_literals\n\nimport io\nimport re\nfrom six import text_type\n\nfrom nltk.corpus import perluniprops\nfrom nltk.tokenize.api import TokenizerI\nfrom nltk.tokenize.util import xml_unescape\n\n\nclass NISTTokenizer(TokenizerI):\n    STRIP_SKIP = re.compile('<skipped>'), ''\n    STRIP_EOL_HYPHEN = re.compile(u'\\u2028'), ' '\n    PUNCT = re.compile('([\\{-\\~\\[-\\` -\\&\\(-\\+\\:-\\@\\/])'), ' \\\\1 '\n    PERIOD_COMMA_PRECEED = re.compile('([^0-9])([\\.,])'), '\\\\1 \\\\2 '\n    PERIOD_COMMA_FOLLOW = re.compile('([\\.,])([^0-9])'), ' \\\\1 \\\\2'\n    DASH_PRECEED_DIGIT = re.compile('([0-9])(-)'), '\\\\1 \\\\2 '\n\n    LANG_DEPENDENT_REGEXES = [PUNCT, PERIOD_COMMA_PRECEED,\n                              PERIOD_COMMA_FOLLOW, DASH_PRECEED_DIGIT]\n\n    pup_number = text_type(''.join(set(perluniprops.chars('Number')))) # i.e. \\p{N}\n    pup_punct = text_type(''.join(set(perluniprops.chars('Punctuation')))) # i.e. \\p{P}\n    pup_symbol = text_type(''.join(set(perluniprops.chars('Symbol')))) # i.e. \\p{S}\n\n    number_regex = re.sub(r'[]^\\\\-]', r'\\\\\\g<0>', pup_number)\n    punct_regex = re.sub(r'[]^\\\\-]', r'\\\\\\g<0>', pup_punct)\n    symbol_regex = re.sub(r'[]^\\\\-]', r'\\\\\\g<0>', pup_symbol)\n\n\n    NONASCII = re.compile('([\\x00-\\x7f]+)'), r' \\1 '\n    PUNCT_1 = re.compile(u\"([{n}])([{p}])\".format(n=number_regex, p=punct_regex)), '\\\\1 \\\\2 '\n    PUNCT_2 = re.compile(u\"([{p}])([{n}])\".format(n=number_regex, p=punct_regex)), ' \\\\1 \\\\2'\n    SYMBOLS = re.compile(u\"([{s}])\".format(s=symbol_regex)), ' \\\\1 '\n\n    INTERNATIONAL_REGEXES = [NONASCII, PUNCT_1, PUNCT_2, SYMBOLS]\n\n    def lang_independent_sub(self, text):\n        regexp, substitution = self.STRIP_SKIP\n        text = regexp.sub(substitution, text)\n        text = xml_unescape(text)\n        regexp, substitution = self.STRIP_EOL_HYPHEN\n        text = regexp.sub(substitution, text)\n        return text\n\n    def tokenize(self, text, lowercase=False,\n                 western_lang=True, return_str=False):\n        text = text_type(text)\n        text = self.lang_independent_sub(text)\n        if western_lang:\n            text = ' ' + text + ' '\n            if lowercase:\n                text = text.lower()\n            for regexp, substitution in self.LANG_DEPENDENT_REGEXES:\n                text = regexp.sub(substitution, text)\n        text = ' '.join(text.split())\n        text = text_type(text.strip())\n        return text if return_str else text.split()\n\n    def international_tokenize(self, text, lowercase=False,\n                               split_non_ascii=True,\n                               return_str=False):\n        text = text_type(text)\n        regexp, substitution = self.STRIP_SKIP\n        text = regexp.sub(substitution, text)\n        regexp, substitution = self.STRIP_EOL_HYPHEN\n        text = regexp.sub(substitution, text)\n        text = xml_unescape(text)\n\n        if lowercase:\n            text = text.lower()\n\n        for regexp, substitution in self.INTERNATIONAL_REGEXES:\n            text = regexp.sub(substitution, text)\n\n        text = ' '.join(text.strip().split())\n        return text if return_str else text.split()\n"], "nltk\\tokenize\\punkt": [".py", "\nr\"\"\"\nPunkt Sentence Tokenizer\n\nThis tokenizer divides a text into a list of sentences\nby using an unsupervised algorithm to build a model for abbreviation\nwords, collocations, and words that start sentences.  It must be\ntrained on a large collection of plaintext in the target language\nbefore it can be used.\n\nThe NLTK data package includes a pre-trained Punkt tokenizer for\nEnglish.\n\n    >>> import nltk.data\n    >>> text = '''\n    ... Punkt knows that the periods in Mr. Smith and Johann S. Bach\n    ... do not mark sentence boundaries.  And sometimes sentences\n    ... can start with non-capitalized words.  i is a good variable\n    ... name.\n    ... '''\n    >>> sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n    >>> print('\\n-----\\n'.join(sent_detector.tokenize(text.strip())))\n    Punkt knows that the periods in Mr. Smith and Johann S. Bach\n    do not mark sentence boundaries.\n    -----\n    And sometimes sentences\n    can start with non-capitalized words.\n    -----\n    i is a good variable\n    name.\n\n(Note that whitespace from the original text, including newlines, is\nretained in the output.)\n\nPunctuation following sentences is also included by default\n(from NLTK 3.0 onwards). It can be excluded with the realign_boundaries\nflag.\n\n    >>> text = '''\n    ... (How does it deal with this parenthesis?)  \"It should be part of the\n    ... previous sentence.\" \"(And the same with this one.)\" ('And this one!')\n    ... \"('(And (this)) '?)\" [(and this. )]\n    ... '''\n    >>> print('\\n-----\\n'.join(\n    ...     sent_detector.tokenize(text.strip())))\n    (How does it deal with this parenthesis?)\n    -----\n    \"It should be part of the\n    previous sentence.\"\n    -----\n    \"(And the same with this one.)\"\n    -----\n    ('And this one!')\n    -----\n    \"('(And (this)) '?)\"\n    -----\n    [(and this. )]\n    >>> print('\\n-----\\n'.join(\n    ...     sent_detector.tokenize(text.strip(), realign_boundaries=False)))\n    (How does it deal with this parenthesis?\n    -----\n    )  \"It should be part of the\n    previous sentence.\n    -----\n    \" \"(And the same with this one.\n    -----\n    )\" ('And this one!\n    -----\n    ')\n    \"('(And (this)) '?\n    -----\n    )\" [(and this.\n    -----\n    )]\n\nHowever, Punkt is designed to learn parameters (a list of abbreviations, etc.)\nunsupervised from a corpus similar to the target domain. The pre-packaged models\nmay therefore be unsuitable: use ``PunktSentenceTokenizer(text)`` to learn\nparameters from the given text.\n\n:class:`.PunktTrainer` learns parameters such as a list of abbreviations\n(without supervision) from portions of text. Using a ``PunktTrainer`` directly\nallows for incremental training and modification of the hyper-parameters used\nto decide what is considered an abbreviation, etc.\n\nThe algorithm for this tokenizer is described in::\n\n  Kiss, Tibor and Strunk, Jan (2006): Unsupervised Multilingual Sentence\n    Boundary Detection.  Computational Linguistics 32: 485-525.\n\"\"\"\nfrom __future__ import print_function, unicode_literals, division\n\n\nimport re\nimport math\nfrom collections import defaultdict\n\nfrom six import string_types\n\nfrom nltk.compat import unicode_repr, python_2_unicode_compatible\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize.api import TokenizerI\n\n\n_ORTHO_BEG_UC = 1 << 1\n\n_ORTHO_MID_UC = 1 << 2\n\n_ORTHO_UNK_UC = 1 << 3\n\n_ORTHO_BEG_LC = 1 << 4\n\n_ORTHO_MID_LC = 1 << 5\n\n_ORTHO_UNK_LC = 1 << 6\n\n_ORTHO_UC = _ORTHO_BEG_UC + _ORTHO_MID_UC + _ORTHO_UNK_UC\n\n_ORTHO_LC = _ORTHO_BEG_LC + _ORTHO_MID_LC + _ORTHO_UNK_LC\n\n_ORTHO_MAP = {\n    ('initial', 'upper'): _ORTHO_BEG_UC,\n    ('internal', 'upper'): _ORTHO_MID_UC,\n    ('unknown', 'upper'): _ORTHO_UNK_UC,\n    ('initial', 'lower'): _ORTHO_BEG_LC,\n    ('internal', 'lower'): _ORTHO_MID_LC,\n    ('unknown', 'lower'): _ORTHO_UNK_LC,\n}\n\"\"\"A map from context position and first-letter case to the\nappropriate orthographic context flag.\"\"\"\n\n\n\nREASON_DEFAULT_DECISION = 'default decision'\nREASON_KNOWN_COLLOCATION = 'known collocation (both words)'\nREASON_ABBR_WITH_ORTHOGRAPHIC_HEURISTIC = 'abbreviation + orthographic heuristic'\nREASON_ABBR_WITH_SENTENCE_STARTER = 'abbreviation + frequent sentence starter'\nREASON_INITIAL_WITH_ORTHOGRAPHIC_HEURISTIC = 'initial + orthographic heuristic'\nREASON_NUMBER_WITH_ORTHOGRAPHIC_HEURISTIC = 'initial + orthographic heuristic'\nREASON_INITIAL_WITH_SPECIAL_ORTHOGRAPHIC_HEURISTIC = 'initial + special orthographic heuristic'\n\n\n\n\nclass PunktLanguageVars(object):\n    \"\"\"\n    Stores variables, mostly regular expressions, which may be\n    language-dependent for correct application of the algorithm.\n    An extension of this class may modify its properties to suit\n    a language other than English; an instance can then be passed\n    as an argument to PunktSentenceTokenizer and PunktTrainer\n    constructors.\n    \"\"\"\n\n    __slots__ = ('_re_period_context', '_re_word_tokenizer')\n\n    def __getstate__(self):\n        return 1\n\n    def __setstate__(self, state):\n        return 1\n\n    sent_end_chars = ('.', '?', '!')\n\n    @property\n    def _re_sent_end_chars(self):\n        return '[%s]' % re.escape(''.join(self.sent_end_chars))\n\n    internal_punctuation = ',:;'  # might want to extend this..\n    \"\"\"sentence internal punctuation, which indicates an abbreviation if\n    preceded by a period-final token.\"\"\"\n\n    re_boundary_realignment = re.compile(r'[\"\\')\\]}]+?(?:\\s+|(?=--)|$)',\n                                         re.MULTILINE)\n    \"\"\"Used to realign punctuation that should be included in a sentence\n    although it follows the period (or ?, !).\"\"\"\n\n    _re_word_start = r\"[^\\(\\\"\\`{\\[:;&\\#\\*@\\)}\\]\\-,]\"\n\n    _re_non_word_chars = r\"(?:[?!)\\\";}\\]\\*:@\\'\\({\\[])\"\n\n    _re_multi_char_punct = r\"(?:\\-{2,}|\\.{2,}|(?:\\.\\s){2,}\\.)\"\n\n    _word_tokenize_fmt = r'''(\n        %(MultiChar)s\n        |\n        (?=%(WordStart)s)\\S+?  # Accept word characters until end is found\n        (?= # Sequences marking a word's end\n            \\s|                                 # White-space\n            $|                                  # End-of-string\n            %(NonWord)s|%(MultiChar)s|          # Punctuation\n            ,(?=$|\\s|%(NonWord)s|%(MultiChar)s) # Comma if at end of word\n        )\n        |\n        \\S\n    )'''\n    \"\"\"Format of a regular expression to split punctuation from words,\n    excluding period.\"\"\"\n\n    def _word_tokenizer_re(self):\n        try:\n            return self._re_word_tokenizer\n        except AttributeError:\n            self._re_word_tokenizer = re.compile(\n                self._word_tokenize_fmt %\n                {\n                    'NonWord': self._re_non_word_chars,\n                    'MultiChar': self._re_multi_char_punct,\n                    'WordStart': self._re_word_start,\n                },\n                re.UNICODE | re.VERBOSE\n            )\n            return self._re_word_tokenizer\n\n    def word_tokenize(self, s):\n        return self._word_tokenizer_re().findall(s)\n\n    _period_context_fmt = r\"\"\"\n        \\S*                          # some word material\n        %(SentEndChars)s             # a potential sentence ending\n        (?=(?P<after_tok>\n            %(NonWord)s              # either other punctuation\n            |\n            \\s+(?P<next_tok>\\S+)     # or whitespace and some other token\n        ))\"\"\"\n    \"\"\"Format of a regular expression to find contexts including possible\n    sentence boundaries. Matches token which the possible sentence boundary\n    ends, and matches the following token within a lookahead expression.\"\"\"\n\n    def period_context_re(self):\n        \"\"\"Compiles and returns a regular expression to find contexts\n        including possible sentence boundaries.\"\"\"\n        try:\n            return self._re_period_context\n        except:\n            self._re_period_context = re.compile(\n                self._period_context_fmt %\n                {\n                    'NonWord': self._re_non_word_chars,\n                    'SentEndChars': self._re_sent_end_chars,\n                },\n                re.UNICODE | re.VERBOSE)\n            return self._re_period_context\n\n\n_re_non_punct = re.compile(r'[^\\W\\d]', re.UNICODE)\n\"\"\"Matches token types that are not merely punctuation. (Types for\nnumeric tokens are changed to ##number## and hence contain alpha.)\"\"\"\n\n\n\n\n\ndef _pair_iter(it):\n    \"\"\"\n    Yields pairs of tokens from the given iterator such that each input\n    token will appear as the first element in a yielded tuple. The last\n    pair will have None as its second element.\n    \"\"\"\n    it = iter(it)\n    prev = next(it)\n    for el in it:\n        yield (prev, el)\n        prev = el\n    yield (prev, None)\n\n\n\nclass PunktParameters(object):\n\n    def __init__(self):\n        self.abbrev_types = set()\n\n        self.collocations = set()\n        \"\"\"A set of word type tuples for known common collocations\n        where the first word ends in a period.  E.g., ('S.', 'Bach')\n        is a common collocation in a text that discusses 'Johann\n        S. Bach'.  These count as negative evidence for sentence\n        boundaries.\"\"\"\n\n        self.sent_starters = set()\n        \"\"\"A set of word types for words that often appear at the\n        beginning of sentences.\"\"\"\n\n        self.ortho_context = defaultdict(int)\n        \"\"\"A dictionary mapping word types to the set of orthographic\n        contexts that word type appears in.  Contexts are represented\n        by adding orthographic context flags: ...\"\"\"\n\n    def clear_abbrevs(self):\n        self.abbrev_types = set()\n\n    def clear_collocations(self):\n        self.collocations = set()\n\n    def clear_sent_starters(self):\n        self.sent_starters = set()\n\n    def clear_ortho_context(self):\n        self.ortho_context = defaultdict(int)\n\n    def add_ortho_context(self, typ, flag):\n        self.ortho_context[typ] |= flag\n\n    def _debug_ortho_context(self, typ):\n        c = self.ortho_context[typ]\n        if c & _ORTHO_BEG_UC:\n            yield 'BEG-UC'\n        if c & _ORTHO_MID_UC:\n            yield 'MID-UC'\n        if c & _ORTHO_UNK_UC:\n            yield 'UNK-UC'\n        if c & _ORTHO_BEG_LC:\n            yield 'BEG-LC'\n        if c & _ORTHO_MID_LC:\n            yield 'MID-LC'\n        if c & _ORTHO_UNK_LC:\n            yield 'UNK-LC'\n\n\n\n@python_2_unicode_compatible\nclass PunktToken(object):\n    \"\"\"Stores a token of text with annotations produced during\n    sentence boundary detection.\"\"\"\n\n    _properties = [\n        'parastart', 'linestart',\n        'sentbreak', 'abbr', 'ellipsis'\n    ]\n    __slots__ = ['tok', 'type', 'period_final'] + _properties\n\n    def __init__(self, tok, **params):\n        self.tok = tok\n        self.type = self._get_type(tok)\n        self.period_final = tok.endswith('.')\n\n        for p in self._properties:\n            setattr(self, p, None)\n        for k in params:\n            setattr(self, k, params[k])\n\n    _RE_ELLIPSIS = re.compile(r'\\.\\.+$')\n    _RE_NUMERIC = re.compile(r'^-?[\\.,]?\\d[\\d,\\.-]*\\.?$')\n    _RE_INITIAL = re.compile(r'[^\\W\\d]\\.$', re.UNICODE)\n    _RE_ALPHA = re.compile(r'[^\\W\\d]+$', re.UNICODE)\n\n\n    def _get_type(self, tok):\n        return self._RE_NUMERIC.sub('##number##', tok.lower())\n\n    @property\n    def type_no_period(self):\n        \"\"\"\n        The type with its final period removed if it has one.\n        \"\"\"\n        if len(self.type) > 1 and self.type[-1] == '.':\n            return self.type[:-1]\n        return self.type\n\n    @property\n    def type_no_sentperiod(self):\n        \"\"\"\n        The type with its final period removed if it is marked as a\n        sentence break.\n        \"\"\"\n        if self.sentbreak:\n            return self.type_no_period\n        return self.type\n\n    @property\n    def first_upper(self):\n        return self.tok[0].isupper()\n\n    @property\n    def first_lower(self):\n        return self.tok[0].islower()\n\n    @property\n    def first_case(self):\n        if self.first_lower:\n            return 'lower'\n        elif self.first_upper:\n            return 'upper'\n        return 'none'\n\n    @property\n    def is_ellipsis(self):\n        return self._RE_ELLIPSIS.match(self.tok)\n\n    @property\n    def is_number(self):\n        return self.type.startswith('##number##')\n\n    @property\n    def is_initial(self):\n        return self._RE_INITIAL.match(self.tok)\n\n    @property\n    def is_alpha(self):\n        return self._RE_ALPHA.match(self.tok)\n\n    @property\n    def is_non_punct(self):\n        return _re_non_punct.search(self.type)\n\n\n    def __repr__(self):\n        \"\"\"\n        A string representation of the token that can reproduce it\n        with eval(), which lists all the token's non-default\n        annotations.\n        \"\"\"\n        typestr = (' type=%s,' % unicode_repr(self.type)\n        if self.type != self.tok else '')\n\n        propvals = ', '.join(\n            '%s=%s' % (p, unicode_repr(getattr(self, p)))\n            for p in self._properties\n            if getattr(self, p)\n        )\n\n        return '%s(%s,%s %s)' % (self.__class__.__name__,\n                                 unicode_repr(self.tok), typestr, propvals)\n\n    def __str__(self):\n        \"\"\"\n        A string representation akin to that used by Kiss and Strunk.\n        \"\"\"\n        res = self.tok\n        if self.abbr:\n            res += '<A>'\n        if self.ellipsis:\n            res += '<E>'\n        if self.sentbreak:\n            res += '<S>'\n        return res\n\n\n\nclass PunktBaseClass(object):\n    \"\"\"\n    Includes common components of PunktTrainer and PunktSentenceTokenizer.\n    \"\"\"\n\n    def __init__(self, lang_vars=PunktLanguageVars(), token_cls=PunktToken,\n                 params=None):\n        if params is None:\n            params = PunktParameters()\n        self._params = params\n        self._lang_vars = lang_vars\n        self._Token = token_cls\n        \"\"\"The collection of parameters that determines the behavior\n        of the punkt tokenizer.\"\"\"\n\n\n    def _tokenize_words(self, plaintext):\n        \"\"\"\n        Divide the given text into tokens, using the punkt word\n        segmentation regular expression, and generate the resulting list\n        of tokens augmented as three-tuples with two boolean values for whether\n        the given token occurs at the start of a paragraph or a new line,\n        respectively.\n        \"\"\"\n        parastart = False\n        for line in plaintext.split('\\n'):\n            if line.strip():\n                line_toks = iter(self._lang_vars.word_tokenize(line))\n\n                yield self._Token(next(line_toks),\n                                  parastart=parastart, linestart=True)\n                parastart = False\n\n                for t in line_toks:\n                    yield self._Token(t)\n            else:\n                parastart = True\n\n\n    def _annotate_first_pass(self, tokens):\n        \"\"\"\n        Perform the first pass of annotation, which makes decisions\n        based purely based on the word type of each word:\n\n          - '?', '!', and '.' are marked as sentence breaks.\n          - sequences of two or more periods are marked as ellipsis.\n          - any word ending in '.' that's a known abbreviation is\n            marked as an abbreviation.\n          - any other word ending in '.' is marked as a sentence break.\n\n        Return these annotations as a tuple of three sets:\n\n          - sentbreak_toks: The indices of all sentence breaks.\n          - abbrev_toks: The indices of all abbreviations.\n          - ellipsis_toks: The indices of all ellipsis marks.\n        \"\"\"\n        for aug_tok in tokens:\n            self._first_pass_annotation(aug_tok)\n            yield aug_tok\n\n    def _first_pass_annotation(self, aug_tok):\n        \"\"\"\n        Performs type-based annotation on a single token.\n        \"\"\"\n\n        tok = aug_tok.tok\n\n        if tok in self._lang_vars.sent_end_chars:\n            aug_tok.sentbreak = True\n        elif aug_tok.is_ellipsis:\n            aug_tok.ellipsis = True\n        elif aug_tok.period_final and not tok.endswith('..'):\n            if (tok[:-1].lower() in self._params.abbrev_types or\n                    tok[:-1].lower().split('-')[-1] in self._params.abbrev_types):\n\n                aug_tok.abbr = True\n            else:\n                aug_tok.sentbreak = True\n\n        return\n\n\n\n\nclass PunktTrainer(PunktBaseClass):\n\n    def __init__(self, train_text=None, verbose=False,\n                 lang_vars=PunktLanguageVars(), token_cls=PunktToken):\n\n        PunktBaseClass.__init__(self, lang_vars=lang_vars,\n                                token_cls=token_cls)\n\n        self._type_fdist = FreqDist()\n        \"\"\"A frequency distribution giving the frequency of each\n        case-normalized token type in the training data.\"\"\"\n\n        self._num_period_toks = 0\n\n        self._collocation_fdist = FreqDist()\n        \"\"\"A frequency distribution giving the frequency of all\n        bigrams in the training data where the first word ends in a\n        period.  Bigrams are encoded as tuples of word types.\n        Especially common collocations are extracted from this\n        frequency distribution, and stored in\n        ``_params``.``collocations <PunktParameters.collocations>``.\"\"\"\n\n        self._sent_starter_fdist = FreqDist()\n        \"\"\"A frequency distribution giving the frequency of all words\n        that occur at the training data at the beginning of a sentence\n        (after the first pass of annotation).  Especially common\n        sentence starters are extracted from this frequency\n        distribution, and stored in ``_params.sent_starters``.\n        \"\"\"\n\n        self._sentbreak_count = 0\n        \"\"\"The total number of sentence breaks identified in training, used for\n        calculating the frequent sentence starter heuristic.\"\"\"\n\n        self._finalized = True\n        \"\"\"A flag as to whether the training has been finalized by finding\n        collocations and sentence starters, or whether finalize_training()\n        still needs to be called.\"\"\"\n\n        if train_text:\n            self.train(train_text, verbose, finalize=True)\n\n    def get_params(self):\n        \"\"\"\n        Calculates and returns parameters for sentence boundary detection as\n        derived from training.\"\"\"\n        if not self._finalized:\n            self.finalize_training()\n        return self._params\n\n\n    ABBREV = 0.3\n\n    IGNORE_ABBREV_PENALTY = False\n    \"\"\"allows the disabling of the abbreviation penalty heuristic, which\n    exponentially disadvantages words that are found at times without a\n    final period.\"\"\"\n\n    ABBREV_BACKOFF = 5\n\n    COLLOCATION = 7.88\n    \"\"\"minimal log-likelihood value that two tokens need to be considered\n    as a collocation\"\"\"\n\n    SENT_STARTER = 30\n    \"\"\"minimal log-likelihood value that a token requires to be considered\n    as a frequent sentence starter\"\"\"\n\n    INCLUDE_ALL_COLLOCS = False\n    \"\"\"this includes as potential collocations all word pairs where the first\n    word ends in a period. It may be useful in corpora where there is a lot\n    of variation that makes abbreviations like Mr difficult to identify.\"\"\"\n\n    INCLUDE_ABBREV_COLLOCS = False\n    \"\"\"this includes as potential collocations all word pairs where the first\n    word is an abbreviation. Such collocations override the orthographic\n    heuristic, but not the sentence starter heuristic. This is overridden by\n    INCLUDE_ALL_COLLOCS, and if both are false, only collocations with initials\n    and ordinals are considered.\"\"\"\n\n    MIN_COLLOC_FREQ = 1\n    \"\"\"this sets a minimum bound on the number of times a bigram needs to\n    appear before it can be considered a collocation, in addition to log\n    likelihood statistics. This is useful when INCLUDE_ALL_COLLOCS is True.\"\"\"\n\n\n    def train(self, text, verbose=False, finalize=True):\n        \"\"\"\n        Collects training data from a given text. If finalize is True, it\n        will determine all the parameters for sentence boundary detection. If\n        not, this will be delayed until get_params() or finalize_training() is\n        called. If verbose is True, abbreviations found will be listed.\n        \"\"\"\n        self._train_tokens(self._tokenize_words(text), verbose)\n        if finalize:\n            self.finalize_training(verbose)\n\n    def train_tokens(self, tokens, verbose=False, finalize=True):\n        \"\"\"\n        Collects training data from a given list of tokens.\n        \"\"\"\n        self._train_tokens((self._Token(t) for t in tokens), verbose)\n        if finalize:\n            self.finalize_training(verbose)\n\n    def _train_tokens(self, tokens, verbose):\n        self._finalized = False\n\n        tokens = list(tokens)\n\n        for aug_tok in tokens:\n            self._type_fdist[aug_tok.type] += 1\n            if aug_tok.period_final:\n                self._num_period_toks += 1\n\n        unique_types = self._unique_types(tokens)\n        for abbr, score, is_add in self._reclassify_abbrev_types(unique_types):\n            if score >= self.ABBREV:\n                if is_add:\n                    self._params.abbrev_types.add(abbr)\n                    if verbose:\n                        print(('  Abbreviation: [%6.4f] %s' %\n                               (score, abbr)))\n            else:\n                if not is_add:\n                    self._params.abbrev_types.remove(abbr)\n                    if verbose:\n                        print(('  Removed abbreviation: [%6.4f] %s' %\n                               (score, abbr)))\n\n        tokens = list(self._annotate_first_pass(tokens))\n\n        self._get_orthography_data(tokens)\n\n        self._sentbreak_count += self._get_sentbreak_count(tokens)\n\n        for aug_tok1, aug_tok2 in _pair_iter(tokens):\n            if not aug_tok1.period_final or not aug_tok2:\n                continue\n\n            if self._is_rare_abbrev_type(aug_tok1, aug_tok2):\n                self._params.abbrev_types.add(aug_tok1.type_no_period)\n                if verbose:\n                    print(('  Rare Abbrev: %s' % aug_tok1.type))\n\n            if self._is_potential_sent_starter(aug_tok2, aug_tok1):\n                self._sent_starter_fdist[aug_tok2.type] += 1\n\n            if self._is_potential_collocation(aug_tok1, aug_tok2):\n                self._collocation_fdist[\n                    (aug_tok1.type_no_period, aug_tok2.type_no_sentperiod)] += 1\n\n    def _unique_types(self, tokens):\n        return set(aug_tok.type for aug_tok in tokens)\n\n    def finalize_training(self, verbose=False):\n        \"\"\"\n        Uses data that has been gathered in training to determine likely\n        collocations and sentence starters.\n        \"\"\"\n        self._params.clear_sent_starters()\n        for typ, ll in self._find_sent_starters():\n            self._params.sent_starters.add(typ)\n            if verbose:\n                print(('  Sent Starter: [%6.4f] %r' % (ll, typ)))\n\n        self._params.clear_collocations()\n        for (typ1, typ2), ll in self._find_collocations():\n            self._params.collocations.add((typ1, typ2))\n            if verbose:\n                print(('  Collocation: [%6.4f] %r+%r' %\n                       (ll, typ1, typ2)))\n\n        self._finalized = True\n\n\n    def freq_threshold(self, ortho_thresh=2, type_thresh=2, colloc_thres=2,\n                       sentstart_thresh=2):\n        \"\"\"\n        Allows memory use to be reduced after much training by removing data\n        about rare tokens that are unlikely to have a statistical effect with\n        further training. Entries occurring above the given thresholds will be\n        retained.\n        \"\"\"\n        if ortho_thresh > 1:\n            old_oc = self._params.ortho_context\n            self._params.clear_ortho_context()\n            for tok in self._type_fdist:\n                count = self._type_fdist[tok]\n                if count >= ortho_thresh:\n                    self._params.ortho_context[tok] = old_oc[tok]\n\n        self._type_fdist = self._freq_threshold(self._type_fdist, type_thresh)\n        self._collocation_fdist = self._freq_threshold(\n            self._collocation_fdist, colloc_thres)\n        self._sent_starter_fdist = self._freq_threshold(\n            self._sent_starter_fdist, sentstart_thresh)\n\n    def _freq_threshold(self, fdist, threshold):\n        \"\"\"\n        Returns a FreqDist containing only data with counts below a given\n        threshold, as well as a mapping (None -> count_removed).\n        \"\"\"\n        res = FreqDist()\n        num_removed = 0\n        for tok in fdist:\n            count = fdist[tok]\n            if count < threshold:\n                num_removed += 1\n            else:\n                res[tok] += count\n        res[None] += num_removed\n        return res\n\n\n    def _get_orthography_data(self, tokens):\n        \"\"\"\n        Collect information about whether each token type occurs\n        with different case patterns (i) overall, (ii) at\n        sentence-initial positions, and (iii) at sentence-internal\n        positions.\n        \"\"\"\n        context = 'internal'\n        tokens = list(tokens)\n\n        for aug_tok in tokens:\n            if aug_tok.parastart and context != 'unknown':\n                context = 'initial'\n\n            if aug_tok.linestart and context == 'internal':\n                context = 'unknown'\n\n            typ = aug_tok.type_no_sentperiod\n\n            flag = _ORTHO_MAP.get((context, aug_tok.first_case), 0)\n            if flag:\n                self._params.add_ortho_context(typ, flag)\n\n            if aug_tok.sentbreak:\n                if not (aug_tok.is_number or aug_tok.is_initial):\n                    context = 'initial'\n                else:\n                    context = 'unknown'\n            elif aug_tok.ellipsis or aug_tok.abbr:\n                context = 'unknown'\n            else:\n                context = 'internal'\n\n\n    def _reclassify_abbrev_types(self, types):\n        \"\"\"\n        (Re)classifies each given token if\n          - it is period-final and not a known abbreviation; or\n          - it is not period-final and is otherwise a known abbreviation\n        by checking whether its previous classification still holds according\n        to the heuristics of section 3.\n        Yields triples (abbr, score, is_add) where abbr is the type in question,\n        score is its log-likelihood with penalties applied, and is_add specifies\n        whether the present type is a candidate for inclusion or exclusion as an\n        abbreviation, such that:\n          - (is_add and score >= 0.3)    suggests a new abbreviation; and\n          - (not is_add and score < 0.3) suggests excluding an abbreviation.\n        \"\"\"\n\n        for typ in types:\n            if not _re_non_punct.search(typ) or typ == '##number##':\n                continue\n\n            if typ.endswith('.'):\n                if typ in self._params.abbrev_types:\n                    continue\n                typ = typ[:-1]\n                is_add = True\n            else:\n                if typ not in self._params.abbrev_types:\n                    continue\n                is_add = False\n\n            num_periods = typ.count('.') + 1\n            num_nonperiods = len(typ) - num_periods + 1\n\n            count_with_period = self._type_fdist[typ + '.']\n            count_without_period = self._type_fdist[typ]\n            ll = self._dunning_log_likelihood(\n                count_with_period + count_without_period,\n                self._num_period_toks, count_with_period,\n                self._type_fdist.N())\n\n            f_length = math.exp(-num_nonperiods)\n            f_periods = num_periods\n            f_penalty = (int(self.IGNORE_ABBREV_PENALTY)\n                         or math.pow(num_nonperiods, -count_without_period))\n            score = ll * f_length * f_periods * f_penalty\n\n            yield typ, score, is_add\n\n    def find_abbrev_types(self):\n        \"\"\"\n        Recalculates abbreviations given type frequencies, despite no prior\n        determination of abbreviations.\n        This fails to include abbreviations otherwise found as \"rare\".\n        \"\"\"\n        self._params.clear_abbrevs()\n        tokens = (typ for typ in self._type_fdist if typ and typ.endswith('.'))\n        for abbr, score, is_add in self._reclassify_abbrev_types(tokens):\n            if score >= self.ABBREV:\n                self._params.abbrev_types.add(abbr)\n\n    def _is_rare_abbrev_type(self, cur_tok, next_tok):\n        \"\"\"\n        A word type is counted as a rare abbreviation if...\n          - it's not already marked as an abbreviation\n          - it occurs fewer than ABBREV_BACKOFF times\n          - either it is followed by a sentence-internal punctuation\n            mark, *or* it is followed by a lower-case word that\n            sometimes appears with upper case, but never occurs with\n            lower case at the beginning of sentences.\n        \"\"\"\n        if cur_tok.abbr or not cur_tok.sentbreak:\n            return False\n\n        typ = cur_tok.type_no_sentperiod\n\n        count = self._type_fdist[typ] + self._type_fdist[typ[:-1]]\n        if (typ in self._params.abbrev_types or count >= self.ABBREV_BACKOFF):\n            return False\n\n        if next_tok.tok[:1] in self._lang_vars.internal_punctuation:\n            return True\n\n        elif next_tok.first_lower:\n            typ2 = next_tok.type_no_sentperiod\n            typ2ortho_context = self._params.ortho_context[typ2]\n            if ((typ2ortho_context & _ORTHO_BEG_UC) and\n                    not (typ2ortho_context & _ORTHO_MID_UC)):\n                return True\n\n\n    @staticmethod\n    def _dunning_log_likelihood(count_a, count_b, count_ab, N):\n        \"\"\"\n        A function that calculates the modified Dunning log-likelihood\n        ratio scores for abbreviation candidates.  The details of how\n        this works is available in the paper.\n        \"\"\"\n        p1 = count_b / N\n        p2 = 0.99\n\n        null_hypo = (count_ab * math.log(p1) +\n                     (count_a - count_ab) * math.log(1.0 - p1))\n        alt_hypo = (count_ab * math.log(p2) +\n                    (count_a - count_ab) * math.log(1.0 - p2))\n\n        likelihood = null_hypo - alt_hypo\n\n        return (-2.0 * likelihood)\n\n    @staticmethod\n    def _col_log_likelihood(count_a, count_b, count_ab, N):\n        \"\"\"\n        A function that will just compute log-likelihood estimate, in\n        the original paper it's described in algorithm 6 and 7.\n\n        This *should* be the original Dunning log-likelihood values,\n        unlike the previous log_l function where it used modified\n        Dunning log-likelihood values\n        \"\"\"\n        p = count_b / N\n        p1 = count_ab / count_a\n        try:\n            p2 = (count_b - count_ab) / (N - count_a)\n        except ZeroDivisionError as e:\n            p2 = 1\n\n        try:\n            summand1 = (count_ab * math.log(p) +\n                        (count_a - count_ab) * math.log(1.0 - p))\n        except ValueError as e:\n            summand1 = 0\n\n        try:\n            summand2 = ((count_b - count_ab) * math.log(p) +\n                        (N - count_a - count_b + count_ab) * math.log(1.0 - p))\n        except ValueError as e:\n            summand2 = 0\n\n        if count_a == count_ab or p1 <= 0 or p1 >= 1:\n            summand3 = 0\n        else:\n            summand3 = (count_ab * math.log(p1) +\n                        (count_a - count_ab) * math.log(1.0 - p1))\n\n        if count_b == count_ab or p2 <= 0 or p2 >= 1:\n            summand4 = 0\n        else:\n            summand4 = ((count_b - count_ab) * math.log(p2) +\n                        (N - count_a - count_b + count_ab) * math.log(1.0 - p2))\n\n        likelihood = summand1 + summand2 - summand3 - summand4\n\n        return (-2.0 * likelihood)\n\n\n    def _is_potential_collocation(self, aug_tok1, aug_tok2):\n        \"\"\"\n        Returns True if the pair of tokens may form a collocation given\n        log-likelihood statistics.\n        \"\"\"\n        return ((self.INCLUDE_ALL_COLLOCS or\n                 (self.INCLUDE_ABBREV_COLLOCS and aug_tok1.abbr) or\n                 (aug_tok1.sentbreak and\n                  (aug_tok1.is_number or aug_tok1.is_initial)))\n                and aug_tok1.is_non_punct\n                and aug_tok2.is_non_punct)\n\n    def _find_collocations(self):\n        \"\"\"\n        Generates likely collocations and their log-likelihood.\n        \"\"\"\n        for types in self._collocation_fdist:\n            try:\n                typ1, typ2 = types\n            except TypeError:\n                continue\n            if typ2 in self._params.sent_starters:\n                continue\n\n            col_count = self._collocation_fdist[types]\n            typ1_count = self._type_fdist[typ1] + self._type_fdist[typ1 + '.']\n            typ2_count = self._type_fdist[typ2] + self._type_fdist[typ2 + '.']\n            if (typ1_count > 1 and typ2_count > 1\n                    and self.MIN_COLLOC_FREQ <\n                    col_count <= min(typ1_count, typ2_count)):\n\n                ll = self._col_log_likelihood(typ1_count, typ2_count,\n                                              col_count, self._type_fdist.N())\n                if (ll >= self.COLLOCATION and\n                        (self._type_fdist.N() / typ1_count >\n                         typ2_count / col_count)):\n                    yield (typ1, typ2), ll\n\n\n    def _is_potential_sent_starter(self, cur_tok, prev_tok):\n        \"\"\"\n        Returns True given a token and the token that preceds it if it\n        seems clear that the token is beginning a sentence.\n        \"\"\"\n        return (prev_tok.sentbreak and\n                not (prev_tok.is_number or prev_tok.is_initial) and\n                cur_tok.is_alpha)\n\n    def _find_sent_starters(self):\n        \"\"\"\n        Uses collocation heuristics for each candidate token to\n        determine if it frequently starts sentences.\n        \"\"\"\n        for typ in self._sent_starter_fdist:\n            if not typ:\n                continue\n\n            typ_at_break_count = self._sent_starter_fdist[typ]\n            typ_count = self._type_fdist[typ] + self._type_fdist[typ + '.']\n            if typ_count < typ_at_break_count:\n                continue\n\n            ll = self._col_log_likelihood(self._sentbreak_count, typ_count,\n                                          typ_at_break_count,\n                                          self._type_fdist.N())\n\n            if (ll >= self.SENT_STARTER and\n                    self._type_fdist.N() / self._sentbreak_count >\n                    typ_count / typ_at_break_count):\n                yield typ, ll\n\n    def _get_sentbreak_count(self, tokens):\n        \"\"\"\n        Returns the number of sentence breaks marked in a given set of\n        augmented tokens.\n        \"\"\"\n        return sum(1 for aug_tok in tokens if aug_tok.sentbreak)\n\n\n\n\nclass PunktSentenceTokenizer(PunktBaseClass, TokenizerI):\n    \"\"\"\n    A sentence tokenizer which uses an unsupervised algorithm to build\n    a model for abbreviation words, collocations, and words that start\n    sentences; and then uses that model to find sentence boundaries.\n    This approach has been shown to work well for many European\n    languages.\n    \"\"\"\n\n    def __init__(self, train_text=None, verbose=False,\n                 lang_vars=PunktLanguageVars(), token_cls=PunktToken):\n        \"\"\"\n        train_text can either be the sole training text for this sentence\n        boundary detector, or can be a PunktParameters object.\n        \"\"\"\n        PunktBaseClass.__init__(self, lang_vars=lang_vars,\n                                token_cls=token_cls)\n\n        if train_text:\n            self._params = self.train(train_text, verbose)\n\n    def train(self, train_text, verbose=False):\n        \"\"\"\n        Derives parameters from a given training text, or uses the parameters\n        given. Repeated calls to this method destroy previous parameters. For\n        incremental training, instantiate a separate PunktTrainer instance.\n        \"\"\"\n        if not isinstance(train_text, string_types):\n            return train_text\n        return PunktTrainer(train_text, lang_vars=self._lang_vars,\n                            token_cls=self._Token).get_params()\n\n\n    def tokenize(self, text, realign_boundaries=True):\n        \"\"\"\n        Given a text, returns a list of the sentences in that text.\n        \"\"\"\n        return list(self.sentences_from_text(text, realign_boundaries))\n\n    def debug_decisions(self, text):\n        \"\"\"\n        Classifies candidate periods as sentence breaks, yielding a dict for\n        each that may be used to understand why the decision was made.\n\n        See format_debug_decision() to help make this output readable.\n        \"\"\"\n\n        for match in self._lang_vars.period_context_re().finditer(text):\n            decision_text = match.group() + match.group('after_tok')\n            tokens = self._tokenize_words(decision_text)\n            tokens = list(self._annotate_first_pass(tokens))\n            while not tokens[0].period_final:\n                tokens.pop(0)\n            yield dict(period_index=match.end() - 1,\n                       text=decision_text,\n                       type1=tokens[0].type,\n                       type2=tokens[1].type,\n                       type1_in_abbrs=bool(tokens[0].abbr),\n                       type1_is_initial=bool(tokens[0].is_initial),\n                       type2_is_sent_starter=tokens[1].type_no_sentperiod in self._params.sent_starters,\n                       type2_ortho_heuristic=self._ortho_heuristic(tokens[1]),\n                       type2_ortho_contexts=set(self._params._debug_ortho_context(tokens[1].type_no_sentperiod)),\n                       collocation=(tokens[0].type_no_sentperiod,\n                                    tokens[1].type_no_sentperiod) in self._params.collocations,\n\n                       reason=self._second_pass_annotation(tokens[0], tokens[1]) or REASON_DEFAULT_DECISION,\n                       break_decision=tokens[0].sentbreak,\n                       )\n\n    def span_tokenize(self, text, realign_boundaries=True):\n        \"\"\"\n        Given a text, generates (start, end) spans of sentences\n        in the text.\n        \"\"\"\n        slices = self._slices_from_text(text)\n        if realign_boundaries:\n            slices = self._realign_boundaries(text, slices)\n        for sl in slices:\n            yield (sl.start, sl.stop)\n\n    def sentences_from_text(self, text, realign_boundaries=True):\n        \"\"\"\n        Given a text, generates the sentences in that text by only\n        testing candidate sentence breaks. If realign_boundaries is\n        True, includes in the sentence closing punctuation that\n        follows the period.\n        \"\"\"\n        return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]\n\n    def _slices_from_text(self, text):\n        last_break = 0\n        for match in self._lang_vars.period_context_re().finditer(text):\n            context = match.group() + match.group('after_tok')\n            if self.text_contains_sentbreak(context):\n                yield slice(last_break, match.end())\n                if match.group('next_tok'):\n                    last_break = match.start('next_tok')\n                else:\n                    last_break = match.end()\n        yield slice(last_break, len(text.rstrip()))\n\n    def _realign_boundaries(self, text, slices):\n        \"\"\"\n        Attempts to realign punctuation that falls after the period but\n        should otherwise be included in the same sentence.\n\n        For example: \"(Sent1.) Sent2.\" will otherwise be split as::\n\n            [\"(Sent1.\", \") Sent1.\"].\n\n        This method will produce::\n\n            [\"(Sent1.)\", \"Sent2.\"].\n        \"\"\"\n        realign = 0\n        for sl1, sl2 in _pair_iter(slices):\n            sl1 = slice(sl1.start + realign, sl1.stop)\n            if not sl2:\n                if text[sl1]:\n                    yield sl1\n                continue\n\n            m = self._lang_vars.re_boundary_realignment.match(text[sl2])\n            if m:\n                yield slice(sl1.start, sl2.start + len(m.group(0).rstrip()))\n                realign = m.end()\n            else:\n                realign = 0\n                if text[sl1]:\n                    yield sl1\n\n    def text_contains_sentbreak(self, text):\n        \"\"\"\n        Returns True if the given text includes a sentence break.\n        \"\"\"\n        found = False  # used to ignore last token\n        for t in self._annotate_tokens(self._tokenize_words(text)):\n            if found:\n                return True\n            if t.sentbreak:\n                found = True\n        return False\n\n    def sentences_from_text_legacy(self, text):\n        \"\"\"\n        Given a text, generates the sentences in that text. Annotates all\n        tokens, rather than just those with possible sentence breaks. Should\n        produce the same results as ``sentences_from_text``.\n        \"\"\"\n        tokens = self._annotate_tokens(self._tokenize_words(text))\n        return self._build_sentence_list(text, tokens)\n\n    def sentences_from_tokens(self, tokens):\n        \"\"\"\n        Given a sequence of tokens, generates lists of tokens, each list\n        corresponding to a sentence.\n        \"\"\"\n        tokens = iter(self._annotate_tokens(self._Token(t) for t in tokens))\n        sentence = []\n        for aug_tok in tokens:\n            sentence.append(aug_tok.tok)\n            if aug_tok.sentbreak:\n                yield sentence\n                sentence = []\n        if sentence:\n            yield sentence\n\n    def _annotate_tokens(self, tokens):\n        \"\"\"\n        Given a set of tokens augmented with markers for line-start and\n        paragraph-start, returns an iterator through those tokens with full\n        annotation including predicted sentence breaks.\n        \"\"\"\n        tokens = self._annotate_first_pass(tokens)\n\n        tokens = self._annotate_second_pass(tokens)\n\n\n        return tokens\n\n    def _build_sentence_list(self, text, tokens):\n        \"\"\"\n        Given the original text and the list of augmented word tokens,\n        construct and return a tokenized list of sentence strings.\n        \"\"\"\n\n        pos = 0\n\n        WS_REGEXP = re.compile(r'\\s*')\n\n        sentence = ''\n        for aug_tok in tokens:\n            tok = aug_tok.tok\n\n            ws = WS_REGEXP.match(text, pos).group()\n            pos += len(ws)\n\n            if text[pos:pos + len(tok)] != tok:\n                pat = '\\s*'.join(re.escape(c) for c in tok)\n                m = re.compile(pat).match(text, pos)\n                if m: tok = m.group()\n\n            assert text[pos:pos + len(tok)] == tok\n            pos += len(tok)\n\n            if sentence:\n                sentence += ws\n            sentence += tok\n\n            if aug_tok.sentbreak:\n                yield sentence\n                sentence = ''\n\n        if sentence:\n            yield sentence\n\n    def dump(self, tokens):\n        print('writing to /tmp/punkt.new...')\n        with open('/tmp/punkt.new', 'w') as outfile:\n            for aug_tok in tokens:\n                if aug_tok.parastart:\n                    outfile.write('\\n\\n')\n                elif aug_tok.linestart:\n                    outfile.write('\\n')\n                else:\n                    outfile.write(' ')\n\n                outfile.write(str(aug_tok))\n\n\n    PUNCTUATION = tuple(';:,.!?')\n\n\n    def _annotate_second_pass(self, tokens):\n        \"\"\"\n        Performs a token-based classification (section 4) over the given\n        tokens, making use of the orthographic heuristic (4.1.1), collocation\n        heuristic (4.1.2) and frequent sentence starter heuristic (4.1.3).\n        \"\"\"\n        for t1, t2 in _pair_iter(tokens):\n            self._second_pass_annotation(t1, t2)\n            yield t1\n\n    def _second_pass_annotation(self, aug_tok1, aug_tok2):\n        \"\"\"\n        Performs token-based classification over a pair of contiguous tokens\n        updating the first.\n        \"\"\"\n        if not aug_tok2:\n            return\n\n        tok = aug_tok1.tok\n        if not aug_tok1.period_final:\n            return\n\n        typ = aug_tok1.type_no_period\n        next_tok = aug_tok2.tok\n        next_typ = aug_tok2.type_no_sentperiod\n        tok_is_initial = aug_tok1.is_initial\n\n        if (typ, next_typ) in self._params.collocations:\n            aug_tok1.sentbreak = False\n            aug_tok1.abbr = True\n            return REASON_KNOWN_COLLOCATION\n\n        if ((aug_tok1.abbr or aug_tok1.ellipsis) and\n                (not tok_is_initial)):\n            is_sent_starter = self._ortho_heuristic(aug_tok2)\n            if is_sent_starter == True:\n                aug_tok1.sentbreak = True\n                return REASON_ABBR_WITH_ORTHOGRAPHIC_HEURISTIC\n\n            if (aug_tok2.first_upper and\n                    next_typ in self._params.sent_starters):\n                aug_tok1.sentbreak = True\n                return REASON_ABBR_WITH_SENTENCE_STARTER\n\n        if tok_is_initial or typ == '##number##':\n\n            is_sent_starter = self._ortho_heuristic(aug_tok2)\n\n            if is_sent_starter == False:\n                aug_tok1.sentbreak = False\n                aug_tok1.abbr = True\n                if tok_is_initial:\n                    return REASON_INITIAL_WITH_ORTHOGRAPHIC_HEURISTIC\n                else:\n                    return REASON_NUMBER_WITH_ORTHOGRAPHIC_HEURISTIC\n\n            if (is_sent_starter == 'unknown' and tok_is_initial and\n                    aug_tok2.first_upper and\n                    not (self._params.ortho_context[next_typ] & _ORTHO_LC)):\n                aug_tok1.sentbreak = False\n                aug_tok1.abbr = True\n                return REASON_INITIAL_WITH_SPECIAL_ORTHOGRAPHIC_HEURISTIC\n\n        return\n\n    def _ortho_heuristic(self, aug_tok):\n        \"\"\"\n        Decide whether the given token is the first token in a sentence.\n        \"\"\"\n        if aug_tok.tok in self.PUNCTUATION:\n            return False\n\n        ortho_context = self._params.ortho_context[aug_tok.type_no_sentperiod]\n\n        if (aug_tok.first_upper and\n                (ortho_context & _ORTHO_LC) and\n                not (ortho_context & _ORTHO_MID_UC)):\n            return True\n\n        if (aug_tok.first_lower and\n                ((ortho_context & _ORTHO_UC) or\n                 not (ortho_context & _ORTHO_BEG_LC))):\n            return False\n\n        return 'unknown'\n\n\nDEBUG_DECISION_FMT = '''Text: %(text)r (at offset %(period_index)d)\nSentence break? %(break_decision)s (%(reason)s)\nCollocation? %(collocation)s\n%(type1)r:\n    known abbreviation: %(type1_in_abbrs)s\n    is initial: %(type1_is_initial)s\n%(type2)r:\n    known sentence starter: %(type2_is_sent_starter)s\n    orthographic heuristic suggests is a sentence starter? %(type2_ortho_heuristic)s\n    orthographic contexts in training: %(type2_ortho_contexts)s\n'''\n\n\ndef format_debug_decision(d):\n    return DEBUG_DECISION_FMT % d\n\n\ndef demo(text, tok_cls=PunktSentenceTokenizer, train_cls=PunktTrainer):\n    cleanup = lambda s: re.compile(r'(?:\\r|^\\s+)', re.MULTILINE).sub('', s).replace('\\n', ' ')\n    trainer = train_cls()\n    trainer.INCLUDE_ALL_COLLOCS = True\n    trainer.train(text)\n    sbd = tok_cls(trainer.get_params())\n    for l in sbd.sentences_from_text(text):\n        print(cleanup(l))\n"], "nltk\\tokenize\\regexp": [".py", "\nr\"\"\"\nRegular-Expression Tokenizers\n\nA ``RegexpTokenizer`` splits a string into substrings using a regular expression.\nFor example, the following tokenizer forms tokens out of alphabetic sequences,\nmoney expressions, and any other non-whitespace sequences:\n\n    >>> from nltk.tokenize import RegexpTokenizer\n    >>> s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n    >>> tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n    >>> tokenizer.tokenize(s)\n    ['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '.',\n    'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n\nA ``RegexpTokenizer`` can use its regexp to match delimiters instead:\n\n    >>> tokenizer = RegexpTokenizer('\\s+', gaps=True)\n    >>> tokenizer.tokenize(s)\n    ['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.',\n    'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']\n\nNote that empty tokens are not returned when the delimiter appears at\nthe start or end of the string.\n\nThe material between the tokens is discarded.  For example,\nthe following tokenizer selects just the capitalized words:\n\n    >>> capword_tokenizer = RegexpTokenizer('[A-Z]\\w+')\n    >>> capword_tokenizer.tokenize(s)\n    ['Good', 'New', 'York', 'Please', 'Thanks']\n\nThis module contains several subclasses of ``RegexpTokenizer``\nthat use pre-defined regular expressions.\n\n    >>> from nltk.tokenize import BlanklineTokenizer\n    >>> # Uses '\\s*\\n\\s*\\n\\s*':\n    >>> BlanklineTokenizer().tokenize(s)\n    ['Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.',\n    'Thanks.']\n\nAll of the regular expression tokenizers are also available as functions:\n\n    >>> from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize\n    >>> regexp_tokenize(s, pattern='\\w+|\\$[\\d\\.]+|\\S+')\n    ['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '.',\n    'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n    >>> wordpunct_tokenize(s)\n    ['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York',\n     '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n    >>> blankline_tokenize(s)\n    ['Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.', 'Thanks.']\n\nCaution: The function ``regexp_tokenize()`` takes the text as its\nfirst argument, and the regular expression pattern as its second\nargument.  This differs from the conventions used by Python's\n``re`` functions, where the pattern is always the first argument.\n(This is for consistency with the other NLTK tokenizers.)\n\"\"\"\nfrom __future__ import unicode_literals\n\nimport re\n\nfrom nltk.tokenize.api import TokenizerI\nfrom nltk.tokenize.util import regexp_span_tokenize\nfrom nltk.compat import python_2_unicode_compatible\n\n\n@python_2_unicode_compatible\nclass RegexpTokenizer(TokenizerI):\n    \"\"\"\n    A tokenizer that splits a string using a regular expression, which\n    matches either the tokens or the separators between tokens.\n\n        >>> tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n\n    :type pattern: str\n    :param pattern: The pattern used to build this tokenizer.\n        (This pattern must not contain capturing parentheses;\n        Use non-capturing parentheses, e.g. (?:...), instead)\n    :type gaps: bool\n    :param gaps: True if this tokenizer's pattern should be used\n        to find separators between tokens; False if this\n        tokenizer's pattern should be used to find the tokens\n        themselves.\n    :type discard_empty: bool\n    :param discard_empty: True if any empty tokens `''`\n        generated by the tokenizer should be discarded.  Empty\n        tokens can only be generated if `_gaps == True`.\n    :type flags: int\n    :param flags: The regexp flags used to compile this\n        tokenizer's pattern.  By default, the following flags are\n        used: `re.UNICODE | re.MULTILINE | re.DOTALL`.\n\n    \"\"\"\n\n    def __init__(self, pattern, gaps=False, discard_empty=True,\n                 flags=re.UNICODE | re.MULTILINE | re.DOTALL):\n        pattern = getattr(pattern, 'pattern', pattern)\n\n        self._pattern = pattern\n        self._gaps = gaps\n        self._discard_empty = discard_empty\n        self._flags = flags\n        self._regexp = None\n\n    def _check_regexp(self):\n        if self._regexp is None:\n            self._regexp = re.compile(self._pattern, self._flags)\n\n    def tokenize(self, text):\n        self._check_regexp()\n        if self._gaps:\n            if self._discard_empty:\n                return [tok for tok in self._regexp.split(text) if tok]\n            else:\n                return self._regexp.split(text)\n\n        else:\n            return self._regexp.findall(text)\n\n    def span_tokenize(self, text):\n        self._check_regexp()\n\n        if self._gaps:\n            for left, right in regexp_span_tokenize(text, self._regexp):\n                if not (self._discard_empty and left == right):\n                    yield left, right\n        else:\n            for m in re.finditer(self._regexp, text):\n                yield m.span()\n\n    def __repr__(self):\n        return ('%s(pattern=%r, gaps=%r, discard_empty=%r, flags=%r)' %\n                (self.__class__.__name__, self._pattern, self._gaps,\n                 self._discard_empty, self._flags))\n\n\nclass WhitespaceTokenizer(RegexpTokenizer):\n    r\"\"\"\n    Tokenize a string on whitespace (space, tab, newline).\n    In general, users should use the string ``split()`` method instead.\n\n        >>> from nltk.tokenize import WhitespaceTokenizer\n        >>> s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n        >>> WhitespaceTokenizer().tokenize(s)\n        ['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.',\n        'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']\n    \"\"\"\n\n    def __init__(self):\n        RegexpTokenizer.__init__(self, r'\\s+', gaps=True)\n\n\nclass BlanklineTokenizer(RegexpTokenizer):\n    \"\"\"\n    Tokenize a string, treating any sequence of blank lines as a delimiter.\n    Blank lines are defined as lines containing no characters, except for\n    space or tab characters.\n    \"\"\"\n\n    def __init__(self):\n        RegexpTokenizer.__init__(self, r'\\s*\\n\\s*\\n\\s*', gaps=True)\n\n\nclass WordPunctTokenizer(RegexpTokenizer):\n    \"\"\"\n    Tokenize a text into a sequence of alphabetic and\n    non-alphabetic characters, using the regexp ``\\w+|[^\\w\\s]+``.\n\n        >>> from nltk.tokenize import WordPunctTokenizer\n        >>> s = \"Good muffins cost $3.88\\\\nin New York.  Please buy me\\\\ntwo of them.\\\\n\\\\nThanks.\"\n        >>> WordPunctTokenizer().tokenize(s)\n        ['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York',\n        '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n    \"\"\"\n\n    def __init__(self):\n        RegexpTokenizer.__init__(self, r'\\w+|[^\\w\\s]+')\n\n\n\ndef regexp_tokenize(text, pattern, gaps=False, discard_empty=True,\n                    flags=re.UNICODE | re.MULTILINE | re.DOTALL):\n    \"\"\"\n    Return a tokenized copy of *text*.  See :class:`.RegexpTokenizer`\n    for descriptions of the arguments.\n    \"\"\"\n    tokenizer = RegexpTokenizer(pattern, gaps, discard_empty, flags)\n    return tokenizer.tokenize(text)\n\n\nblankline_tokenize = BlanklineTokenizer().tokenize\nwordpunct_tokenize = WordPunctTokenizer().tokenize\n"], "nltk\\tokenize\\repp": [".py", "\nfrom __future__ import unicode_literals, print_function\nfrom six import text_type\n\nimport os\nimport re\nimport sys\nimport subprocess\nimport tempfile\n\n\nfrom nltk.data import ZipFilePathPointer\nfrom nltk.internals import find_dir\n\nfrom nltk.tokenize.api import TokenizerI\n\nclass ReppTokenizer(TokenizerI):\n    def __init__(self, repp_dir, encoding='utf8'):\n        self.repp_dir = self.find_repptokenizer(repp_dir)\n        self.working_dir = tempfile.gettempdir()\n        self.encoding = encoding\n        \n    def tokenize(self, sentence):\n        return next(self.tokenize_sents([sentence]))\n    \n    def tokenize_sents(self, sentences, keep_token_positions=False):\n        with tempfile.NamedTemporaryFile(prefix='repp_input.', \n            dir=self.working_dir, mode='w', delete=False) as input_file:\n            for sent in sentences:\n                input_file.write(text_type(sent) + '\\n')\n            input_file.close()\n            cmd =self.generate_repp_command(input_file.name)\n            repp_output = self._execute(cmd).decode(self.encoding).strip()\n            for tokenized_sent in self.parse_repp_outputs(repp_output):\n                if not keep_token_positions:\n                    tokenized_sent, starts, ends = zip(*tokenized_sent)\n                yield tokenized_sent      \n        \n    def generate_repp_command(self, inputfilename):\n        cmd = [self.repp_dir + '/src/repp']\n        cmd+= ['-c', self.repp_dir + '/erg/repp.set']\n        cmd+= ['--format', 'triple']\n        cmd+= [inputfilename]\n        return cmd  \n\n    @staticmethod\n    def _execute(cmd):\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = p.communicate()\n        return stdout\n    \n    @staticmethod    \n    def parse_repp_outputs(repp_output):\n        line_regex = re.compile('^\\((\\d+), (\\d+), (.+)\\)$', re.MULTILINE)\n        for section in repp_output.split('\\n\\n'):\n            words_with_positions = [(token, int(start), int(end))\n                                    for start, end, token in \n                                    line_regex.findall(section)]\n            words = tuple(t[2] for t in words_with_positions)\n            yield words_with_positions\n    \n    def find_repptokenizer(self, repp_dirname):\n        if os.path.exists(repp_dirname): # If a full path is given.\n            _repp_dir = repp_dirname\n        else: # Try to find path to REPP directory in environment variables.\n            _repp_dir = find_dir(repp_dirname, env_vars=('REPP_TOKENIZER',))\n        assert os.path.exists(_repp_dir+'/src/repp')\n        assert os.path.exists(_repp_dir+'/erg/repp.set')\n        return _repp_dir\n"], "nltk\\tokenize\\sexpr": [".py", "\n\nimport re\n\nfrom nltk.tokenize.api import TokenizerI\n\n\nclass SExprTokenizer(TokenizerI):\n\n    def __init__(self, parens='()', strict=True):\n        if len(parens) != 2:\n            raise ValueError('parens must contain exactly two strings')\n        self._strict = strict\n        self._open_paren = parens[0]\n        self._close_paren = parens[1]\n        self._paren_regexp = re.compile('%s|%s' % (re.escape(parens[0]),\n                                                   re.escape(parens[1])))\n\n    def tokenize(self, text):\n        result = []\n        pos = 0\n        depth = 0\n        for m in self._paren_regexp.finditer(text):\n            paren = m.group()\n            if depth == 0:\n                result += text[pos:m.start()].split()\n                pos = m.start()\n            if paren == self._open_paren:\n                depth += 1\n            if paren == self._close_paren:\n                if self._strict and depth == 0:\n                    raise ValueError('Un-matched close paren at char %d'\n                                     % m.start())\n                depth = max(0, depth - 1)\n                if depth == 0:\n                    result.append(text[pos:m.end()])\n                    pos = m.end()\n        if self._strict and depth > 0:\n            raise ValueError('Un-matched open paren at char %d' % pos)\n        if pos < len(text):\n            result.append(text[pos:])\n        return result\n\n\nsexpr_tokenize = SExprTokenizer().tokenize\n"], "nltk\\tokenize\\simple": [".py", "\nr\"\"\"\nSimple Tokenizers\n\nThese tokenizers divide strings into substrings using the string\n``split()`` method.\nWhen tokenizing using a particular delimiter string, use\nthe string ``split()`` method directly, as this is more efficient.\n\nThe simple tokenizers are *not* available as separate functions;\ninstead, you should just use the string ``split()`` method directly:\n\n    >>> s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n    >>> s.split()\n    ['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.',\n    'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']\n    >>> s.split(' ')\n    ['Good', 'muffins', 'cost', '$3.88\\nin', 'New', 'York.', '',\n    'Please', 'buy', 'me\\ntwo', 'of', 'them.\\n\\nThanks.']\n    >>> s.split('\\n')\n    ['Good muffins cost $3.88', 'in New York.  Please buy me',\n    'two of them.', '', 'Thanks.']\n\nThe simple tokenizers are mainly useful because they follow the\nstandard ``TokenizerI`` interface, and so can be used with any code\nthat expects a tokenizer.  For example, these tokenizers can be used\nto specify the tokenization conventions when building a `CorpusReader`.\n\n\"\"\"\nfrom __future__ import unicode_literals\nfrom nltk.tokenize.api import TokenizerI, StringTokenizer\nfrom nltk.tokenize.util import string_span_tokenize, regexp_span_tokenize\n\n\nclass SpaceTokenizer(StringTokenizer):\n    r\"\"\"Tokenize a string using the space character as a delimiter,\n    which is the same as ``s.split(' ')``.\n\n        >>> from nltk.tokenize import SpaceTokenizer\n        >>> s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n        >>> SpaceTokenizer().tokenize(s)\n        ['Good', 'muffins', 'cost', '$3.88\\nin', 'New', 'York.', '',\n        'Please', 'buy', 'me\\ntwo', 'of', 'them.\\n\\nThanks.']\n    \"\"\"\n\n    _string = ' '\n\n\nclass TabTokenizer(StringTokenizer):\n    r\"\"\"Tokenize a string use the tab character as a delimiter,\n    the same as ``s.split('\\t')``.\n\n        >>> from nltk.tokenize import TabTokenizer\n        >>> TabTokenizer().tokenize('a\\tb c\\n\\t d')\n        ['a', 'b c\\n', ' d']\n    \"\"\"\n\n    _string = '\\t'\n\n\nclass CharTokenizer(StringTokenizer):\n    \"\"\"Tokenize a string into individual characters.  If this functionality\n    is ever required directly, use ``for char in string``.\n    \"\"\"\n\n    def tokenize(self, s):\n        return list(s)\n\n    def span_tokenize(self, s):\n        for i, j in enumerate(range(1, len(s) + 1)):\n            yield i, j\n\n\nclass LineTokenizer(TokenizerI):\n    r\"\"\"Tokenize a string into its lines, optionally discarding blank lines.\n    This is similar to ``s.split('\\n')``.\n\n        >>> from nltk.tokenize import LineTokenizer\n        >>> s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n        >>> LineTokenizer(blanklines='keep').tokenize(s)\n        ['Good muffins cost $3.88', 'in New York.  Please buy me',\n        'two of them.', '', 'Thanks.']\n        >>> # same as [l for l in s.split('\\n') if l.strip()]:\n        >>> LineTokenizer(blanklines='discard').tokenize(s)\n        ['Good muffins cost $3.88', 'in New York.  Please buy me',\n        'two of them.', 'Thanks.']\n\n    :param blanklines: Indicates how blank lines should be handled.  Valid values are:\n\n        - ``discard``: strip blank lines out of the token list before returning it.\n           A line is considered blank if it contains only whitespace characters.\n        - ``keep``: leave all blank lines in the token list.\n        - ``discard-eof``: if the string ends with a newline, then do not generate\n           a corresponding token ``''`` after that newline.\n    \"\"\"\n\n    def __init__(self, blanklines='discard'):\n        valid_blanklines = ('discard', 'keep', 'discard-eof')\n        if blanklines not in valid_blanklines:\n            raise ValueError('Blank lines must be one of: %s' %\n                             ' '.join(valid_blanklines))\n\n        self._blanklines = blanklines\n\n    def tokenize(self, s):\n        lines = s.splitlines()\n        if self._blanklines == 'discard':\n            lines = [l for l in lines if l.rstrip()]\n        elif self._blanklines == 'discard-eof':\n            if lines and not lines[-1].strip():\n                lines.pop()\n        return lines\n\n    def span_tokenize(self, s):\n        if self._blanklines == 'keep':\n            for span in string_span_tokenize(s, r'\\n'):\n                yield span\n        else:\n            for span in regexp_span_tokenize(s, r'\\n(\\s+\\n)*'):\n                yield span\n\n\n\ndef line_tokenize(text, blanklines='discard'):\n    return LineTokenizer(blanklines).tokenize(text)\n"], "nltk\\tokenize\\stanford": [".py", "\nfrom __future__ import unicode_literals, print_function\n\nimport tempfile\nimport os\nimport json\nfrom subprocess import PIPE\nimport warnings\n\nfrom six import text_type\n\nfrom nltk.internals import find_jar, config_java, java, _java_options\nfrom nltk.tokenize.api import TokenizerI\nfrom nltk.parse.corenlp import CoreNLPParser\n\n_stanford_url = 'https://nlp.stanford.edu/software/tokenizer.shtml'\n\n\nclass StanfordTokenizer(TokenizerI):\n    r\"\"\"\n    Interface to the Stanford Tokenizer\n\n    >>> from nltk.tokenize.stanford import StanfordTokenizer\n    >>> s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\nThanks.\"\n    >>> StanfordTokenizer().tokenize(s)\n    ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n    >>> s = \"The colour of the wall is blue.\"\n    >>> StanfordTokenizer(options={\"americanize\": True}).tokenize(s)\n    ['The', 'color', 'of', 'the', 'wall', 'is', 'blue', '.']\n    \"\"\"\n\n    _JAR = 'stanford-postagger.jar'\n\n    def __init__(self, path_to_jar=None, encoding='utf8', options=None, verbose=False, java_options='-mx1000m'):\n        warnings.warn(str(\"\\nThe StanfordTokenizer will \"\n                          \"be deprecated in version 3.2.5.\\n\"\n                          \"Please use \\033[91mnltk.parse.corenlp.CoreNLPParser\\033[0m instead.'\"),\n                      DeprecationWarning, stacklevel=2)\n\n        self._stanford_jar = find_jar(\n            self._JAR, path_to_jar,\n            env_vars=('STANFORD_POSTAGGER',),\n            searchpath=(), url=_stanford_url,\n            verbose=verbose\n        )\n\n        self._encoding = encoding\n        self.java_options = java_options\n\n        options = {} if options is None else options\n        self._options_cmd = ','.join('{0}={1}'.format(key, val) for key, val in options.items())\n\n    @staticmethod\n    def _parse_tokenized_output(s):\n        return s.splitlines()\n\n    def tokenize(self, s):\n        \"\"\"\n        Use stanford tokenizer's PTBTokenizer to tokenize multiple sentences.\n        \"\"\"\n        cmd = [\n            'edu.stanford.nlp.process.PTBTokenizer',\n        ]\n        return self._parse_tokenized_output(self._execute(cmd, s))\n\n    def _execute(self, cmd, input_, verbose=False):\n        encoding = self._encoding\n        cmd.extend(['-charset', encoding])\n        _options_cmd = self._options_cmd\n        if _options_cmd:\n            cmd.extend(['-options', self._options_cmd])\n\n        default_options = ' '.join(_java_options)\n\n        config_java(options=self.java_options, verbose=verbose)\n\n        with tempfile.NamedTemporaryFile(mode='wb', delete=False) as input_file:\n            if isinstance(input_, text_type) and encoding:\n                input_ = input_.encode(encoding)\n            input_file.write(input_)\n            input_file.flush()\n\n            cmd.append(input_file.name)\n\n            stdout, stderr = java(cmd, classpath=self._stanford_jar,\n                                  stdout=PIPE, stderr=PIPE)\n            stdout = stdout.decode(encoding)\n\n        os.unlink(input_file.name)\n\n        config_java(options=default_options, verbose=False)\n\n        return stdout\n\n\ndef setup_module(module):\n    from nose import SkipTest\n\n    try:\n        StanfordTokenizer()\n    except LookupError:\n        raise SkipTest(\n            'doctests from nltk.tokenize.stanford are skipped because the stanford postagger jar doesn\\'t exist')\n"], "nltk\\tokenize\\stanford_segmenter": [".py", "\nfrom __future__ import unicode_literals, print_function\n\nimport tempfile\nimport os\nimport json\nfrom subprocess import PIPE\nimport warnings\n\nfrom nltk import compat\nfrom nltk.internals import find_jar, find_file, find_dir, \\\n                           config_java, java, _java_options\nfrom nltk.tokenize.api import TokenizerI\n\nfrom six import text_type\n\n_stanford_url = 'https://nlp.stanford.edu/software'\n\n\nclass StanfordSegmenter(TokenizerI):\n\n    _JAR = 'stanford-segmenter.jar'\n\n    def __init__(self,\n                 path_to_jar=None,\n                 path_to_slf4j=None,\n                 java_class=None,\n                 path_to_model=None,\n                 path_to_dict=None,\n                 path_to_sihan_corpora_dict=None,\n                 sihan_post_processing='false',\n                 keep_whitespaces='false',\n                 encoding='UTF-8', options=None,\n                 verbose=False, java_options='-mx2g'):\n        warnings.simplefilter('always', DeprecationWarning)\n        warnings.warn(str(\"\\nThe StanfordTokenizer will \"\n                          \"be deprecated in version 3.2.5.\\n\"\n                          \"Please use \\033[91mnltk.parse.corenlp.CoreNLPTokenizer\\033[0m instead.'\"),\n                      DeprecationWarning, stacklevel=2)\n        warnings.simplefilter('ignore', DeprecationWarning)\n\n        stanford_segmenter = find_jar(\n                self._JAR, path_to_jar,\n                env_vars=('STANFORD_SEGMENTER',),\n                searchpath=(), url=_stanford_url,\n                verbose=verbose)\n        if path_to_slf4j is not None:\n            slf4j = find_jar(\n                'slf4j-api.jar', path_to_slf4j,\n                env_vars=('SLF4J', 'STANFORD_SEGMENTER',),\n                searchpath=(), url=_stanford_url,\n                verbose=verbose)\n        else:\n            slf4j = None\n\n        self._stanford_jar = os.pathsep.join(\n            _ for _ in [stanford_segmenter, slf4j] if _ is not None\n        )\n\n        self._java_class = java_class\n        self._model = path_to_model\n        self._sihan_corpora_dict = path_to_sihan_corpora_dict\n        self._sihan_post_processing = sihan_post_processing\n        self._keep_whitespaces = keep_whitespaces\n        self._dict = path_to_dict\n\n        self._encoding = encoding\n        self.java_options = java_options\n        options = {} if options is None else options\n        self._options_cmd = ','.join('{0}={1}'.format(key, json.dumps(val)) for key, val in options.items())\n\n    def default_config(self, lang):\n\n        search_path = ()\n        if os.environ.get('STANFORD_SEGMENTER'):\n            search_path = {os.path.join(os.environ.get('STANFORD_SEGMENTER'), 'data')}\n\n        self._dict = None\n        self._sihan_corpora_dict = None\n        self._sihan_post_processing = 'false'\n\n        if lang == 'ar':\n            self._java_class = 'edu.stanford.nlp.international.arabic.process.ArabicSegmenter'\n            model = 'arabic-segmenter-atb+bn+arztrain.ser.gz'\n\n        elif lang == 'zh':\n            self._java_class = 'edu.stanford.nlp.ie.crf.CRFClassifier'\n            model = 'pku.gz'\n            self._sihan_post_processing = 'true'\n\n            path_to_dict = 'dict-chris6.ser.gz'\n            try:\n                self._dict = find_file(path_to_dict, searchpath=search_path,\n                                       url=_stanford_url, verbose=False,\n                                       env_vars=('STANFORD_MODELS',))\n            except LookupError:\n                raise LookupError(\"Could not find '%s' (tried using env. \"\n                    \"variables STANFORD_MODELS and <STANFORD_SEGMENTER>/data/)\" % path_to_dict)\n\n            sihan_dir = './data/'\n            try:\n                path_to_sihan_dir = find_dir(sihan_dir,\n                                             url=_stanford_url, verbose=False,\n                                             env_vars=('STANFORD_SEGMENTER',))\n                self._sihan_corpora_dict = os.path.join(path_to_sihan_dir, sihan_dir)\n            except LookupError:\n                raise LookupError(\"Could not find '%s' (tried using the \"\n                    \"STANFORD_SEGMENTER environment variable)\" % sihan_dir)\n        else:\n            raise LookupError(\"Unsupported language '%'\" % lang)\n\n        try:\n            self._model = find_file(model, searchpath=search_path,\n                                    url=_stanford_url, verbose=False,\n                                    env_vars=('STANFORD_MODELS', 'STANFORD_SEGMENTER',))\n        except LookupError:\n            raise LookupError(\"Could not find '%s' (tried using env. \"\n                \"variables STANFORD_MODELS and <STANFORD_SEGMENTER>/data/)\" % model)\n\n    def tokenize(self, s):\n        super().tokenize(s)\n\n    def segment_file(self, input_file_path):\n        cmd = [\n            self._java_class,\n            '-loadClassifier', self._model,\n            '-keepAllWhitespaces', self._keep_whitespaces,\n            '-textFile', input_file_path\n        ]\n        if self._sihan_corpora_dict is not None:\n            cmd.extend(['-serDictionary', self._dict,\n                        '-sighanCorporaDict', self._sihan_corpora_dict,\n                        '-sighanPostProcessing', self._sihan_post_processing])\n\n        stdout = self._execute(cmd)\n\n        return stdout\n\n    def segment(self, tokens):\n        return self.segment_sents([tokens])\n\n    def segment_sents(self, sentences):\n        encoding = self._encoding\n        _input_fh, self._input_file_path = tempfile.mkstemp(text=True)\n\n        _input_fh = os.fdopen(_input_fh, 'wb')\n        _input = '\\n'.join((' '.join(x) for x in sentences))\n        if isinstance(_input, text_type) and encoding:\n            _input = _input.encode(encoding)\n        _input_fh.write(_input)\n        _input_fh.close()\n\n        cmd = [\n            self._java_class,\n            '-loadClassifier', self._model,\n            '-keepAllWhitespaces', self._keep_whitespaces,\n            '-textFile', self._input_file_path\n        ]\n        if self._sihan_corpora_dict is not None:\n            cmd.extend(['-serDictionary', self._dict,\n                        '-sighanCorporaDict', self._sihan_corpora_dict,\n                        '-sighanPostProcessing', self._sihan_post_processing])\n\n        stdout = self._execute(cmd)\n\n        os.unlink(self._input_file_path)\n\n        return stdout\n\n    def _execute(self, cmd, verbose=False):\n        encoding = self._encoding\n        cmd.extend(['-inputEncoding', encoding])\n        _options_cmd = self._options_cmd\n        if _options_cmd:\n            cmd.extend(['-options', self._options_cmd])\n\n        default_options = ' '.join(_java_options)\n\n        config_java(options=self.java_options, verbose=verbose)\n\n        stdout, _stderr = java(cmd, classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE)\n        stdout = stdout.decode(encoding)\n\n        config_java(options=default_options, verbose=False)\n\n        return stdout\n\n\ndef setup_module(module):\n    from nose import SkipTest\n\n    try:\n        seg = StanfordSegmenter()\n        seg.default_config('ar')\n        seg.default_config('zh')\n    except LookupError as e:\n        raise SkipTest('Tests for nltk.tokenize.stanford_segmenter skipped: %s' % str(e))\n"], "nltk\\tokenize\\texttiling": [".py", "\nimport re\nimport math\n\ntry:\n    import numpy\nexcept ImportError:\n    pass\n\nfrom nltk.tokenize.api import TokenizerI\n\nBLOCK_COMPARISON, VOCABULARY_INTRODUCTION = 0, 1\nLC, HC = 0, 1\nDEFAULT_SMOOTHING = [0]\n\n\nclass TextTilingTokenizer(TokenizerI):\n\n    def __init__(self,\n                 w=20,\n                 k=10,\n                 similarity_method=BLOCK_COMPARISON,\n                 stopwords=None,\n                 smoothing_method=DEFAULT_SMOOTHING,\n                 smoothing_width=2,\n                 smoothing_rounds=1,\n                 cutoff_policy=HC,\n                 demo_mode=False):\n\n        if stopwords is None:\n            from nltk.corpus import stopwords\n            stopwords = stopwords.words('english')\n        self.__dict__.update(locals())\n        del self.__dict__['self']\n\n    def tokenize(self, text):\n\n    if x.ndim != 1:\n        raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n\n    if x.size < window_len:\n        raise ValueError(\"Input vector needs to be bigger than window size.\")\n\n    if window_len < 3:\n        return x\n\n    if window not in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n        raise ValueError(\"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n\n    s = numpy.r_[2*x[0]-x[window_len:1:-1], x, 2*x[-1]-x[-1:-window_len:-1]]\n\n    if window == 'flat':  # moving average\n        w = numpy.ones(window_len, 'd')\n    else:\n        w = eval('numpy.' + window + '(window_len)')\n\n    y = numpy.convolve(w/w.sum(), s, mode='same')\n\n    return y[window_len-1:-window_len+1]\n\n\ndef demo(text=None):\n    from nltk.corpus import brown\n    from matplotlib import pylab\n    tt = TextTilingTokenizer(demo_mode=True)\n    if text is None:\n        text = brown.raw()[:10000]\n    s, ss, d, b = tt.tokenize(text)\n    pylab.xlabel(\"Sentence Gap index\")\n    pylab.ylabel(\"Gap Scores\")\n    pylab.plot(range(len(s)), s, label=\"Gap Scores\")\n    pylab.plot(range(len(ss)), ss, label=\"Smoothed Gap scores\")\n    pylab.plot(range(len(d)), d, label=\"Depth scores\")\n    pylab.stem(range(len(b)), b)\n    pylab.legend()\n    pylab.show()\n"], "nltk\\tokenize\\toktok": [".py", "\n\nimport re\nfrom six import text_type\n\nfrom nltk.tokenize.api import TokenizerI\n\nclass ToktokTokenizer(TokenizerI):\n    NON_BREAKING = re.compile(u\"\\u00A0\"), \" \"\n    \n    FUNKY_PUNCT_1 = re.compile(u'([\u060c;\u061b\u00bf!\"\\])}\u00bb\u203a\u201d\u061f\u00a1%\u066a\u00b0\u00b1\u00a9\u00ae\u0964\u0965\u2026])'), r\" \\1 \"\n    FUNKY_PUNCT_2 = re.compile(u'([({\\[\u201c\u2018\u201e\u201a\u00ab\u2039\u300c\u300e])'), r\" \\1 \"\n    EN_EM_DASHES = re.compile(u'([\u2013\u2014])'), r\" \\1 \"\n    \n    AMPERCENT = re.compile('& '), '&amp; '\n    TAB = re.compile('\\t'), ' &#9; '\n    PIPE = re.compile('\\|'), ' &#124; '\n    \n    COMMA_IN_NUM = re.compile(r'(?<!,)([,\u060c])(?![,\\d])'), r' \\1 '\n    \n    PROB_SINGLE_QUOTES = re.compile(r\"(['\u2019`])\"), r' \\1 '\n    STUPID_QUOTES_1 = re.compile(r\" ` ` \"), r\" `` \"\n    STUPID_QUOTES_2 = re.compile(r\" ' ' \"), r\" '' \"\n    \n    FINAL_PERIOD_1 = re.compile(r\"(?<!\\.)\\.$\"), r\" .\"\n    FINAL_PERIOD_2 = re.compile(r\"\"\"(?<!\\.)\\.\\s*([\"'\u2019\u00bb\u203a\u201d]) *$\"\"\"), r\" . \\1\"\n\n    MULTI_COMMAS = re.compile(r'(,{2,})'), r' \\1 '\n    MULTI_DASHES = re.compile(r'(-{2,})'), r' \\1 '\n    MULTI_DOTS = re.compile(r'(\\.{2,})'), r' \\1 '\n\n    OPEN_PUNCT = text_type(u'([{\\u0f3a\\u0f3c\\u169b\\u201a\\u201e\\u2045\\u207d'\n                            u'\\u208d\\u2329\\u2768\\u276a\\u276c\\u276e\\u2770\\u2772'\n                            u'\\u2774\\u27c5\\u27e6\\u27e8\\u27ea\\u27ec\\u27ee\\u2983'\n                            u'\\u2985\\u2987\\u2989\\u298b\\u298d\\u298f\\u2991\\u2993'\n                            u'\\u2995\\u2997\\u29d8\\u29da\\u29fc\\u2e22\\u2e24\\u2e26'\n                            u'\\u2e28\\u3008\\u300a\\u300c\\u300e\\u3010\\u3014\\u3016'\n                            u'\\u3018\\u301a\\u301d\\ufd3e\\ufe17\\ufe35\\ufe37\\ufe39'\n                            u'\\ufe3b\\ufe3d\\ufe3f\\ufe41\\ufe43\\ufe47\\ufe59\\ufe5b'\n                            u'\\ufe5d\\uff08\\uff3b\\uff5b\\uff5f\\uff62')\n    CLOSE_PUNCT = text_type(u')]}\\u0f3b\\u0f3d\\u169c\\u2046\\u207e\\u208e\\u232a'\n                            u'\\u2769\\u276b\\u276d\\u276f\\u2771\\u2773\\u2775\\u27c6'\n                            u'\\u27e7\\u27e9\\u27eb\\u27ed\\u27ef\\u2984\\u2986\\u2988'\n                            u'\\u298a\\u298c\\u298e\\u2990\\u2992\\u2994\\u2996\\u2998'\n                            u'\\u29d9\\u29db\\u29fd\\u2e23\\u2e25\\u2e27\\u2e29\\u3009'\n                            u'\\u300b\\u300d\\u300f\\u3011\\u3015\\u3017\\u3019\\u301b'\n                            u'\\u301e\\u301f\\ufd3f\\ufe18\\ufe36\\ufe38\\ufe3a\\ufe3c'\n                            u'\\ufe3e\\ufe40\\ufe42\\ufe44\\ufe48\\ufe5a\\ufe5c\\ufe5e'\n                            u'\\uff09\\uff3d\\uff5d\\uff60\\uff63')\n    CURRENCY_SYM = text_type(u'$\\xa2\\xa3\\xa4\\xa5\\u058f\\u060b\\u09f2\\u09f3\\u09fb'\n                             u'\\u0af1\\u0bf9\\u0e3f\\u17db\\u20a0\\u20a1\\u20a2\\u20a3'\n                             u'\\u20a4\\u20a5\\u20a6\\u20a7\\u20a8\\u20a9\\u20aa\\u20ab'\n                             u'\\u20ac\\u20ad\\u20ae\\u20af\\u20b0\\u20b1\\u20b2\\u20b3'\n                             u'\\u20b4\\u20b5\\u20b6\\u20b7\\u20b8\\u20b9\\u20ba\\ua838'\n                             u'\\ufdfc\\ufe69\\uff04\\uffe0\\uffe1\\uffe5\\uffe6')\n    \n    OPEN_PUNCT_RE = re.compile(u'([{}])'.format(OPEN_PUNCT)), r'\\1 '\n    CLOSE_PUNCT_RE = re.compile(u'([{}])'.format(CLOSE_PUNCT)), r'\\1 '\n    CURRENCY_SYM_RE = re.compile(u'([{}])'.format(CURRENCY_SYM)), r'\\1 '\n    \n    URL_FOE_1 = re.compile(r':(?!//)'), r' : ' # in perl s{:(?!//)}{ : }g;\n    URL_FOE_2 = re.compile(r'\\?(?!\\S)'), r' ? ' # in perl s{\\?(?!\\S)}{ ? }g;\n    URL_FOE_3 = re.compile(r'(:\\/\\/)[\\S+\\.\\S+\\/\\S+][\\/]'), ' / '\n    URL_FOE_4 = re.compile(r' /'), r' / ' # s{ /}{ / }g;\n    \n    LSTRIP = re.compile(r'^ +'), ''\n    RSTRIP = re.compile(r'\\s+$'),'\\n' \n    ONE_SPACE = re.compile(r' {2,}'), ' '\n    \n    TOKTOK_REGEXES = [NON_BREAKING, FUNKY_PUNCT_1, \n                      URL_FOE_1, URL_FOE_2, URL_FOE_3, URL_FOE_4,\n                      AMPERCENT, TAB, PIPE,\n                      OPEN_PUNCT_RE, CLOSE_PUNCT_RE, \n                      MULTI_COMMAS, COMMA_IN_NUM, FINAL_PERIOD_2,\n                      PROB_SINGLE_QUOTES, STUPID_QUOTES_1, STUPID_QUOTES_2,\n                      CURRENCY_SYM_RE, EN_EM_DASHES, MULTI_DASHES, MULTI_DOTS,\n                      FINAL_PERIOD_1, FINAL_PERIOD_2, ONE_SPACE]\n    \n    def tokenize(self, text, return_str=False):\n        text = text_type(text) # Converts input string into unicode.\n        for regexp, subsitution in self.TOKTOK_REGEXES:\n            text = regexp.sub(subsitution, text)\n        text = text_type(text.strip()) \n        return text if return_str else text.split()"], "nltk\\tokenize\\treebank": [".py", "\nr\"\"\"\n\nPenn Treebank Tokenizer\n\nThe Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.\nThis implementation is a port of the tokenizer sed script written by Robert McIntyre\nand available at http://www.cis.upenn.edu/~treebank/tokenizer.sed.\n\"\"\"\n\nimport re\nfrom nltk.tokenize.api import TokenizerI\nfrom nltk.tokenize.util import align_tokens\n\n\nclass MacIntyreContractions:\n    \"\"\"\n    List of contractions adapted from Robert MacIntyre's tokenizer.\n    \"\"\"\n    CONTRACTIONS2 = [r\"(?i)\\b(can)(?#X)(not)\\b\",\n                     r\"(?i)\\b(d)(?#X)('ye)\\b\",\n                     r\"(?i)\\b(gim)(?#X)(me)\\b\",\n                     r\"(?i)\\b(gon)(?#X)(na)\\b\",\n                     r\"(?i)\\b(got)(?#X)(ta)\\b\",\n                     r\"(?i)\\b(lem)(?#X)(me)\\b\",\n                     r\"(?i)\\b(mor)(?#X)('n)\\b\",\n                     r\"(?i)\\b(wan)(?#X)(na)\\s\"]\n    CONTRACTIONS3 = [r\"(?i) ('t)(?#X)(is)\\b\", r\"(?i) ('t)(?#X)(was)\\b\"]\n    CONTRACTIONS4 = [r\"(?i)\\b(whad)(dd)(ya)\\b\",\n                     r\"(?i)\\b(wha)(t)(cha)\\b\"]\n\n\nclass TreebankWordTokenizer(TokenizerI):\n    \"\"\"\n    The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.\n    This is the method that is invoked by ``word_tokenize()``.  It assumes that the\n    text has already been segmented into sentences, e.g. using ``sent_tokenize()``.\n\n    This tokenizer performs the following steps:\n\n    - split standard contractions, e.g. ``don't`` -> ``do n't`` and ``they'll`` -> ``they 'll``\n    - treat most punctuation characters as separate tokens\n    - split off commas and single quotes, when followed by whitespace\n    - separate periods that appear at the end of line\n\n        >>> from nltk.tokenize import TreebankWordTokenizer\n        >>> s = '''Good muffins cost $3.88\\\\nin New York.  Please buy me\\\\ntwo of them.\\\\nThanks.'''\n        >>> TreebankWordTokenizer().tokenize(s)\n        ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks', '.']\n        >>> s = \"They'll save and invest more.\"\n        >>> TreebankWordTokenizer().tokenize(s)\n        ['They', \"'ll\", 'save', 'and', 'invest', 'more', '.']\n        >>> s = \"hi, my name can't hello,\"\n        >>> TreebankWordTokenizer().tokenize(s)\n        ['hi', ',', 'my', 'name', 'ca', \"n't\", 'hello', ',']\n    \"\"\"\n\n    STARTING_QUOTES = [\n        (re.compile(r'^\\\"'), r'``'),\n        (re.compile(r'(``)'), r' \\1 '),\n        (re.compile(r\"([ \\(\\[{<])(\\\"|\\'{2})\"), r'\\1 `` '),\n    ]\n\n    PUNCTUATION = [\n        (re.compile(r'([:,])([^\\d])'), r' \\1 \\2'),\n        (re.compile(r'([:,])$'), r' \\1 '),\n        (re.compile(r'\\.\\.\\.'), r' ... '),\n        (re.compile(r'[;@#$%&]'), r' \\g<0> '),\n        (re.compile(r'([^\\.])(\\.)([\\]\\)}>\"\\']*)\\s*$'), r'\\1 \\2\\3 '),  # Handles the final period.\n        (re.compile(r'[?!]'), r' \\g<0> '),\n\n        (re.compile(r\"([^'])' \"), r\"\\1 ' \"),\n    ]\n\n    PARENS_BRACKETS = (re.compile(r'[\\]\\[\\(\\)\\{\\}\\<\\>]'), r' \\g<0> ')\n\n    CONVERT_PARENTHESES = [\n        (re.compile(r'\\('), '-LRB-'), (re.compile(r'\\)'), '-RRB-'),\n        (re.compile(r'\\['), '-LSB-'), (re.compile(r'\\]'), '-RSB-'),\n        (re.compile(r'\\{'), '-LCB-'), (re.compile(r'\\}'), '-RCB-')\n    ]\n\n    DOUBLE_DASHES = (re.compile(r'--'), r' -- ')\n\n    ENDING_QUOTES = [\n        (re.compile(r'\"'), \" '' \"),\n        (re.compile(r'(\\S)(\\'\\')'), r'\\1 \\2 '),\n        (re.compile(r\"([^' ])('[sS]|'[mM]|'[dD]|') \"), r\"\\1 \\2 \"),\n        (re.compile(r\"([^' ])('ll|'LL|'re|'RE|'ve|'VE|n't|N'T) \"), r\"\\1 \\2 \"),\n    ]\n\n    _contractions = MacIntyreContractions()\n    CONTRACTIONS2 = list(map(re.compile, _contractions.CONTRACTIONS2))\n    CONTRACTIONS3 = list(map(re.compile, _contractions.CONTRACTIONS3))\n\n    def tokenize(self, text, convert_parentheses=False, return_str=False):\n        for regexp, substitution in self.STARTING_QUOTES:\n            text = regexp.sub(substitution, text)\n\n        for regexp, substitution in self.PUNCTUATION:\n            text = regexp.sub(substitution, text)\n\n        regexp, substitution = self.PARENS_BRACKETS\n        text = regexp.sub(substitution, text)\n        if convert_parentheses:\n            for regexp, substitution in self.CONVERT_PARENTHESES:\n                text = regexp.sub(substitution, text)\n\n        regexp, substitution = self.DOUBLE_DASHES\n        text = regexp.sub(substitution, text)\n\n        text = \" \" + text + \" \"\n\n        for regexp, substitution in self.ENDING_QUOTES:\n            text = regexp.sub(substitution, text)\n\n        for regexp in self.CONTRACTIONS2:\n            text = regexp.sub(r' \\1 \\2 ', text)\n        for regexp in self.CONTRACTIONS3:\n            text = regexp.sub(r' \\1 \\2 ', text)\n\n\n        return text if return_str else text.split()\n\n    def span_tokenize(self, text):\n        \"\"\"\n        Uses the post-hoc nltk.tokens.align_tokens to return the offset spans.\n\n            >>> from nltk.tokenize import TreebankWordTokenizer\n            >>> s = '''Good muffins cost $3.88\\\\nin New (York).  Please (buy) me\\\\ntwo of them.\\\\n(Thanks).'''\n            >>> expected = [(0, 4), (5, 12), (13, 17), (18, 19), (19, 23),\n            ... (24, 26), (27, 30), (31, 32), (32, 36), (36, 37), (37, 38),\n            ... (40, 46), (47, 48), (48, 51), (51, 52), (53, 55), (56, 59),\n            ... (60, 62), (63, 68), (69, 70), (70, 76), (76, 77), (77, 78)]\n            >>> list(TreebankWordTokenizer().span_tokenize(s)) == expected\n            True\n            >>> expected = ['Good', 'muffins', 'cost', '$', '3.88', 'in',\n            ... 'New', '(', 'York', ')', '.', 'Please', '(', 'buy', ')',\n            ... 'me', 'two', 'of', 'them.', '(', 'Thanks', ')', '.']\n            >>> [s[start:end] for start, end in TreebankWordTokenizer().span_tokenize(s)] == expected\n            True\n\n            Additional example\n            >>> from nltk.tokenize import TreebankWordTokenizer\n            >>> s = '''I said, \"I'd like to buy some ''good muffins\" which cost $3.88\\\\n each in New (York).\"'''\n            >>> expected = [(0, 1), (2, 6), (6, 7), (8, 9), (9, 10), (10, 12),\n            ... (13, 17), (18, 20), (21, 24), (25, 29), (30, 32), (32, 36),\n            ... (37, 44), (44, 45), (46, 51), (52, 56), (57, 58), (58, 62),\n            ... (64, 68), (69, 71), (72, 75), (76, 77), (77, 81), (81, 82),\n            ... (82, 83), (83, 84)]\n            >>> list(TreebankWordTokenizer().span_tokenize(s)) == expected\n            True\n            >>> expected = ['I', 'said', ',', '\"', 'I', \"'d\", 'like', 'to',\n            ... 'buy', 'some', \"''\", \"good\", 'muffins', '\"', 'which', 'cost',\n            ... '$', '3.88', 'each', 'in', 'New', '(', 'York', ')', '.', '\"']\n            >>> [s[start:end] for start, end in TreebankWordTokenizer().span_tokenize(s)] == expected\n            True\n\n        \"\"\"\n        raw_tokens = self.tokenize(text)\n\n        if ('\"' in text) or (\"''\" in text):\n            matched = [m.group() for m in re.finditer(r\"``|'{2}|\\\"\", text)]\n\n            tokens = [matched.pop(0) if tok in ['\"', \"``\", \"''\"] else tok\n                      for tok in raw_tokens]\n        else:\n            tokens = raw_tokens\n\n        for tok in align_tokens(tokens, text):\n            yield tok\n\n\nclass TreebankWordDetokenizer(TokenizerI):\n    \"\"\"\n    The Treebank detokenizer uses the reverse regex operations corresponding to\n    the Treebank tokenizer's regexes.\n\n    Note:\n    - There're additional assumption mades when undoing the padding of [;@#$%&]\n      punctuation symbols that isn't presupposed in the TreebankTokenizer.\n    - There're additional regexes added in reversing the parentheses tokenization,\n       - the r'([\\]\\)\\}\\>])\\s([:;,.])' removes the additional right padding added\n         to the closing parentheses precedding [:;,.].\n    - It's not possible to return the original whitespaces as they were because\n      there wasn't explicit records of where '\\n', '\\t' or '\\s' were removed at\n      the text.split() operation.\n\n        >>> from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer\n        >>> s = '''Good muffins cost $3.88\\\\nin New York.  Please buy me\\\\ntwo of them.\\\\nThanks.'''\n        >>> d = TreebankWordDetokenizer()\n        >>> t = TreebankWordTokenizer()\n        >>> toks = t.tokenize(s)\n        >>> d.detokenize(toks)\n        'Good muffins cost $3.88 in New York. Please buy me two of them. Thanks.'\n\n    The MXPOST parentheses substitution can be undone using the `convert_parentheses`\n    parameter:\n\n    >>> s = '''Good muffins cost $3.88\\\\nin New (York).  Please (buy) me\\\\ntwo of them.\\\\n(Thanks).'''\n    >>> expected_tokens = ['Good', 'muffins', 'cost', '$', '3.88', 'in',\n    ... 'New', '-LRB-', 'York', '-RRB-', '.', 'Please', '-LRB-', 'buy',\n    ... '-RRB-', 'me', 'two', 'of', 'them.', '-LRB-', 'Thanks', '-RRB-', '.']\n    >>> expected_tokens == t.tokenize(s, convert_parentheses=True)\n    True\n    >>> expected_detoken = 'Good muffins cost $3.88 in New (York). Please (buy) me two of them. (Thanks).'\n    >>> expected_detoken == d.detokenize(t.tokenize(s, convert_parentheses=True), convert_parentheses=True)\n    True\n\n    During tokenization it's safe to add more spaces but during detokenization,\n    simply undoing the padding doesn't really help.\n\n    - During tokenization, left and right pad is added to [!?], when\n      detokenizing, only left shift the [!?] is needed.\n      Thus (re.compile(r'\\s([?!])'), r'\\g<1>')\n\n    - During tokenization [:,] are left and right padded but when detokenizing,\n      only left shift is necessary and we keep right pad after comma/colon\n      if the string after is a non-digit.\n      Thus (re.compile(r'\\s([:,])\\s([^\\d])'), r'\\1 \\2')\n\n    >>> from nltk.tokenize.treebank import TreebankWordDetokenizer\n    >>> toks = ['hello', ',', 'i', 'ca', \"n't\", 'feel', 'my', 'feet', '!', 'Help', '!', '!']\n    >>> twd = TreebankWordDetokenizer()\n    >>> twd.detokenize(toks)\n    \"hello, i can't feel my feet! Help!!\"\n\n    >>> toks = ['hello', ',', 'i', \"can't\", 'feel', ';', 'my', 'feet', '!',\n    ... 'Help', '!', '!', 'He', 'said', ':', 'Help', ',', 'help', '?', '!']\n    >>> twd.detokenize(toks)\n    \"hello, i can't feel; my feet! Help!! He said: Help, help?!\"\n    \"\"\"\n    _contractions = MacIntyreContractions()\n    CONTRACTIONS2 = [re.compile(pattern.replace('(?#X)', '\\s'))\n                     for pattern in _contractions.CONTRACTIONS2]\n    CONTRACTIONS3 = [re.compile(pattern.replace('(?#X)', '\\s'))\n                     for pattern in _contractions.CONTRACTIONS3]\n\n    ENDING_QUOTES = [\n        (re.compile(r\"([^' ])\\s('ll|'LL|'re|'RE|'ve|'VE|n't|N'T) \"), r\"\\1\\2 \"),\n        (re.compile(r\"([^' ])\\s('[sS]|'[mM]|'[dD]|') \"), r\"\\1\\2 \"),\n        (re.compile(r'(\\S)(\\'\\')'), r'\\1\\2 '),\n        (re.compile(r\" '' \"), '\"')\n    ]\n\n    DOUBLE_DASHES = (re.compile(r' -- '), r'--')\n\n    CONVERT_PARENTHESES = [\n        (re.compile('-LRB-'), '('), (re.compile('-RRB-'), ')'),\n        (re.compile('-LSB-'), '['), (re.compile('-RSB-'), ']'),\n        (re.compile('-LCB-'), '{'), (re.compile('-RCB-'), '}')\n    ]\n\n    PARENS_BRACKETS = [(re.compile(r'\\s([\\[\\(\\{\\<])\\s'), r' \\g<1>'),\n                       (re.compile(r'\\s([\\]\\)\\}\\>])\\s'), r'\\g<1> '),\n                       (re.compile(r'([\\]\\)\\}\\>])\\s([:;,.])'), r'\\1\\2')]\n\n    PUNCTUATION = [\n        (re.compile(r\"([^'])\\s'\\s\"), r\"\\1' \"),\n        (re.compile(r'\\s([?!])'), r'\\g<1>'),  # Strip left pad for [?!]\n        (re.compile(r'([^\\.])\\s(\\.)([\\]\\)}>\"\\']*)\\s*$'), r'\\1\\2\\3'),\n        (re.compile(r'\\s([#$])\\s'), r' \\g<1>'),  # Left pad.\n        (re.compile(r'\\s([;%])\\s'), r'\\g<1> '),  # Right pad.\n        (re.compile(r'\\s([&])\\s'), r' \\g<1> '),  # Unknown pad.\n        (re.compile(r'\\s\\.\\.\\.\\s'), r'...'),\n        (re.compile(r'\\s([:,])\\s$'), r'\\1'),\n        (re.compile(r'\\s([:,])\\s([^\\d])'), r'\\1 \\2')  # Keep right pad after comma/colon before non-digits.\n    ]\n\n    STARTING_QUOTES = [\n        (re.compile(r'([ (\\[{<])\\s``'), r'\\1\"'),\n        (re.compile(r'\\s(``)\\s'), r'\\1'),\n        (re.compile(r'^``'), r'\\\"'),\n    ]\n\n    def tokenize(self, tokens, convert_parentheses=False):\n        \"\"\"\n        Python port of the Moses detokenizer.\n\n        :param tokens: A list of strings, i.e. tokenized text.\n        :type tokens: list(str)\n        :return: str\n        \"\"\"\n        text = ' '.join(tokens)\n        for regexp in self.CONTRACTIONS3:\n            text = regexp.sub(r'\\1\\2', text)\n        for regexp in self.CONTRACTIONS2:\n            text = regexp.sub(r'\\1\\2', text)\n\n        for regexp, substitution in self.ENDING_QUOTES:\n            text = regexp.sub(substitution, text)\n\n        text = text.strip()\n\n        regexp, substitution = self.DOUBLE_DASHES\n        text = regexp.sub(substitution, text)\n\n        if convert_parentheses:\n            for regexp, substitution in self.CONVERT_PARENTHESES:\n                text = regexp.sub(substitution, text)\n\n        for regexp, substitution in self.PARENS_BRACKETS:\n            text = regexp.sub(substitution, text)\n\n        for regexp, substitution in self.PUNCTUATION:\n            text = regexp.sub(substitution, text)\n\n        for regexp, substitution in self.STARTING_QUOTES:\n            text = regexp.sub(substitution, text)\n\n        return text.strip()\n\n    def detokenize(self, tokens, convert_parentheses=False):\n        return self.tokenize(tokens, convert_parentheses)\n"], "nltk\\tokenize\\util": [".py", "\nfrom re import finditer\nfrom xml.sax.saxutils import escape, unescape\n\n\ndef string_span_tokenize(s, sep):\n    r\"\"\"\n    Return the offsets of the tokens in *s*, as a sequence of ``(start, end)``\n    tuples, by splitting the string at each occurrence of *sep*.\n\n        >>> from nltk.tokenize.util import string_span_tokenize\n        >>> s = '''Good muffins cost $3.88\\nin New York.  Please buy me\n        ... two of them.\\n\\nThanks.'''\n        >>> list(string_span_tokenize(s, \" \"))\n        [(0, 4), (5, 12), (13, 17), (18, 26), (27, 30), (31, 36), (37, 37),\n        (38, 44), (45, 48), (49, 55), (56, 58), (59, 73)]\n\n    :param s: the string to be tokenized\n    :type s: str\n    :param sep: the token separator\n    :type sep: str\n    :rtype: iter(tuple(int, int))\n    \"\"\"\n    if len(sep) == 0:\n        raise ValueError(\"Token delimiter must not be empty\")\n    left = 0\n    while True:\n        try:\n            right = s.index(sep, left)\n            if right != 0:\n                yield left, right\n        except ValueError:\n            if left != len(s):\n                yield left, len(s)\n            break\n\n        left = right + len(sep)\n\n\ndef regexp_span_tokenize(s, regexp):\n    r\"\"\"\n    Return the offsets of the tokens in *s*, as a sequence of ``(start, end)``\n    tuples, by splitting the string at each successive match of *regexp*.\n\n        >>> from nltk.tokenize.util import regexp_span_tokenize\n        >>> s = '''Good muffins cost $3.88\\nin New York.  Please buy me\n        ... two of them.\\n\\nThanks.'''\n        >>> list(regexp_span_tokenize(s, r'\\s'))\n        [(0, 4), (5, 12), (13, 17), (18, 23), (24, 26), (27, 30), (31, 36),\n        (38, 44), (45, 48), (49, 51), (52, 55), (56, 58), (59, 64), (66, 73)]\n\n    :param s: the string to be tokenized\n    :type s: str\n    :param regexp: regular expression that matches token separators (must not be empty)\n    :type regexp: str\n    :rtype: iter(tuple(int, int))\n    \"\"\"\n    left = 0\n    for m in finditer(regexp, s):\n        right, next = m.span()\n        if right != left:\n            yield left, right\n        left = next\n    yield left, len(s)\n\n\ndef spans_to_relative(spans):\n    r\"\"\"\n    Return a sequence of relative spans, given a sequence of spans.\n\n        >>> from nltk.tokenize import WhitespaceTokenizer\n        >>> from nltk.tokenize.util import spans_to_relative\n        >>> s = '''Good muffins cost $3.88\\nin New York.  Please buy me\n        ... two of them.\\n\\nThanks.'''\n        >>> list(spans_to_relative(WhitespaceTokenizer().span_tokenize(s)))\n        [(0, 4), (1, 7), (1, 4), (1, 5), (1, 2), (1, 3), (1, 5), (2, 6),\n        (1, 3), (1, 2), (1, 3), (1, 2), (1, 5), (2, 7)]\n\n    :param spans: a sequence of (start, end) offsets of the tokens\n    :type spans: iter(tuple(int, int))\n    :rtype: iter(tuple(int, int))\n    \"\"\"\n    prev = 0\n    for left, right in spans:\n        yield left - prev, right - left\n        prev = right\n\n\nclass CJKChars(object):\n    \"\"\"\n    An object that enumerates the code points of the CJK characters as listed on\n    http://en.wikipedia.org/wiki/Basic_Multilingual_Plane#Basic_Multilingual_Plane\n\n    This is a Python port of the CJK code point enumerations of Moses tokenizer:\n    https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/detokenizer.perl#L309\n    \"\"\"\n    Hangul_Jamo = (4352, 4607)  # (ord(u\"\\u1100\"), ord(u\"\\u11ff\"))\n\n    CJK_Radicals = (11904, 42191)  # (ord(u\"\\u2e80\"), ord(u\"\\ua4cf\"))\n\n    Phags_Pa = (43072, 43135)  # (ord(u\"\\ua840\"), ord(u\"\\ua87f\"))\n\n    Hangul_Syllables = (44032, 55215)  # (ord(u\"\\uAC00\"), ord(u\"\\uD7AF\"))\n\n    CJK_Compatibility_Ideographs = (63744, 64255)  # (ord(u\"\\uF900\"), ord(u\"\\uFAFF\"))\n\n    CJK_Compatibility_Forms = (65072, 65103)  # (ord(u\"\\uFE30\"), ord(u\"\\uFE4F\"))\n\n    Katakana_Hangul_Halfwidth = (65381, 65500)  # (ord(u\"\\uFF65\"), ord(u\"\\uFFDC\"))\n\n    Supplementary_Ideographic_Plane = (131072, 196607)  # (ord(u\"\\U00020000\"), ord(u\"\\U0002FFFF\"))\n\n    ranges = [Hangul_Jamo, CJK_Radicals, Phags_Pa, Hangul_Syllables,\n              CJK_Compatibility_Ideographs, CJK_Compatibility_Forms,\n              Katakana_Hangul_Halfwidth, Supplementary_Ideographic_Plane]\n\n\ndef is_cjk(character):\n    \"\"\"\n    Python port of Moses' code to check for CJK character.\n\n    >>> CJKChars().ranges\n    [(4352, 4607), (11904, 42191), (43072, 43135), (44032, 55215), (63744, 64255), (65072, 65103), (65381, 65500), (131072, 196607)]\n    >>> is_cjk(u'\\u33fe')\n    True\n    >>> is_cjk(u'\\uFE5F')\n    False\n\n    :param character: The character that needs to be checked.\n    :type character: char\n    :return: bool\n    \"\"\"\n    return any([start <= ord(character) <= end for start, end in\n                [(4352, 4607), (11904, 42191), (43072, 43135), (44032, 55215),\n                 (63744, 64255), (65072, 65103), (65381, 65500),\n                 (131072, 196607)]\n                ])\n\n\ndef xml_escape(text):\n    \"\"\"\n    This function transforms the input text into an \"escaped\" version suitable\n    for well-formed XML formatting.\n\n    Note that the default xml.sax.saxutils.escape() function don't escape\n    some characters that Moses does so we have to manually add them to the\n    entities dictionary.\n\n        >>> input_str = ''')| & < > ' \" ] ['''\n        >>> expected_output =  ''')| &amp; &lt; &gt; ' \" ] ['''\n        >>> escape(input_str) == expected_output\n        True\n        >>> xml_escape(input_str)\n        ')&#124; &amp; &lt; &gt; &apos; &quot; &#93; &#91;'\n\n    :param text: The text that needs to be escaped.\n    :type text: str\n    :rtype: str\n    \"\"\"\n    return escape(text, entities={r\"'\": r\"&apos;\", r'\"': r\"&quot;\",\n                                  r\"|\": r\"&#124;\",\n                                  r\"[\": r\"&#91;\", r\"]\": r\"&#93;\", })\n\n\ndef xml_unescape(text):\n    \"\"\"\n    This function transforms the \"escaped\" version suitable\n    for well-formed XML formatting into humanly-readable string.\n\n    Note that the default xml.sax.saxutils.unescape() function don't unescape\n    some characters that Moses does so we have to manually add them to the\n    entities dictionary.\n\n        >>> from xml.sax.saxutils import unescape\n        >>> s = ')&#124; &amp; &lt; &gt; &apos; &quot; &#93; &#91;'\n        >>> expected = ''')| & < > \\' \" ] ['''\n        >>> xml_unescape(s) == expected\n        True\n\n    :param text: The text that needs to be unescaped.\n    :type text: str\n    :rtype: str\n    \"\"\"\n    return unescape(text, entities={r\"&apos;\": r\"'\", r\"&quot;\": r'\"',\n                                    r\"&#124;\": r\"|\",\n                                    r\"&#91;\": r\"[\", r\"&#93;\": r\"]\", })\n\n\ndef align_tokens(tokens, sentence):\n    \"\"\"\n    This module attempt to find the offsets of the tokens in *s*, as a sequence\n    of ``(start, end)`` tuples, given the tokens and also the source string.\n\n        >>> from nltk.tokenize import TreebankWordTokenizer\n        >>> from nltk.tokenize.util import align_tokens\n        >>> s = str(\"The plane, bound for St Petersburg, crashed in Egypt's \"\n        ... \"Sinai desert just 23 minutes after take-off from Sharm el-Sheikh \"\n        ... \"on Saturday.\")\n        >>> tokens = TreebankWordTokenizer().tokenize(s)\n        >>> expected = [(0, 3), (4, 9), (9, 10), (11, 16), (17, 20), (21, 23),\n        ... (24, 34), (34, 35), (36, 43), (44, 46), (47, 52), (52, 54),\n        ... (55, 60), (61, 67), (68, 72), (73, 75), (76, 83), (84, 89),\n        ... (90, 98), (99, 103), (104, 109), (110, 119), (120, 122),\n        ... (123, 131), (131, 132)]\n        >>> output = list(align_tokens(tokens, s))\n        >>> len(tokens) == len(expected) == len(output)  # Check that length of tokens and tuples are the same.\n        True\n        >>> expected == list(align_tokens(tokens, s))  # Check that the output is as expected.\n        True\n        >>> tokens == [s[start:end] for start, end in output]  # Check that the slices of the string corresponds to the tokens.\n        True\n\n    :param tokens: The list of strings that are the result of tokenization\n    :type tokens: list(str)\n    :param sentence: The original string\n    :type sentence: str\n    :rtype: list(tuple(int,int))\n    \"\"\"\n    point = 0\n    offsets = []\n    for token in tokens:\n        try:\n            start = sentence.index(token, point)\n        except ValueError:\n            raise ValueError('substring \"{}\" not found in \"{}\"'.format(token, sentence))\n        point = start + len(token)\n        offsets.append((start, point))\n    return offsets\n"], "nltk\\tokenize\\__init__": [".py", "\nr\"\"\"\nNLTK Tokenizer Package\n\nTokenizers divide strings into lists of substrings.  For example,\ntokenizers can be used to find the words and punctuation in a string:\n\n    >>> from nltk.tokenize import word_tokenize\n    >>> s = '''Good muffins cost $3.88\\nin New York.  Please buy me\n    ... two of them.\\n\\nThanks.'''\n    >>> word_tokenize(s)\n    ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.',\n    'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n\nThis particular tokenizer requires the Punkt sentence tokenization\nmodels to be installed. NLTK also provides a simpler,\nregular-expression based tokenizer, which splits text on whitespace\nand punctuation:\n\n    >>> from nltk.tokenize import wordpunct_tokenize\n    >>> wordpunct_tokenize(s)\n    ['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.',\n    'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n\nWe can also operate at the level of sentences, using the sentence\ntokenizer directly as follows:\n\n    >>> from nltk.tokenize import sent_tokenize, word_tokenize\n    >>> sent_tokenize(s)\n    ['Good muffins cost $3.88\\nin New York.', 'Please buy me\\ntwo of them.', 'Thanks.']\n    >>> [word_tokenize(t) for t in sent_tokenize(s)]\n    [['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.'],\n    ['Please', 'buy', 'me', 'two', 'of', 'them', '.'], ['Thanks', '.']]\n\nCaution: when tokenizing a Unicode string, make sure you are not\nusing an encoded version of the string (it may be necessary to\ndecode it first, e.g. with ``s.decode(\"utf8\")``.\n\nNLTK tokenizers can produce token-spans, represented as tuples of integers\nhaving the same semantics as string slices, to support efficient comparison\nof tokenizers.  (These methods are implemented as generators.)\n\n    >>> from nltk.tokenize import WhitespaceTokenizer\n    >>> list(WhitespaceTokenizer().span_tokenize(s))\n    [(0, 4), (5, 12), (13, 17), (18, 23), (24, 26), (27, 30), (31, 36), (38, 44),\n    (45, 48), (49, 51), (52, 55), (56, 58), (59, 64), (66, 73)]\n\nThere are numerous ways to tokenize text.  If you need more control over\ntokenization, see the other methods provided in this package.\n\nFor further information, please see Chapter 3 of the NLTK book.\n\"\"\"\n\nimport re\n\nfrom nltk.data              import load\nfrom nltk.tokenize.casual   import (TweetTokenizer, casual_tokenize)\nfrom nltk.tokenize.mwe      import MWETokenizer\nfrom nltk.tokenize.punkt    import PunktSentenceTokenizer\nfrom nltk.tokenize.regexp   import (RegexpTokenizer, WhitespaceTokenizer,\n                                    BlanklineTokenizer, WordPunctTokenizer,\n                                    wordpunct_tokenize, regexp_tokenize,\n                                    blankline_tokenize)\nfrom nltk.tokenize.repp     import ReppTokenizer\nfrom nltk.tokenize.sexpr    import SExprTokenizer, sexpr_tokenize\nfrom nltk.tokenize.simple   import (SpaceTokenizer, TabTokenizer, LineTokenizer,\n                                    line_tokenize)\nfrom nltk.tokenize.texttiling import TextTilingTokenizer\nfrom nltk.tokenize.toktok   import ToktokTokenizer\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\nfrom nltk.tokenize.util     import string_span_tokenize, regexp_span_tokenize\nfrom nltk.tokenize.stanford_segmenter import StanfordSegmenter\n\ndef sent_tokenize(text, language='english'):\n    \"\"\"\n    Return a sentence-tokenized copy of *text*,\n    using NLTK's recommended sentence tokenizer\n    (currently :class:`.PunktSentenceTokenizer`\n    for the specified language).\n\n    :param text: text to split into sentences\n    :param language: the model name in the Punkt corpus\n    \"\"\"\n    tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))\n    return tokenizer.tokenize(text)\n\n_treebank_word_tokenizer = TreebankWordTokenizer()\n\n\nimproved_open_quote_regex = re.compile(u'([\u00ab\u201c\u2018\u201e]|[`]+)', re.U)\nimproved_close_quote_regex = re.compile(u'([\u00bb\u201d\u2019])', re.U)\nimproved_punct_regex = re.compile(r'([^\\.])(\\.)([\\]\\)}>\"\\'' u'\u00bb\u201d\u2019 ' r']*)\\s*$', re.U)\n_treebank_word_tokenizer.STARTING_QUOTES.insert(0, (improved_open_quote_regex, r' \\1 '))\n_treebank_word_tokenizer.ENDING_QUOTES.insert(0, (improved_close_quote_regex, r' \\1 '))\n_treebank_word_tokenizer.PUNCTUATION.insert(0, (improved_punct_regex, r'\\1 \\2 \\3 '))\n\n\ndef word_tokenize(text, language='english', preserve_line=False):\n    \"\"\"\n    Return a tokenized copy of *text*,\n    using NLTK's recommended word tokenizer\n    (currently an improved :class:`.TreebankWordTokenizer`\n    along with :class:`.PunktSentenceTokenizer`\n    for the specified language).\n\n    :param text: text to split into words\n    :type text: str\n    :param language: the model name in the Punkt corpus\n    :type language: str\n    :param preserve_line: An option to keep the preserve the sentence and not sentence tokenize it.\n    :type preserver_line: bool\n    \"\"\"\n    sentences = [text] if preserve_line else sent_tokenize(text, language)\n    return [token for sent in sentences\n            for token in _treebank_word_tokenizer.tokenize(sent)]\n", 1], "nltk\\toolbox": [".py", "\nfrom __future__ import print_function\n\nimport os, re, codecs\nfrom xml.etree.ElementTree import ElementTree, TreeBuilder, Element, SubElement\n\nfrom six import u\n\nfrom nltk.compat import StringIO, PY3\nfrom nltk.data import PathPointer, ZipFilePathPointer, find\n\n\nclass StandardFormat(object):\n    def __init__(self, filename=None, encoding=None):\n        self._encoding = encoding\n        if filename is not None:\n            self.open(filename)\n\n    def open(self, sfm_file):\n        if isinstance(sfm_file, PathPointer):\n            self._file = sfm_file.open(self._encoding)\n        else:\n            self._file = codecs.open(sfm_file, 'rU', self._encoding)\n\n    def open_string(self, s):\n        self._file = StringIO(s)\n\n    def raw_fields(self):\n        join_string = '\\n'\n        line_regexp = r'^%s(?:\\\\(\\S+)\\s*)?(.*)$'\n        first_line_pat = re.compile(line_regexp % '(?:\\xef\\xbb\\xbf)?')\n        line_pat = re.compile(line_regexp % '')\n        file_iter = iter(self._file)\n        line = next(file_iter)\n        mobj = re.match(first_line_pat, line)\n        mkr, line_value = mobj.groups()\n        value_lines = [line_value,]\n        self.line_num = 0\n        for line in file_iter:\n            self.line_num += 1\n            mobj = re.match(line_pat, line)\n            line_mkr, line_value = mobj.groups()\n            if line_mkr:\n                yield (mkr, join_string.join(value_lines))\n                mkr = line_mkr\n                value_lines = [line_value,]\n            else:\n                value_lines.append(line_value)\n        self.line_num += 1\n        yield (mkr, join_string.join(value_lines))\n\n    def fields(self, strip=True, unwrap=True, encoding=None, errors='strict', unicode_fields=None):\n        if encoding is None and unicode_fields is not None:\n            raise ValueError('unicode_fields is set but not encoding.')\n        unwrap_pat = re.compile(r'\\n+')\n        for mkr, val in self.raw_fields():\n            if encoding and not PY3: # kludge - already decoded in PY3?\n                if unicode_fields is not None and mkr in unicode_fields:\n                    val = val.decode('utf8', errors)\n                else:\n                    val = val.decode(encoding, errors)\n                mkr = mkr.decode(encoding, errors)\n            if unwrap:\n                val = unwrap_pat.sub(' ', val)\n            if strip:\n                val = val.rstrip()\n            yield (mkr, val)\n\n    def close(self):\n        self._file.close()\n        try:\n            del self.line_num\n        except AttributeError:\n            pass\n\nclass ToolboxData(StandardFormat):\n    def parse(self, grammar=None,  **kwargs):\n        if grammar:\n            return self._chunk_parse(grammar=grammar,  **kwargs)\n        else:\n            return self._record_parse(**kwargs)\n\n    def _record_parse(self, key=None, **kwargs):\n        builder = TreeBuilder()\n        builder.start('toolbox_data', {})\n        builder.start('header', {})\n        in_records = False\n        for mkr, value in self.fields(**kwargs):\n            if key is None and not in_records and mkr[0] != '_':\n                key = mkr\n            if mkr == key:\n                if in_records:\n                    builder.end('record')\n                else:\n                    builder.end('header')\n                    in_records = True\n                builder.start('record', {})\n            builder.start(mkr, {})\n            builder.data(value)\n            builder.end(mkr)\n        if in_records:\n            builder.end('record')\n        else:\n            builder.end('header')\n        builder.end('toolbox_data')\n        return builder.close()\n\n    def _tree2etree(self, parent):\n        from nltk.tree import Tree\n\n        root = Element(parent.label())\n        for child in parent:\n            if isinstance(child, Tree):\n                root.append(self._tree2etree(child))\n            else:\n                text, tag = child\n                e = SubElement(root, tag)\n                e.text = text\n        return root\n\n    def _chunk_parse(self, grammar=None, root_label='record', trace=0, **kwargs):\n        from nltk import chunk\n        from nltk.tree import Tree\n\n        cp = chunk.RegexpParser(grammar, root_label=root_label, trace=trace)\n        db = self.parse(**kwargs)\n        tb_etree = Element('toolbox_data')\n        header = db.find('header')\n        tb_etree.append(header)\n        for record in db.findall('record'):\n            parsed = cp.parse([(elem.text, elem.tag) for elem in record])\n            tb_etree.append(self._tree2etree(parsed))\n        return tb_etree\n\n_is_value = re.compile(r\"\\S\")\n\ndef to_sfm_string(tree, encoding=None, errors='strict', unicode_fields=None):\n    if tree.tag == 'record':\n        root = Element('toolbox_data')\n        root.append(tree)\n        tree = root\n\n    if tree.tag != 'toolbox_data':\n        raise ValueError(\"not a toolbox_data element structure\")\n    if encoding is None and unicode_fields is not None:\n        raise ValueError(\"if encoding is not specified then neither should unicode_fields\")\n    l = []\n    for rec in tree:\n        l.append('\\n')\n        for field in rec:\n            mkr = field.tag\n            value = field.text\n            if encoding is not None:\n                if unicode_fields is not None and mkr in unicode_fields:\n                    cur_encoding = 'utf8'\n                else:\n                    cur_encoding = encoding\n                if re.search(_is_value, value):\n                    l.append((u(\"\\\\%s %s\\n\") % (mkr, value)).encode(cur_encoding, errors))\n                else:\n                    l.append((u(\"\\\\%s%s\\n\") % (mkr, value)).encode(cur_encoding, errors))\n            else:\n                if re.search(_is_value, value):\n                    l.append(\"\\\\%s %s\\n\" % (mkr, value))\n                else:\n                    l.append(\"\\\\%s%s\\n\" % (mkr, value))\n    return ''.join(l[1:])\n\nclass ToolboxSettings(StandardFormat):\n\n    def __init__(self):\n        super(ToolboxSettings, self).__init__()\n\n    def parse(self, encoding=None, errors='strict', **kwargs):\n        builder = TreeBuilder()\n        for mkr, value in self.fields(encoding=encoding, errors=errors, **kwargs):\n            block=mkr[0]\n            if block in (\"+\", \"-\"):\n                mkr=mkr[1:]\n            else:\n                block=None\n            if block == \"+\":\n                builder.start(mkr, {})\n                builder.data(value)\n            elif block == '-':\n                builder.end(mkr)\n            else:\n                builder.start(mkr, {})\n                builder.data(value)\n                builder.end(mkr)\n        return builder.close()\n\ndef to_settings_string(tree, encoding=None, errors='strict', unicode_fields=None):\n    l = list()\n    _to_settings_string(tree.getroot(), l, encoding=encoding, errors=errors, unicode_fields=unicode_fields)\n    return ''.join(l)\n\ndef _to_settings_string(node, l, **kwargs):\n    tag = node.tag\n    text = node.text\n    if len(node) == 0:\n        if text:\n            l.append('\\\\%s %s\\n' % (tag, text))\n        else:\n            l.append('\\\\%s\\n' % tag)\n    else:\n        if text:\n            l.append('\\\\+%s %s\\n' % (tag, text))\n        else:\n            l.append('\\\\+%s\\n' % tag)\n        for n in node:\n            _to_settings_string(n, l, **kwargs)\n        l.append('\\\\-%s\\n' % tag)\n    return\n\ndef remove_blanks(elem):\n    out = list()\n    for child in elem:\n        remove_blanks(child)\n        if child.text or len(child) > 0:\n            out.append(child)\n    elem[:] = out\n\ndef add_default_fields(elem, default_fields):\n    for field in default_fields.get(elem.tag,  []):\n        if elem.find(field) is None:\n            SubElement(elem, field)\n    for child in elem:\n        add_default_fields(child, default_fields)\n\ndef sort_fields(elem, field_orders):\n    order_dicts = dict()\n    for field, order in field_orders.items():\n        order_dicts[field] = order_key = dict()\n        for i, subfield in enumerate(order):\n            order_key[subfield] = i\n    _sort_fields(elem, order_dicts)\n\ndef _sort_fields(elem, orders_dicts):\n    try:\n        order = orders_dicts[elem.tag]\n    except KeyError:\n        pass\n    else:\n        tmp = sorted([((order.get(child.tag, 1e9), i), child) for i, child in enumerate(elem)])\n        elem[:] = [child for key, child in tmp]\n    for child in elem:\n        if len(child):\n            _sort_fields(child, orders_dicts)\n\ndef add_blank_lines(tree, blanks_before, blanks_between):\n    try:\n        before = blanks_before[tree.tag]\n        between = blanks_between[tree.tag]\n    except KeyError:\n        for elem in tree:\n            if len(elem):\n                add_blank_lines(elem, blanks_before, blanks_between)\n    else:\n        last_elem = None\n        for elem in tree:\n            tag = elem.tag\n            if last_elem is not None and last_elem.tag != tag:\n                if tag in before and last_elem is not None:\n                    e = last_elem.getiterator()[-1]\n                    e.text = (e.text or \"\") + \"\\n\"\n            else:\n                if tag in between:\n                    e = last_elem.getiterator()[-1]\n                    e.text = (e.text or \"\") + \"\\n\"\n            if len(elem):\n                add_blank_lines(elem, blanks_before, blanks_between)\n            last_elem = elem\n\ndef demo():\n    from itertools import islice\n\n    file_path = find('corpora/toolbox/rotokas.dic')\n    lexicon = ToolboxData(file_path).parse()\n    print('first field in fourth record:')\n    print(lexicon[3][0].tag)\n    print(lexicon[3][0].text)\n\n    print('\\nfields in sequential order:')\n    for field in islice(lexicon.find('record'), 10):\n        print(field.tag, field.text)\n\n    print('\\nlx fields:')\n    for field in islice(lexicon.findall('record/lx'), 10):\n        print(field.text)\n\n    settings = ToolboxSettings()\n    file_path = find('corpora/toolbox/MDF/MDF_AltH.typ')\n    settings.open(file_path)\n    tree = settings.parse(unwrap=False, encoding='cp1252')\n    print(tree.find('expset/expMDF/rtfPageSetup/paperSize').text)\n    settings_tree = ElementTree(tree)\n    print(to_settings_string(settings_tree).encode('utf8'))\n\nif __name__ == '__main__':\n    demo()\n"], "nltk\\translate\\api": [".py", "\nfrom __future__ import print_function, unicode_literals\nimport subprocess\nfrom collections import namedtuple\n\nfrom nltk.compat import python_2_unicode_compatible\n\n\n@python_2_unicode_compatible\nclass AlignedSent(object):\n\n    def __init__(self, words, mots, alignment=None):\n        self._words = words\n        self._mots = mots\n        if alignment is None:\n            self.alignment = Alignment([])\n        else:\n            assert type(alignment) is Alignment\n            self.alignment = alignment\n\n    @property\n    def words(self):\n        return self._words\n\n    @property\n    def mots(self):\n        return self._mots\n\n    def _get_alignment(self):\n        return self._alignment\n\n    def _set_alignment(self, alignment):\n        _check_alignment(len(self.words), len(self.mots), alignment)\n        self._alignment = alignment\n\n    alignment = property(_get_alignment, _set_alignment)\n\n    def __repr__(self):\n        words = \"[%s]\" % (\", \".join(\"'%s'\" % w for w in self._words))\n        mots = \"[%s]\" % (\", \".join(\"'%s'\" % w for w in self._mots))\n\n        return \"AlignedSent(%s, %s, %r)\" % (words, mots, self._alignment)\n\n    def _to_dot(self):\n        s = 'graph align {\\n'\n        s += 'node[shape=plaintext]\\n'\n\n        for w in self._words:\n            s += '\"%s_source\" [label=\"%s\"] \\n' % (w, w)\n\n        for w in self._mots:\n            s += '\"%s_target\" [label=\"%s\"] \\n' % (w, w)\n\n        for u, v in self._alignment:\n            s += '\"%s_source\" -- \"%s_target\" \\n' % (self._words[u], self._mots[v])\n\n        for i in range(len(self._words) - 1):\n            s += '\"%s_source\" -- \"%s_source\" [style=invis]\\n' % (self._words[i], self._words[i + 1])\n\n        for i in range(len(self._mots) - 1):\n            s += '\"%s_target\" -- \"%s_target\" [style=invis]\\n' % (self._mots[i], self._mots[i + 1])\n\n        s += '{rank = same; %s}\\n' % (' '.join('\"%s_source\"' % w for w in self._words))\n        s += '{rank = same; %s}\\n' % (' '.join('\"%s_target\"' % w for w in self._mots))\n\n        s += '}'\n\n        return s\n\n    def _repr_svg_(self):\n        dot_string = self._to_dot().encode('utf8')\n        output_format = 'svg'\n        try:\n            process = subprocess.Popen(['dot', '-T%s' % output_format], stdin=subprocess.PIPE,\n                                       stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        except OSError:\n            raise Exception('Cannot find the dot binary from Graphviz package')\n        out, err = process.communicate(dot_string)\n\n        return out.decode('utf8')\n\n    def __str__(self):\n        source = \" \".join(self._words)[:20] + \"...\"\n        target = \" \".join(self._mots)[:20] + \"...\"\n        return \"<AlignedSent: '%s' -> '%s'>\" % (source, target)\n\n    def invert(self):\n        return AlignedSent(self._mots, self._words,\n                           self._alignment.invert())\n\n\n@python_2_unicode_compatible\nclass Alignment(frozenset):\n\n    def __new__(cls, pairs):\n        self = frozenset.__new__(cls, pairs)\n        self._len = (max(p[0] for p in self) if self != frozenset([]) else 0)\n        self._index = None\n        return self\n\n    @classmethod\n    def fromstring(cls, s):\n\n        return Alignment([_giza2pair(a) for a in s.split()])\n\n    def __getitem__(self, key):\n        if not self._index:\n            self._build_index()\n        return self._index.__getitem__(key)\n\n    def invert(self):\n        return Alignment(((p[1], p[0]) + p[2:]) for p in self)\n\n    def range(self, positions=None):\n        image = set()\n        if not self._index:\n            self._build_index()\n        if not positions:\n            positions = list(range(len(self._index)))\n        for p in positions:\n            image.update(f for _, f in self._index[p])\n        return sorted(image)\n\n    def __repr__(self):\n        return \"Alignment(%r)\" % sorted(self)\n\n    def __str__(self):\n        return \" \".join(\"%d-%d\" % p[:2] for p in sorted(self))\n\n    def _build_index(self):\n        self._index = [[] for _ in range(self._len + 1)]\n        for p in self:\n            self._index[p[0]].append(p)\n\n\ndef _giza2pair(pair_string):\n    i, j = pair_string.split(\"-\")\n    return int(i), int(j)\n\n\ndef _naacl2pair(pair_string):\n    i, j, p = pair_string.split(\"-\")\n    return int(i), int(j)\n\n\ndef _check_alignment(num_words, num_mots, alignment):\n\n    assert type(alignment) is Alignment\n\n    if not all(0 <= pair[0] < num_words for pair in alignment):\n        raise IndexError(\"Alignment is outside boundary of words\")\n    if not all(pair[1] is None or 0 <= pair[1] < num_mots for pair in alignment):\n        raise IndexError(\"Alignment is outside boundary of mots\")\n\n\nPhraseTableEntry = namedtuple('PhraseTableEntry', ['trg_phrase', 'log_prob'])\n\n\nclass PhraseTable(object):\n\n    def __init__(self):\n        self.src_phrases = dict()\n\n    def translations_for(self, src_phrase):\n        return self.src_phrases[src_phrase]\n\n    def add(self, src_phrase, trg_phrase, log_prob):\n        entry = PhraseTableEntry(trg_phrase=trg_phrase, log_prob=log_prob)\n        if src_phrase not in self.src_phrases:\n            self.src_phrases[src_phrase] = []\n        self.src_phrases[src_phrase].append(entry)\n        self.src_phrases[src_phrase].sort(key=lambda e: e.log_prob,\n                                          reverse=True)\n\n    def __contains__(self, src_phrase):\n        return src_phrase in self.src_phrases\n"], "nltk\\translate\\bleu_score": [".py", "\nfrom __future__ import division\n\nimport math\nimport sys\nimport fractions\nimport warnings\nfrom collections import Counter\n\nfrom nltk.util import ngrams\n\ntry:\n    fractions.Fraction(0, 1000, _normalize=False)\n    from fractions import Fraction\nexcept TypeError:\n    from nltk.compat import Fraction\n\n\ndef sentence_bleu(references, hypothesis, weights=(0.25, 0.25, 0.25, 0.25),\n                  smoothing_function=None, auto_reweigh=False):\n    return corpus_bleu([references], [hypothesis],\n                       weights, smoothing_function, auto_reweigh)\n\n\ndef corpus_bleu(list_of_references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25),\n                smoothing_function=None, auto_reweigh=False):\n\n    p_numerators = Counter()  # Key = ngram order, and value = no. of ngram matches.\n    p_denominators = Counter()  # Key = ngram order, and value = no. of ngram in ref.\n    hyp_lengths, ref_lengths = 0, 0\n\n    assert len(list_of_references) == len(hypotheses), \"The number of hypotheses and their reference(s) should be the \" \\\n                                                       \"same \"\n\n    for references, hypothesis in zip(list_of_references, hypotheses):\n        for i, _ in enumerate(weights, start=1):\n            p_i = modified_precision(references, hypothesis, i)\n            p_numerators[i] += p_i.numerator\n            p_denominators[i] += p_i.denominator\n\n        hyp_len = len(hypothesis)\n        hyp_lengths += hyp_len\n        ref_lengths += closest_ref_length(references, hyp_len)\n\n    bp = brevity_penalty(ref_lengths, hyp_lengths)\n\n    if auto_reweigh:\n        if hyp_lengths < 4 and weights == (0.25, 0.25, 0.25, 0.25):\n            weights = (1 / hyp_lengths,) * hyp_lengths\n\n    p_n = [Fraction(p_numerators[i], p_denominators[i], _normalize=False)\n           for i, _ in enumerate(weights, start=1)]\n\n    if p_numerators[1] == 0:\n        return 0\n\n    if not smoothing_function:\n        smoothing_function = SmoothingFunction().method0\n    p_n = smoothing_function(p_n, references=references, hypothesis=hypothesis,\n                             hyp_len=hyp_len)\n    s = (w_i * math.log(p_i) for w_i, p_i in zip(weights, p_n))\n    s =  bp * math.exp(math.fsum(s))\n    return s\n\n\ndef modified_precision(references, hypothesis, n):\n    counts = Counter(ngrams(hypothesis, n)) if len(hypothesis) >= n else Counter()\n    max_counts = {}\n    for reference in references:\n        reference_counts = Counter(ngrams(reference, n)) if len(reference) >= n else Counter()\n        for ngram in counts:\n            max_counts[ngram] = max(max_counts.get(ngram, 0),\n                                    reference_counts[ngram])\n\n    clipped_counts = {ngram: min(count, max_counts[ngram])\n                      for ngram, count in counts.items()}\n\n    numerator = sum(clipped_counts.values())\n    denominator = max(1, sum(counts.values()))\n\n    return Fraction(numerator, denominator, _normalize=False)\n\n\ndef closest_ref_length(references, hyp_len):\n    ref_lens = (len(reference) for reference in references)\n    closest_ref_len = min(ref_lens, key=lambda ref_len:\n    (abs(ref_len - hyp_len), ref_len))\n    return closest_ref_len\n\n\ndef brevity_penalty(closest_ref_len, hyp_len):\n    if hyp_len > closest_ref_len:\n        return 1\n    elif hyp_len == 0:\n        return 0\n    else:\n        return math.exp(1 - closest_ref_len / hyp_len)\n\n\nclass SmoothingFunction:\n\n    def __init__(self, epsilon=0.1, alpha=5, k=5):\n        self.epsilon = epsilon\n        self.alpha = alpha\n        self.k = k\n\n    def method0(self, p_n, *args, **kwargs):\n        p_n_new = []\n        for i, p_i in enumerate(p_n):\n            if p_i.numerator != 0:\n                p_n_new.append(p_i)\n            else:\n                _msg = str(\"\\nThe hypothesis contains 0 counts of {}-gram overlaps.\\n\"\n                           \"Therefore the BLEU score evaluates to 0, independently of\\n\"\n                           \"how many N-gram overlaps of lower order it contains.\\n\"\n                           \"Consider using lower n-gram order or use \"\n                           \"SmoothingFunction()\").format(i+1)\n                warnings.warn(_msg)\n                p_n_new.append(sys.float_info.min)\n        return p_n_new\n\n    def method1(self, p_n, *args, **kwargs):\n        return [(p_i.numerator + self.epsilon) / p_i.denominator\n                if p_i.numerator == 0 else p_i for p_i in p_n]\n\n    def method2(self, p_n, *args, **kwargs):\n        return [Fraction(p_i.numerator + 1, p_i.denominator + 1, _normalize=False) for p_i in p_n]\n\n    def method3(self, p_n, *args, **kwargs):\n        incvnt = 1  # From the mteval-v13a.pl, it's referred to as k.\n        for i, p_i in enumerate(p_n):\n            if p_i.numerator == 0:\n                p_n[i] = 1 / (2 ** incvnt * p_i.denominator)\n                incvnt += 1\n        return p_n\n\n    def method4(self, p_n, references, hypothesis, hyp_len, *args, **kwargs):\n        for i, p_i in enumerate(p_n):\n            if p_i.numerator == 0 and hyp_len != 0:\n                incvnt = i + 1 * self.k / math.log(hyp_len)  # Note that this K is different from the K from NIST.\n                p_n[i] = 1 / incvnt\n        return p_n\n\n    def method5(self, p_n, references, hypothesis, hyp_len, *args, **kwargs):\n        m = {}\n        p_n_plus1 = p_n + [modified_precision(references, hypothesis, 5)]\n        m[-1] = p_n[0] + 1\n        for i, p_i in enumerate(p_n):\n            p_n[i] = (m[i - 1] + p_i + p_n_plus1[i + 1]) / 3\n            m[i] = p_n[i]\n        return p_n\n\n    def method6(self, p_n, references, hypothesis, hyp_len, *args, **kwargs):\n        assert p_n[2], \"This smoothing method requires non-zero precision for bigrams.\"\n        for i, p_i in enumerate(p_n):\n            if i in [0, 1]:  # Skips the first 2 orders of ngrams.\n                continue\n            else:\n                pi0 = 0 if p_n[i - 2] == 0 else p_n[i - 1] ** 2 / p_n[i - 2]\n                m = p_i.numerator\n                l = sum(1 for _ in ngrams(hypothesis, i + 1))\n                p_n[i] = (m + self.alpha * pi0) / (l + self.alpha)\n        return p_n\n\n    def method7(self, p_n, references, hypothesis, hyp_len, *args, **kwargs):\n        p_n = self.method4(p_n, references, hypothesis, hyp_len)\n        p_n = self.method5(p_n, references, hypothesis, hyp_len)\n        return p_n\n"], "nltk\\translate\\chrf_score": [".py", "\nfrom __future__ import division\nfrom collections import Counter\n\nfrom nltk.util import ngrams, everygrams\n\ndef sentence_chrf(reference, hypothesis, min_len=1, max_len=6, beta=3.0):\n    return corpus_chrf([reference], [hypothesis], min_len, max_len, beta=beta)\n\n\ndef corpus_chrf(list_of_references, hypotheses, min_len=1, max_len=6, beta=3.0):\n\n    assert len(list_of_references) == len(hypotheses), \"The number of hypotheses and their references should be the same\"\n\n    for reference, hypothesis in zip(list_of_references, hypotheses):\n        if type(reference) and type(hypothesis) != str:\n            reference, hypothesis = ' '.join(reference), ' '.join(hypothesis)\n        ref_ngrams = Counter(everygrams(reference, min_len, max_len))\n        hyp_ngrams = Counter(everygrams(hypothesis, min_len, max_len))\n        overlap_ngrams = ref_ngrams & hyp_ngrams\n        tp = sum(overlap_ngrams.values()) # True positives.\n        tpfp = sum(hyp_ngrams.values()) # True positives + False positives.\n        tffn = sum(ref_ngrams.values()) # True posities + False negatives.\n\n    precision = tp / tpfp\n    recall = tp / tffn\n    factor = beta**2\n    score = (1+ factor ) * (precision * recall) / ( factor * precision + recall)\n    return score\n"], "nltk\\translate\\gale_church": [".py", "\n\n\nfrom __future__ import division\nimport math\n\ntry:\n    from scipy.stats import norm\n    from norm import logsf as norm_logsf\nexcept ImportError:\n    def erfcc(x):\n        z = abs(x)\n        t = 1 / (1 + 0.5 * z)\n        r = t * math.exp(-z * z -\n                         1.26551223 + t *\n                         (1.00002368 + t *\n                          (.37409196 + t *\n                           (.09678418 + t *\n                            (-.18628806 + t *\n                             (.27886807 + t *\n                              (-1.13520398 + t *\n                               (1.48851587 + t *\n                                (-.82215223 + t * .17087277)))))))))\n        if x >= 0.:\n            return r\n        else:\n            return 2. - r\n\n\n    def norm_cdf(x):\n        return 1 - 0.5 * erfcc(x / math.sqrt(2))\n\n\n    def norm_logsf(x):\n        try:\n            return math.log(1 - norm_cdf(x))\n        except ValueError:\n            return float('-inf')\n\n\nLOG2 = math.log(2)\n\n\nclass LanguageIndependent(object):\n\n    PRIORS = {\n        (1, 0): 0.0099,\n        (0, 1): 0.0099,\n        (1, 1): 0.89,\n        (2, 1): 0.089,\n        (1, 2): 0.089,\n        (2, 2): 0.011,\n    }\n\n    AVERAGE_CHARACTERS = 1\n    VARIANCE_CHARACTERS = 6.8\n\n\ndef trace(backlinks, source_sents_lens, target_sents_lens):\n    links = []\n    position = (len(source_sents_lens), len(target_sents_lens))\n    while position != (0, 0) and all(p >=0 for p in position):\n        try:\n            s, t = backlinks[position]\n        except TypeError:\n            position = (position[0]-1 , position[1]-1)\n            continue\n        for i in range(s):\n            for j in range(t):\n                links.append((position[0] - i - 1, position[1] - j - 1))\n        position = (position[0] - s, position[1] - t)\n\n    return links[::-1]\n\n\ndef align_log_prob(i, j, source_sents, target_sents, alignment, params):\n    l_s = sum(source_sents[i - offset - 1] for offset in range(alignment[0]))\n    l_t = sum(target_sents[j - offset - 1] for offset in range(alignment[1]))\n    try:\n        m = (l_s + l_t / params.AVERAGE_CHARACTERS) / 2\n        delta = (l_s * params.AVERAGE_CHARACTERS - l_t) / math.sqrt(m * params.VARIANCE_CHARACTERS)\n    except ZeroDivisionError:\n        return float('-inf')\n\n    return - (LOG2 + norm_logsf(abs(delta)) + math.log(params.PRIORS[alignment]))\n\n\ndef align_blocks(source_sents_lens, target_sents_lens, params = LanguageIndependent):\n\n    alignment_types = list(params.PRIORS.keys())\n\n    D = [[]]\n\n    backlinks = {}\n\n    for i in range(len(source_sents_lens) + 1): \n        for j in range(len(target_sents_lens) + 1):\n            min_dist = float('inf')\n            min_align = None\n            for a in alignment_types:\n                prev_i = - 1 - a[0]\n                prev_j = j - a[1]\n                if prev_i < -len(D) or prev_j < 0:\n                    continue\n                p = D[prev_i][prev_j] + align_log_prob(i, j, source_sents_lens, \n                                                       target_sents_lens, a, params)\n                if p < min_dist:\n                    min_dist = p\n                    min_align = a\n\n            if min_dist == float('inf'):\n                min_dist = 0\n\n            backlinks[(i, j)] = min_align\n            D[-1].append(min_dist)\n\n        if len(D) > 2:\n            D.pop(0)\n        D.append([])\n    \n    return trace(backlinks, source_sents_lens, target_sents_lens)\n\n\ndef align_texts(source_blocks, target_blocks, params = LanguageIndependent):\n    if len(source_blocks) != len(target_blocks):\n        raise ValueError(\"Source and target texts do not have the same number of blocks.\")\n    \n    return [align_blocks(source_block, target_block, params) \n            for source_block, target_block in zip(source_blocks, target_blocks)]\n\n\n\ndef split_at(it, split_value):\n    def _chunk_iterator(first):\n        v = first\n        while v != split_value:\n            yield v\n            v = it.next()\n    \n    while True:\n        yield _chunk_iterator(it.next())\n        \n\ndef parse_token_stream(stream, soft_delimiter, hard_delimiter):\n    return [\n        [sum(len(token) for token in sentence_it) \n         for sentence_it in split_at(block_it, soft_delimiter)]\n        for block_it in split_at(stream, hard_delimiter)]\n\n\n\n\n\n\n"], "nltk\\translate\\gdfa": [".py", "\nfrom collections import defaultdict\n\ndef grow_diag_final_and(srclen, trglen, e2f, f2e):\n\n    e2f = [tuple(map(int,a.split('-'))) for a in e2f.split()]\n    f2e = [tuple(map(int,a.split('-'))) for a in f2e.split()]\n    \n    neighbors = [(-1,0),(0,-1),(1,0),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n    alignment = set(e2f).intersection(set(f2e)) # Find the intersection.\n    union = set(e2f).union(set(f2e))\n    \n    aligned = defaultdict(set)\n    for i,j in alignment:\n        aligned['e'].add(i)\n        aligned['f'].add(j)\n    \n    def grow_diag():\n        prev_len = len(alignment) - 1\n        while prev_len < len(alignment):\n            no_new_points = True\n            for e in range(srclen):\n                for f in range(trglen): \n                    if (e,f) in alignment:\n                        for neighbor in neighbors:\n                            neighbor = tuple(i+j for i,j in zip((e,f),neighbor))\n                            e_new, f_new = neighbor\n                            if (e_new not in aligned and f_new not in aligned)\\\n                            and neighbor in union:\n                                alignment.add(neighbor)\n                                aligned['e'].add(e_new); aligned['f'].add(f_new)\n                                prev_len+=1\n                                no_new_points = False\n            if no_new_points:\n                break\n\n                                                                    \n    def final_and(a):\n        for e_new in range(srclen):\n            for f_new in range(trglen):\n                if (e_new not in aligned\n                    and f_new not in aligned\n                    and (e_new, f_new) in union):\n                    alignment.add((e_new, f_new))\n                    aligned['e'].add(e_new); aligned['f'].add(f_new)\n\n    \n    grow_diag()\n    final_and(e2f)\n    final_and(f2e)\n    return sorted(alignment)\n"], "nltk\\translate\\gleu_score": [".py", "\nfrom __future__ import division\nfrom collections import Counter\n\nfrom nltk.util import ngrams, everygrams\n\n\ndef sentence_gleu(references, hypothesis, min_len=1, max_len=4):\n    return corpus_gleu(\n        [references],\n        [hypothesis],\n        min_len=min_len,\n        max_len=max_len\n    )\n\ndef corpus_gleu(list_of_references, hypotheses, min_len=1, max_len=4):\n    assert len(list_of_references) == len(hypotheses), \"The number of hypotheses and their reference(s) should be the same\"\n\n    corpus_n_match = 0\n    corpus_n_all = 0\n\n    for references, hypothesis in zip(list_of_references, hypotheses):\n        hyp_ngrams = Counter(everygrams(hypothesis, min_len, max_len))\n        tpfp = sum(hyp_ngrams.values())  # True positives + False positives.\n        \n        hyp_counts = []\n        for reference in references:\n            ref_ngrams = Counter(everygrams(reference, min_len, max_len))\n            tpfn = sum(ref_ngrams.values())  # True positives + False negatives.\n\n            overlap_ngrams = ref_ngrams & hyp_ngrams\n            tp = sum(overlap_ngrams.values())  # True positives.\n\n            n_all = max(tpfp, tpfn)\n\n            if n_all > 0:\n                hyp_counts.append((tp, n_all))\n\n        if hyp_counts:\n            n_match, n_all = max(hyp_counts, key=lambda hc: hc[0]/hc[1])\n            corpus_n_match += n_match\n            corpus_n_all += n_all\n\n    if corpus_n_all == 0:\n        gleu_score = 0.0\n    else:\n        gleu_score = corpus_n_match / corpus_n_all\n\n    return gleu_score\n"], "nltk\\translate\\ibm1": [".py", "\n\nfrom __future__ import division\nfrom collections import defaultdict\nfrom nltk.translate import AlignedSent\nfrom nltk.translate import Alignment\nfrom nltk.translate import IBMModel\nfrom nltk.translate.ibm_model import Counts\nimport warnings\n\n\nclass IBMModel1(IBMModel):\n\n    def __init__(self, sentence_aligned_corpus, iterations,\n                 probability_tables=None):\n        super(IBMModel1, self).__init__(sentence_aligned_corpus)\n\n        if probability_tables is None:\n            self.set_uniform_probabilities(sentence_aligned_corpus)\n        else:\n            self.translation_table = probability_tables['translation_table']\n\n        for n in range(0, iterations):\n            self.train(sentence_aligned_corpus)\n\n        self.align_all(sentence_aligned_corpus)\n\n    def set_uniform_probabilities(self, sentence_aligned_corpus):\n        initial_prob = 1 / len(self.trg_vocab)\n        if initial_prob < IBMModel.MIN_PROB:\n            warnings.warn(\"Target language vocabulary is too large (\" +\n                          str(len(self.trg_vocab)) + \" words). \"\n                          \"Results may be less accurate.\")\n\n        for t in self.trg_vocab:\n            self.translation_table[t] = defaultdict(lambda: initial_prob)\n\n    def train(self, parallel_corpus):\n        counts = Counts()\n        for aligned_sentence in parallel_corpus:\n            trg_sentence = aligned_sentence.words\n            src_sentence = [None] + aligned_sentence.mots\n\n            total_count = self.prob_all_alignments(src_sentence, trg_sentence)\n\n            for t in trg_sentence:\n                for s in src_sentence:\n                    count = self.prob_alignment_point(s, t)\n                    normalized_count = count / total_count[t]\n                    counts.t_given_s[t][s] += normalized_count\n                    counts.any_t_given_s[s] += normalized_count\n\n        self.maximize_lexical_translation_probabilities(counts)\n\n    def prob_all_alignments(self, src_sentence, trg_sentence):\n        alignment_prob_for_t = defaultdict(lambda: 0.0)\n        for t in trg_sentence:\n            for s in src_sentence:\n                alignment_prob_for_t[t] += self.prob_alignment_point(s, t)\n        return alignment_prob_for_t\n\n    def prob_alignment_point(self, s, t):\n        return self.translation_table[t][s]\n\n    def prob_t_a_given_s(self, alignment_info):\n        prob = 1.0\n\n        for j, i in enumerate(alignment_info.alignment):\n            if j == 0:\n                continue  # skip the dummy zeroeth element\n            trg_word = alignment_info.trg_sentence[j]\n            src_word = alignment_info.src_sentence[i]\n            prob *= self.translation_table[trg_word][src_word]\n\n        return max(prob, IBMModel.MIN_PROB)\n\n    def align_all(self, parallel_corpus):\n        for sentence_pair in parallel_corpus:\n            self.align(sentence_pair)\n\n    def align(self, sentence_pair):\n        best_alignment = []\n\n        for j, trg_word in enumerate(sentence_pair.words):\n            best_prob = max(self.translation_table[trg_word][None],\n                            IBMModel.MIN_PROB)\n            best_alignment_point = None\n            for i, src_word in enumerate(sentence_pair.mots):\n                align_prob = self.translation_table[trg_word][src_word]\n                if align_prob >= best_prob:  # prefer newer word in case of tie\n                    best_prob = align_prob\n                    best_alignment_point = i\n\n            best_alignment.append((j, best_alignment_point))\n\n        sentence_pair.alignment = Alignment(best_alignment)\n"], "nltk\\translate\\ibm2": [".py", "\n\nfrom __future__ import division\nfrom collections import defaultdict\nfrom nltk.translate import AlignedSent\nfrom nltk.translate import Alignment\nfrom nltk.translate import IBMModel\nfrom nltk.translate import IBMModel1\nfrom nltk.translate.ibm_model import Counts\nimport warnings\n\n\nclass IBMModel2(IBMModel):\n\n    def __init__(self, sentence_aligned_corpus, iterations,\n                 probability_tables=None):\n        super(IBMModel2, self).__init__(sentence_aligned_corpus)\n\n        if probability_tables is None:\n            ibm1 = IBMModel1(sentence_aligned_corpus, 2 * iterations)\n            self.translation_table = ibm1.translation_table\n            self.set_uniform_probabilities(sentence_aligned_corpus)\n        else:\n            self.translation_table = probability_tables['translation_table']\n            self.alignment_table = probability_tables['alignment_table']\n\n        for n in range(0, iterations):\n            self.train(sentence_aligned_corpus)\n\n        self.align_all(sentence_aligned_corpus)\n\n    def set_uniform_probabilities(self, sentence_aligned_corpus):\n        l_m_combinations = set()\n        for aligned_sentence in sentence_aligned_corpus:\n            l = len(aligned_sentence.mots)\n            m = len(aligned_sentence.words)\n            if (l, m) not in l_m_combinations:\n                l_m_combinations.add((l, m))\n                initial_prob = 1 / (l + 1)\n                if initial_prob < IBMModel.MIN_PROB:\n                    warnings.warn(\"A source sentence is too long (\" + str(l) +\n                                  \" words). Results may be less accurate.\")\n\n                for i in range(0, l + 1):\n                    for j in range(1, m + 1):\n                        self.alignment_table[i][j][l][m] = initial_prob\n\n    def train(self, parallel_corpus):\n        counts = Model2Counts()\n        for aligned_sentence in parallel_corpus:\n            src_sentence = [None] + aligned_sentence.mots\n            trg_sentence = ['UNUSED'] + aligned_sentence.words  # 1-indexed\n            l = len(aligned_sentence.mots)\n            m = len(aligned_sentence.words)\n\n            total_count = self.prob_all_alignments(src_sentence, trg_sentence)\n\n            for j in range(1, m + 1):\n                t = trg_sentence[j]\n                for i in range(0, l + 1):\n                    s = src_sentence[i]\n                    count = self.prob_alignment_point(\n                        i, j, src_sentence, trg_sentence)\n                    normalized_count = count / total_count[t]\n\n                    counts.update_lexical_translation(normalized_count, s, t)\n                    counts.update_alignment(normalized_count, i, j, l, m)\n\n        self.maximize_lexical_translation_probabilities(counts)\n        self.maximize_alignment_probabilities(counts)\n\n    def maximize_alignment_probabilities(self, counts):\n        MIN_PROB = IBMModel.MIN_PROB\n        for i, j_s in counts.alignment.items():\n            for j, src_sentence_lengths in j_s.items():\n                for l, trg_sentence_lengths in src_sentence_lengths.items():\n                    for m in trg_sentence_lengths:\n                        estimate = (counts.alignment[i][j][l][m] /\n                                    counts.alignment_for_any_i[j][l][m])\n                        self.alignment_table[i][j][l][m] = max(estimate,\n                                                               MIN_PROB)\n\n    def prob_all_alignments(self, src_sentence, trg_sentence):\n        alignment_prob_for_t = defaultdict(lambda: 0.0)\n        for j in range(1, len(trg_sentence)):\n            t = trg_sentence[j]\n            for i in range(0, len(src_sentence)):\n                alignment_prob_for_t[t] += self.prob_alignment_point(\n                    i, j, src_sentence, trg_sentence)\n        return alignment_prob_for_t\n\n    def prob_alignment_point(self, i, j, src_sentence, trg_sentence):\n        l = len(src_sentence) - 1\n        m = len(trg_sentence) - 1\n        s = src_sentence[i]\n        t = trg_sentence[j]\n        return self.translation_table[t][s] * self.alignment_table[i][j][l][m]\n\n    def prob_t_a_given_s(self, alignment_info):\n        prob = 1.0\n        l = len(alignment_info.src_sentence) - 1\n        m = len(alignment_info.trg_sentence) - 1\n\n        for j, i in enumerate(alignment_info.alignment):\n            if j == 0:\n                continue  # skip the dummy zeroeth element\n            trg_word = alignment_info.trg_sentence[j]\n            src_word = alignment_info.src_sentence[i]\n            prob *= (self.translation_table[trg_word][src_word] *\n                     self.alignment_table[i][j][l][m])\n\n        return max(prob, IBMModel.MIN_PROB)\n\n    def align_all(self, parallel_corpus):\n        for sentence_pair in parallel_corpus:\n            self.align(sentence_pair)\n\n    def align(self, sentence_pair):\n        best_alignment = []\n\n        l = len(sentence_pair.mots)\n        m = len(sentence_pair.words)\n\n        for j, trg_word in enumerate(sentence_pair.words):\n            best_prob = (self.translation_table[trg_word][None] *\n                         self.alignment_table[0][j + 1][l][m])\n            best_prob = max(best_prob, IBMModel.MIN_PROB)\n            best_alignment_point = None\n            for i, src_word in enumerate(sentence_pair.mots):\n                align_prob = (self.translation_table[trg_word][src_word] *\n                              self.alignment_table[i + 1][j + 1][l][m])\n                if align_prob >= best_prob:\n                    best_prob = align_prob\n                    best_alignment_point = i\n\n            best_alignment.append((j, best_alignment_point))\n\n        sentence_pair.alignment = Alignment(best_alignment)\n\n\nclass Model2Counts(Counts):\n    def __init__(self):\n        super(Model2Counts, self).__init__()\n        self.alignment = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(\n                lambda: 0.0))))\n        self.alignment_for_any_i = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: 0.0)))\n\n    def update_lexical_translation(self, count, s, t):\n        self.t_given_s[t][s] += count\n        self.any_t_given_s[s] += count\n\n    def update_alignment(self, count, i, j, l, m):\n        self.alignment[i][j][l][m] += count\n        self.alignment_for_any_i[j][l][m] += count\n"], "nltk\\translate\\ibm3": [".py", "\n\nfrom __future__ import division\nfrom collections import defaultdict\nfrom math import factorial\nfrom nltk.translate import AlignedSent\nfrom nltk.translate import Alignment\nfrom nltk.translate import IBMModel\nfrom nltk.translate import IBMModel2\nfrom nltk.translate.ibm_model import Counts\nimport warnings\n\n\nclass IBMModel3(IBMModel):\n\n    def __init__(self, sentence_aligned_corpus, iterations,\n                 probability_tables=None):\n        super(IBMModel3, self).__init__(sentence_aligned_corpus)\n        self.reset_probabilities()\n\n        if probability_tables is None:\n            ibm2 = IBMModel2(sentence_aligned_corpus, iterations)\n            self.translation_table = ibm2.translation_table\n            self.alignment_table = ibm2.alignment_table\n            self.set_uniform_probabilities(sentence_aligned_corpus)\n        else:\n            self.translation_table = probability_tables['translation_table']\n            self.alignment_table = probability_tables['alignment_table']\n            self.fertility_table = probability_tables['fertility_table']\n            self.p1 = probability_tables['p1']\n            self.distortion_table = probability_tables['distortion_table']\n\n        for n in range(0, iterations):\n            self.train(sentence_aligned_corpus)\n\n    def reset_probabilities(self):\n        super(IBMModel3, self).reset_probabilities()\n        self.distortion_table = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(\n                lambda: self.MIN_PROB))))\n\n    def set_uniform_probabilities(self, sentence_aligned_corpus):\n        l_m_combinations = set()\n        for aligned_sentence in sentence_aligned_corpus:\n            l = len(aligned_sentence.mots)\n            m = len(aligned_sentence.words)\n            if (l, m) not in l_m_combinations:\n                l_m_combinations.add((l, m))\n                initial_prob = 1 / m\n                if initial_prob < IBMModel.MIN_PROB:\n                    warnings.warn(\"A target sentence is too long (\" + str(m) +\n                                  \" words). Results may be less accurate.\")\n                for j in range(1, m + 1):\n                    for i in range(0, l + 1):\n                        self.distortion_table[j][i][l][m] = initial_prob\n\n        self.fertility_table[0] = defaultdict(lambda: 0.2)\n        self.fertility_table[1] = defaultdict(lambda: 0.65)\n        self.fertility_table[2] = defaultdict(lambda: 0.1)\n        self.fertility_table[3] = defaultdict(lambda: 0.04)\n        MAX_FERTILITY = 10\n        initial_fert_prob = 0.01 / (MAX_FERTILITY - 4)\n        for phi in range(4, MAX_FERTILITY):\n            self.fertility_table[phi] = defaultdict(lambda: initial_fert_prob)\n\n        self.p1 = 0.5\n\n    def train(self, parallel_corpus):\n        counts = Model3Counts()\n        for aligned_sentence in parallel_corpus:\n            l = len(aligned_sentence.mots)\n            m = len(aligned_sentence.words)\n\n            sampled_alignments, best_alignment = self.sample(aligned_sentence)\n            aligned_sentence.alignment = Alignment(\n                best_alignment.zero_indexed_alignment())\n\n            total_count = self.prob_of_alignments(sampled_alignments)\n\n            for alignment_info in sampled_alignments:\n                count = self.prob_t_a_given_s(alignment_info)\n                normalized_count = count / total_count\n\n                for j in range(1, m + 1):\n                    counts.update_lexical_translation(\n                        normalized_count, alignment_info, j)\n                    counts.update_distortion(\n                        normalized_count, alignment_info, j, l, m)\n\n                counts.update_null_generation(normalized_count, alignment_info)\n                counts.update_fertility(normalized_count, alignment_info)\n\n        existing_alignment_table = self.alignment_table\n        self.reset_probabilities()\n        self.alignment_table = existing_alignment_table  # don't retrain\n\n        self.maximize_lexical_translation_probabilities(counts)\n        self.maximize_distortion_probabilities(counts)\n        self.maximize_fertility_probabilities(counts)\n        self.maximize_null_generation_probabilities(counts)\n\n    def maximize_distortion_probabilities(self, counts):\n        MIN_PROB = IBMModel.MIN_PROB\n        for j, i_s in counts.distortion.items():\n            for i, src_sentence_lengths in i_s.items():\n                for l, trg_sentence_lengths in src_sentence_lengths.items():\n                    for m in trg_sentence_lengths:\n                        estimate = (counts.distortion[j][i][l][m] /\n                                    counts.distortion_for_any_j[i][l][m])\n                        self.distortion_table[j][i][l][m] = max(estimate,\n                                                                MIN_PROB)\n\n    def prob_t_a_given_s(self, alignment_info):\n        src_sentence = alignment_info.src_sentence\n        trg_sentence = alignment_info.trg_sentence\n        l = len(src_sentence) - 1  # exclude NULL\n        m = len(trg_sentence) - 1\n        p1 = self.p1\n        p0 = 1 - p1\n\n        probability = 1.0\n        MIN_PROB = IBMModel.MIN_PROB\n\n        null_fertility = alignment_info.fertility_of_i(0)\n        probability *= (pow(p1, null_fertility) *\n                        pow(p0, m - 2 * null_fertility))\n        if probability < MIN_PROB:\n            return MIN_PROB\n\n        for i in range(1, null_fertility + 1):\n            probability *= (m - null_fertility - i + 1) / i\n            if probability < MIN_PROB:\n                return MIN_PROB\n\n        for i in range(1, l + 1):\n            fertility = alignment_info.fertility_of_i(i)\n            probability *= (factorial(fertility) *\n                self.fertility_table[fertility][src_sentence[i]])\n            if probability < MIN_PROB:\n                return MIN_PROB\n\n        for j in range(1, m + 1):\n            t = trg_sentence[j]\n            i = alignment_info.alignment[j]\n            s = src_sentence[i]\n\n            probability *= (self.translation_table[t][s] *\n                self.distortion_table[j][i][l][m])\n            if probability < MIN_PROB:\n                return MIN_PROB\n\n        return probability\n\n\nclass Model3Counts(Counts):\n    def __init__(self):\n        super(Model3Counts, self).__init__()\n        self.distortion = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(\n                lambda: 0.0))))\n        self.distortion_for_any_j = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: 0.0)))\n\n    def update_distortion(self, count, alignment_info, j, l, m):\n        i = alignment_info.alignment[j]\n        self.distortion[j][i][l][m] += count\n        self.distortion_for_any_j[i][l][m] += count\n"], "nltk\\translate\\ibm4": [".py", "\n\nfrom __future__ import division\nfrom collections import defaultdict\nfrom math import factorial\nfrom nltk.translate import AlignedSent\nfrom nltk.translate import Alignment\nfrom nltk.translate import IBMModel\nfrom nltk.translate import IBMModel3\nfrom nltk.translate.ibm_model import Counts\nfrom nltk.translate.ibm_model import longest_target_sentence_length\nimport warnings\n\n\nclass IBMModel4(IBMModel):\n\n    def __init__(self, sentence_aligned_corpus, iterations,\n                 source_word_classes, target_word_classes,\n                 probability_tables=None):\n        super(IBMModel4, self).__init__(sentence_aligned_corpus)\n        self.reset_probabilities()\n        self.src_classes = source_word_classes\n        self.trg_classes = target_word_classes\n\n        if probability_tables is None:\n            ibm3 = IBMModel3(sentence_aligned_corpus, iterations)\n            self.translation_table = ibm3.translation_table\n            self.alignment_table = ibm3.alignment_table\n            self.fertility_table = ibm3.fertility_table\n            self.p1 = ibm3.p1\n            self.set_uniform_probabilities(sentence_aligned_corpus)\n        else:\n            self.translation_table = probability_tables['translation_table']\n            self.alignment_table = probability_tables['alignment_table']\n            self.fertility_table = probability_tables['fertility_table']\n            self.p1 = probability_tables['p1']\n            self.head_distortion_table = probability_tables[\n                'head_distortion_table']\n            self.non_head_distortion_table = probability_tables[\n                'non_head_distortion_table']\n\n        for n in range(0, iterations):\n            self.train(sentence_aligned_corpus)\n\n    def reset_probabilities(self):\n        super(IBMModel4, self).reset_probabilities()\n        self.head_distortion_table = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: self.MIN_PROB)))\n\n        self.non_head_distortion_table = defaultdict(\n            lambda: defaultdict(lambda: self.MIN_PROB))\n\n    def set_uniform_probabilities(self, sentence_aligned_corpus):\n        max_m = longest_target_sentence_length(sentence_aligned_corpus)\n\n        if max_m <= 1:\n            initial_prob = IBMModel.MIN_PROB\n        else:\n            initial_prob = 1 / (2 * (max_m - 1))\n        if initial_prob < IBMModel.MIN_PROB:\n            warnings.warn(\"A target sentence is too long (\" + str(max_m) +\n                          \" words). Results may be less accurate.\")\n\n        for dj in range(1, max_m):\n            self.head_distortion_table[dj] = defaultdict(\n                lambda: defaultdict(lambda: initial_prob))\n            self.head_distortion_table[-dj] = defaultdict(\n                lambda: defaultdict(lambda: initial_prob))\n            self.non_head_distortion_table[dj] = defaultdict(\n                lambda: initial_prob)\n            self.non_head_distortion_table[-dj] = defaultdict(\n                lambda: initial_prob)\n\n    def train(self, parallel_corpus):\n        counts = Model4Counts()\n        for aligned_sentence in parallel_corpus:\n            m = len(aligned_sentence.words)\n\n            sampled_alignments, best_alignment = self.sample(aligned_sentence)\n            aligned_sentence.alignment = Alignment(\n                best_alignment.zero_indexed_alignment())\n\n            total_count = self.prob_of_alignments(sampled_alignments)\n\n            for alignment_info in sampled_alignments:\n                count = self.prob_t_a_given_s(alignment_info)\n                normalized_count = count / total_count\n\n                for j in range(1, m + 1):\n                    counts.update_lexical_translation(\n                        normalized_count, alignment_info, j)\n                    counts.update_distortion(\n                        normalized_count, alignment_info, j,\n                        self.src_classes, self.trg_classes)\n\n                counts.update_null_generation(normalized_count, alignment_info)\n                counts.update_fertility(normalized_count, alignment_info)\n\n        existing_alignment_table = self.alignment_table\n        self.reset_probabilities()\n        self.alignment_table = existing_alignment_table  # don't retrain\n\n        self.maximize_lexical_translation_probabilities(counts)\n        self.maximize_distortion_probabilities(counts)\n        self.maximize_fertility_probabilities(counts)\n        self.maximize_null_generation_probabilities(counts)\n\n    def maximize_distortion_probabilities(self, counts):\n        head_d_table = self.head_distortion_table\n        for dj, src_classes in counts.head_distortion.items():\n            for s_cls, trg_classes in src_classes.items():\n                for t_cls in trg_classes:\n                    estimate = (counts.head_distortion[dj][s_cls][t_cls] /\n                                counts.head_distortion_for_any_dj[s_cls][t_cls])\n                    head_d_table[dj][s_cls][t_cls] = max(estimate,\n                                                         IBMModel.MIN_PROB)\n\n        non_head_d_table = self.non_head_distortion_table\n        for dj, trg_classes in counts.non_head_distortion.items():\n            for t_cls in trg_classes:\n                estimate = (counts.non_head_distortion[dj][t_cls] /\n                            counts.non_head_distortion_for_any_dj[t_cls])\n                non_head_d_table[dj][t_cls] = max(estimate, IBMModel.MIN_PROB)\n\n    def prob_t_a_given_s(self, alignment_info):\n        return IBMModel4.model4_prob_t_a_given_s(alignment_info, self)\n\n    @staticmethod  # exposed for Model 5 to use\n    def model4_prob_t_a_given_s(alignment_info, ibm_model):\n        probability = 1.0\n        MIN_PROB = IBMModel.MIN_PROB\n\n        def null_generation_term():\n            value = 1.0\n            p1 = ibm_model.p1\n            p0 = 1 - p1\n            null_fertility = alignment_info.fertility_of_i(0)\n            m = len(alignment_info.trg_sentence) - 1\n            value *= (pow(p1, null_fertility) * pow(p0, m - 2 * null_fertility))\n            if value < MIN_PROB:\n                return MIN_PROB\n\n            for i in range(1, null_fertility + 1):\n                value *= (m - null_fertility - i + 1) / i\n            return value\n\n        def fertility_term():\n            value = 1.0\n            src_sentence = alignment_info.src_sentence\n            for i in range(1, len(src_sentence)):\n                fertility = alignment_info.fertility_of_i(i)\n                value *= (factorial(fertility) *\n                          ibm_model.fertility_table[fertility][src_sentence[i]])\n                if value < MIN_PROB:\n                    return MIN_PROB\n            return value\n\n        def lexical_translation_term(j):\n            t = alignment_info.trg_sentence[j]\n            i = alignment_info.alignment[j]\n            s = alignment_info.src_sentence[i]\n            return ibm_model.translation_table[t][s]\n\n        def distortion_term(j):\n            t = alignment_info.trg_sentence[j]\n            i = alignment_info.alignment[j]\n            if i == 0:\n                return 1.0\n            if alignment_info.is_head_word(j):\n                previous_cept = alignment_info.previous_cept(j)\n                src_class = None\n                if previous_cept is not None:\n                    previous_s = alignment_info.src_sentence[previous_cept]\n                    src_class = ibm_model.src_classes[previous_s]\n                trg_class = ibm_model.trg_classes[t]\n                dj = j - alignment_info.center_of_cept(previous_cept)\n                return ibm_model.head_distortion_table[dj][src_class][trg_class]\n\n            previous_position = alignment_info.previous_in_tablet(j)\n            trg_class = ibm_model.trg_classes[t]\n            dj = j - previous_position\n            return ibm_model.non_head_distortion_table[dj][trg_class]\n\n        probability *= null_generation_term()\n        if probability < MIN_PROB:\n            return MIN_PROB\n\n        probability *= fertility_term()\n        if probability < MIN_PROB:\n            return MIN_PROB\n\n        for j in range(1, len(alignment_info.trg_sentence)):\n            probability *= lexical_translation_term(j)\n            if probability < MIN_PROB:\n                return MIN_PROB\n\n            probability *= distortion_term(j)\n            if probability < MIN_PROB:\n                return MIN_PROB\n\n        return probability\n\n\nclass Model4Counts(Counts):\n    def __init__(self):\n        super(Model4Counts, self).__init__()\n        self.head_distortion = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: 0.0)))\n        self.head_distortion_for_any_dj = defaultdict(\n            lambda: defaultdict(lambda: 0.0))\n        self.non_head_distortion = defaultdict(\n            lambda: defaultdict(lambda: 0.0))\n        self.non_head_distortion_for_any_dj = defaultdict(lambda: 0.0)\n\n    def update_distortion(self, count, alignment_info, j,\n                          src_classes, trg_classes):\n        i = alignment_info.alignment[j]\n        t = alignment_info.trg_sentence[j]\n        if i == 0:\n            pass\n        elif alignment_info.is_head_word(j):\n            previous_cept = alignment_info.previous_cept(j)\n            if previous_cept is not None:\n                previous_src_word = alignment_info.src_sentence[previous_cept]\n                src_class = src_classes[previous_src_word]\n            else:\n                src_class = None\n            trg_class = trg_classes[t]\n            dj = j - alignment_info.center_of_cept(previous_cept)\n            self.head_distortion[dj][src_class][trg_class] += count\n            self.head_distortion_for_any_dj[src_class][trg_class] += count\n        else:\n            previous_j = alignment_info.previous_in_tablet(j)\n            trg_class = trg_classes[t]\n            dj = j - previous_j\n            self.non_head_distortion[dj][trg_class] += count\n            self.non_head_distortion_for_any_dj[trg_class] += count\n"], "nltk\\translate\\ibm5": [".py", "\n\nfrom __future__ import division\nfrom collections import defaultdict\nfrom math import factorial\nfrom nltk.translate import AlignedSent\nfrom nltk.translate import Alignment\nfrom nltk.translate import IBMModel\nfrom nltk.translate import IBMModel4\nfrom nltk.translate.ibm_model import Counts\nfrom nltk.translate.ibm_model import longest_target_sentence_length\nimport warnings\n\n\nclass IBMModel5(IBMModel):\n    MIN_SCORE_FACTOR = 0.2\n\n    def __init__(self, sentence_aligned_corpus, iterations,\n                 source_word_classes, target_word_classes,\n                 probability_tables=None):\n        super(IBMModel5, self).__init__(sentence_aligned_corpus)\n        self.reset_probabilities()\n        self.src_classes = source_word_classes\n        self.trg_classes = target_word_classes\n\n        if probability_tables is None:\n            ibm4 = IBMModel4(sentence_aligned_corpus, iterations,\n                             source_word_classes, target_word_classes)\n            self.translation_table = ibm4.translation_table\n            self.alignment_table = ibm4.alignment_table\n            self.fertility_table = ibm4.fertility_table\n            self.p1 = ibm4.p1\n            self.head_distortion_table = ibm4.head_distortion_table\n            self.non_head_distortion_table = ibm4.non_head_distortion_table\n            self.set_uniform_probabilities(sentence_aligned_corpus)\n        else:\n            self.translation_table = probability_tables['translation_table']\n            self.alignment_table = probability_tables['alignment_table']\n            self.fertility_table = probability_tables['fertility_table']\n            self.p1 = probability_tables['p1']\n            self.head_distortion_table = probability_tables[\n                'head_distortion_table']\n            self.non_head_distortion_table = probability_tables[\n                'non_head_distortion_table']\n            self.head_vacancy_table = probability_tables[\n                'head_vacancy_table']\n            self.non_head_vacancy_table = probability_tables[\n                'non_head_vacancy_table']\n\n        for n in range(0, iterations):\n            self.train(sentence_aligned_corpus)\n\n    def reset_probabilities(self):\n        super(IBMModel5, self).reset_probabilities()\n        self.head_vacancy_table = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: self.MIN_PROB)))\n\n        self.non_head_vacancy_table = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: self.MIN_PROB)))\n\n    def set_uniform_probabilities(self, sentence_aligned_corpus):\n        max_m = longest_target_sentence_length(sentence_aligned_corpus)\n\n        if max_m > 0 and (1 / (2 * max_m)) < IBMModel.MIN_PROB:\n            warnings.warn(\"A target sentence is too long (\" + str(max_m) +\n                          \" words). Results may be less accurate.\")\n\n        for max_v in range(1, max_m + 1):\n            for dv in range(1, max_m + 1):\n                initial_prob = 1 / (2 * max_v)\n                self.head_vacancy_table[dv][max_v] = defaultdict(\n                    lambda: initial_prob)\n                self.head_vacancy_table[-(dv-1)][max_v] = defaultdict(\n                    lambda: initial_prob)\n                self.non_head_vacancy_table[dv][max_v] = defaultdict(\n                    lambda: initial_prob)\n                self.non_head_vacancy_table[-(dv-1)][max_v] = defaultdict(\n                    lambda: initial_prob)\n\n    def train(self, parallel_corpus):\n        counts = Model5Counts()\n        for aligned_sentence in parallel_corpus:\n            l = len(aligned_sentence.mots)\n            m = len(aligned_sentence.words)\n\n            sampled_alignments, best_alignment = self.sample(aligned_sentence)\n            aligned_sentence.alignment = Alignment(\n                best_alignment.zero_indexed_alignment())\n\n            total_count = self.prob_of_alignments(sampled_alignments)\n\n            for alignment_info in sampled_alignments:\n                count = self.prob_t_a_given_s(alignment_info)\n                normalized_count = count / total_count\n\n                for j in range(1, m + 1):\n                    counts.update_lexical_translation(\n                        normalized_count, alignment_info, j)\n\n                slots = Slots(m)\n                for i in range(1, l + 1):\n                    counts.update_vacancy(\n                        normalized_count, alignment_info, i,\n                        self.trg_classes, slots)\n\n                counts.update_null_generation(normalized_count, alignment_info)\n                counts.update_fertility(normalized_count, alignment_info)\n\n        existing_alignment_table = self.alignment_table\n        self.reset_probabilities()\n        self.alignment_table = existing_alignment_table  # don't retrain\n\n        self.maximize_lexical_translation_probabilities(counts)\n        self.maximize_vacancy_probabilities(counts)\n        self.maximize_fertility_probabilities(counts)\n        self.maximize_null_generation_probabilities(counts)\n\n    def sample(self, sentence_pair):\n        sampled_alignments, best_alignment = super(\n            IBMModel5, self).sample(sentence_pair)\n        return self.prune(sampled_alignments), best_alignment\n\n    def prune(self, alignment_infos):\n        alignments = []\n        best_score = 0\n\n        for alignment_info in alignment_infos:\n            score = IBMModel4.model4_prob_t_a_given_s(alignment_info, self)\n            best_score = max(score, best_score)\n            alignments.append((alignment_info, score))\n\n        threshold = IBMModel5.MIN_SCORE_FACTOR * best_score\n        alignments = [a[0] for a in alignments if a[1] > threshold]\n        return set(alignments)\n\n    def hillclimb(self, alignment_info, j_pegged=None):\n        alignment = alignment_info  # alias with shorter name\n        max_probability = IBMModel4.model4_prob_t_a_given_s(alignment, self)\n\n        while True:\n            old_alignment = alignment\n            for neighbor_alignment in self.neighboring(alignment, j_pegged):\n                neighbor_probability = IBMModel4.model4_prob_t_a_given_s(\n                    neighbor_alignment, self)\n\n                if neighbor_probability > max_probability:\n                    alignment = neighbor_alignment\n                    max_probability = neighbor_probability\n\n            if alignment == old_alignment:\n                break\n\n        alignment.score = max_probability\n        return alignment\n\n    def prob_t_a_given_s(self, alignment_info):\n        probability = 1.0\n        MIN_PROB = IBMModel.MIN_PROB\n        slots = Slots(len(alignment_info.trg_sentence) - 1)\n\n        def null_generation_term():\n            value = 1.0\n            p1 = self.p1\n            p0 = 1 - p1\n            null_fertility = alignment_info.fertility_of_i(0)\n            m = len(alignment_info.trg_sentence) - 1\n            value *= (pow(p1, null_fertility) * pow(p0, m - 2 * null_fertility))\n            if value < MIN_PROB:\n                return MIN_PROB\n\n            for i in range(1, null_fertility + 1):\n                value *= (m - null_fertility - i + 1) / i\n            return value\n\n        def fertility_term():\n            value = 1.0\n            src_sentence = alignment_info.src_sentence\n            for i in range(1, len(src_sentence)):\n                fertility = alignment_info.fertility_of_i(i)\n                value *= (factorial(fertility) *\n                          self.fertility_table[fertility][src_sentence[i]])\n                if value < MIN_PROB:\n                    return MIN_PROB\n            return value\n\n        def lexical_translation_term(j):\n            t = alignment_info.trg_sentence[j]\n            i = alignment_info.alignment[j]\n            s = alignment_info.src_sentence[i]\n            return self.translation_table[t][s]\n\n        def vacancy_term(i):\n            value = 1.0\n            tablet = alignment_info.cepts[i]\n            tablet_length = len(tablet)\n            total_vacancies = slots.vacancies_at(len(slots))\n\n            if tablet_length == 0:\n                return value\n\n            j = tablet[0]\n            previous_cept = alignment_info.previous_cept(j)\n            previous_center = alignment_info.center_of_cept(previous_cept)\n            dv = slots.vacancies_at(j) - slots.vacancies_at(previous_center)\n            max_v = total_vacancies - tablet_length + 1\n            trg_class = self.trg_classes[alignment_info.trg_sentence[j]]\n            value *= self.head_vacancy_table[dv][max_v][trg_class]\n            slots.occupy(j)  # mark position as occupied\n            total_vacancies -= 1\n            if value < MIN_PROB:\n                return MIN_PROB\n\n            for k in range(1, tablet_length):\n                previous_position = tablet[k - 1]\n                previous_vacancies = slots.vacancies_at(previous_position)\n                j = tablet[k]\n                dv = slots.vacancies_at(j) - previous_vacancies\n                max_v = (total_vacancies - tablet_length + k + 1 -\n                         previous_vacancies)\n                trg_class = self.trg_classes[alignment_info.trg_sentence[j]]\n                value *= self.non_head_vacancy_table[dv][max_v][trg_class]\n                slots.occupy(j)  # mark position as occupied\n                total_vacancies -= 1\n                if value < MIN_PROB:\n                    return MIN_PROB\n\n            return value\n\n        probability *= null_generation_term()\n        if probability < MIN_PROB:\n            return MIN_PROB\n\n        probability *= fertility_term()\n        if probability < MIN_PROB:\n            return MIN_PROB\n\n        for j in range(1, len(alignment_info.trg_sentence)):\n            probability *= lexical_translation_term(j)\n            if probability < MIN_PROB:\n                return MIN_PROB\n\n        for i in range(1, len(alignment_info.src_sentence)):\n            probability *= vacancy_term(i)\n            if probability < MIN_PROB:\n                return MIN_PROB\n\n        return probability\n\n    def maximize_vacancy_probabilities(self, counts):\n        MIN_PROB = IBMModel.MIN_PROB\n        head_vacancy_table = self.head_vacancy_table\n        for dv, max_vs in counts.head_vacancy.items():\n            for max_v, trg_classes in max_vs.items():\n                for t_cls in trg_classes:\n                    estimate = (counts.head_vacancy[dv][max_v][t_cls] /\n                                counts.head_vacancy_for_any_dv[max_v][t_cls])\n                    head_vacancy_table[dv][max_v][t_cls] = max(estimate,\n                                                               MIN_PROB)\n\n        non_head_vacancy_table = self.non_head_vacancy_table\n        for dv, max_vs in counts.non_head_vacancy.items():\n            for max_v, trg_classes in max_vs.items():\n                for t_cls in trg_classes:\n                    estimate = (\n                        counts.non_head_vacancy[dv][max_v][t_cls] /\n                        counts.non_head_vacancy_for_any_dv[max_v][t_cls])\n                    non_head_vacancy_table[dv][max_v][t_cls] = max(estimate,\n                                                                   MIN_PROB)\n\n\nclass Model5Counts(Counts):\n    def __init__(self):\n        super(Model5Counts, self).__init__()\n        self.head_vacancy = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: 0.0)))\n        self.head_vacancy_for_any_dv = defaultdict(\n            lambda: defaultdict(lambda: 0.0))\n        self.non_head_vacancy = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: 0.0)))\n        self.non_head_vacancy_for_any_dv = defaultdict(\n            lambda: defaultdict(lambda: 0.0))\n\n    def update_vacancy(self, count, alignment_info, i, trg_classes, slots):\n        tablet = alignment_info.cepts[i]\n        tablet_length = len(tablet)\n        total_vacancies = slots.vacancies_at(len(slots))\n\n        if tablet_length == 0:\n            return  # ignore zero fertility words\n\n        j = tablet[0]\n        previous_cept = alignment_info.previous_cept(j)\n        previous_center = alignment_info.center_of_cept(previous_cept)\n        dv = slots.vacancies_at(j) - slots.vacancies_at(previous_center)\n        max_v = total_vacancies - tablet_length + 1\n        trg_class = trg_classes[alignment_info.trg_sentence[j]]\n        self.head_vacancy[dv][max_v][trg_class] += count\n        self.head_vacancy_for_any_dv[max_v][trg_class] += count\n        slots.occupy(j)  # mark position as occupied\n        total_vacancies -= 1\n\n        for k in range(1, tablet_length):\n            previous_position = tablet[k - 1]\n            previous_vacancies = slots.vacancies_at(previous_position)\n            j = tablet[k]\n            dv = slots.vacancies_at(j) - previous_vacancies\n            max_v = (total_vacancies - tablet_length + k + 1 -\n                     previous_vacancies)\n            trg_class = trg_classes[alignment_info.trg_sentence[j]]\n            self.non_head_vacancy[dv][max_v][trg_class] += count\n            self.non_head_vacancy_for_any_dv[max_v][trg_class] += count\n            slots.occupy(j)  # mark position as occupied\n            total_vacancies -= 1\n\n\nclass Slots(object):\n    def __init__(self, target_sentence_length):\n        self._slots = [False] * (target_sentence_length + 1)  # 1-indexed\n\n    def occupy(self, position):\n        self._slots[position] = True\n\n    def vacancies_at(self, position):\n        vacancies = 0\n        for k in range(1, position + 1):\n            if not self._slots[k]:\n                vacancies += 1\n        return vacancies\n\n    def __len__(self):\n        return len(self._slots) - 1  # exclude dummy zeroeth element\n"], "nltk\\translate\\ibm_model": [".py", "\nfrom __future__ import division\nfrom bisect import insort_left\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom math import ceil\n\n\ndef longest_target_sentence_length(sentence_aligned_corpus):\n    max_m = 0\n    for aligned_sentence in sentence_aligned_corpus:\n        m = len(aligned_sentence.words)\n        max_m = max(m, max_m)\n    return max_m\n\n\nclass IBMModel(object):\n    MIN_PROB = 1.0e-12  # GIZA++ is more liberal and uses 1.0e-7\n\n    def __init__(self, sentence_aligned_corpus):\n        self.init_vocab(sentence_aligned_corpus)\n        self.reset_probabilities()\n\n    def reset_probabilities(self):\n        self.translation_table = defaultdict(\n            lambda: defaultdict(lambda: IBMModel.MIN_PROB))\n\n        self.alignment_table = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(\n                lambda: IBMModel.MIN_PROB))))\n\n        self.fertility_table = defaultdict(\n            lambda: defaultdict(lambda: self.MIN_PROB))\n\n        self.p1 = 0.5\n\n    def set_uniform_probabilities(self, sentence_aligned_corpus):\n        pass\n\n    def init_vocab(self, sentence_aligned_corpus):\n        src_vocab = set()\n        trg_vocab = set()\n        for aligned_sentence in sentence_aligned_corpus:\n            trg_vocab.update(aligned_sentence.words)\n            src_vocab.update(aligned_sentence.mots)\n        src_vocab.add(None)\n\n        self.src_vocab = src_vocab\n\n        self.trg_vocab = trg_vocab\n\n    def sample(self, sentence_pair):\n        sampled_alignments = set()\n        l = len(sentence_pair.mots)\n        m = len(sentence_pair.words)\n\n        initial_alignment = self.best_model2_alignment(sentence_pair)\n        potential_alignment = self.hillclimb(initial_alignment)\n        sampled_alignments.update(self.neighboring(potential_alignment))\n        best_alignment = potential_alignment\n\n        for j in range(1, m + 1):\n            for i in range(0, l + 1):\n                initial_alignment = self.best_model2_alignment(\n                    sentence_pair, j, i)\n                potential_alignment = self.hillclimb(initial_alignment, j)\n                neighbors = self.neighboring(potential_alignment, j)\n                sampled_alignments.update(neighbors)\n                if potential_alignment.score > best_alignment.score:\n                    best_alignment = potential_alignment\n\n        return sampled_alignments, best_alignment\n\n    def best_model2_alignment(self, sentence_pair, j_pegged=None, i_pegged=0):\n        src_sentence = [None] + sentence_pair.mots\n        trg_sentence = ['UNUSED'] + sentence_pair.words  # 1-indexed\n\n        l = len(src_sentence) - 1  # exclude NULL\n        m = len(trg_sentence) - 1\n\n        alignment = [0] * (m + 1)  # init all alignments to NULL\n        cepts = [[] for i in range((l + 1))]  # init all cepts to empty list\n\n        for j in range(1, m + 1):\n            if j == j_pegged:\n                best_i = i_pegged\n            else:\n                best_i = 0\n                max_alignment_prob = IBMModel.MIN_PROB\n                t = trg_sentence[j]\n\n                for i in range(0, l + 1):\n                    s = src_sentence[i]\n                    alignment_prob = (self.translation_table[t][s] *\n                                      self.alignment_table[i][j][l][m])\n\n                    if alignment_prob >= max_alignment_prob:\n                        max_alignment_prob = alignment_prob\n                        best_i = i\n\n            alignment[j] = best_i\n            cepts[best_i].append(j)\n\n        return AlignmentInfo(tuple(alignment), tuple(src_sentence),\n                             tuple(trg_sentence), cepts)\n\n    def hillclimb(self, alignment_info, j_pegged=None):\n        alignment = alignment_info  # alias with shorter name\n        max_probability = self.prob_t_a_given_s(alignment)\n\n        while True:\n            old_alignment = alignment\n            for neighbor_alignment in self.neighboring(alignment, j_pegged):\n                neighbor_probability = self.prob_t_a_given_s(neighbor_alignment)\n\n                if neighbor_probability > max_probability:\n                    alignment = neighbor_alignment\n                    max_probability = neighbor_probability\n\n            if alignment == old_alignment:\n                break\n\n        alignment.score = max_probability\n        return alignment\n\n    def neighboring(self, alignment_info, j_pegged=None):\n        neighbors = set()\n\n        l = len(alignment_info.src_sentence) - 1  # exclude NULL\n        m = len(alignment_info.trg_sentence) - 1\n        original_alignment = alignment_info.alignment\n        original_cepts = alignment_info.cepts\n\n        for j in range(1, m + 1):\n            if j != j_pegged:\n                for i in range(0, l + 1):\n                    new_alignment = list(original_alignment)\n                    new_cepts = deepcopy(original_cepts)\n                    old_i = original_alignment[j]\n\n                    new_alignment[j] = i\n\n                    insort_left(new_cepts[i], j)\n                    new_cepts[old_i].remove(j)\n\n                    new_alignment_info = AlignmentInfo(\n                        tuple(new_alignment), alignment_info.src_sentence,\n                        alignment_info.trg_sentence, new_cepts)\n                    neighbors.add(new_alignment_info)\n\n        for j in range(1, m + 1):\n            if j != j_pegged:\n                for other_j in range(1, m + 1):\n                    if other_j != j_pegged and other_j != j:\n                        new_alignment = list(original_alignment)\n                        new_cepts = deepcopy(original_cepts)\n                        other_i = original_alignment[other_j]\n                        i = original_alignment[j]\n\n                        new_alignment[j] = other_i\n                        new_alignment[other_j] = i\n\n                        new_cepts[other_i].remove(other_j)\n                        insort_left(new_cepts[other_i], j)\n                        new_cepts[i].remove(j)\n                        insort_left(new_cepts[i], other_j)\n\n                        new_alignment_info = AlignmentInfo(\n                            tuple(new_alignment), alignment_info.src_sentence,\n                            alignment_info.trg_sentence, new_cepts)\n                        neighbors.add(new_alignment_info)\n\n        return neighbors\n\n    def maximize_lexical_translation_probabilities(self, counts):\n        for t, src_words in counts.t_given_s.items():\n            for s in src_words:\n                estimate = counts.t_given_s[t][s] / counts.any_t_given_s[s]\n                self.translation_table[t][s] = max(estimate, IBMModel.MIN_PROB)\n\n    def maximize_fertility_probabilities(self, counts):\n        for phi, src_words in counts.fertility.items():\n            for s in src_words:\n                estimate = (counts.fertility[phi][s] /\n                            counts.fertility_for_any_phi[s])\n                self.fertility_table[phi][s] = max(estimate, IBMModel.MIN_PROB)\n\n    def maximize_null_generation_probabilities(self, counts):\n        p1_estimate = counts.p1 / (counts.p1 + counts.p0)\n        p1_estimate = max(p1_estimate, IBMModel.MIN_PROB)\n        self.p1 = min(p1_estimate, 1 - IBMModel.MIN_PROB)\n\n    def prob_of_alignments(self, alignments):\n        probability = 0\n        for alignment_info in alignments:\n            probability += self.prob_t_a_given_s(alignment_info)\n        return probability\n\n    def prob_t_a_given_s(self, alignment_info):\n        return 0.0\n\n\nclass AlignmentInfo(object):\n\n    def __init__(self, alignment, src_sentence, trg_sentence, cepts):\n        if not isinstance(alignment, tuple):\n            raise TypeError(\"The alignment must be a tuple because it is used \"\n                            \"to uniquely identify AlignmentInfo objects.\")\n\n        self.alignment = alignment\n\n        self.src_sentence = src_sentence\n\n        self.trg_sentence = trg_sentence\n\n        self.cepts = cepts\n\n        self.score = None\n\n    def fertility_of_i(self, i):\n        return len(self.cepts[i])\n\n    def is_head_word(self, j):\n        i = self.alignment[j]\n        return self.cepts[i][0] == j\n\n    def center_of_cept(self, i):\n        if i is None:\n            return 0\n\n        average_position = sum(self.cepts[i]) / len(self.cepts[i])\n        return int(ceil(average_position))\n\n    def previous_cept(self, j):\n        i = self.alignment[j]\n        if i == 0:\n            raise ValueError(\"Words aligned to NULL cannot have a previous \"\n                             \"cept because NULL has no position\")\n        previous_cept = i - 1\n        while previous_cept > 0 and self.fertility_of_i(previous_cept) == 0:\n            previous_cept -= 1\n\n        if previous_cept <= 0:\n            previous_cept = None\n        return previous_cept\n\n    def previous_in_tablet(self, j):\n        i = self.alignment[j]\n        tablet_position = self.cepts[i].index(j)\n        if tablet_position == 0:\n            return None\n        return self.cepts[i][tablet_position - 1]\n\n    def zero_indexed_alignment(self):\n        zero_indexed_alignment = []\n        for j in range(1, len(self.trg_sentence)):\n            i = self.alignment[j] - 1\n            if i < 0:\n                i = None  # alignment to NULL token\n            zero_indexed_alignment.append((j - 1, i))\n        return zero_indexed_alignment\n\n    def __eq__(self, other):\n        return self.alignment == other.alignment\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash(self.alignment)\n\n\nclass Counts(object):\n    def __init__(self):\n        self.t_given_s = defaultdict(lambda: defaultdict(lambda: 0.0))\n        self.any_t_given_s = defaultdict(lambda: 0.0)\n        self.p0 = 0.0\n        self.p1 = 0.0\n        self.fertility = defaultdict(lambda: defaultdict(lambda: 0.0))\n        self.fertility_for_any_phi = defaultdict(lambda: 0.0)\n\n    def update_lexical_translation(self, count, alignment_info, j):\n        i = alignment_info.alignment[j]\n        t = alignment_info.trg_sentence[j]\n        s = alignment_info.src_sentence[i]\n        self.t_given_s[t][s] += count\n        self.any_t_given_s[s] += count\n\n    def update_null_generation(self, count, alignment_info):\n        m = len(alignment_info.trg_sentence) - 1\n        fertility_of_null = alignment_info.fertility_of_i(0)\n        self.p1 += fertility_of_null * count\n        self.p0 += (m - 2 * fertility_of_null) * count\n\n    def update_fertility(self, count, alignment_info):\n        for i in range(0, len(alignment_info.src_sentence)):\n            s = alignment_info.src_sentence[i]\n            phi = alignment_info.fertility_of_i(i)\n            self.fertility[phi][s] += count\n            self.fertility_for_any_phi[s] += count\n"], "nltk\\translate\\metrics": [".py", "from __future__ import division\n\ndef alignment_error_rate(reference, hypothesis, possible=None):\n\n    if possible is None:\n        possible = reference\n    else:\n        assert(reference.issubset(possible)) # sanity check\n\n    return (1.0 - (len(hypothesis & reference) + len(hypothesis & possible)) /\n            float(len(hypothesis) + len(reference)))\n"], "nltk\\translate\\nist_score": [".py", "\nfrom __future__ import division\n\nimport math\nimport fractions\nfrom collections import Counter\n\nfrom nltk.util import ngrams\n\ndef sentence_nist(references, hypothesis, n=5):\n    return corpus_nist([references], [hypothesis], n)\n\ndef corpus_nist(list_of_references, hypotheses, n=5):\n    assert len(list_of_references) == len(hypotheses), \"The number of hypotheses and their reference(s) should be the same\"\n\n    ngram_freq = Counter()\n    total_reference_words = 0\n    for references in list_of_references: # For each source sent, there's a list of reference sents.\n        for reference in references:\n            for i in range(1,n+1):\n                ngram_freq.update(ngrams(reference, i))\n            total_reference_words += len(reference)\n\n    information_weights = {}\n    for _ngram in ngram_freq: # w_1 ... w_n\n        _mgram = _ngram[:-1] #  w_1 ... w_n-1\n        if _mgram and _mgram in ngram_freq:\n            numerator = ngram_freq[_mgram]\n        else:\n            numerator = total_reference_words\n        information_weights[_ngram] = math.log(numerator/ngram_freq[_ngram], 2)\n\n    nist_precision_numerator_per_ngram = Counter()\n    nist_precision_denominator_per_ngram = Counter()\n    l_ref, l_sys = 0, 0\n    for i in range(1,n+1):\n        for references, hypothesis in zip(list_of_references, hypotheses):\n            hyp_len = len(hypothesis)\n\n            nist_score_per_ref = []\n            for reference in references:\n                _ref_len = len(reference)\n                hyp_ngrams = Counter(ngrams(hypothesis, i)) if len(hypothesis) >= i else Counter()\n                ref_ngrams = Counter(ngrams(reference, i)) if len(reference) >=i else Counter()\n                ngram_overlaps = hyp_ngrams & ref_ngrams\n                _numerator = sum(information_weights[_ngram] * count\n                                for _ngram, count in ngram_overlaps.items())\n                _denominator = sum(hyp_ngrams.values())\n                _precision = 0 if _denominator == 0 else _numerator/_denominator\n                nist_score_per_ref.append((_precision, _numerator, _denominator, _ref_len))\n            precision, numerator, denominator, ref_len = max(nist_score_per_ref)\n            nist_precision_numerator_per_ngram[i] += numerator\n            nist_precision_denominator_per_ngram[i] += denominator\n            l_ref += ref_len\n            l_sys += hyp_len\n\n    nist_precision = 0\n    for i in nist_precision_numerator_per_ngram:\n        precision = nist_precision_numerator_per_ngram[i] / nist_precision_denominator_per_ngram[i]\n        nist_precision+= precision\n    return nist_precision * nist_length_penalty(l_ref, l_sys)\n\n\ndef nist_length_penalty(ref_len, hyp_len):\n    ratio = hyp_len / ref_len\n    if 0 < ratio < 1:\n        ratio_x, score_x = 1.5, 0.5\n        beta = math.log(score_x) / math.log(ratio_x)**2\n        return math.exp(beta * math.log(ratio)**2)\n    else: # ratio <= 0 or ratio >= 1\n        return max(min(ratio, 1.0), 0.0)\n"], "nltk\\translate\\phrase_based": [".py", "\ndef extract(f_start, f_end, e_start, e_end, \n            alignment, f_aligned,\n            srctext, trgtext, srclen, trglen, max_phrase_length):\n\n    if f_end < 0:  # 0-based indexing.\n        return {}\n    for e,f in alignment:\n        if ((f_start <= f <= f_end) and (e < e_start or e > e_end)):\n            return {}\n\n    phrases = set()\n    fs = f_start\n    while True:\n        fe = min(f_end, f_start + max_phrase_length - 1)\n        while True:\n            src_phrase = \" \".join(srctext[e_start:e_end+1])\n            trg_phrase = \" \".join(trgtext[fs:fe+1])\n            phrases.add(((e_start, e_end+1), (f_start, f_end+1), \n                         src_phrase, trg_phrase))\n            fe += 1\n            if fe in f_aligned or fe == trglen:\n                break\n        fs -=1 \n        if fs in f_aligned or fs < 0:\n            break\n    return phrases\n\ndef phrase_extraction(srctext, trgtext, alignment, max_phrase_length=0):\n\n    srctext = srctext.split()   # e\n    trgtext = trgtext.split()   # f\n    srclen = len(srctext)       # len(e)\n    trglen = len(trgtext)       # len(f)\n    f_aligned = [j for _,j in alignment]\n    max_phrase_length = max_phrase_length or max(srclen,trglen)\n\n    bp = set()\n\n    for e_start in range(srclen):\n        max_idx = min(srclen, e_start + max_phrase_length)\n        for e_end in range(e_start, max_idx):\n            f_start, f_end = trglen-1 , -1  #  0-based indexing\n \n            for e,f in alignment:\n                if e_start <= e <= e_end:\n                    f_start = min(f, f_start)\n                    f_end = max(f, f_end)\n            phrases = extract(f_start, f_end, e_start, e_end, \n                              alignment, f_aligned,\n                              srctext, trgtext, srclen, trglen,\n                              max_phrase_length)\n            if phrases:\n                bp.update(phrases)\n    return bp\n\n"], "nltk\\translate\\ribes_score": [".py", "from __future__ import division\nfrom itertools import islice\nimport math\n\nfrom nltk.util import ngrams, choose\n\n\ndef sentence_ribes(references, hypothesis, alpha=0.25, beta=0.10):\n    best_ribes = -1.0\n    for reference in references:\n        worder = word_rank_alignment(reference, hypothesis)\n        nkt = kendall_tau(worder)\n            \n        bp = min(1.0, math.exp(1.0 - len(reference)/len(hypothesis)))\n        \n        p1 = len(worder) / len(hypothesis)\n        \n        _ribes = nkt * (p1 ** alpha) *  (bp ** beta)\n        \n        if _ribes > best_ribes: # Keeps the best score.\n            best_ribes = _ribes\n        \n    return best_ribes\n\n\ndef corpus_ribes(list_of_references, hypotheses, alpha=0.25, beta=0.10):\n    corpus_best_ribes = 0.0\n    for references, hypothesis in zip(list_of_references, hypotheses):\n        corpus_best_ribes += sentence_ribes(references, hypothesis, alpha, beta)\n    return corpus_best_ribes / len(hypotheses)\n    \n        \ndef position_of_ngram(ngram, sentence):\n    for i,sublist in enumerate(ngrams(sentence, len(ngram))):\n        if ngram == sublist:\n            return i\n\n\ndef word_rank_alignment(reference, hypothesis, character_based=False):\n    worder = []\n    hyp_len = len(hypothesis)\n    ref_ngrams = []\n    hyp_ngrams = []\n    for n in range(1, len(reference)+1):\n        for ng in ngrams(reference, n):\n            ref_ngrams.append(ng)\n        for ng in ngrams(hypothesis, n):\n            hyp_ngrams.append(ng)\n    for i, h_word in enumerate(hypothesis):\n        if h_word not in reference:\n            continue\n        elif hypothesis.count(h_word) == reference.count(h_word) == 1:\n            worder.append(reference.index(h_word))\n        else:\n            max_window_size = max(i, hyp_len-i+1)\n            for window in range(1, max_window_size):\n                if i+window < hyp_len: # If searching the right context is possible.\n                    right_context_ngram = tuple(islice(hypothesis, i, i+window+1))\n                    num_times_in_ref = ref_ngrams.count(right_context_ngram)\n                    num_times_in_hyp = hyp_ngrams.count(right_context_ngram) \n                    if num_times_in_ref == num_times_in_hyp == 1:\n                        pos = position_of_ngram(right_context_ngram, reference)\n                        worder.append(pos)  # Add the positions of the ngram.\n                        break\n                if window <= i: # If searching the left context is possible.\n                    left_context_ngram = tuple(islice(hypothesis, i-window, i+1))\n                    num_times_in_ref = ref_ngrams.count(left_context_ngram)\n                    num_times_in_hyp = hyp_ngrams.count(left_context_ngram)\n                    if num_times_in_ref == num_times_in_hyp == 1:\n                        pos = position_of_ngram(left_context_ngram, reference)\n                        worder.append(pos+ len(left_context_ngram) -1)  \n                        break\n    return worder\n\n    \ndef find_increasing_sequences(worder):\n    items = iter(worder)\n    a, b = None, next(items, None)\n    result = [b]\n    while b is not None:\n        a, b = b, next(items, None)\n        if b is not None and a + 1 == b:\n            result.append(b)\n        else:\n            if len(result) > 1:\n                yield tuple(result)\n            result = [b]\n\n\ndef kendall_tau(worder, normalize=True):\n    worder_len = len(worder)\n    increasing_sequences = find_increasing_sequences(worder)\n    num_increasing_pairs = sum(choose(len(seq),2) for seq in increasing_sequences) \n    num_possible_pairs = choose(worder_len, 2)\n    tau = 2 * num_increasing_pairs / num_possible_pairs -1\n    if normalize: # If normalized, the tau output falls between 0.0 to 1.0\n        return (tau + 1) /2\n    else: # Otherwise, the tau outputs falls between -1.0 to +1.0\n        return tau\n\n\ndef spearman_rho(worder, normalize=True):\n    worder_len = len(worder)\n    sum_d_square = sum((wi - i)**2 for wi, i in zip(worder, range(worder_len)))\n    rho = 1 - sum_d_square / choose(worder_len+1, 3)\n    \n    if normalize: # If normalized, the rho output falls between 0.0 to 1.0\n        return (rho + 1) /2\n    else: # Otherwise, the rho outputs falls between -1.0 to +1.0\n        return rho\n"], "nltk\\translate\\stack_decoder": [".py", "\n\nimport warnings\nfrom collections import defaultdict\nfrom math import log\n\n\nclass StackDecoder(object):\n    def __init__(self, phrase_table, language_model):\n        self.phrase_table = phrase_table\n        self.language_model = language_model\n\n        self.word_penalty = 0.0\n\n        self.beam_threshold = 0.0\n\n        self.stack_size = 100\n\n        self.__distortion_factor = 0.5\n        self.__compute_log_distortion()\n\n    @property\n    def distortion_factor(self):\n        return self.__distortion_factor\n\n    @distortion_factor.setter\n    def distortion_factor(self, d):\n        self.__distortion_factor = d\n        self.__compute_log_distortion()\n\n    def __compute_log_distortion(self):\n        if self.__distortion_factor == 0.0:\n            self.__log_distortion_factor = log(1e-9)  # 1e-9 is almost zero\n        else:\n            self.__log_distortion_factor = log(self.__distortion_factor)\n\n    def translate(self, src_sentence):\n        sentence = tuple(src_sentence)  # prevent accidental modification\n        sentence_length = len(sentence)\n        stacks = [_Stack(self.stack_size, self.beam_threshold)\n                  for _ in range(0, sentence_length + 1)]\n        empty_hypothesis = _Hypothesis()\n        stacks[0].push(empty_hypothesis)\n\n        all_phrases = self.find_all_src_phrases(sentence)\n        future_score_table = self.compute_future_scores(sentence)\n        for stack in stacks:\n            for hypothesis in stack:\n                possible_expansions = StackDecoder.valid_phrases(all_phrases,\n                                                                 hypothesis)\n                for src_phrase_span in possible_expansions:\n                    src_phrase = sentence[src_phrase_span[0]:src_phrase_span[1]]\n                    for translation_option in (self.phrase_table.\n                                               translations_for(src_phrase)):\n                        raw_score = self.expansion_score(\n                            hypothesis, translation_option, src_phrase_span)\n                        new_hypothesis = _Hypothesis(\n                            raw_score=raw_score,\n                            src_phrase_span=src_phrase_span,\n                            trg_phrase=translation_option.trg_phrase,\n                            previous=hypothesis\n                        )\n                        new_hypothesis.future_score = self.future_score(\n                            new_hypothesis, future_score_table, sentence_length)\n                        total_words = new_hypothesis.total_translated_words()\n                        stacks[total_words].push(new_hypothesis)\n\n        if not stacks[sentence_length]:\n            warnings.warn('Unable to translate all words. '\n                          'The source sentence contains words not in '\n                          'the phrase table')\n            return []\n\n        best_hypothesis = stacks[sentence_length].best()\n        return best_hypothesis.translation_so_far()\n\n    def find_all_src_phrases(self, src_sentence):\n        sentence_length = len(src_sentence)\n        phrase_indices = [[] for _ in src_sentence]\n        for start in range(0, sentence_length):\n            for end in range(start + 1, sentence_length + 1):\n                potential_phrase = src_sentence[start:end]\n                if potential_phrase in self.phrase_table:\n                    phrase_indices[start].append(end)\n        return phrase_indices\n\n    def compute_future_scores(self, src_sentence):\n        scores = defaultdict(lambda: defaultdict(lambda: float('-inf')))\n        for seq_length in range(1, len(src_sentence) + 1):\n            for start in range(0, len(src_sentence) - seq_length + 1):\n                end = start + seq_length\n                phrase = src_sentence[start:end]\n                if phrase in self.phrase_table:\n                    score = self.phrase_table.translations_for(\n                        phrase)[0].log_prob  # pick best (first) translation\n                    score += self.language_model.probability(phrase)\n                    scores[start][end] = score\n\n                for mid in range(start + 1, end):\n                    combined_score = (scores[start][mid] +\n                                      scores[mid][end])\n                    if combined_score > scores[start][end]:\n                        scores[start][end] = combined_score\n        return scores\n\n    def future_score(self, hypothesis, future_score_table, sentence_length):\n        score = 0.0\n        for span in hypothesis.untranslated_spans(sentence_length):\n            score += future_score_table[span[0]][span[1]]\n        return score\n\n    def expansion_score(self, hypothesis, translation_option, src_phrase_span):\n        score = hypothesis.raw_score\n        score += translation_option.log_prob\n        score += self.language_model.probability_change(\n            hypothesis, translation_option.trg_phrase)\n        score += self.distortion_score(hypothesis, src_phrase_span)\n        score -= self.word_penalty * len(translation_option.trg_phrase)\n        return score\n\n    def distortion_score(self, hypothesis, next_src_phrase_span):\n        if not hypothesis.src_phrase_span:\n            return 0.0\n        next_src_phrase_start = next_src_phrase_span[0]\n        prev_src_phrase_end = hypothesis.src_phrase_span[1]\n        distortion_distance = next_src_phrase_start - prev_src_phrase_end\n        return abs(distortion_distance) * self.__log_distortion_factor\n\n    @staticmethod\n    def valid_phrases(all_phrases_from, hypothesis):\n        untranslated_spans = hypothesis.untranslated_spans(\n            len(all_phrases_from))\n        valid_phrases = []\n        for available_span in untranslated_spans:\n            start = available_span[0]\n            available_end = available_span[1]\n            while start < available_end:\n                for phrase_end in all_phrases_from[start]:\n                    if phrase_end > available_end:\n                        break\n                    valid_phrases.append((start, phrase_end))\n                start += 1\n        return valid_phrases\n\n\nclass _Hypothesis(object):\n    def __init__(self, raw_score=0.0, src_phrase_span=(), trg_phrase=(),\n                 previous=None, future_score=0.0):\n        self.raw_score = raw_score\n        self.src_phrase_span = src_phrase_span\n        self.trg_phrase = trg_phrase\n        self.previous = previous\n        self.future_score = future_score\n\n    def score(self):\n        return self.raw_score + self.future_score\n\n    def untranslated_spans(self, sentence_length):\n        translated_positions = self.translated_positions()\n        translated_positions.sort()\n        translated_positions.append(sentence_length)  # add sentinel position\n\n        untranslated_spans = []\n        start = 0\n        for end in translated_positions:\n            if start < end:\n                untranslated_spans.append((start, end))\n            start = end + 1\n\n        return untranslated_spans\n\n    def translated_positions(self):\n        translated_positions = []\n        current_hypothesis = self\n        while current_hypothesis.previous is not None:\n            translated_span = current_hypothesis.src_phrase_span\n            translated_positions.extend(range(translated_span[0],\n                                              translated_span[1]))\n            current_hypothesis = current_hypothesis.previous\n        return translated_positions\n\n    def total_translated_words(self):\n        return len(self.translated_positions())\n\n    def translation_so_far(self):\n        translation = []\n        self.__build_translation(self, translation)\n        return translation\n\n    def __build_translation(self, hypothesis, output):\n        if hypothesis.previous is None:\n            return\n        self.__build_translation(hypothesis.previous, output)\n        output.extend(hypothesis.trg_phrase)\n\n\nclass _Stack(object):\n    def __init__(self, max_size=100, beam_threshold=0.0):\n        self.max_size = max_size\n        self.items = []\n\n        if beam_threshold == 0.0:\n            self.__log_beam_threshold = float('-inf')\n        else:\n            self.__log_beam_threshold = log(beam_threshold)\n\n    def push(self, hypothesis):\n        self.items.append(hypothesis)\n        self.items.sort(key=lambda h: h.score(), reverse=True)\n        while len(self.items) > self.max_size:\n            self.items.pop()\n        self.threshold_prune()\n\n    def threshold_prune(self):\n        if not self.items:\n            return\n        threshold = self.items[0].score() + self.__log_beam_threshold\n        for hypothesis in reversed(self.items):\n            if hypothesis.score() < threshold:\n                self.items.pop()\n            else:\n                break\n\n    def best(self):\n        if self.items:\n            return self.items[0]\n        return None\n\n    def __iter__(self):\n        return iter(self.items)\n\n    def __contains__(self, hypothesis):\n        return hypothesis in self.items\n\n    def __bool__(self):\n        return len(self.items) != 0\n    __nonzero__=__bool__\n"], "nltk\\translate\\__init__": [".py", "\n\nfrom nltk.translate.api import AlignedSent, Alignment, PhraseTable\nfrom nltk.translate.ibm_model import IBMModel\nfrom nltk.translate.ibm1 import IBMModel1\nfrom nltk.translate.ibm2 import IBMModel2\nfrom nltk.translate.ibm3 import IBMModel3\nfrom nltk.translate.ibm4 import IBMModel4\nfrom nltk.translate.ibm5 import IBMModel5\nfrom nltk.translate.bleu_score import sentence_bleu as bleu\nfrom nltk.translate.ribes_score import sentence_ribes as ribes\nfrom nltk.translate.metrics import alignment_error_rate\nfrom nltk.translate.stack_decoder import StackDecoder\n", 1], "nltk\\tree": [".py", "\nfrom __future__ import print_function, unicode_literals\nfrom abc import ABCMeta, abstractmethod\nfrom six import add_metaclass\n\n\nimport re\n\nfrom six import string_types\n\nfrom nltk.grammar import Production, Nonterminal\nfrom nltk.probability import ProbabilisticMixIn\nfrom nltk.util import slice_bounds\nfrom nltk.compat import python_2_unicode_compatible, unicode_repr\nfrom nltk.internals import raise_unorderable_types\n\n\n@python_2_unicode_compatible\nclass Tree(list):\n    def __init__(self, node, children=None):\n        if children is None:\n            raise TypeError(\"%s: Expected a node value and child list \"\n                                % type(self).__name__)\n        elif isinstance(children, string_types):\n            raise TypeError(\"%s() argument 2 should be a list, not a \"\n                            \"string\" % type(self).__name__)\n        else:\n            list.__init__(self, children)\n            self._label = node\n\n\n    def __eq__(self, other):\n        return (self.__class__ is other.__class__ and\n                (self._label, list(self)) == (other._label, list(other)))\n\n    def __lt__(self, other):\n        if not isinstance(other, Tree):\n            return self.__class__.__name__ < other.__class__.__name__\n        elif self.__class__ is other.__class__:\n            return (self._label, list(self)) < (other._label, list(other))\n        else:\n            return self.__class__.__name__ < other.__class__.__name__\n\n    __ne__ = lambda self, other: not self == other\n    __gt__ = lambda self, other: not (self < other or self == other)\n    __le__ = lambda self, other: self < other or self == other\n    __ge__ = lambda self, other: not self < other\n\n\n    def __mul__(self, v):\n        raise TypeError('Tree does not support multiplication')\n    def __rmul__(self, v):\n        raise TypeError('Tree does not support multiplication')\n    def __add__(self, v):\n        raise TypeError('Tree does not support addition')\n    def __radd__(self, v):\n        raise TypeError('Tree does not support addition')\n\n\n    def __getitem__(self, index):\n        if isinstance(index, (int, slice)):\n            return list.__getitem__(self, index)\n        elif isinstance(index, (list, tuple)):\n            if len(index) == 0:\n                return self\n            elif len(index) == 1:\n                return self[index[0]]\n            else:\n                return self[index[0]][index[1:]]\n        else:\n            raise TypeError(\"%s indices must be integers, not %s\" %\n                            (type(self).__name__, type(index).__name__))\n\n    def __setitem__(self, index, value):\n        if isinstance(index, (int, slice)):\n            return list.__setitem__(self, index, value)\n        elif isinstance(index, (list, tuple)):\n            if len(index) == 0:\n                raise IndexError('The tree position () may not be '\n                                 'assigned to.')\n            elif len(index) == 1:\n                self[index[0]] = value\n            else:\n                self[index[0]][index[1:]] = value\n        else:\n            raise TypeError(\"%s indices must be integers, not %s\" %\n                            (type(self).__name__, type(index).__name__))\n\n    def __delitem__(self, index):\n        if isinstance(index, (int, slice)):\n            return list.__delitem__(self, index)\n        elif isinstance(index, (list, tuple)):\n            if len(index) == 0:\n                raise IndexError('The tree position () may not be deleted.')\n            elif len(index) == 1:\n                del self[index[0]]\n            else:\n                del self[index[0]][index[1:]]\n        else:\n            raise TypeError(\"%s indices must be integers, not %s\" %\n                            (type(self).__name__, type(index).__name__))\n\n\n    def _get_node(self):\n        raise NotImplementedError(\"Use label() to access a node label.\")\n    def _set_node(self, value):\n        raise NotImplementedError(\"Use set_label() method to set a node label.\")\n    node = property(_get_node, _set_node)\n\n    def label(self):\n        return self._label\n\n    def set_label(self, label):\n        self._label = label\n\n    def leaves(self):\n        leaves = []\n        for child in self:\n            if isinstance(child, Tree):\n                leaves.extend(child.leaves())\n            else:\n                leaves.append(child)\n        return leaves\n\n    def flatten(self):\n        return Tree(self.label(), self.leaves())\n\n    def height(self):\n        max_child_height = 0\n        for child in self:\n            if isinstance(child, Tree):\n                max_child_height = max(max_child_height, child.height())\n            else:\n                max_child_height = max(max_child_height, 1)\n        return 1 + max_child_height\n\n    def treepositions(self, order='preorder'):\n        positions = []\n        if order in ('preorder', 'bothorder'): positions.append( () )\n        for i, child in enumerate(self):\n            if isinstance(child, Tree):\n                childpos = child.treepositions(order)\n                positions.extend((i,)+p for p in childpos)\n            else:\n                positions.append( (i,) )\n        if order in ('postorder', 'bothorder'): positions.append( () )\n        return positions\n\n    def subtrees(self, filter=None):\n        if not filter or filter(self):\n            yield self\n        for child in self:\n            if isinstance(child, Tree):\n                for subtree in child.subtrees(filter):\n                    yield subtree\n\n    def productions(self):\n\n        if not isinstance(self._label, string_types):\n            raise TypeError('Productions can only be generated from trees having node labels that are strings')\n\n        prods = [Production(Nonterminal(self._label), _child_names(self))]\n        for child in self:\n            if isinstance(child, Tree):\n                prods += child.productions()\n        return prods\n\n    def pos(self):\n        pos = []\n        for child in self:\n            if isinstance(child, Tree):\n                pos.extend(child.pos())\n            else:\n                pos.append((child, self._label))\n        return pos\n\n    def leaf_treeposition(self, index):\n        if index < 0: raise IndexError('index must be non-negative')\n\n        stack = [(self, ())]\n        while stack:\n            value, treepos = stack.pop()\n            if not isinstance(value, Tree):\n                if index == 0: return treepos\n                else: index -= 1\n            else:\n                for i in range(len(value)-1, -1, -1):\n                    stack.append( (value[i], treepos+(i,)) )\n\n        raise IndexError('index must be less than or equal to len(self)')\n\n    def treeposition_spanning_leaves(self, start, end):\n        if end <= start:\n            raise ValueError('end must be greater than start')\n        start_treepos = self.leaf_treeposition(start)\n        end_treepos = self.leaf_treeposition(end-1)\n        for i in range(len(start_treepos)):\n            if i == len(end_treepos) or start_treepos[i] != end_treepos[i]:\n                return start_treepos[:i]\n        return start_treepos\n\n\n    def chomsky_normal_form(self, factor=\"right\", horzMarkov=None, vertMarkov=0, childChar=\"|\", parentChar=\"^\"):\n        from nltk.treetransforms import chomsky_normal_form\n        chomsky_normal_form(self, factor, horzMarkov, vertMarkov, childChar, parentChar)\n\n    def un_chomsky_normal_form(self, expandUnary = True, childChar = \"|\", parentChar = \"^\", unaryChar = \"+\"):\n        from nltk.treetransforms import un_chomsky_normal_form\n        un_chomsky_normal_form(self, expandUnary, childChar, parentChar, unaryChar)\n\n    def collapse_unary(self, collapsePOS = False, collapseRoot = False, joinChar = \"+\"):\n        from nltk.treetransforms import collapse_unary\n        collapse_unary(self, collapsePOS, collapseRoot, joinChar)\n\n\n    @classmethod\n    def convert(cls, tree):\n        if isinstance(tree, Tree):\n            children = [cls.convert(child) for child in tree]\n            return cls(tree._label, children)\n        else:\n            return tree\n\n    def copy(self, deep=False):\n        if not deep: return type(self)(self._label, self)\n        else: return type(self).convert(self)\n\n    def _frozen_class(self): return ImmutableTree\n    def freeze(self, leaf_freezer=None):\n        frozen_class = self._frozen_class()\n        if leaf_freezer is None:\n            newcopy = frozen_class.convert(self)\n        else:\n            newcopy = self.copy(deep=True)\n            for pos in newcopy.treepositions('leaves'):\n                newcopy[pos] = leaf_freezer(newcopy[pos])\n            newcopy = frozen_class.convert(newcopy)\n        hash(newcopy) # Make sure the leaves are hashable.\n        return newcopy\n\n\n    @classmethod\n    def fromstring(cls, s, brackets='()', read_node=None, read_leaf=None,\n              node_pattern=None, leaf_pattern=None,\n              remove_empty_top_bracketing=False):\n        if not isinstance(brackets, string_types) or len(brackets) != 2:\n            raise TypeError('brackets must be a length-2 string')\n        if re.search('\\s', brackets):\n            raise TypeError('whitespace brackets not allowed')\n        open_b, close_b = brackets\n        open_pattern, close_pattern = (re.escape(open_b), re.escape(close_b))\n        if node_pattern is None:\n            node_pattern = '[^\\s%s%s]+' % (open_pattern, close_pattern)\n        if leaf_pattern is None:\n            leaf_pattern = '[^\\s%s%s]+' % (open_pattern, close_pattern)\n        token_re = re.compile('%s\\s*(%s)?|%s|(%s)' % (\n            open_pattern, node_pattern, close_pattern, leaf_pattern))\n        stack = [(None, [])] # list of (node, children) tuples\n        for match in token_re.finditer(s):\n            token = match.group()\n            if token[0] == open_b:\n                if len(stack) == 1 and len(stack[0][1]) > 0:\n                    cls._parse_error(s, match, 'end-of-string')\n                label = token[1:].lstrip()\n                if read_node is not None: label = read_node(label)\n                stack.append((label, []))\n            elif token == close_b:\n                if len(stack) == 1:\n                    if len(stack[0][1]) == 0:\n                        cls._parse_error(s, match, open_b)\n                    else:\n                        cls._parse_error(s, match, 'end-of-string')\n                label, children = stack.pop()\n                stack[-1][1].append(cls(label, children))\n            else:\n                if len(stack) == 1:\n                    cls._parse_error(s, match, open_b)\n                if read_leaf is not None: token = read_leaf(token)\n                stack[-1][1].append(token)\n\n        if len(stack) > 1:\n            cls._parse_error(s, 'end-of-string', close_b)\n        elif len(stack[0][1]) == 0:\n            cls._parse_error(s, 'end-of-string', open_b)\n        else:\n            assert stack[0][0] is None\n            assert len(stack[0][1]) == 1\n        tree = stack[0][1][0]\n\n        if remove_empty_top_bracketing and tree._label == '' and len(tree) == 1:\n            tree = tree[0]\n        return tree\n\n    @classmethod\n    def _parse_error(cls, s, match, expecting):\n        if match == 'end-of-string':\n            pos, token = len(s), 'end-of-string'\n        else:\n            pos, token = match.start(), match.group()\n        msg = '%s.read(): expected %r but got %r\\n%sat index %d.' % (\n            cls.__name__, expecting, token, ' '*12, pos)\n        s = s.replace('\\n', ' ').replace('\\t', ' ')\n        offset = pos\n        if len(s) > pos+10:\n            s = s[:pos+10]+'...'\n        if pos > 10:\n            s = '...'+s[pos-10:]\n            offset = 13\n        msg += '\\n%s\"%s\"\\n%s^' % (' '*16, s, ' '*(17+offset))\n        raise ValueError(msg)\n\n\n    def draw(self):\n        from nltk.draw.tree import draw_trees\n        draw_trees(self)\n\n    def pretty_print(self, sentence=None, highlight=(), stream=None, **kwargs):\n        from nltk.treeprettyprinter import TreePrettyPrinter\n        print(TreePrettyPrinter(self, sentence, highlight).text(**kwargs),\n              file=stream)\n\n    def __repr__(self):\n        childstr = \", \".join(unicode_repr(c) for c in self)\n        return '%s(%s, [%s])' % (type(self).__name__, unicode_repr(self._label), childstr)\n\n    def _repr_png_(self):\n        import os\n        import base64\n        import subprocess\n        import tempfile\n        from nltk.draw.tree import tree_to_treesegment\n        from nltk.draw.util import CanvasFrame\n        from nltk.internals import find_binary\n        _canvas_frame = CanvasFrame()\n        widget = tree_to_treesegment(_canvas_frame.canvas(), self)\n        _canvas_frame.add_widget(widget)\n        x, y, w, h = widget.bbox()\n        _canvas_frame.canvas()['scrollregion'] = (0, 0, w, h)\n        with tempfile.NamedTemporaryFile() as file:\n            in_path = '{0:}.ps'.format(file.name)\n            out_path = '{0:}.png'.format(file.name)\n            _canvas_frame.print_to_file(in_path)\n            _canvas_frame.destroy_widget(widget)\n            subprocess.call([find_binary('gs', binary_names=['gswin32c.exe', 'gswin64c.exe'], env_vars=['PATH'], verbose=False)] +\n                            '-q -dEPSCrop -sDEVICE=png16m -r90 -dTextAlphaBits=4 -dGraphicsAlphaBits=4 -dSAFER -dBATCH -dNOPAUSE -sOutputFile={0:} {1:}'\n                            .format(out_path, in_path).split())\n            with open(out_path, 'rb') as sr:\n                res = sr.read()\n            os.remove(in_path)\n            os.remove(out_path)\n            return base64.b64encode(res).decode()\n\n    def __str__(self):\n        return self.pformat()\n\n    def pprint(self, **kwargs):\n\n        if \"stream\" in kwargs:\n            stream = kwargs[\"stream\"]\n            del kwargs[\"stream\"]\n        else:\n            stream = None\n        print(self.pformat(**kwargs), file=stream)\n\n    def pformat(self, margin=70, indent=0, nodesep='', parens='()', quotes=False):\n\n        s = self._pformat_flat(nodesep, parens, quotes)\n        if len(s) + indent < margin:\n            return s\n\n        if isinstance(self._label, string_types):\n            s = '%s%s%s' % (parens[0], self._label, nodesep)\n        else:\n            s = '%s%s%s' % (parens[0], unicode_repr(self._label), nodesep)\n        for child in self:\n            if isinstance(child, Tree):\n                s += '\\n'+' '*(indent+2)+child.pformat(margin, indent+2,\n                                                  nodesep, parens, quotes)\n            elif isinstance(child, tuple):\n                s += '\\n'+' '*(indent+2)+ \"/\".join(child)\n            elif isinstance(child, string_types) and not quotes:\n                s += '\\n'+' '*(indent+2)+ '%s' % child\n            else:\n                s += '\\n'+' '*(indent+2)+ unicode_repr(child)\n        return s+parens[1]\n\n    def pformat_latex_qtree(self):\n        r\"\"\"\n        Returns a representation of the tree compatible with the\n        LaTeX qtree package. This consists of the string ``\\Tree``\n        followed by the tree represented in bracketed notation.\n\n        For example, the following result was generated from a parse tree of\n        the sentence ``The announcement astounded us``::\n\n          \\Tree [.I'' [.N'' [.D The ] [.N' [.N announcement ] ] ]\n              [.I' [.V'' [.V' [.V astounded ] [.N'' [.N' [.N us ] ] ] ] ] ] ]\n\n        See http://www.ling.upenn.edu/advice/latex.html for the LaTeX\n        style file for the qtree package.\n\n        :return: A latex qtree representation of this tree.\n        :rtype: str\n        \"\"\"\n        reserved_chars = re.compile('([#\\$%&~_\\{\\}])')\n\n        pformat = self.pformat(indent=6, nodesep='', parens=('[.', ' ]'))\n        return r'\\Tree ' + re.sub(reserved_chars, r'\\\\\\1', pformat)\n\n    def _pformat_flat(self, nodesep, parens, quotes):\n        childstrs = []\n        for child in self:\n            if isinstance(child, Tree):\n                childstrs.append(child._pformat_flat(nodesep, parens, quotes))\n            elif isinstance(child, tuple):\n                childstrs.append(\"/\".join(child))\n            elif isinstance(child, string_types) and not quotes:\n                childstrs.append('%s' % child)\n            else:\n                childstrs.append(unicode_repr(child))\n        if isinstance(self._label, string_types):\n            return '%s%s%s %s%s' % (parens[0], self._label, nodesep,\n                                    \" \".join(childstrs), parens[1])\n        else:\n            return '%s%s%s %s%s' % (parens[0], unicode_repr(self._label), nodesep,\n                                    \" \".join(childstrs), parens[1])\n\n\nclass ImmutableTree(Tree):\n    def __init__(self, node, children=None):\n        super(ImmutableTree, self).__init__(node, children)\n        try:\n            self._hash = hash((self._label, tuple(self)))\n        except (TypeError, ValueError):\n            raise ValueError(\"%s: node value and children \"\n                             \"must be immutable\" % type(self).__name__)\n\n    def __setitem__(self, index, value):\n        raise ValueError('%s may not be modified' % type(self).__name__)\n    def __setslice__(self, i, j, value):\n        raise ValueError('%s may not be modified' % type(self).__name__)\n    def __delitem__(self, index):\n        raise ValueError('%s may not be modified' % type(self).__name__)\n    def __delslice__(self, i, j):\n        raise ValueError('%s may not be modified' % type(self).__name__)\n    def __iadd__(self, other):\n        raise ValueError('%s may not be modified' % type(self).__name__)\n    def __imul__(self, other):\n        raise ValueError('%s may not be modified' % type(self).__name__)\n    def append(self, v):\n        raise ValueError('%s may not be modified' % type(self).__name__)\n    def extend(self, v):\n        raise ValueError('%s may not be modified' % type(self).__name__)\n    def pop(self, v=None):\n        raise ValueError('%s may not be modified' % type(self).__name__)\n    def remove(self, v):\n        raise ValueError('%s may not be modified' % type(self).__name__)\n    def reverse(self):\n        raise ValueError('%s may not be modified' % type(self).__name__)\n    def sort(self):\n        raise ValueError('%s may not be modified' % type(self).__name__)\n    def __hash__(self):\n        return self._hash\n\n    def set_label(self, value):\n        \"\"\"\n        Set the node label.  This will only succeed the first time the\n        node label is set, which should occur in ImmutableTree.__init__().\n        \"\"\"\n        if hasattr(self, '_label'):\n            raise ValueError('%s may not be modified' % type(self).__name__)\n        self._label = value\n\n\n@add_metaclass(ABCMeta)\nclass AbstractParentedTree(Tree):\n    \"\"\"\n    An abstract base class for a ``Tree`` that automatically maintains\n    pointers to parent nodes.  These parent pointers are updated\n    whenever any change is made to a tree's structure.  Two subclasses\n    are currently defined:\n\n      - ``ParentedTree`` is used for tree structures where each subtree\n        has at most one parent.  This class should be used in cases\n        where there is no\"sharing\" of subtrees.\n\n      - ``MultiParentedTree`` is used for tree structures where a\n        subtree may have zero or more parents.  This class should be\n        used in cases where subtrees may be shared.\n\n    Subclassing\n    ===========\n    The ``AbstractParentedTree`` class redefines all operations that\n    modify a tree's structure to call two methods, which are used by\n    subclasses to update parent information:\n\n      - ``_setparent()`` is called whenever a new child is added.\n      - ``_delparent()`` is called whenever a child is removed.\n    \"\"\"\n\n    def __init__(self, node, children=None):\n        super(AbstractParentedTree, self).__init__(node, children)\n        if children is not None:\n            for i, child in enumerate(self):\n                if isinstance(child, Tree):\n                    self._setparent(child, i, dry_run=True)\n            for i, child in enumerate(self):\n                if isinstance(child, Tree):\n                    self._setparent(child, i)\n\n    @abstractmethod\n    def _setparent(self, child, index, dry_run=False):\n        \"\"\"\n        Update the parent pointer of ``child`` to point to ``self``.  This\n        method is only called if the type of ``child`` is ``Tree``;\n        i.e., it is not called when adding a leaf to a tree.  This method\n        is always called before the child is actually added to the\n        child list of ``self``.\n\n        :type child: Tree\n        :type index: int\n        :param index: The index of ``child`` in ``self``.\n        :raise TypeError: If ``child`` is a tree with an impropriate\n            type.  Typically, if ``child`` is a tree, then its type needs\n            to match the type of ``self``.  This prevents mixing of\n            different tree types (single-parented, multi-parented, and\n            non-parented).\n        :param dry_run: If true, the don't actually set the child's\n            parent pointer; just check for any error conditions, and\n            raise an exception if one is found.\n        \"\"\"\n\n    @abstractmethod\n    def _delparent(self, child, index):\n        \"\"\"\n        Update the parent pointer of ``child`` to not point to self.  This\n        method is only called if the type of ``child`` is ``Tree``; i.e., it\n        is not called when removing a leaf from a tree.  This method\n        is always called before the child is actually removed from the\n        child list of ``self``.\n\n        :type child: Tree\n        :type index: int\n        :param index: The index of ``child`` in ``self``.\n        \"\"\"\n\n\n    def __delitem__(self, index):\n        if isinstance(index, slice):\n            start, stop, step = slice_bounds(self, index, allow_step=True)\n            for i in range(start, stop, step):\n                if isinstance(self[i], Tree):\n                    self._delparent(self[i], i)\n            super(AbstractParentedTree, self).__delitem__(index)\n\n        elif isinstance(index, int):\n            if index < 0: index += len(self)\n            if index < 0: raise IndexError('index out of range')\n            if isinstance(self[index], Tree):\n                self._delparent(self[index], index)\n            super(AbstractParentedTree, self).__delitem__(index)\n\n        elif isinstance(index, (list, tuple)):\n            if len(index) == 0:\n                raise IndexError('The tree position () may not be deleted.')\n            elif len(index) == 1:\n                del self[index[0]]\n            else:\n                del self[index[0]][index[1:]]\n\n        else:\n            raise TypeError(\"%s indices must be integers, not %s\" %\n                            (type(self).__name__, type(index).__name__))\n\n    def __setitem__(self, index, value):\n        if isinstance(index, slice):\n            start, stop, step = slice_bounds(self, index, allow_step=True)\n            if not isinstance(value, (list, tuple)):\n                value = list(value)\n            for i, child in enumerate(value):\n                if isinstance(child, Tree):\n                    self._setparent(child, start + i*step, dry_run=True)\n            for i in range(start, stop, step):\n                if isinstance(self[i], Tree):\n                    self._delparent(self[i], i)\n            for i, child in enumerate(value):\n                if isinstance(child, Tree):\n                    self._setparent(child, start + i*step)\n            super(AbstractParentedTree, self).__setitem__(index, value)\n\n        elif isinstance(index, int):\n            if index < 0: index += len(self)\n            if index < 0: raise IndexError('index out of range')\n            if value is self[index]:\n                return\n            if isinstance(value, Tree):\n                self._setparent(value, index)\n            if isinstance(self[index], Tree):\n                self._delparent(self[index], index)\n            super(AbstractParentedTree, self).__setitem__(index, value)\n\n        elif isinstance(index, (list, tuple)):\n            if len(index) == 0:\n                raise IndexError('The tree position () may not be assigned to.')\n            elif len(index) == 1:\n                self[index[0]] = value\n            else:\n                self[index[0]][index[1:]] = value\n\n        else:\n            raise TypeError(\"%s indices must be integers, not %s\" %\n                            (type(self).__name__, type(index).__name__))\n\n    def append(self, child):\n        if isinstance(child, Tree):\n            self._setparent(child, len(self))\n        super(AbstractParentedTree, self).append(child)\n\n    def extend(self, children):\n        for child in children:\n            if isinstance(child, Tree):\n                self._setparent(child, len(self))\n            super(AbstractParentedTree, self).append(child)\n\n    def insert(self, index, child):\n        if index < 0: index += len(self)\n        if index < 0: index = 0\n        if isinstance(child, Tree):\n            self._setparent(child, index)\n        super(AbstractParentedTree, self).insert(index, child)\n\n    def pop(self, index=-1):\n        if index < 0: index += len(self)\n        if index < 0: raise IndexError('index out of range')\n        if isinstance(self[index], Tree):\n            self._delparent(self[index], index)\n        return super(AbstractParentedTree, self).pop(index)\n\n    def remove(self, child):\n        index = self.index(child)\n        if isinstance(self[index], Tree):\n            self._delparent(self[index], index)\n        super(AbstractParentedTree, self).remove(child)\n\n    if hasattr(list, '__getslice__'):\n        def __getslice__(self, start, stop):\n            return self.__getitem__(slice(max(0, start), max(0, stop)))\n        def __delslice__(self, start, stop):\n            return self.__delitem__(slice(max(0, start), max(0, stop)))\n        def __setslice__(self, start, stop, value):\n            return self.__setitem__(slice(max(0, start), max(0, stop)), value)\n\nclass ParentedTree(AbstractParentedTree):\n    \"\"\"\n    A ``Tree`` that automatically maintains parent pointers for\n    single-parented trees.  The following are methods for querying\n    the structure of a parented tree: ``parent``, ``parent_index``,\n    ``left_sibling``, ``right_sibling``, ``root``, ``treeposition``.\n\n    Each ``ParentedTree`` may have at most one parent.  In\n    particular, subtrees may not be shared.  Any attempt to reuse a\n    single ``ParentedTree`` as a child of more than one parent (or\n    as multiple children of the same parent) will cause a\n    ``ValueError`` exception to be raised.\n\n    ``ParentedTrees`` should never be used in the same tree as ``Trees``\n    or ``MultiParentedTrees``.  Mixing tree implementations may result\n    in incorrect parent pointers and in ``TypeError`` exceptions.\n    \"\"\"\n    def __init__(self, node, children=None):\n        self._parent = None\n        super(ParentedTree, self).__init__(node, children)\n        if children is None:\n            for i, child in enumerate(self):\n                if isinstance(child, Tree):\n                    child._parent = None\n                    self._setparent(child, i)\n\n    def _frozen_class(self): return ImmutableParentedTree\n\n\n    def parent(self):\n        return self._parent\n\n    def parent_index(self):\n        \"\"\"\n        The index of this tree in its parent.  I.e.,\n        ``ptree.parent()[ptree.parent_index()] is ptree``.  Note that\n        ``ptree.parent_index()`` is not necessarily equal to\n        ``ptree.parent.index(ptree)``, since the ``index()`` method\n        returns the first child that is equal to its argument.\n        \"\"\"\n        if self._parent is None: return None\n        for i, child in enumerate(self._parent):\n            if child is self: return i\n        assert False, 'expected to find self in self._parent!'\n\n    def left_sibling(self):\n        parent_index = self.parent_index()\n        if self._parent and parent_index > 0:\n            return self._parent[parent_index-1]\n        return None # no left sibling\n\n    def right_sibling(self):\n        parent_index = self.parent_index()\n        if self._parent and parent_index < (len(self._parent)-1):\n            return self._parent[parent_index+1]\n        return None # no right sibling\n\n    def root(self):\n        \"\"\"\n        The root of this tree.  I.e., the unique ancestor of this tree\n        whose parent is None.  If ``ptree.parent()`` is None, then\n        ``ptree`` is its own root.\n        \"\"\"\n        root = self\n        while root.parent() is not None:\n            root = root.parent()\n        return root\n\n    def treeposition(self):\n        \"\"\"\n        The tree position of this tree, relative to the root of the\n        tree.  I.e., ``ptree.root[ptree.treeposition] is ptree``.\n        \"\"\"\n        if self.parent() is None:\n            return ()\n        else:\n            return self.parent().treeposition() + (self.parent_index(),)\n\n\n\n    def _delparent(self, child, index):\n        assert isinstance(child, ParentedTree)\n        assert self[index] is child\n        assert child._parent is self\n\n        child._parent = None\n\n    def _setparent(self, child, index, dry_run=False):\n        if not isinstance(child, ParentedTree):\n            raise TypeError('Can not insert a non-ParentedTree '+\n                            'into a ParentedTree')\n\n        if child._parent is not None:\n            raise ValueError('Can not insert a subtree that already '\n                             'has a parent.')\n\n        if not dry_run:\n            child._parent = self\n\n\nclass MultiParentedTree(AbstractParentedTree):\n    \"\"\"\n    A ``Tree`` that automatically maintains parent pointers for\n    multi-parented trees.  The following are methods for querying the\n    structure of a multi-parented tree: ``parents()``, ``parent_indices()``,\n    ``left_siblings()``, ``right_siblings()``, ``roots``, ``treepositions``.\n\n    Each ``MultiParentedTree`` may have zero or more parents.  In\n    particular, subtrees may be shared.  If a single\n    ``MultiParentedTree`` is used as multiple children of the same\n    parent, then that parent will appear multiple times in its\n    ``parents()`` method.\n\n    ``MultiParentedTrees`` should never be used in the same tree as\n    ``Trees`` or ``ParentedTrees``.  Mixing tree implementations may\n    result in incorrect parent pointers and in ``TypeError`` exceptions.\n    \"\"\"\n    def __init__(self, node, children=None):\n        self._parents = []\n        \"\"\"A list of this tree's parents.  This list should not\n           contain duplicates, even if a parent contains this tree\n           multiple times.\"\"\"\n        super(MultiParentedTree, self).__init__(node, children)\n        if children is None:\n            for i, child in enumerate(self):\n                if isinstance(child, Tree):\n                    child._parents = []\n                    self._setparent(child, i)\n\n    def _frozen_class(self): return ImmutableMultiParentedTree\n\n\n    def parents(self):\n        \"\"\"\n        The set of parents of this tree.  If this tree has no parents,\n        then ``parents`` is the empty set.  To check if a tree is used\n        as multiple children of the same parent, use the\n        ``parent_indices()`` method.\n\n        :type: list(MultiParentedTree)\n        \"\"\"\n        return list(self._parents)\n\n    def left_siblings(self):\n        \"\"\"\n        A list of all left siblings of this tree, in any of its parent\n        trees.  A tree may be its own left sibling if it is used as\n        multiple contiguous children of the same parent.  A tree may\n        appear multiple times in this list if it is the left sibling\n        of this tree with respect to multiple parents.\n\n        :type: list(MultiParentedTree)\n        \"\"\"\n        return [parent[index-1]\n                for (parent, index) in self._get_parent_indices()\n                if index > 0]\n\n    def right_siblings(self):\n        \"\"\"\n        A list of all right siblings of this tree, in any of its parent\n        trees.  A tree may be its own right sibling if it is used as\n        multiple contiguous children of the same parent.  A tree may\n        appear multiple times in this list if it is the right sibling\n        of this tree with respect to multiple parents.\n\n        :type: list(MultiParentedTree)\n        \"\"\"\n        return [parent[index+1]\n                for (parent, index) in self._get_parent_indices()\n                if index < (len(parent)-1)]\n\n    def _get_parent_indices(self):\n        return [(parent, index)\n                for parent in self._parents\n                for index, child in enumerate(parent)\n                if child is self]\n\n    def roots(self):\n        \"\"\"\n        The set of all roots of this tree.  This set is formed by\n        tracing all possible parent paths until trees with no parents\n        are found.\n\n        :type: list(MultiParentedTree)\n        \"\"\"\n        return list(self._get_roots_helper({}).values())\n\n    def _get_roots_helper(self, result):\n        if self._parents:\n            for parent in self._parents:\n                parent._get_roots_helper(result)\n        else:\n            result[id(self)] = self\n        return result\n\n    def parent_indices(self, parent):\n        \"\"\"\n        Return a list of the indices where this tree occurs as a child\n        of ``parent``.  If this child does not occur as a child of\n        ``parent``, then the empty list is returned.  The following is\n        always true::\n\n          for parent_index in ptree.parent_indices(parent):\n              parent[parent_index] is ptree\n        \"\"\"\n        if parent not in self._parents: return []\n        else: return [index for (index, child) in enumerate(parent)\n                      if child is self]\n\n    def treepositions(self, root):\n        \"\"\"\n        Return a list of all tree positions that can be used to reach\n        this multi-parented tree starting from ``root``.  I.e., the\n        following is always true::\n\n          for treepos in ptree.treepositions(root):\n              root[treepos] is ptree\n        \"\"\"\n        if self is root:\n            return [()]\n        else:\n            return [treepos+(index,)\n                    for parent in self._parents\n                    for treepos in parent.treepositions(root)\n                    for (index, child) in enumerate(parent) if child is self]\n\n\n\n    def _delparent(self, child, index):\n        assert isinstance(child, MultiParentedTree)\n        assert self[index] is child\n        assert len([p for p in child._parents if p is self]) == 1\n\n        for i, c in enumerate(self):\n            if c is child and i != index: break\n        else:\n            child._parents.remove(self)\n\n    def _setparent(self, child, index, dry_run=False):\n        if not isinstance(child, MultiParentedTree):\n            raise TypeError('Can not insert a non-MultiParentedTree '+\n                            'into a MultiParentedTree')\n\n        if not dry_run:\n            for parent in child._parents:\n                if parent is self: break\n            else:\n                child._parents.append(self)\n\nclass ImmutableParentedTree(ImmutableTree, ParentedTree):\n    pass\n\nclass ImmutableMultiParentedTree(ImmutableTree, MultiParentedTree):\n    pass\n\n\n\n@python_2_unicode_compatible\nclass ProbabilisticTree(Tree, ProbabilisticMixIn):\n    def __init__(self, node, children=None, **prob_kwargs):\n        Tree.__init__(self, node, children)\n        ProbabilisticMixIn.__init__(self, **prob_kwargs)\n\n    def _frozen_class(self): return ImmutableProbabilisticTree\n    def __repr__(self):\n        return '%s (p=%r)' % (Tree.unicode_repr(self), self.prob())\n    def __str__(self):\n        return '%s (p=%.6g)' % (self.pformat(margin=60), self.prob())\n    def copy(self, deep=False):\n        if not deep: return type(self)(self._label, self, prob=self.prob())\n        else: return type(self).convert(self)\n    @classmethod\n    def convert(cls, val):\n        if isinstance(val, Tree):\n            children = [cls.convert(child) for child in val]\n            if isinstance(val, ProbabilisticMixIn):\n                return cls(val._label, children, prob=val.prob())\n            else:\n                return cls(val._label, children, prob=1.0)\n        else:\n            return val\n\n    def __eq__(self, other):\n        return (self.__class__ is other.__class__ and\n                (self._label, list(self), self.prob()) ==\n                (other._label, list(other), other.prob()))\n\n    def __lt__(self, other):\n        if not isinstance(other, Tree):\n            raise_unorderable_types(\"<\", self, other)\n        if self.__class__ is other.__class__:\n            return ((self._label, list(self), self.prob()) <\n                    (other._label, list(other), other.prob()))\n        else:\n            return self.__class__.__name__ < other.__class__.__name__\n\n\n@python_2_unicode_compatible\nclass ImmutableProbabilisticTree(ImmutableTree, ProbabilisticMixIn):\n    def __init__(self, node, children=None, **prob_kwargs):\n        ImmutableTree.__init__(self, node, children)\n        ProbabilisticMixIn.__init__(self, **prob_kwargs)\n        self._hash = hash((self._label, tuple(self), self.prob()))\n\n    def _frozen_class(self): return ImmutableProbabilisticTree\n    def __repr__(self):\n        return '%s [%s]' % (Tree.unicode_repr(self), self.prob())\n    def __str__(self):\n        return '%s [%s]' % (self.pformat(margin=60), self.prob())\n    def copy(self, deep=False):\n        if not deep: return type(self)(self._label, self, prob=self.prob())\n        else: return type(self).convert(self)\n    @classmethod\n    def convert(cls, val):\n        if isinstance(val, Tree):\n            children = [cls.convert(child) for child in val]\n            if isinstance(val, ProbabilisticMixIn):\n                return cls(val._label, children, prob=val.prob())\n            else:\n                return cls(val._label, children, prob=1.0)\n        else:\n            return val\n\n\ndef _child_names(tree):\n    names = []\n    for child in tree:\n        if isinstance(child, Tree):\n            names.append(Nonterminal(child._label))\n        else:\n            names.append(child)\n    return names\n\n\ndef bracket_parse(s):\n    \"\"\"\n    Use Tree.read(s, remove_empty_top_bracketing=True) instead.\n    \"\"\"\n    raise NameError(\"Use Tree.read(s, remove_empty_top_bracketing=True) instead.\")\n\ndef sinica_parse(s):\n    \"\"\"\n    Parse a Sinica Treebank string and return a tree.  Trees are represented as nested brackettings,\n    as shown in the following example (X represents a Chinese character):\n    S(goal:NP(Head:Nep:XX)|theme:NP(Head:Nhaa:X)|quantity:Dab:X|Head:VL2:X)#0(PERIODCATEGORY)\n\n    :return: A tree corresponding to the string representation.\n    :rtype: Tree\n    :param s: The string to be converted\n    :type s: str\n    \"\"\"\n    tokens = re.split(r'([()| ])', s)\n    for i in range(len(tokens)):\n        if tokens[i] == '(':\n            tokens[i-1], tokens[i] = tokens[i], tokens[i-1]     # pull nonterminal inside parens\n        elif ':' in tokens[i]:\n            fields = tokens[i].split(':')\n            if len(fields) == 2:                                # non-terminal\n                tokens[i] = fields[1]\n            else:\n                tokens[i] = \"(\" + fields[-2] + \" \" + fields[-1] + \")\"\n        elif tokens[i] == '|':\n            tokens[i] = ''\n\n    treebank_string = \" \".join(tokens)\n    return Tree.fromstring(treebank_string, remove_empty_top_bracketing=True)\n\n\n\n\ndef demo():\n    \"\"\"\n    A demonstration showing how Trees and Trees can be\n    used.  This demonstration creates a Tree, and loads a\n    Tree from the Treebank corpus,\n    and shows the results of calling several of their methods.\n    \"\"\"\n\n    from nltk import Tree, ProbabilisticTree\n\n    s = '(S (NP (DT the) (NN cat)) (VP (VBD ate) (NP (DT a) (NN cookie))))'\n    t = Tree.fromstring(s)\n    print(\"Convert bracketed string into tree:\")\n    print(t)\n    print(t.__repr__())\n\n    print(\"Display tree properties:\")\n    print(t.label())         # tree's constituent type\n    print(t[0])             # tree's first child\n    print(t[1])             # tree's second child\n    print(t.height())\n    print(t.leaves())\n    print(t[1])\n    print(t[1,1])\n    print(t[1,1,0])\n\n    the_cat = t[0]\n    the_cat.insert(1, Tree.fromstring('(JJ big)'))\n    print(\"Tree modification:\")\n    print(t)\n    t[1,1,1] = Tree.fromstring('(NN cake)')\n    print(t)\n    print()\n\n    print(\"Collapse unary:\")\n    t.collapse_unary()\n    print(t)\n    print(\"Chomsky normal form:\")\n    t.chomsky_normal_form()\n    print(t)\n    print()\n\n    pt = ProbabilisticTree('x', ['y', 'z'], prob=0.5)\n    print(\"Probabilistic Tree:\")\n    print(pt)\n    print()\n\n    t = Tree.fromstring(t.pformat())\n    print(\"Convert tree to bracketed string and back again:\")\n    print(t)\n    print()\n\n    print(\"LaTeX output:\")\n    print(t.pformat_latex_qtree())\n    print()\n\n    print(\"Production output:\")\n    print(t.productions())\n    print()\n\n    t.set_label(('test', 3))\n    print(t)\n\n__all__ = ['ImmutableProbabilisticTree', 'ImmutableTree', 'ProbabilisticMixIn',\n           'ProbabilisticTree', 'Tree', 'bracket_parse',\n           'sinica_parse', 'ParentedTree', 'MultiParentedTree',\n           'ImmutableParentedTree', 'ImmutableMultiParentedTree']\n"], "nltk\\treeprettyprinter": [".py", "\n\nfrom __future__ import division, print_function, unicode_literals\n\nfrom nltk.util import slice_bounds, OrderedDict\nfrom nltk.compat import python_2_unicode_compatible, unicode_repr\nfrom nltk.internals import raise_unorderable_types\nfrom nltk.tree import Tree\n\nimport re\nimport sys\nimport codecs\nfrom cgi import escape\nfrom collections import defaultdict\nfrom operator import itemgetter\nfrom itertools import chain, islice\n\n\nANSICOLOR = {\n    'black': 30,\n    'red': 31,\n    'green': 32,\n    'yellow': 33,\n    'blue': 34,\n    'magenta': 35,\n    'cyan': 36,\n    'white': 37,\n}\n\n\n@python_2_unicode_compatible\nclass TreePrettyPrinter(object):\n\n    def __init__(self, tree, sentence=None, highlight=()):\n        if sentence is None:\n            leaves = tree.leaves()\n            if (leaves and not any(len(a) == 0 for a in tree.subtrees())\n                    and all(isinstance(a, int) for a in leaves)):\n                sentence = [str(a) for a in leaves]\n            else:\n                tree = tree.copy(True)\n                sentence = []\n                for a in tree.subtrees():\n                    if len(a) == 0:\n                        a.append(len(sentence))\n                        sentence.append(None)\n                    elif any(not isinstance(b, Tree) for b in a):\n                        for n, b in enumerate(a):\n                            if not isinstance(b, Tree):\n                                a[n] = len(sentence)\n                                sentence.append('%s' % b)\n        self.nodes, self.coords, self.edges, self.highlight = self.nodecoords(\n                tree, sentence, highlight)\n\n    def __str__(self):\n        return self.text()\n\n    def __repr__(self):\n        return '<TreePrettyPrinter with %d nodes>' % len(self.nodes)\n\n\n    @staticmethod\n    def nodecoords(tree, sentence, highlight):\n        def findcell(m, matrix, startoflevel, children):\n            candidates = [a for _, a in children[m]]\n            minidx, maxidx = min(candidates), max(candidates)\n            leaves = tree[m].leaves()\n            center = scale * sum(leaves) // len(leaves)  # center of gravity\n            if minidx < maxidx and not minidx < center < maxidx:\n                center = sum(candidates) // len(candidates)\n            if max(candidates) - min(candidates) > 2 * scale:\n                center -= center % scale  # round to unscaled coordinate\n                if minidx < maxidx and not minidx < center < maxidx:\n                    center += scale\n            if ids[m] == 0:\n                startoflevel = len(matrix)\n            for rowidx in range(startoflevel, len(matrix) + 1):\n                if rowidx == len(matrix):  # need to add a new row\n                    matrix.append([vertline if a not in (corner, None)\n                            else None for a in matrix[-1]])\n                row = matrix[rowidx]\n                i = j = center\n                if len(children[m]) == 1:  # place unaries directly above child\n                    return rowidx, next(iter(children[m]))[1]\n                elif all(a is None or a == vertline for a\n                        in row[min(candidates):max(candidates) + 1]):\n                    for n in range(scale):\n                        i = j = center + n\n                        while j > minidx or i < maxidx:\n                            if i < maxidx and (matrix[rowidx][i] is None\n                                    or i in candidates):\n                                return rowidx, i\n                            elif j > minidx and (matrix[rowidx][j] is None\n                                    or j in candidates):\n                                return rowidx, j\n                            i += scale\n                            j -= scale\n            raise ValueError('could not find a free cell for:\\n%s\\n%s'\n                    'min=%d; max=%d' % (tree[m], minidx, maxidx, dumpmatrix()))\n\n        def dumpmatrix():\n            return '\\n'.join(\n                '%2d: %s' % (n, ' '.join(('%2r' % i)[:2] for i in row))\n                for n, row in enumerate(matrix))\n\n        leaves = tree.leaves()\n        if not all(isinstance(n, int) for n in leaves):\n            raise ValueError('All leaves must be integer indices.')\n        if len(leaves) != len(set(leaves)):\n            raise ValueError('Indices must occur at most once.')\n        if not all(0 <= n < len(sentence) for n in leaves):\n            raise ValueError('All leaves must be in the interval 0..n '\n                    'with n=len(sentence)\\ntokens: %d indices: '\n                    '%r\\nsentence: %s' % (len(sentence), tree.leaves(), sentence))\n        vertline, corner = -1, -2  # constants\n        tree = tree.copy(True)\n        for a in tree.subtrees():\n            a.sort(key=lambda n: min(n.leaves()) if isinstance(n, Tree) else n)\n        scale = 2\n        crossed = set()\n        positions = tree.treepositions()\n        maxdepth = max(map(len, positions)) + 1\n        childcols = defaultdict(set)\n        matrix = [[None] * (len(sentence) * scale)]\n        nodes = {}\n        ids = dict((a, n) for n, a in enumerate(positions))\n        highlighted_nodes = set(n for a, n in ids.items()\n                                if not highlight or tree[a] in highlight)\n        levels = dict((n, []) for n in range(maxdepth - 1))\n        terminals = []\n        for a in positions:\n            node = tree[a]\n            if isinstance(node, Tree):\n                levels[maxdepth - node.height()].append(a)\n            else:\n                terminals.append(a)\n\n        for n in levels:\n            levels[n].sort(key=lambda n: max(tree[n].leaves())\n                    - min(tree[n].leaves()))\n        terminals.sort()\n        positions = set(positions)\n\n        for m in terminals:\n            i = int(tree[m]) * scale\n            assert matrix[0][i] is None, (matrix[0][i], m, i)\n            matrix[0][i] = ids[m]\n            nodes[ids[m]] = sentence[tree[m]]\n            if nodes[ids[m]] is None:\n                nodes[ids[m]] = '...'\n                highlighted_nodes.discard(ids[m])\n            positions.remove(m)\n            childcols[m[:-1]].add((0, i))\n\n        for n in sorted(levels, reverse=True):\n            nodesatdepth = levels[n]\n            startoflevel = len(matrix)\n            matrix.append([vertline if a not in (corner, None) else None\n                    for a in matrix[-1]])\n            for m in nodesatdepth:  # [::-1]:\n                if n < maxdepth - 1 and childcols[m]:\n                    _, pivot = min(childcols[m], key=itemgetter(1))\n                    if (set(a[:-1] for row in matrix[:-1] for a in row[:pivot]\n                            if isinstance(a, tuple)) &\n                        set(a[:-1] for row in matrix[:-1] for a in row[pivot:]\n                            if isinstance(a, tuple))):\n                        crossed.add(m)\n\n                rowidx, i = findcell(m, matrix, startoflevel, childcols)\n                positions.remove(m)\n\n                for _, x in childcols[m]:\n                    matrix[rowidx][x] = corner\n                matrix[rowidx][i] = ids[m]\n                nodes[ids[m]] = tree[m]\n                if m != ():\n                    childcols[m[:-1]].add((rowidx, i))\n        assert len(positions) == 0\n\n        for m in range(scale * len(sentence) - 1, -1, -1):\n            if not any(isinstance(row[m], (Tree, int))\n                    for row in matrix):\n                for row in matrix:\n                    del row[m]\n\n        matrix = [row for row in reversed(matrix)\n                if not all(a is None or a == vertline for a in row)]\n\n        coords = {}\n        for n, _ in enumerate(matrix):\n            for m, i in enumerate(matrix[n]):\n                if isinstance(i, int) and i >= 0:\n                    coords[i] = n, m\n\n        positions = sorted([a for level in levels.values()\n                for a in level], key=lambda a: a[:-1] in crossed)\n\n        edges = OrderedDict()\n        for i in reversed(positions):\n            for j, _ in enumerate(tree[i]):\n                edges[ids[i + (j, )]] = ids[i]\n\n        return nodes, coords, edges, highlighted_nodes\n\n\n    def text(self, nodedist=1, unicodelines=False, html=False, ansi=False,\n             nodecolor='blue', leafcolor='red', funccolor='green',\n             abbreviate=None, maxwidth=16):\n        if abbreviate == True:\n            abbreviate = 5\n        if unicodelines:\n            horzline = '\\u2500'\n            leftcorner = '\\u250c'\n            rightcorner = '\\u2510'\n            vertline = ' \\u2502 '\n            tee = horzline + '\\u252C' + horzline\n            bottom = horzline + '\\u2534' + horzline\n            cross = horzline + '\\u253c' + horzline\n            ellipsis = '\\u2026'\n        else:\n            horzline = '_'\n            leftcorner = rightcorner = ' '\n            vertline = ' | '\n            tee = 3 * horzline\n            cross = bottom = '_|_'\n            ellipsis = '.'\n\n        def crosscell(cur, x=vertline):\n            splitl = len(cur) - len(cur) // 2 - len(x) // 2 - 1\n            lst = list(cur)\n            lst[splitl:splitl + len(x)] = list(x)\n            return ''.join(lst)\n\n        result = []\n        matrix = defaultdict(dict)\n        maxnodewith = defaultdict(lambda: 3)\n        maxnodeheight = defaultdict(lambda: 1)\n        maxcol = 0\n        minchildcol = {}\n        maxchildcol = {}\n        childcols = defaultdict(set)\n        labels = {}\n        wrapre = re.compile('(.{%d,%d}\\\\b\\\\W*|.{%d})' % (\n                maxwidth - 4, maxwidth, maxwidth))\n        for a in self.nodes:\n            row, column = self.coords[a]\n            matrix[row][column] = a\n            maxcol = max(maxcol, column)\n            label = (self.nodes[a].label() if isinstance(self.nodes[a], Tree)\n                     else self.nodes[a])\n            if abbreviate and len(label) > abbreviate:\n                label = label[:abbreviate] + ellipsis\n            if maxwidth and len(label) > maxwidth:\n                label = wrapre.sub(r'\\1\\n', label).strip()\n            label = label.split('\\n')\n            maxnodeheight[row] = max(maxnodeheight[row], len(label))\n            maxnodewith[column] = max(maxnodewith[column], max(map(len, label)))\n            labels[a] = label\n            if a not in self.edges:\n                continue  # e.g., root\n            parent = self.edges[a]\n            childcols[parent].add((row, column))\n            minchildcol[parent] = min(minchildcol.get(parent, column), column)\n            maxchildcol[parent] = max(maxchildcol.get(parent, column), column)\n        for row in sorted(matrix, reverse=True):\n            noderows = [[''.center(maxnodewith[col]) for col in range(maxcol + 1)]\n                    for _ in range(maxnodeheight[row])]\n            branchrow = [''.center(maxnodewith[col]) for col in range(maxcol + 1)]\n            for col in matrix[row]:\n                n = matrix[row][col]\n                node = self.nodes[n]\n                text = labels[n]\n                if isinstance(node, Tree):\n                    if n in minchildcol and minchildcol[n] < maxchildcol[n]:\n                        i, j = minchildcol[n], maxchildcol[n]\n                        a, b = (maxnodewith[i] + 1) // 2 - 1, maxnodewith[j] // 2\n                        branchrow[i] = ((' ' * a) + leftcorner).ljust(\n                                maxnodewith[i], horzline)\n                        branchrow[j] = (rightcorner + (' ' * b)).rjust(\n                                maxnodewith[j], horzline)\n                        for i in range(minchildcol[n] + 1, maxchildcol[n]):\n                            if i == col and any(\n                                    a == i for _, a in childcols[n]):\n                                line = cross\n                            elif i == col:\n                                line = bottom\n                            elif any(a == i for _, a in childcols[n]):\n                                line = tee\n                            else:\n                                line = horzline\n                            branchrow[i] = line.center(maxnodewith[i], horzline)\n                    else:  # if n and n in minchildcol:\n                        branchrow[col] = crosscell(branchrow[col])\n                text = [a.center(maxnodewith[col]) for a in text]\n                color = nodecolor if isinstance(node, Tree) else leafcolor\n                if isinstance(node, Tree) and node.label().startswith('-'):\n                    color = funccolor\n                if html:\n                    text = [escape(a) for a in text]\n                    if n in self.highlight:\n                        text = ['<font color=%s>%s</font>' % (\n                                color, a) for a in text]\n                elif ansi and n in self.highlight:\n                    text = ['\\x1b[%d;1m%s\\x1b[0m' % (\n                            ANSICOLOR[color], a) for a in text]\n                for x in range(maxnodeheight[row]):\n                    noderows[x][col] = (text[x] if x < len(text)\n                            else (vertline if childcols[n] else ' ').center(\n                                maxnodewith[col], ' '))\n            if row != max(matrix):\n                for n, (childrow, col) in self.coords.items():\n                    if (n > 0 and\n                            self.coords[self.edges[n]][0] < row < childrow):\n                        branchrow[col] = crosscell(branchrow[col])\n                        if col not in matrix[row]:\n                            for noderow in noderows:\n                                noderow[col] = crosscell(noderow[col])\n                branchrow = [a + ((a[-1] if a[-1] != ' ' else b[0]) * nodedist)\n                        for a, b in zip(branchrow, branchrow[1:] + [' '])]\n                result.append(''.join(branchrow))\n            result.extend((' ' * nodedist).join(noderow)\n                    for noderow in reversed(noderows))\n        return '\\n'.join(reversed(result)) + '\\n'\n\n\n    def svg(self, nodecolor='blue', leafcolor='red', funccolor='green'):\n        fontsize = 12\n        hscale = 40\n        vscale = 25\n        hstart = vstart = 20\n        width = max(col for _, col in self.coords.values())\n        height = max(row for row, _ in self.coords.values())\n        result = ['<svg version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" '\n                  'width=\"%dem\" height=\"%dem\" viewBox=\"%d %d %d %d\">' % (\n                      width * 3,\n                      height * 2.5,\n                      -hstart, -vstart,\n                      width * hscale + 3 * hstart,\n                      height * vscale + 3 * vstart)\n                      ]\n\n        children = defaultdict(set)\n        for n in self.nodes:\n            if n:\n                children[self.edges[n]].add(n)\n\n        for node in self.nodes:\n            if not children[node]:\n                continue\n            y, x = self.coords[node]\n            x *= hscale\n            y *= vscale\n            x += hstart\n            y += vstart + fontsize // 2\n            childx = [self.coords[c][1] for c in children[node]]\n            xmin = hstart + hscale * min(childx)\n            xmax = hstart + hscale * max(childx)\n            result.append(\n                '\\t<polyline style=\"stroke:black; stroke-width:1; fill:none;\" '\n                'points=\"%g,%g %g,%g\" />' % (xmin, y, xmax, y))\n            result.append(\n                '\\t<polyline style=\"stroke:black; stroke-width:1; fill:none;\" '\n                'points=\"%g,%g %g,%g\" />' % (x, y, x, y - fontsize // 3))\n\n        for child, parent in self.edges.items():\n            y, _ = self.coords[parent]\n            y *= vscale\n            y += vstart + fontsize // 2\n            childy, childx = self.coords[child]\n            childx *= hscale\n            childy *= vscale\n            childx += hstart\n            childy += vstart - fontsize\n            result += [\n                '\\t<polyline style=\"stroke:white; stroke-width:10; fill:none;\"'\n                ' points=\"%g,%g %g,%g\" />' % (childx, childy, childx, y + 5),\n                '\\t<polyline style=\"stroke:black; stroke-width:1; fill:none;\"'\n                ' points=\"%g,%g %g,%g\" />' % (childx, childy, childx, y),\n                ]\n\n        for n, (row, column) in self.coords.items():\n            node = self.nodes[n]\n            x = column * hscale + hstart\n            y = row * vscale + vstart\n            if n in self.highlight:\n                color = nodecolor if isinstance(node, Tree) else leafcolor\n                if isinstance(node, Tree) and node.label().startswith('-'):\n                    color = funccolor\n            else:\n                color = 'black'\n            result += ['\\t<text style=\"text-anchor: middle; fill: %s; '\n                       'font-size: %dpx;\" x=\"%g\" y=\"%g\">%s</text>' % (\n                           color, fontsize, x, y,\n                           escape(node.label() if isinstance(node, Tree)\n                                  else node))]\n\n        result += ['</svg>']\n        return '\\n'.join(result)\n\n\ndef test():\n    def print_tree(n, tree, sentence=None, ansi=True, **xargs):\n        print()\n        print('{0}: \"{1}\"'.format(n, ' '.join(sentence or tree.leaves())))\n        print(tree)\n        print()\n        drawtree = TreePrettyPrinter(tree, sentence)\n        try:\n            print(drawtree.text(unicodelines=ansi, ansi=ansi, **xargs))\n        except (UnicodeDecodeError, UnicodeEncodeError):\n            print(drawtree.text(unicodelines=False, ansi=False, **xargs))\n\n    from nltk.corpus import treebank\n    for n in [0, 1440, 1591, 2771, 2170]:\n        tree = treebank.parsed_sents()[n]\n        print_tree(n, tree, nodedist=2, maxwidth=8)\n    print()\n    print('ASCII version:')\n    print(TreePrettyPrinter(tree).text(nodedist=2))\n\n    tree = Tree.fromstring(\n        '(top (punct 8) (smain (noun 0) (verb 1) (inf (verb 5) (inf (verb 6) '\n        '(conj (inf (pp (prep 2) (np (det 3) (noun 4))) (verb 7)) (inf (verb 9)) '\n        '(vg 10) (inf (verb 11)))))) (punct 12))', read_leaf=int)\n    sentence = ('Ze had met haar moeder kunnen gaan winkelen ,'\n                ' zwemmen of terrassen .'.split())\n    print_tree('Discontinuous tree', tree, sentence, nodedist=2)\n\n\n__all__ = ['TreePrettyPrinter']\n\nif __name__ == '__main__':\n    test()\n"], "nltk\\treetransforms": [".py", "\nfrom __future__ import print_function\n\nfrom nltk.tree import Tree\n\ndef chomsky_normal_form(tree, factor=\"right\", horzMarkov=None, vertMarkov=0, childChar=\"|\", parentChar=\"^\"):\n\n    if horzMarkov is None: horzMarkov = 999\n\n\n    nodeList = [(tree, [tree.label()])]\n    while nodeList != []:\n        node, parent = nodeList.pop()\n        if isinstance(node,Tree):\n\n            parentString = \"\"\n            originalNode = node.label()\n            if vertMarkov != 0 and node != tree and isinstance(node[0],Tree):\n                parentString = \"%s<%s>\" % (parentChar, \"-\".join(parent))\n                node.set_label(node.label() + parentString)\n                parent = [originalNode] + parent[:vertMarkov - 1]\n\n            for child in node:\n                nodeList.append((child, parent))\n\n            if len(node) > 2:\n                childNodes = [child.label() for child in node]\n                nodeCopy = node.copy()\n                node[0:] = [] # delete the children\n\n                curNode = node\n                numChildren = len(nodeCopy)\n                for i in range(1,numChildren - 1):\n                    if factor == \"right\":\n                        newHead = \"%s%s<%s>%s\" % (originalNode, childChar, \"-\".join(childNodes[i:min([i+horzMarkov,numChildren])]),parentString) # create new head\n                        newNode = Tree(newHead, [])\n                        curNode[0:] = [nodeCopy.pop(0), newNode]\n                    else:\n                        newHead = \"%s%s<%s>%s\" % (originalNode, childChar, \"-\".join(childNodes[max([numChildren-i-horzMarkov,0]):-i]),parentString)\n                        newNode = Tree(newHead, [])\n                        curNode[0:] = [newNode, nodeCopy.pop()]\n\n                    curNode = newNode\n\n                curNode[0:] = [child for child in nodeCopy]\n\n\ndef un_chomsky_normal_form(tree, expandUnary = True, childChar = \"|\", parentChar = \"^\", unaryChar = \"+\"):\n    nodeList = [(tree,[])]\n    while nodeList != []:\n        node,parent = nodeList.pop()\n        if isinstance(node,Tree):\n            childIndex = node.label().find(childChar)\n            if childIndex != -1:\n                nodeIndex = parent.index(node)\n                parent.remove(parent[nodeIndex])\n                if nodeIndex == 0:\n                    parent.insert(0,node[0])\n                    parent.insert(1,node[1])\n                else:\n                    parent.extend([node[0],node[1]])\n\n                node = parent\n            else:\n                parentIndex = node.label().find(parentChar)\n                if parentIndex != -1:\n                    node.set_label(node.label()[:parentIndex])\n\n                if expandUnary == True:\n                    unaryIndex = node.label().find(unaryChar)\n                    if unaryIndex != -1:\n                        newNode = Tree(node.label()[unaryIndex + 1:], [i for i in node])\n                        node.set_label(node.label()[:unaryIndex])\n                        node[0:] = [newNode]\n\n            for child in node:\n                nodeList.append((child,node))\n\n\ndef collapse_unary(tree, collapsePOS = False, collapseRoot = False, joinChar = \"+\"):\n\n    if collapseRoot == False and isinstance(tree, Tree) and len(tree) == 1:\n        nodeList = [tree[0]]\n    else:\n        nodeList = [tree]\n\n    while nodeList != []:\n        node = nodeList.pop()\n        if isinstance(node,Tree):\n            if len(node) == 1 and isinstance(node[0], Tree) and (collapsePOS == True or isinstance(node[0,0], Tree)):\n                node.set_label(node.label() + joinChar + node[0].label())\n                node[0:] = [child for child in node[0]]\n                nodeList.append(node)\n            else:\n                for child in node:\n                    nodeList.append(child)\n\n\ndef demo():\n\n    from nltk.draw.tree import draw_trees\n    from nltk import tree, treetransforms\n    from copy import deepcopy\n\n    sentence = \"\"\"(TOP\n  (S\n    (S\n      (VP\n        (VBN Turned)\n        (ADVP (RB loose))\n        (PP\n          (IN in)\n          (NP\n            (NP (NNP Shane) (NNP Longman) (POS 's))\n            (NN trading)\n            (NN room)))))\n    (, ,)\n    (NP (DT the) (NN yuppie) (NNS dealers))\n    (VP (AUX do) (NP (NP (RB little)) (ADJP (RB right))))\n    (. .)))\"\"\"\n    t = tree.Tree.fromstring(sentence, remove_empty_top_bracketing=True)\n\n    collapsedTree = deepcopy(t)\n    treetransforms.collapse_unary(collapsedTree)\n\n    cnfTree = deepcopy(collapsedTree)\n    treetransforms.chomsky_normal_form(cnfTree)\n\n    parentTree = deepcopy(collapsedTree)\n    treetransforms.chomsky_normal_form(parentTree, horzMarkov=2, vertMarkov=1)\n\n    original = deepcopy(parentTree)\n    treetransforms.un_chomsky_normal_form(original)\n\n    sentence2 = original.pprint()\n    print(sentence)\n    print(sentence2)\n    print(\"Sentences the same? \", sentence == sentence2)\n\n    draw_trees(t, collapsedTree, cnfTree, parentTree, original)\n\nif __name__ == '__main__':\n    demo()\n\n__all__ = [\"chomsky_normal_form\", \"un_chomsky_normal_form\", \"collapse_unary\"]\n"], "nltk\\twitter\\api": [".py", "\n\nfrom abc import ABCMeta, abstractmethod\nfrom six import add_metaclass\nfrom datetime import tzinfo, timedelta, datetime\nfrom nltk.compat import UTC\nimport time as _time\n\n\nclass LocalTimezoneOffsetWithUTC(tzinfo):\n    STDOFFSET = timedelta(seconds=-_time.timezone)\n\n    if _time.daylight:\n        DSTOFFSET = timedelta(seconds=-_time.altzone)\n    else:\n        DSTOFFSET = STDOFFSET\n\n    def utcoffset(self, dt):\n        return self.DSTOFFSET\n\n\nLOCAL = LocalTimezoneOffsetWithUTC()\n\n\n@add_metaclass(ABCMeta)\nclass BasicTweetHandler(object):\n    def __init__(self, limit=20):\n        self.limit = limit\n        self.counter = 0\n\n        self.do_stop = False\n\n        self.max_id = None\n\n    def do_continue(self):\n        return self.counter < self.limit and not self.do_stop\n\nclass TweetHandlerI(BasicTweetHandler):\n    def __init__(self, limit=20, upper_date_limit=None, lower_date_limit=None):\n        BasicTweetHandler.__init__(self, limit)\n\n        self.upper_date_limit = None\n        self.lower_date_limit = None\n        if upper_date_limit:\n            self.upper_date_limit = datetime(*upper_date_limit, tzinfo=LOCAL)\n        if lower_date_limit:\n            self.lower_date_limit = datetime(*lower_date_limit, tzinfo=LOCAL)\n\n        self.startingup = True\n\n    @abstractmethod\n    def handle(self, data):\n\n    @abstractmethod\n    def on_finish(self):\n\n    def check_date_limit(self, data, verbose=False):\n        if self.upper_date_limit or self.lower_date_limit:\n            date_fmt = '%a %b %d %H:%M:%S +0000 %Y'\n            tweet_date = \\\n                datetime.strptime(data['created_at'],\n                                  date_fmt).replace(tzinfo=UTC)\n            if (self.upper_date_limit and tweet_date > self.upper_date_limit) or \\\n               (self.lower_date_limit and tweet_date < self.lower_date_limit):\n                if self.upper_date_limit:\n                    message = \"earlier\"\n                    date_limit = self.upper_date_limit\n                else:\n                    message = \"later\"\n                    date_limit = self.lower_date_limit\n                if verbose:\n                    print(\"Date limit {0} is {1} than date of current tweet {2}\".\\\n                      format(date_limit, message, tweet_date))\n                self.do_stop = True\n"], "nltk\\twitter\\common": [".py", "\nfrom __future__ import print_function\n\nimport csv\nimport gzip\nimport json\n\nimport nltk.compat as compat\n\nHIER_SEPARATOR = \".\"\n\n\ndef extract_fields(tweet, fields):\n    out = []\n    for field in fields:\n        try:\n            _add_field_to_out(tweet, field, out)\n        except TypeError:\n            raise RuntimeError('Fatal error when extracting fields. Cannot find field ', field)\n    return out\n\n\ndef _add_field_to_out(json, field, out):\n    if _is_composed_key(field):\n        key, value = _get_key_value_composed(field)\n        _add_field_to_out(json[key], value, out)\n    else:\n        out += [json[field]]\n\n\ndef _is_composed_key(field):\n    if HIER_SEPARATOR in field:\n        return True\n    return False\n\n\ndef _get_key_value_composed(field):\n    out = field.split(HIER_SEPARATOR)\n    key = out[0]\n    value = HIER_SEPARATOR.join(out[1:])\n    return key, value\n\n\ndef _get_entity_recursive(json, entity):\n    if not json:\n        return None\n    elif isinstance(json, dict):\n        for key, value in json.items():\n            if key == entity:\n                return value\n\n            if key == 'entities' or key == 'extended_entities':\n                candidate = _get_entity_recursive(value, entity)\n                if candidate is not None:\n                    return candidate\n        return None\n    elif isinstance(json, list):\n        for item in json:\n            candidate = _get_entity_recursive(item, entity)\n            if candidate is not None:\n                return candidate\n        return None\n    else:\n        return None\n\n\ndef json2csv(fp, outfile, fields, encoding='utf8', errors='replace',\n             gzip_compress=False):\n    (writer, outf) = outf_writer_compat(outfile, encoding, errors, gzip_compress)\n    writer.writerow(fields)\n    for line in fp:\n        tweet = json.loads(line)\n        row = extract_fields(tweet, fields)\n        writer.writerow(row)\n    outf.close()\n\n\ndef outf_writer_compat(outfile, encoding, errors, gzip_compress=False):\n    if compat.PY3:\n        if gzip_compress:\n            outf = gzip.open(outfile, 'wt', encoding=encoding, errors=errors)\n        else:\n            outf = open(outfile, 'w', encoding=encoding, errors=errors)\n        writer = csv.writer(outf)\n    else:\n        if gzip_compress:\n            outf = gzip.open(outfile, 'wb')\n        else:\n            outf = open(outfile, 'wb')\n        writer = compat.UnicodeWriter(outf, encoding=encoding, errors=errors)\n    return (writer, outf)\n\n\ndef json2csv_entities(tweets_file, outfile, main_fields, entity_type, entity_fields,\n                      encoding='utf8', errors='replace', gzip_compress=False):\n\n    (writer, outf) = outf_writer_compat(outfile, encoding, errors, gzip_compress)\n    header = get_header_field_list(main_fields, entity_type, entity_fields)\n    writer.writerow(header)\n    for line in tweets_file:\n        tweet = json.loads(line)\n        if _is_composed_key(entity_type):\n            key, value = _get_key_value_composed(entity_type)\n            object_json = _get_entity_recursive(tweet, key)\n            if not object_json:\n                continue\n            object_fields = extract_fields(object_json, main_fields)\n            items = _get_entity_recursive(object_json, value)\n            _write_to_file(object_fields, items, entity_fields, writer)\n        else:\n            tweet_fields = extract_fields(tweet, main_fields)\n            items = _get_entity_recursive(tweet, entity_type)\n            _write_to_file(tweet_fields, items, entity_fields, writer)\n    outf.close()\n\n\ndef get_header_field_list(main_fields, entity_type, entity_fields):\n    if _is_composed_key(entity_type):\n        key, value = _get_key_value_composed(entity_type)\n        main_entity = key\n        sub_entity = value\n    else:\n        main_entity = None\n        sub_entity = entity_type\n\n    if main_entity:\n        output1 = [HIER_SEPARATOR.join([main_entity, x]) for x in main_fields]\n    else:\n        output1 = main_fields\n    output2 = [HIER_SEPARATOR.join([sub_entity, x]) for x in entity_fields]\n    return output1 + output2\n\n\ndef _write_to_file(object_fields, items, entity_fields, writer):\n    if not items:\n        return\n    if isinstance(items, dict):\n        row = object_fields\n        entity_field_values = [x for x in entity_fields if not _is_composed_key(x)]\n        entity_field_composed = [x for x in entity_fields if _is_composed_key(x)]\n        for field in entity_field_values:\n            value = items[field]\n            if isinstance(value, list):\n                row += value\n            else:\n                row += [value]\n        for d in entity_field_composed:\n            kd, vd = _get_key_value_composed(d)\n            json_dict = items[kd]\n            if not isinstance(json_dict, dict):\n                raise RuntimeError(\"\"\"Key {0} does not contain a dictionary\n                in the json file\"\"\".format(kd))\n            row += [json_dict[vd]]\n        writer.writerow(row)\n        return\n    for item in items:\n        row = object_fields + extract_fields(item, entity_fields)\n        writer.writerow(row)\n"], "nltk\\twitter\\twitterclient": [".py", "\n\n\nimport datetime\nimport itertools\nimport json\nimport os\nimport requests\nimport time\nimport gzip\n\n\nfrom twython import Twython, TwythonStreamer\nfrom twython.exceptions import TwythonRateLimitError, TwythonError\n\nfrom nltk.twitter.util import credsfromfile, guess_path\nfrom nltk.twitter.api import TweetHandlerI, BasicTweetHandler\n\n\n\nclass Streamer(TwythonStreamer):\n    def __init__(self, app_key, app_secret, oauth_token, oauth_token_secret):\n\n        self.handler = None\n        self.do_continue = True\n        TwythonStreamer.__init__(self, app_key, app_secret, oauth_token,\n                                 oauth_token_secret)\n\n    def register(self, handler):\n        self.handler = handler\n\n    def on_success(self, data):\n        if self.do_continue:\n            if self.handler is not None:\n                if 'text' in data:\n                    self.handler.counter += 1\n                    self.handler.handle(data)\n                    self.do_continue = self.handler.do_continue()\n            else:\n                raise ValueError(\"No data handler has been registered.\")\n        else:\n            self.disconnect()\n            self.handler.on_finish()\n\n\n    def on_error(self, status_code, data):\n        print(status_code)\n\n    def sample(self):\n        while self.do_continue:\n\n\n            try:\n                self.statuses.sample()\n            except requests.exceptions.ChunkedEncodingError as e:\n                if e is not None:\n                    print(\"Error (stream will continue): {0}\".format(e))\n                continue\n\n    def filter(self, track='', follow='', lang='en'):\n        while self.do_continue:\n\n            try:\n                if track == '' and follow == '':\n                    msg = \"Please supply a value for 'track', 'follow'\"\n                    raise ValueError(msg)\n                self.statuses.filter(track=track, follow=follow, lang=lang)\n            except requests.exceptions.ChunkedEncodingError as e:\n                if e is not None:\n                    print(\"Error (stream will continue): {0}\".format(e))\n                continue\n\n\nclass Query(Twython):\n    def __init__(self, app_key, app_secret, oauth_token,\n                 oauth_token_secret):\n        self.handler = None\n        self.do_continue = True\n        Twython.__init__(self, app_key, app_secret, oauth_token, oauth_token_secret)\n\n    def register(self, handler):\n        self.handler = handler\n\n    def expand_tweetids(self, ids_f, verbose=True):\n        ids = [line.strip() for line in ids_f if line]\n\n        if verbose:\n            print(\"Counted {0} Tweet IDs in {1}.\".format(len(ids), ids_f))\n\n        id_chunks = [ids[i:i+100] for i in range(0, len(ids), 100)]\n\n        chunked_tweets = (self.lookup_status(id=chunk) for chunk in\n                          id_chunks)\n\n        return itertools.chain.from_iterable(chunked_tweets)\n\n\n\n    def _search_tweets(self, keywords, limit=100, lang='en'):\n        while True:\n            tweets = self.search_tweets(keywords=keywords, limit=limit, lang=lang,\n                                        max_id=self.handler.max_id)\n            for tweet in tweets:\n                self.handler.handle(tweet)\n            if not (self.handler.do_continue() and self.handler.repeat):\n                break\n        self.handler.on_finish()\n\n    def search_tweets(self, keywords, limit=100, lang='en', max_id=None,\n                      retries_after_twython_exception=0):\n        if not self.handler:\n            self.handler = BasicTweetHandler(limit=limit)\n\n        count_from_query = 0\n        if max_id:\n            self.handler.max_id = max_id\n        else:\n            results = self.search(q=keywords, count=min(100, limit), lang=lang,\n                                  result_type='recent')\n            count = len(results['statuses'])\n            if count == 0:\n                print(\"No Tweets available through REST API for those keywords\")\n                return\n            count_from_query = count\n            self.handler.max_id = results['statuses'][count - 1]['id'] - 1\n\n            for result in results['statuses']:\n                yield result\n                self.handler.counter += 1\n                if self.handler.do_continue() == False:\n                    return\n\n\n        retries = 0\n        while count_from_query < limit:\n            try:\n                mcount = min(100, limit-count_from_query)\n                results = self.search(q=keywords, count=mcount, lang=lang,\n                                      max_id=self.handler.max_id, result_type='recent')\n            except TwythonRateLimitError as e:\n                print(\"Waiting for 15 minutes -{0}\".format(e))\n                time.sleep(15*60) # wait 15 minutes\n                continue\n            except TwythonError as e:\n                print(\"Fatal error in Twython request -{0}\".format(e))\n                if retries_after_twython_exception == retries:\n                    raise e\n                retries += 1\n\n            count = len(results['statuses'])\n            if count == 0:\n                print(\"No more Tweets available through rest api\")\n                return\n            count_from_query += count\n            self.handler.max_id = results['statuses'][count - 1]['id'] - 1\n\n            for result in results['statuses']:\n                yield result\n                self.handler.counter += 1\n                if self.handler.do_continue() == False:\n                    return\n\n    def user_info_from_id(self, userids):\n        return [self.show_user(user_id=userid) for userid in userids]\n\n    def user_tweets(self, screen_name, limit, include_rts='false'):\n        data = self.get_user_timeline(screen_name=screen_name, count=limit,\n                                      include_rts=include_rts)\n        for item in data:\n            self.handler.handle(item)\n\n\n\n\nclass Twitter(object):\n    def __init__(self):\n        self._oauth = credsfromfile()\n        self.streamer = Streamer(**self._oauth)\n        self.query = Query(**self._oauth)\n\n\n    def tweets(self, keywords='', follow='', to_screen=True, stream=True,\n               limit=100, date_limit=None, lang='en', repeat=False,\n               gzip_compress=False):\n        if stream:\n            upper_date_limit = date_limit\n            lower_date_limit = None\n        else:\n            upper_date_limit = None\n            lower_date_limit = date_limit\n\n        if to_screen:\n            handler = TweetViewer(limit=limit,\n                                  upper_date_limit=upper_date_limit,\n                                  lower_date_limit=lower_date_limit)\n        else:\n            handler = TweetWriter(limit=limit,\n                                  upper_date_limit=upper_date_limit,\n                                  lower_date_limit=lower_date_limit, repeat=repeat,\n                                  gzip_compress=gzip_compress)\n\n\n\n        if to_screen:\n            handler = TweetViewer(limit=limit)\n        else:\n            if stream:\n                upper_date_limit = date_limit\n                lower_date_limit = None\n            else:\n                upper_date_limit = None\n                lower_date_limit = date_limit\n\n            handler = TweetWriter(limit=limit, upper_date_limit=upper_date_limit,\n                                  lower_date_limit=lower_date_limit, repeat=repeat,\n                                  gzip_compress=gzip_compress)\n\n        if stream:\n            self.streamer.register(handler)\n            if keywords == '' and follow == '':\n                self.streamer.sample()\n            else:\n                self.streamer.filter(track=keywords, follow=follow, lang=lang)\n        else:\n            self.query.register(handler)\n            if keywords == '':\n                raise ValueError(\"Please supply at least one keyword to search for.\")\n            else:\n                self.query._search_tweets(keywords, limit=limit, lang=lang)\n\n\n\nclass TweetViewer(TweetHandlerI):\n\n    def handle(self, data):\n        text = data['text']\n        print(text)\n\n        self.check_date_limit(data)\n        if self.do_stop:\n            return\n\n    def on_finish(self):\n        print('Written {0} Tweets'.format(self.counter))\n\n\nclass TweetWriter(TweetHandlerI):\n    def __init__(self, limit=2000, upper_date_limit=None, lower_date_limit=None,\n                 fprefix='tweets', subdir='twitter-files', repeat=False,\n                 gzip_compress=False):\n        self.fprefix = fprefix\n        self.subdir = guess_path(subdir)\n        self.gzip_compress = gzip_compress\n        self.fname = self.timestamped_file()\n        self.repeat = repeat\n        self.output = None\n        TweetHandlerI.__init__(self, limit, upper_date_limit, lower_date_limit)\n\n\n    def timestamped_file(self):\n        subdir = self.subdir\n        fprefix = self.fprefix\n        if subdir:\n            if not os.path.exists(subdir):\n                os.mkdir(subdir)\n\n        fname = os.path.join(subdir, fprefix)\n        fmt = '%Y%m%d-%H%M%S'\n        timestamp = datetime.datetime.now().strftime(fmt)\n        if self.gzip_compress:\n            suffix = '.gz'\n        else:\n            suffix = ''\n        outfile = '{0}.{1}.json{2}'.format(fname, timestamp, suffix)\n        return outfile\n\n\n    def handle(self, data):\n        if self.startingup:\n            if self.gzip_compress:\n                self.output = gzip.open(self.fname, 'w')\n            else:\n                self.output = open(self.fname, 'w')\n            print('Writing to {0}'.format(self.fname))\n\n        json_data = json.dumps(data)\n        if self.gzip_compress:\n            self.output.write((json_data + \"\\n\").encode('utf-8'))\n        else:\n            self.output.write(json_data + \"\\n\")\n\n        self.check_date_limit(data)\n        if self.do_stop:\n            return\n\n        self.startingup = False\n\n    def on_finish(self):\n        print('Written {0} Tweets'.format(self.counter))\n        if self.output:\n            self.output.close()\n\n    def do_continue(self):\n        if self.repeat == False:\n            return TweetHandlerI.do_continue(self)\n\n        if self.do_stop:\n            return False\n\n        if self.counter == self.limit:\n            self._restart_file()\n        return True\n\n\n    def _restart_file(self):\n        self.on_finish()\n        self.fname = self.timestamped_file()\n        self.startingup = True\n        self.counter = 0\n"], "nltk\\twitter\\twitter_demo": [".py", "\nfrom __future__ import print_function\n\nimport datetime\nfrom functools import wraps\nimport json\n\nfrom nltk.compat import StringIO\n\nfrom nltk.twitter import Query, Streamer, Twitter, TweetViewer, TweetWriter,\\\n     credsfromfile\n\n\nSPACER = '###################################'\n\ndef verbose(func):\n    @wraps(func)\n    def with_formatting(*args, **kwargs):\n        print()\n        print(SPACER)\n        print(\"Using %s\" % (func.__name__))\n        print(SPACER)\n        return func(*args, **kwargs)\n    return with_formatting\n\ndef yesterday():\n    date =  datetime.datetime.now()\n    date -= datetime.timedelta(days=1)\n    date_tuple = date.timetuple()[:6]\n    return date_tuple\n\ndef setup():\n    global USERIDS, FIELDS\n\n    USERIDS = ['759251', '612473', '15108702', '6017542', '2673523800']\n    FIELDS = ['id_str']\n\n\n@verbose\ndef twitterclass_demo():\n    tw = Twitter()\n    print(\"Track from the public stream\\n\")\n    tw.tweets(keywords='love, hate', limit=10) #public stream\n    print(SPACER)\n    print(\"Search past Tweets\\n\")\n    tw = Twitter()\n    tw.tweets(keywords='love, hate', stream=False, limit=10) # search past tweets\n    print(SPACER)\n    print(\"Follow two accounts in the public stream\" +\n          \" -- be prepared to wait a few minutes\\n\")\n    tw = Twitter()\n    tw.tweets(follow=['759251', '6017542'], stream=True, limit=5) #public stream\n\n\n@verbose\ndef sampletoscreen_demo(limit=20):\n    oauth = credsfromfile()\n    client = Streamer(**oauth)\n    client.register(TweetViewer(limit=limit))\n    client.sample()\n\n\n@verbose\ndef tracktoscreen_demo(track=\"taylor swift\", limit=10):\n    oauth = credsfromfile()\n    client = Streamer(**oauth)\n    client.register(TweetViewer(limit=limit))\n    client.filter(track=track)\n\n\n@verbose\ndef search_demo(keywords='nltk'):\n    oauth = credsfromfile()\n    client = Query(**oauth)\n    for tweet in client.search_tweets(keywords=keywords, limit=10):\n        print(tweet['text'])\n\n\n@verbose\ndef tweets_by_user_demo(user='NLTK_org', count=200):\n    oauth = credsfromfile()\n    client = Query(**oauth)\n    client.register(TweetWriter())\n    client.user_tweets(user, count)\n\n\n@verbose\ndef lookup_by_userid_demo():\n    oauth = credsfromfile()\n    client = Query(**oauth)\n    user_info = client.user_info_from_id(USERIDS)\n    for info in user_info:\n        name = info['screen_name']\n        followers = info['followers_count']\n        following = info['friends_count']\n        print(\"{0}, followers: {1}, following: {2}\".format(name, followers, following))\n\n\n@verbose\ndef followtoscreen_demo(limit=10):\n    oauth = credsfromfile()\n    client = Streamer(**oauth)\n    client.register(TweetViewer(limit=limit))\n    client.statuses.filter(follow=USERIDS)\n\n\n@verbose\ndef streamtofile_demo(limit=20):\n    oauth = credsfromfile()\n    client = Streamer(**oauth)\n    client.register(TweetWriter(limit=limit, repeat=False))\n    client.statuses.sample()\n\n\n@verbose\ndef limit_by_time_demo(keywords=\"nltk\"):\n    date = yesterday()\n    dt_date = datetime.datetime(*date)\n    oauth = credsfromfile()\n    client = Query(**oauth)\n    client.register(TweetViewer(limit=100, lower_date_limit=date))\n\n    print(\"Cutoff date: {}\\n\".format(dt_date))\n\n    for tweet in client.search_tweets(keywords=keywords):\n        print(\"{} \".format(tweet['created_at']), end='')\n        client.handler.handle(tweet)\n\n\n@verbose\ndef corpusreader_demo():\n    from nltk.corpus import twitter_samples as tweets\n\n    print()\n    print(\"Complete tweet documents\")\n    print(SPACER)\n    for tweet in tweets.docs(\"tweets.20150430-223406.json\")[:1]:\n        print(json.dumps(tweet, indent=1, sort_keys=True))\n\n    print()\n    print(\"Raw tweet strings:\")\n    print(SPACER)\n    for text in tweets.strings(\"tweets.20150430-223406.json\")[:15]:\n        print(text)\n\n    print()\n    print(\"Tokenized tweet strings:\")\n    print(SPACER)\n    for toks in tweets.tokenized(\"tweets.20150430-223406.json\")[:15]:\n        print(toks)\n\n\n@verbose\ndef expand_tweetids_demo():\n    ids_f =\\\n        StringIO(\"\"\"\\\n        588665495492124672\n        588665495487909888\n        588665495508766721\n        588665495513006080\n        588665495517200384\n        588665495487811584\n        588665495525588992\n        588665495487844352\n        588665495492014081\n        588665495512948737\"\"\")\n    oauth = credsfromfile()\n    client = Query(**oauth)\n    hydrated = client.expand_tweetids(ids_f)\n\n    for tweet in hydrated:\n            id_str = tweet['id_str']\n            print('id: {}'.format(id_str))\n            text = tweet['text']\n            if text.startswith('@null'):\n                text = \"[Tweet not available]\"\n            print(text + '\\n')\n\n\n\nALL = [twitterclass_demo, sampletoscreen_demo, tracktoscreen_demo,\n       search_demo, tweets_by_user_demo, lookup_by_userid_demo, followtoscreen_demo,\n       streamtofile_demo, limit_by_time_demo, corpusreader_demo, expand_tweetids_demo]\n\n\"\"\"\nSelect demo functions to run. E.g. replace the following line with \"DEMOS =\nALL[8:]\" to execute only the final three demos.\n\"\"\"\nDEMOS = ALL[:]\n\nif __name__ == \"__main__\":\n    setup()\n\n    for demo in DEMOS:\n        demo()\n\n    print(\"\\n\" + SPACER)\n    print(\"All demos completed\")\n    print(SPACER)\n\n"], "nltk\\twitter\\util": [".py", "\n\nfrom __future__ import print_function\n\nimport os\nimport pprint\nfrom twython import Twython\n\n\ndef credsfromfile(creds_file=None, subdir=None, verbose=False):\n    return Authenticate().load_creds(creds_file=creds_file, subdir=subdir, verbose=verbose)\n\n\nclass Authenticate(object):\n\n    def __init__(self):\n        self.creds_file = 'credentials.txt'\n        self.creds_fullpath = None\n\n        self.oauth = {}\n        try:\n            self.twitter_dir = os.environ['TWITTER']\n            self.creds_subdir = self.twitter_dir\n        except KeyError:\n            self.twitter_dir = None\n            self.creds_subdir = None\n\n    def load_creds(self, creds_file=None, subdir=None, verbose=False):\n        if creds_file is not None:\n            self.creds_file = creds_file\n\n        if subdir is None:\n            if self.creds_subdir is None:\n                msg = \"Supply a value to the 'subdir' parameter or\" + \\\n                      \" set the TWITTER environment variable.\"\n                raise ValueError(msg)\n        else:\n            self.creds_subdir = subdir\n\n        self.creds_fullpath = \\\n            os.path.normpath(os.path.join(self.creds_subdir, self.creds_file))\n\n        if not os.path.isfile(self.creds_fullpath):\n            raise OSError('Cannot find file {}'.format(self.creds_fullpath))\n\n        with open(self.creds_fullpath) as infile:\n            if verbose:\n                print('Reading credentials file {}'.format(self.creds_fullpath))\n\n            for line in infile:\n                if '=' in line:\n                    name, value = line.split('=', 1)\n                    self.oauth[name.strip()] = value.strip()\n\n        self._validate_creds_file(verbose=verbose)\n\n        return self.oauth\n\n    def _validate_creds_file(self, verbose=False):\n        oauth1 = False\n        oauth1_keys = ['app_key', 'app_secret', 'oauth_token', 'oauth_token_secret']\n        oauth2 = False\n        oauth2_keys = ['app_key', 'app_secret', 'access_token']\n        if all(k in self.oauth for k in oauth1_keys):\n            oauth1 = True\n        elif all(k in self.oauth for k in oauth2_keys):\n            oauth2 = True\n\n        if not (oauth1 or oauth2):\n            msg = 'Missing or incorrect entries in {}\\n'.format(self.creds_file)\n            msg += pprint.pformat(self.oauth)\n            raise ValueError(msg)\n        elif verbose:\n            print('Credentials file \"{}\" looks good'.format(self.creds_file))\n\n\ndef add_access_token(creds_file=None):\n    if creds_file is None:\n        path = os.path.dirname(__file__)\n        creds_file = os.path.join(path, 'credentials2.txt')\n    oauth2 = credsfromfile(creds_file=creds_file)\n    app_key = oauth2['app_key']\n    app_secret = oauth2['app_secret']\n\n    twitter = Twython(app_key, app_secret, oauth_version=2)\n    access_token = twitter.obtain_access_token()\n    tok = 'access_token={}\\n'.format(access_token)\n    with open(creds_file, 'a') as infile:\n        print(tok, file=infile)\n\n\ndef guess_path(pth):\n    if os.path.isabs(pth):\n        return pth\n    else:\n        return os.path.expanduser(os.path.join(\"~\", pth))\n"], "nltk\\twitter\\__init__": [".py", "\ntry:\n    import twython\nexcept ImportError:\n    import warnings\n    warnings.warn(\"The twython library has not been installed. \"\n                  \"Some functionality from the twitter package will not be available.\")\nelse:\n    from nltk.twitter.util import Authenticate, credsfromfile\n    from nltk.twitter.twitterclient import Streamer, Query, Twitter,\\\n         TweetViewer, TweetWriter\n\n\nfrom nltk.twitter.common import json2csv\n", 1], "nltk\\util": [".py", "from __future__ import print_function\n\nimport sys\nimport inspect\nimport locale\nimport re\nimport types\nimport textwrap\nimport pydoc\nimport bisect\nimport os\n\nfrom itertools import islice, chain, combinations\nfrom pprint import pprint\nfrom collections import defaultdict, deque\nfrom sys import version_info\n\nfrom six import class_types, string_types, text_type\nfrom six.moves.urllib.request import (build_opener, install_opener, getproxies,\n                                      ProxyHandler, ProxyBasicAuthHandler,\n                                      ProxyDigestAuthHandler,\n                                      HTTPPasswordMgrWithDefaultRealm)\n\nfrom nltk.internals import slice_bounds, raise_unorderable_types\nfrom nltk.collections import *\nfrom nltk.compat import python_2_unicode_compatible\n\n\n\n\ndef usage(obj, selfname='self'):\n    str(obj) # In case it's lazy, this will load it.\n\n    if not isinstance(obj, class_types):\n        obj = obj.__class__\n\n    print('%s supports the following operations:' % obj.__name__)\n    for (name, method) in sorted(pydoc.allmethods(obj).items()):\n        if name.startswith('_'): continue\n        if getattr(method, '__deprecated__', False): continue\n\n        if sys.version_info[0] >= 3:\n            getargspec = inspect.getfullargspec\n        else:\n            getargspec = inspect.getargspec\n        args, varargs, varkw, defaults = getargspec(method)[:4]\n        if (args and args[0]=='self' and\n            (defaults is None or len(args)>len(defaults))):\n            args = args[1:]\n            name = '%s.%s' % (selfname, name)\n        argspec = inspect.formatargspec(\n            args, varargs, varkw, defaults)\n        print(textwrap.fill('%s%s' % (name, argspec),\n                            initial_indent='  - ',\n                            subsequent_indent=' '*(len(name)+5)))\n\n\ndef in_idle():\n    import sys\n    return sys.stdin.__class__.__name__ in ('PyShell', 'RPCProxy')\n\n\ndef pr(data, start=0, end=None):\n    pprint(list(islice(data, start, end)))\n\ndef print_string(s, width=70):\n    print('\\n'.join(textwrap.wrap(s, width=width)))\n\ndef tokenwrap(tokens, separator=\" \", width=70):\n    return '\\n'.join(textwrap.wrap(separator.join(tokens), width=width))\n\n\n\ndef py25():\n    return version_info[0] == 2 and version_info[1] == 5\ndef py26():\n    return version_info[0] == 2 and version_info[1] == 6\ndef py27():\n    return version_info[0] == 2 and version_info[1] == 7\n\n\n\nclass Index(defaultdict):\n\n    def __init__(self, pairs):\n        defaultdict.__init__(self, list)\n        for key, value in pairs:\n            self[key].append(value)\n\n\n\ndef re_show(regexp, string, left=\"{\", right=\"}\"):\n    print(re.compile(regexp, re.M).sub(left + r\"\\g<0>\" + right, string.rstrip()))\n\n\n\ndef filestring(f):\n    if hasattr(f, 'read'):\n        return f.read()\n    elif isinstance(f, string_types):\n        with open(f, 'r') as infile:\n            return infile.read()\n    else:\n        raise ValueError(\"Must be called with a filename or file-like object\")\n\n\ndef breadth_first(tree, children=iter, maxdepth=-1):\n    queue = deque([(tree, 0)])\n\n    while queue:\n        node, depth = queue.popleft()\n        yield node\n\n        if depth != maxdepth:\n            try:\n                queue.extend((c, depth + 1) for c in children(node))\n            except TypeError:\n                pass\n\n\n\ndef guess_encoding(data):\n    successful_encoding = None\n    encodings = ['utf-8']\n    try:\n        encodings.append(locale.nl_langinfo(locale.CODESET))\n    except AttributeError:\n        pass\n    try:\n        encodings.append(locale.getlocale()[1])\n    except (AttributeError, IndexError):\n        pass\n    try:\n        encodings.append(locale.getdefaultlocale()[1])\n    except (AttributeError, IndexError):\n        pass\n    encodings.append('latin-1')\n    for enc in encodings:\n        if not enc:\n            continue\n        try:\n            decoded = text_type(data, enc)\n            successful_encoding = enc\n\n        except (UnicodeError, LookupError):\n            pass\n        else:\n            break\n    if not successful_encoding:\n         raise UnicodeError(\n        'Unable to decode input data.  Tried the following encodings: %s.'\n        % ', '.join([repr(enc) for enc in encodings if enc]))\n    else:\n         return (decoded, successful_encoding)\n\n\n\ndef unique_list(xs):\n    seen = set()\n    return [x for x in xs if x not in seen and not seen.add(x)]\n\n\ndef invert_dict(d):\n    inverted_dict = defaultdict(list)\n    for key in d:\n        if hasattr(d[key], '__iter__'):\n            for term in d[key]:\n                inverted_dict[term].append(key)\n        else:\n            inverted_dict[d[key]] = key\n    return inverted_dict\n\n\n\ndef transitive_closure(graph, reflexive=False):\n    if reflexive:\n        base_set = lambda k: set([k])\n    else:\n        base_set = lambda k: set()\n    agenda_graph = dict((k, graph[k].copy()) for k in graph)\n    closure_graph = dict((k, base_set(k)) for k in graph)\n    for i in graph:\n        agenda = agenda_graph[i]\n        closure = closure_graph[i]\n        while agenda:\n            j = agenda.pop()\n            closure.add(j)\n            closure |= closure_graph.setdefault(j, base_set(j))\n            agenda |= agenda_graph.get(j, base_set(j))\n            agenda -= closure\n    return closure_graph\n\n\ndef invert_graph(graph):\n    inverted = {}\n    for key in graph:\n        for value in graph[key]:\n            inverted.setdefault(value, set()).add(key)\n    return inverted\n\n\n\n\ndef clean_html(html):\n    raise NotImplementedError (\"To remove HTML markup, use BeautifulSoup's get_text() function\")\n\ndef clean_url(url):\n    raise NotImplementedError (\"To remove HTML markup, use BeautifulSoup's get_text() function\")\n\n\ndef flatten(*args):\n\n    x = []\n    for l in args:\n        if not isinstance(l, (list, tuple)): l = [l]\n        for item in l:\n            if isinstance(item, (list, tuple)):\n                x.extend(flatten(item))\n            else:\n                x.append(item)\n    return x\n\n\ndef pad_sequence(sequence, n, pad_left=False, pad_right=False,\n                 left_pad_symbol=None, right_pad_symbol=None):\n    sequence = iter(sequence)\n    if pad_left:\n        sequence = chain((left_pad_symbol,) * (n-1), sequence)\n    if pad_right:\n        sequence = chain(sequence, (right_pad_symbol,) * (n-1))\n    return sequence\n\n\ndef ngrams(sequence, n, pad_left=False, pad_right=False,\n           left_pad_symbol=None, right_pad_symbol=None):\n    sequence = pad_sequence(sequence, n, pad_left, pad_right,\n                            left_pad_symbol, right_pad_symbol)\n\n    history = []\n    while n > 1:\n        history.append(next(sequence))\n        n -= 1\n    for item in sequence:\n        history.append(item)\n        yield tuple(history)\n        del history[0]\n\ndef bigrams(sequence, **kwargs):\n\n    for item in ngrams(sequence, 2, **kwargs):\n        yield item\n\ndef trigrams(sequence, **kwargs):\n\n    for item in ngrams(sequence, 3, **kwargs):\n        yield item\n\ndef everygrams(sequence, min_len=1, max_len=-1, **kwargs):\n\n    if max_len == -1:\n        max_len = len(sequence)\n    for n in range(min_len, max_len+1):\n        for ng in ngrams(sequence, n, **kwargs):\n            yield ng\n\ndef skipgrams(sequence, n, k, **kwargs):\n\n    if 'pad_left' in kwargs or 'pad_right' in kwargs:\n        sequence = pad_sequence(sequence, n, **kwargs)\n\n    SENTINEL = object()\n    for ngram in ngrams(sequence, n + k, pad_right=True, right_pad_symbol=SENTINEL):\n        head = ngram[:1]\n        tail = ngram[1:]\n        for skip_tail in combinations(tail, n - 1):\n            if skip_tail[-1] is SENTINEL:\n                continue\n            yield head + skip_tail\n\n\ndef binary_search_file(file, key, cache={}, cacheDepth=-1):\n\n    key = key + ' '\n    keylen = len(key)\n    start = 0\n    currentDepth = 0\n\n    if hasattr(file, 'name'):\n        end = os.stat(file.name).st_size - 1\n    else:\n        file.seek(0, 2)\n        end = file.tell() - 1\n        file.seek(0)\n\n    while start < end:\n        lastState = start, end\n        middle = (start + end) // 2\n\n        if cache.get(middle):\n            offset, line = cache[middle]\n\n        else:\n            line = \"\"\n            while True:\n                file.seek(max(0, middle - 1))\n                if middle > 0:\n                    file.readline()\n                offset = file.tell()\n                line = file.readline()\n                if line != \"\": break\n                middle = (start + middle)//2\n                if middle == end -1:\n                    return None\n            if currentDepth < cacheDepth:\n                cache[middle] = (offset, line)\n\n        if offset > end:\n            assert end != middle - 1, \"infinite loop\"\n            end = middle - 1\n        elif line[:keylen] == key:\n            return line\n        elif line > key:\n            assert end != middle - 1, \"infinite loop\"\n            end = middle - 1\n        elif line < key:\n            start = offset + len(line) - 1\n\n        currentDepth += 1\n        thisState = start, end\n\n        if lastState == thisState:\n            return None\n\n    return None\n\n\ndef set_proxy(proxy, user=None, password=''):\n    from nltk import compat\n\n    if proxy is None:\n        try:\n            proxy = getproxies()['http']\n        except KeyError:\n            raise ValueError('Could not detect default proxy settings')\n\n    proxy_handler = ProxyHandler({'https': proxy, 'http': proxy})\n    opener = build_opener(proxy_handler)\n\n    if user is not None:\n        password_manager = HTTPPasswordMgrWithDefaultRealm()\n        password_manager.add_password(realm=None, uri=proxy, user=user,\n                passwd=password)\n        opener.add_handler(ProxyBasicAuthHandler(password_manager))\n        opener.add_handler(ProxyDigestAuthHandler(password_manager))\n\n    install_opener(opener)\n\n\n\n\ndef elementtree_indent(elem, level=0):\n\n    i = \"\\n\" + level*\"  \"\n    if len(elem):\n        if not elem.text or not elem.text.strip():\n            elem.text = i + \"  \"\n        for elem in elem:\n            elementtree_indent(elem, level+1)\n        if not elem.tail or not elem.tail.strip():\n            elem.tail = i\n    else:\n        if level and (not elem.tail or not elem.tail.strip()):\n            elem.tail = i\n\n\ndef choose(n, k):\n    if 0 <= k <= n:\n        ntok, ktok = 1, 1\n        for t in range(1, min(k, n - k) + 1):\n            ntok *= n\n            ktok *= t\n            n -= 1\n        return ntok // ktok\n    else:\n        return 0\n"], "nltk\\wsd": [".py", "\nfrom nltk.corpus import wordnet\n\n\ndef lesk(context_sentence, ambiguous_word, pos=None, synsets=None):\n\n    context = set(context_sentence)\n    if synsets is None:\n        synsets = wordnet.synsets(ambiguous_word)\n\n    if pos:\n        synsets = [ss for ss in synsets if str(ss.pos()) == pos]\n\n    if not synsets:\n        return None\n\n    _, sense = max(\n        (len(context.intersection(ss.definition().split())), ss) for ss in synsets\n    )\n\n    return sense\n\n\n"], "nltk\\__init__": [".py", "\nfrom __future__ import print_function, absolute_import\n\nimport os\n\n\ntry:\n    version_file = os.path.join(os.path.dirname(__file__), 'VERSION')\n    with open(version_file, 'r') as infile:\n        __version__ = infile.read().strip()\nexcept NameError:\n    __version__ = 'unknown (running code interactively?)'\nexcept IOError as ex:\n    __version__ = \"unknown (%s)\" % ex\n\nif __doc__ is not None:  # fix for the ``python -OO``\n    __doc__ += '\\n@version: ' + __version__\n\n\n__copyright__ = \"\"\"\\\nCopyright (C) 2001-2018 NLTK Project.\n\nDistributed and Licensed under the Apache License, Version 2.0,\nwhich is included by reference.\n\"\"\"\n\n__license__ = \"Apache License, Version 2.0\"\n__longdescr__ = \"\"\"\\\nThe Natural Language Toolkit (NLTK) is a Python package for\nnatural language processing.  NLTK requires Python 2.6 or higher.\"\"\"\n__keywords__ = ['NLP', 'CL', 'natural language processing',\n                'computational linguistics', 'parsing', 'tagging',\n                'tokenizing', 'syntax', 'linguistics', 'language',\n                'natural language', 'text analytics']\n__url__ = \"http://nltk.org/\"\n\n__maintainer__ = \"Steven Bird, Edward Loper, Ewan Klein\"\n__maintainer_email__ = \"stevenbird1@gmail.com\"\n__author__ = __maintainer__\n__author_email__ = __maintainer_email__\n\n__classifiers__ = [\n    'Development Status :: 5 - Production/Stable',\n    'Intended Audience :: Developers',\n    'Intended Audience :: Education',\n    'Intended Audience :: Information Technology',\n    'Intended Audience :: Science/Research',\n    'License :: OSI Approved :: Apache Software License',\n    'Operating System :: OS Independent',\n    'Programming Language :: Python :: 2.6',\n    'Programming Language :: Python :: 2.7',\n    'Topic :: Scientific/Engineering',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    'Topic :: Scientific/Engineering :: Human Machine Interfaces',\n    'Topic :: Scientific/Engineering :: Information Analysis',\n    'Topic :: Text Processing',\n    'Topic :: Text Processing :: Filters',\n    'Topic :: Text Processing :: General',\n    'Topic :: Text Processing :: Indexing',\n    'Topic :: Text Processing :: Linguistic',\n]\n\nfrom nltk.internals import config_java\n\ntry:\n    import numpypy\nexcept ImportError:\n    pass\n\nimport subprocess\nif not hasattr(subprocess, 'PIPE'):\n    def _fake_PIPE(*args, **kwargs):\n        raise NotImplementedError('subprocess.PIPE is not supported.')\n    subprocess.PIPE = _fake_PIPE\nif not hasattr(subprocess, 'Popen'):\n    def _fake_Popen(*args, **kwargs):\n        raise NotImplementedError('subprocess.Popen is not supported.')\n    subprocess.Popen = _fake_Popen\n\n\n\nfrom nltk.collocations import *\nfrom nltk.decorators import decorator, memoize\nfrom nltk.featstruct import *\nfrom nltk.grammar import *\nfrom nltk.probability import *\nfrom nltk.text import *\nfrom nltk.tree import *\nfrom nltk.util import *\nfrom nltk.jsontags import *\n\n\nfrom nltk.chunk import *\nfrom nltk.classify import *\nfrom nltk.inference import *\nfrom nltk.metrics import *\nfrom nltk.parse import *\nfrom nltk.tag import *\nfrom nltk.tokenize import *\nfrom nltk.translate import *\nfrom nltk.sem import *\nfrom nltk.stem import *\n\n\nfrom nltk import lazyimport\napp = lazyimport.LazyModule('nltk.app', locals(), globals())\nchat = lazyimport.LazyModule('nltk.chat', locals(), globals())\ncorpus = lazyimport.LazyModule('nltk.corpus', locals(), globals())\ndraw = lazyimport.LazyModule('nltk.draw', locals(), globals())\ntoolbox = lazyimport.LazyModule('nltk.toolbox', locals(), globals())\n\n\ntry:\n    import numpy\nexcept ImportError:\n    pass\nelse:\n    from nltk import cluster\n\nfrom nltk.downloader import download, download_shell\ntry:\n    from six.moves import tkinter\nexcept ImportError:\n    pass\nelse:\n    try:\n        from nltk.downloader import download_gui\n    except RuntimeError as e:\n        import warnings\n        warnings.warn(\"Corpus downloader GUI not loaded \"\n                      \"(RuntimeError during import: %s)\" % str(e))\n\n\nfrom nltk import ccg, chunk, classify, collocations\nfrom nltk import data, featstruct, grammar, help, inference, metrics\nfrom nltk import misc, parse, probability, sem, stem, wsd\nfrom nltk import tag, tbl, text, tokenize, translate, tree, treetransforms, util\n\n\ndef demo():\n    print(\"To run the demo code for a module, type nltk.module.demo()\")\n", 1], "inflect": [".py", "'''\n    inflect.py: correctly generate plurals, ordinals, indefinite articles;\n                convert numbers to words\n    Copyright (C) 2010 Paul Dyson\n\n    Based upon the Perl module Lingua::EN::Inflect by Damian Conway.\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n    The original Perl module Lingua::EN::Inflect by Damian Conway is\n    available from http://search.cpan.org/~dconway/\n\n    This module can be downloaded at http://pypi.python.org/pypi/inflect\n\nmethods:\n          classical inflect\n          plural plural_noun plural_verb plural_adj singular_noun no num a an\n          compare compare_nouns compare_verbs compare_adjs\n          present_participle\n          ordinal\n          number_to_words\n          join\n          defnoun defverb defadj defa defan\n\n    INFLECTIONS:    classical inflect\n          plural plural_noun plural_verb plural_adj singular_noun compare\n          no num a an present_participle\n\n    PLURALS:   classical inflect\n          plural plural_noun plural_verb plural_adj singular_noun no num\n          compare compare_nouns compare_verbs compare_adjs\n\n    COMPARISONS:    classical\n          compare compare_nouns compare_verbs compare_adjs\n\n    ARTICLES:   classical inflect num a an\n\n    NUMERICAL:      ordinal number_to_words\n\n    USER_DEFINED:   defnoun defverb defadj defa defan\n\nExceptions:\n UnknownClassicalModeError\n BadNumValueError\n BadChunkingOptionError\n NumOutOfRangeError\n BadUserDefinedPatternError\n BadRcFileError\n BadGenderError\n\n'''\n\nfrom re import match, search, subn, IGNORECASE, VERBOSE\nfrom re import split as splitre\nfrom re import error as reerror\nfrom re import sub as resub\n\n\nclass UnknownClassicalModeError(Exception):\n    pass\n\n\nclass BadNumValueError(Exception):\n    pass\n\n\nclass BadChunkingOptionError(Exception):\n    pass\n\n\nclass NumOutOfRangeError(Exception):\n    pass\n\n\nclass BadUserDefinedPatternError(Exception):\n    pass\n\n\nclass BadRcFileError(Exception):\n    pass\n\n\nclass BadGenderError(Exception):\n    pass\n\n\n__ver_major__ = 0\n__ver_minor__ = 3\n__ver_patch__ = 1\n__ver_sub__ = \"\"\n__version__ = \"%d.%d.%d%s\" % (__ver_major__, __ver_minor__,\n                              __ver_patch__, __ver_sub__)\n\n\nSTDOUT_ON = False\n\n\ndef print3(txt):\n    if STDOUT_ON:\n        print(txt)\n\n\ndef enclose(s):\n    return \"(?:%s)\" % s\n\n\ndef joinstem(cutpoint=0, words=''):\n    '''\n    join stem of each word in words into a string for regex\n    each word is truncated at cutpoint\n    cutpoint is usually negative indicating the number of letters to remove\n    from the end of each word\n\n    e.g.\n    joinstem(-2, [\"ephemeris\", \"iris\", \".*itis\"]) returns\n    (?:ephemer|ir|.*it)\n\n    '''\n    return enclose('|'.join(w[:cutpoint] for w in words))\n\n\ndef bysize(words):\n    '''\n    take a list of words and return a dict of sets sorted by word length\n    e.g.\n    ret[3]=set(['ant', 'cat', 'dog', 'pig'])\n    ret[4]=set(['frog', 'goat'])\n    ret[5]=set(['horse'])\n    ret[8]=set(['elephant'])\n    '''\n    ret = {}\n    for w in words:\n        if len(w) not in ret:\n            ret[len(w)] = set()\n        ret[len(w)].add(w)\n    return ret\n\n\ndef make_pl_si_lists(lst, plending, siendingsize, dojoinstem=True):\n    '''\n    given a list of singular words: lst\n    an ending to append to make the plural: plending\n    the number of characters to remove from the singular before appending plending: siendingsize\n    a flag whether to create a joinstem: dojoinstem\n\n    return:\n    a list of pluralised words: si_list (called si because this is what you need to\n                                         look for to make the singular)\n    the pluralised words as a dict of sets sorted by word length: si_bysize\n    the singular words as a dict of sets sorted by word length: pl_bysize\n    if dojoinstem is True: a regular expression that matches any of the stems: stem\n    '''\n    if siendingsize is not None:\n        siendingsize = -siendingsize\n    si_list = [w[:siendingsize] + plending for w in lst]\n    pl_bysize = bysize(lst)\n    si_bysize = bysize(si_list)\n    if dojoinstem:\n        stem = joinstem(siendingsize, lst)\n        return si_list, si_bysize, pl_bysize, stem\n    else:\n        return si_list, si_bysize, pl_bysize\n\n\n\npl_sb_irregular_s = {\n    \"corpus\": \"corpuses|corpora\",\n    \"opus\":   \"opuses|opera\",\n    \"genus\":  \"genera\",\n    \"mythos\": \"mythoi\",\n    \"penis\":  \"penises|penes\",\n    \"testis\": \"testes\",\n    \"atlas\":  \"atlases|atlantes\",\n    \"yes\":    \"yeses\",\n}\n\npl_sb_irregular = {\n    \"child\":      \"children\",\n    \"brother\":    \"brothers|brethren\",\n    \"loaf\":       \"loaves\",\n    \"hoof\":       \"hoofs|hooves\",\n    \"beef\":       \"beefs|beeves\",\n    \"thief\":      \"thiefs|thieves\",\n    \"money\":      \"monies\",\n    \"mongoose\":   \"mongooses\",\n    \"ox\":         \"oxen\",\n    \"cow\":        \"cows|kine\",\n    \"graffito\":   \"graffiti\",\n    \"octopus\":    \"octopuses|octopodes\",\n    \"genie\":      \"genies|genii\",\n    \"ganglion\":   \"ganglions|ganglia\",\n    \"trilby\":     \"trilbys\",\n    \"turf\":       \"turfs|turves\",\n    \"numen\":      \"numina\",\n    \"atman\":      \"atmas\",\n    \"occiput\":    \"occiputs|occipita\",\n    \"sabretooth\": \"sabretooths\",\n    \"sabertooth\": \"sabertooths\",\n    \"lowlife\":    \"lowlifes\",\n    \"flatfoot\":   \"flatfoots\",\n    \"tenderfoot\": \"tenderfoots\",\n    \"romany\":     \"romanies\",\n    \"jerry\":      \"jerries\",\n    \"mary\":       \"maries\",\n    \"talouse\":    \"talouses\",\n    \"blouse\":     \"blouses\",\n    \"rom\":        \"roma\",\n    \"carmen\":     \"carmina\",\n}\n\npl_sb_irregular.update(pl_sb_irregular_s)\n\npl_sb_irregular_caps = {\n    'Romany': 'Romanies',\n    'Jerry':  'Jerrys',\n    'Mary':   'Marys',\n    'Rom':    'Roma',\n}\n\npl_sb_irregular_compound = {\n    \"prima donna\": \"prima donnas|prime donne\",\n}\n\nsi_sb_irregular = dict([(v, k) for (k, v) in pl_sb_irregular.items()])\nkeys = list(si_sb_irregular.keys())\nfor k in keys:\n    if '|' in k:\n        k1, k2 = k.split('|')\n        si_sb_irregular[k1] = si_sb_irregular[k2] = si_sb_irregular[k]\n        del si_sb_irregular[k]\nsi_sb_irregular_caps = dict([(v, k) for (k, v) in pl_sb_irregular_caps.items()])\nsi_sb_irregular_compound = dict([(v, k) for (k, v) in pl_sb_irregular_compound.items()])\nkeys = list(si_sb_irregular_compound.keys())\nfor k in keys:\n    if '|' in k:\n        k1, k2 = k.split('|')\n        si_sb_irregular_compound[k1] = si_sb_irregular_compound[k2] = si_sb_irregular_compound[k]\n        del si_sb_irregular_compound[k]\n\n\n\npl_sb_z_zes_list = (\n    \"quartz\", \"topaz\",\n)\npl_sb_z_zes_bysize = bysize(pl_sb_z_zes_list)\n\npl_sb_ze_zes_list = ('snooze',)\npl_sb_ze_zes_bysize = bysize(pl_sb_ze_zes_list)\n\n\n\npl_sb_C_is_ides_complete = [\n    \"ephemeris\", \"iris\", \"clitoris\",\n    \"chrysalis\", \"epididymis\",\n]\n\npl_sb_C_is_ides_endings = [\n    \"itis\",\n]\n\npl_sb_C_is_ides = joinstem(-2, pl_sb_C_is_ides_complete + [\n    '.*%s' % w for w in pl_sb_C_is_ides_endings])\n\npl_sb_C_is_ides_list = pl_sb_C_is_ides_complete + pl_sb_C_is_ides_endings\n\n(si_sb_C_is_ides_list, si_sb_C_is_ides_bysize,\n    pl_sb_C_is_ides_bysize) = make_pl_si_lists(pl_sb_C_is_ides_list, 'ides', 2, dojoinstem=False)\n\n\n\npl_sb_C_a_ata_list = (\n    \"anathema\", \"bema\", \"carcinoma\", \"charisma\", \"diploma\",\n    \"dogma\", \"drama\", \"edema\", \"enema\", \"enigma\", \"lemma\",\n    \"lymphoma\", \"magma\", \"melisma\", \"miasma\", \"oedema\",\n    \"sarcoma\", \"schema\", \"soma\", \"stigma\", \"stoma\", \"trauma\",\n    \"gumma\", \"pragma\",\n)\n\n(si_sb_C_a_ata_list, si_sb_C_a_ata_bysize,\n    pl_sb_C_a_ata_bysize, pl_sb_C_a_ata) = make_pl_si_lists(pl_sb_C_a_ata_list, 'ata', 1)\n\n\npl_sb_U_a_ae_list = (\n    \"alumna\", \"alga\", \"vertebra\", \"persona\"\n)\n(si_sb_U_a_ae_list, si_sb_U_a_ae_bysize,\n    pl_sb_U_a_ae_bysize, pl_sb_U_a_ae) = make_pl_si_lists(pl_sb_U_a_ae_list, 'e', None)\n\n\npl_sb_C_a_ae_list = (\n    \"amoeba\", \"antenna\", \"formula\", \"hyperbola\",\n    \"medusa\", \"nebula\", \"parabola\", \"abscissa\",\n    \"hydra\", \"nova\", \"lacuna\", \"aurora\", \"umbra\",\n    \"flora\", \"fauna\",\n)\n(si_sb_C_a_ae_list, si_sb_C_a_ae_bysize,\n    pl_sb_C_a_ae_bysize, pl_sb_C_a_ae) = make_pl_si_lists(pl_sb_C_a_ae_list, 'e', None)\n\n\n\npl_sb_C_en_ina_list = (\n    \"stamen\", \"foramen\", \"lumen\",\n)\n\n(si_sb_C_en_ina_list, si_sb_C_en_ina_bysize,\n    pl_sb_C_en_ina_bysize, pl_sb_C_en_ina) = make_pl_si_lists(pl_sb_C_en_ina_list, 'ina', 2)\n\n\n\npl_sb_U_um_a_list = (\n    \"bacterium\", \"agendum\", \"desideratum\", \"erratum\",\n    \"stratum\", \"datum\", \"ovum\", \"extremum\",\n    \"candelabrum\",\n)\n(si_sb_U_um_a_list, si_sb_U_um_a_bysize,\n    pl_sb_U_um_a_bysize, pl_sb_U_um_a) = make_pl_si_lists(pl_sb_U_um_a_list, 'a', 2)\n\n\npl_sb_C_um_a_list = (\n    \"maximum\", \"minimum\", \"momentum\", \"optimum\",\n    \"quantum\", \"cranium\", \"curriculum\", \"dictum\",\n    \"phylum\", \"aquarium\", \"compendium\", \"emporium\",\n    \"enconium\", \"gymnasium\", \"honorarium\", \"interregnum\",\n    \"lustrum\", \"memorandum\", \"millennium\", \"rostrum\",\n    \"spectrum\", \"speculum\", \"stadium\", \"trapezium\",\n    \"ultimatum\", \"medium\", \"vacuum\", \"velum\",\n    \"consortium\", \"arboretum\",\n)\n\n(si_sb_C_um_a_list, si_sb_C_um_a_bysize,\n    pl_sb_C_um_a_bysize, pl_sb_C_um_a) = make_pl_si_lists(pl_sb_C_um_a_list, 'a', 2)\n\n\n\npl_sb_U_us_i_list = (\n    \"alumnus\", \"alveolus\", \"bacillus\", \"bronchus\",\n    \"locus\", \"nucleus\", \"stimulus\", \"meniscus\",\n    \"sarcophagus\",\n)\n(si_sb_U_us_i_list, si_sb_U_us_i_bysize,\n    pl_sb_U_us_i_bysize, pl_sb_U_us_i) = make_pl_si_lists(pl_sb_U_us_i_list, 'i', 2)\n\n\npl_sb_C_us_i_list = (\n    \"focus\", \"radius\", \"genius\",\n    \"incubus\", \"succubus\", \"nimbus\",\n    \"fungus\", \"nucleolus\", \"stylus\",\n    \"torus\", \"umbilicus\", \"uterus\",\n    \"hippopotamus\", \"cactus\",\n)\n\n(si_sb_C_us_i_list, si_sb_C_us_i_bysize,\n    pl_sb_C_us_i_bysize, pl_sb_C_us_i) = make_pl_si_lists(pl_sb_C_us_i_list, 'i', 2)\n\n\n\npl_sb_C_us_us = (\n    \"status\", \"apparatus\", \"prospectus\", \"sinus\",\n    \"hiatus\", \"impetus\", \"plexus\",\n)\npl_sb_C_us_us_bysize = bysize(pl_sb_C_us_us)\n\n\npl_sb_U_on_a_list = (\n    \"criterion\", \"perihelion\", \"aphelion\",\n    \"phenomenon\", \"prolegomenon\", \"noumenon\",\n    \"organon\", \"asyndeton\", \"hyperbaton\",\n)\n(si_sb_U_on_a_list, si_sb_U_on_a_bysize,\n    pl_sb_U_on_a_bysize, pl_sb_U_on_a) = make_pl_si_lists(pl_sb_U_on_a_list, 'a', 2)\n\n\npl_sb_C_on_a_list = (\n    \"oxymoron\",\n)\n\n(si_sb_C_on_a_list, si_sb_C_on_a_bysize,\n    pl_sb_C_on_a_bysize, pl_sb_C_on_a) = make_pl_si_lists(pl_sb_C_on_a_list, 'a', 2)\n\n\n\npl_sb_C_o_i = [\n    \"solo\", \"soprano\", \"basso\", \"alto\",\n    \"contralto\", \"tempo\", \"piano\", \"virtuoso\",\n]  # list not tuple so can concat for pl_sb_U_o_os\n\npl_sb_C_o_i_bysize = bysize(pl_sb_C_o_i)\nsi_sb_C_o_i_bysize = bysize(['%si' % w[:-1] for w in pl_sb_C_o_i])\n\npl_sb_C_o_i_stems = joinstem(-1, pl_sb_C_o_i)\n\n\npl_sb_U_o_os_complete = set((\n    \"ado\", \"ISO\", \"NATO\", \"NCO\", \"NGO\", \"oto\",\n))\nsi_sb_U_o_os_complete = set('%ss' % w for w in pl_sb_U_o_os_complete)\n\n\npl_sb_U_o_os_endings = [\n    \"aficionado\", \"aggro\",\n    \"albino\", \"allegro\", \"ammo\",\n    \"Antananarivo\", \"archipelago\", \"armadillo\",\n    \"auto\", \"avocado\", \"Bamako\",\n    \"Barquisimeto\", \"bimbo\", \"bingo\",\n    \"Biro\", \"bolero\", \"Bolzano\",\n    \"bongo\", \"Boto\", \"burro\",\n    \"Cairo\", \"canto\", \"cappuccino\",\n    \"casino\", \"cello\", \"Chicago\",\n    \"Chimango\", \"cilantro\", \"cochito\",\n    \"coco\", \"Colombo\", \"Colorado\",\n    \"commando\", \"concertino\", \"contango\",\n    \"credo\", \"crescendo\", \"cyano\",\n    \"demo\", \"ditto\", \"Draco\",\n    \"dynamo\", \"embryo\", \"Esperanto\",\n    \"espresso\", \"euro\", \"falsetto\",\n    \"Faro\", \"fiasco\", \"Filipino\",\n    \"flamenco\", \"furioso\", \"generalissimo\",\n    \"Gestapo\", \"ghetto\", \"gigolo\",\n    \"gizmo\", \"Greensboro\", \"gringo\",\n    \"Guaiabero\", \"guano\", \"gumbo\",\n    \"gyro\", \"hairdo\", \"hippo\",\n    \"Idaho\", \"impetigo\", \"inferno\",\n    \"info\", \"intermezzo\", \"intertrigo\",\n    \"Iquico\", \"jumbo\",\n    \"junto\", \"Kakapo\", \"kilo\",\n    \"Kinkimavo\", \"Kokako\", \"Kosovo\",\n    \"Lesotho\", \"libero\", \"libido\",\n    \"libretto\", \"lido\", \"Lilo\",\n    \"limbo\", \"limo\", \"lineno\",\n    \"lingo\", \"lino\", \"livedo\",\n    \"loco\", \"logo\", \"lumbago\",\n    \"macho\", \"macro\", \"mafioso\",\n    \"magneto\", \"magnifico\", \"Majuro\",\n    \"Malabo\", \"manifesto\", \"Maputo\",\n    \"Maracaibo\", \"medico\", \"memo\",\n    \"metro\", \"Mexico\", \"micro\",\n    \"Milano\", \"Monaco\", \"mono\",\n    \"Montenegro\", \"Morocco\", \"Muqdisho\",\n    \"myo\",\n    \"neutrino\", \"Ningbo\",\n    \"octavo\", \"oregano\", \"Orinoco\",\n    \"Orlando\", \"Oslo\",\n    \"panto\", \"Paramaribo\", \"Pardusco\",\n    \"pedalo\", \"photo\", \"pimento\",\n    \"pinto\", \"pleco\", \"Pluto\",\n    \"pogo\", \"polo\", \"poncho\",\n    \"Porto-Novo\", \"Porto\", \"pro\",\n    \"psycho\", \"pueblo\", \"quarto\",\n    \"Quito\", \"rhino\", \"risotto\",\n    \"rococo\", \"rondo\", \"Sacramento\",\n    \"saddo\", \"sago\", \"salvo\",\n    \"Santiago\", \"Sapporo\", \"Sarajevo\",\n    \"scherzando\", \"scherzo\", \"silo\",\n    \"sirocco\", \"sombrero\", \"staccato\",\n    \"sterno\", \"stucco\", \"stylo\",\n    \"sumo\", \"Taiko\", \"techno\",\n    \"terrazzo\", \"testudo\", \"timpano\",\n    \"tiro\", \"tobacco\", \"Togo\",\n    \"Tokyo\", \"torero\", \"Torino\",\n    \"Toronto\", \"torso\", \"tremolo\",\n    \"typo\", \"tyro\", \"ufo\",\n    \"UNESCO\", \"vaquero\", \"vermicello\",\n    \"verso\", \"vibrato\", \"violoncello\",\n    \"Virgo\", \"weirdo\", \"WHO\",\n    \"WTO\", \"Yamoussoukro\", \"yo-yo\",\n    \"zero\", \"Zibo\",\n] + pl_sb_C_o_i\n\npl_sb_U_o_os_bysize = bysize(pl_sb_U_o_os_endings)\nsi_sb_U_o_os_bysize = bysize(['%ss' % w for w in pl_sb_U_o_os_endings])\n\n\n\npl_sb_U_ch_chs_list = (\n    \"czech\", \"eunuch\", \"stomach\"\n)\n\n(si_sb_U_ch_chs_list, si_sb_U_ch_chs_bysize,\n    pl_sb_U_ch_chs_bysize, pl_sb_U_ch_chs) = make_pl_si_lists(pl_sb_U_ch_chs_list, 's', None)\n\n\n\npl_sb_U_ex_ices_list = (\n    \"codex\", \"murex\", \"silex\",\n)\n(si_sb_U_ex_ices_list, si_sb_U_ex_ices_bysize,\n    pl_sb_U_ex_ices_bysize, pl_sb_U_ex_ices) = make_pl_si_lists(pl_sb_U_ex_ices_list, 'ices', 2)\n\npl_sb_U_ix_ices_list = (\n    \"radix\", \"helix\",\n)\n(si_sb_U_ix_ices_list, si_sb_U_ix_ices_bysize,\n    pl_sb_U_ix_ices_bysize, pl_sb_U_ix_ices) = make_pl_si_lists(pl_sb_U_ix_ices_list, 'ices', 2)\n\n\npl_sb_C_ex_ices_list = (\n    \"vortex\", \"vertex\", \"cortex\", \"latex\",\n    \"pontifex\", \"apex\", \"index\", \"simplex\",\n)\n\n(si_sb_C_ex_ices_list, si_sb_C_ex_ices_bysize,\n    pl_sb_C_ex_ices_bysize, pl_sb_C_ex_ices) = make_pl_si_lists(pl_sb_C_ex_ices_list, 'ices', 2)\n\n\npl_sb_C_ix_ices_list = (\n    \"appendix\",\n)\n\n(si_sb_C_ix_ices_list, si_sb_C_ix_ices_bysize,\n    pl_sb_C_ix_ices_bysize, pl_sb_C_ix_ices) = make_pl_si_lists(pl_sb_C_ix_ices_list, 'ices', 2)\n\n\n\npl_sb_C_i_list = (\n    \"afrit\", \"afreet\", \"efreet\",\n)\n\n(si_sb_C_i_list, si_sb_C_i_bysize,\n    pl_sb_C_i_bysize, pl_sb_C_i) = make_pl_si_lists(pl_sb_C_i_list, 'i', None)\n\n\n\npl_sb_C_im_list = (\n    \"goy\", \"seraph\", \"cherub\",\n)\n\n(si_sb_C_im_list, si_sb_C_im_bysize,\n    pl_sb_C_im_bysize, pl_sb_C_im) = make_pl_si_lists(pl_sb_C_im_list, 'im', None)\n\n\n\npl_sb_U_man_mans_list = \"\"\"\n    ataman caiman cayman ceriman\n    desman dolman farman harman hetman\n    human leman ottoman shaman talisman\n\"\"\".split()\npl_sb_U_man_mans_caps_list = \"\"\"\n    Alabaman Bahaman Burman German\n    Hiroshiman Liman Nakayaman Norman Oklahoman\n    Panaman Roman Selman Sonaman Tacoman Yakiman\n    Yokohaman Yuman\n\"\"\".split()\n\n(si_sb_U_man_mans_list, si_sb_U_man_mans_bysize,\n    pl_sb_U_man_mans_bysize) = make_pl_si_lists(pl_sb_U_man_mans_list, 's', None, dojoinstem=False)\n(si_sb_U_man_mans_caps_list, si_sb_U_man_mans_caps_bysize,\n    pl_sb_U_man_mans_caps_bysize) = make_pl_si_lists(\n        pl_sb_U_man_mans_caps_list, 's', None, dojoinstem=False)\n\n\npl_sb_uninflected_s_complete = [\n    \"breeches\", \"britches\", \"pajamas\", \"pyjamas\", \"clippers\", \"gallows\",\n    \"hijinks\", \"headquarters\", \"pliers\", \"scissors\", \"testes\", \"herpes\",\n    \"pincers\", \"shears\", \"proceedings\", \"trousers\",\n\n\n    \"cantus\", \"coitus\", \"nexus\",\n\n    \"contretemps\", \"corps\", \"debris\",\n    \"siemens\",\n\n    \"mumps\",\n\n    \"diabetes\", \"jackanapes\", \"series\", \"species\", \"subspecies\", \"rabies\",\n    \"chassis\", \"innings\", \"news\", \"mews\", \"haggis\",\n]\n\npl_sb_uninflected_s_endings = [\n    \"ois\",\n\n    \"measles\",\n]\n\npl_sb_uninflected_s = pl_sb_uninflected_s_complete + [\n    '.*%s' % w for w in pl_sb_uninflected_s_endings]\n\npl_sb_uninflected_herd = (\n    \"wildebeest\", \"swine\", \"eland\", \"bison\", \"buffalo\",\n    \"elk\", \"rhinoceros\", 'zucchini',\n    'caribou', 'dace', 'grouse', 'guinea fowl', 'guinea-fowl',\n    'haddock', 'hake', 'halibut', 'herring', 'mackerel',\n    'pickerel', 'pike', 'roe', 'seed', 'shad',\n    'snipe', 'teal', 'turbot', 'water fowl', 'water-fowl',\n)\n\npl_sb_uninflected_complete = [\n    \"tuna\", \"salmon\", \"mackerel\", \"trout\",\n    \"bream\", \"sea-bass\", \"sea bass\", \"carp\", \"cod\", \"flounder\", \"whiting\",\n    \"moose\",\n\n    \"graffiti\", \"djinn\", 'samuri',\n    'offspring', 'pence', 'quid', 'hertz',\n] + pl_sb_uninflected_s_complete\n\npl_sb_uninflected_caps = [\n    \"Portuguese\", \"Amoyese\", \"Borghese\", \"Congoese\", \"Faroese\",\n    \"Foochowese\", \"Genevese\", \"Genoese\", \"Gilbertese\", \"Hottentotese\",\n    \"Kiplingese\", \"Kongoese\", \"Lucchese\", \"Maltese\", \"Nankingese\",\n    \"Niasese\", \"Pekingese\", \"Piedmontese\", \"Pistoiese\", \"Sarawakese\",\n    \"Shavese\", \"Vermontese\", \"Wenchowese\", \"Yengeese\",\n]\n\n\npl_sb_uninflected_endings = [\n    \"fish\",\n\n    \"deer\", \"sheep\",\n\n    \"nese\", \"rese\", \"lese\", \"mese\",\n\n    \"pox\",\n\n\n    'craft',\n] + pl_sb_uninflected_s_endings\n\n\npl_sb_uninflected_bysize = bysize(pl_sb_uninflected_endings)\n\n\n\npl_sb_singular_s_complete = [\n    \"acropolis\", \"aegis\", \"alias\", \"asbestos\", \"bathos\", \"bias\",\n    \"bronchitis\", \"bursitis\", \"caddis\", \"cannabis\",\n    \"canvas\", \"chaos\", \"cosmos\", \"dais\", \"digitalis\",\n    \"epidermis\", \"ethos\", \"eyas\", \"gas\", \"glottis\",\n    \"hubris\", \"ibis\", \"lens\", \"mantis\", \"marquis\", \"metropolis\",\n    \"pathos\", \"pelvis\", \"polis\", \"rhinoceros\",\n    \"sassafras\", \"trellis\",\n] + pl_sb_C_is_ides_complete\n\n\npl_sb_singular_s_endings = [\n    \"ss\", \"us\",\n] + pl_sb_C_is_ides_endings\n\npl_sb_singular_s_bysize = bysize(pl_sb_singular_s_endings)\n\nsi_sb_singular_s_complete = ['%ses' % w for w in pl_sb_singular_s_complete]\nsi_sb_singular_s_endings = ['%ses' % w for w in pl_sb_singular_s_endings]\nsi_sb_singular_s_bysize = bysize(si_sb_singular_s_endings)\n\npl_sb_singular_s_es = [\n    \"[A-Z].*es\",\n]\n\npl_sb_singular_s = enclose('|'.join(pl_sb_singular_s_complete +\n                                    ['.*%s' % w for w in pl_sb_singular_s_endings] +\n                                    pl_sb_singular_s_es))\n\n\n\n\nsi_sb_ois_oi_case = (\n    'Bolshois', 'Hanois'\n)\n\nsi_sb_uses_use_case = (\n    'Betelgeuses', 'Duses', 'Meuses', 'Syracuses', 'Toulouses',\n)\n\nsi_sb_uses_use = (\n    'abuses', 'applauses', 'blouses',\n    'carouses', 'causes', 'chartreuses', 'clauses',\n    'contuses', 'douses', 'excuses', 'fuses',\n    'grouses', 'hypotenuses', 'masseuses',\n    'menopauses', 'misuses', 'muses', 'overuses', 'pauses',\n    'peruses', 'profuses', 'recluses', 'reuses',\n    'ruses', 'souses', 'spouses', 'suffuses', 'transfuses', 'uses',\n)\n\nsi_sb_ies_ie_case = (\n    'Addies', 'Aggies', 'Allies', 'Amies', 'Angies', 'Annies',\n    'Annmaries', 'Archies', 'Arties', 'Aussies', 'Barbies',\n    'Barries', 'Basies', 'Bennies', 'Bernies', 'Berties', 'Bessies',\n    'Betties', 'Billies', 'Blondies', 'Bobbies', 'Bonnies',\n    'Bowies', 'Brandies', 'Bries', 'Brownies', 'Callies',\n    'Carnegies', 'Carries', 'Cassies', 'Charlies', 'Cheries',\n    'Christies', 'Connies', 'Curies', 'Dannies', 'Debbies', 'Dixies',\n    'Dollies', 'Donnies', 'Drambuies', 'Eddies', 'Effies', 'Ellies',\n    'Elsies', 'Eries', 'Ernies', 'Essies', 'Eugenies', 'Fannies',\n    'Flossies', 'Frankies', 'Freddies', 'Gillespies', 'Goldies',\n    'Gracies', 'Guthries', 'Hallies', 'Hatties', 'Hetties',\n    'Hollies', 'Jackies', 'Jamies', 'Janies', 'Jannies', 'Jeanies',\n    'Jeannies', 'Jennies', 'Jessies', 'Jimmies', 'Jodies', 'Johnies',\n    'Johnnies', 'Josies', 'Julies', 'Kalgoorlies', 'Kathies', 'Katies',\n    'Kellies', 'Kewpies', 'Kristies', 'Laramies', 'Lassies', 'Lauries',\n    'Leslies', 'Lessies', 'Lillies', 'Lizzies', 'Lonnies', 'Lories',\n    'Lorries', 'Lotties', 'Louies', 'Mackenzies', 'Maggies', 'Maisies',\n    'Mamies', 'Marcies', 'Margies', 'Maries', 'Marjories', 'Matties',\n    'McKenzies', 'Melanies', 'Mickies', 'Millies', 'Minnies', 'Mollies',\n    'Mounties', 'Nannies', 'Natalies', 'Nellies', 'Netties', 'Ollies',\n    'Ozzies', 'Pearlies', 'Pottawatomies', 'Reggies', 'Richies', 'Rickies',\n    'Robbies', 'Ronnies', 'Rosalies', 'Rosemaries', 'Rosies', 'Roxies',\n    'Rushdies', 'Ruthies', 'Sadies', 'Sallies', 'Sammies', 'Scotties',\n    'Selassies', 'Sherries', 'Sophies', 'Stacies', 'Stefanies', 'Stephanies',\n    'Stevies', 'Susies', 'Sylvies', 'Tammies', 'Terries', 'Tessies',\n    'Tommies', 'Tracies', 'Trekkies', 'Valaries', 'Valeries', 'Valkyries',\n    'Vickies', 'Virgies', 'Willies', 'Winnies', 'Wylies', 'Yorkies',\n)\n\nsi_sb_ies_ie = (\n    'aeries', 'baggies', 'belies', 'biggies', 'birdies', 'bogies',\n    'bonnies', 'boogies', 'bookies', 'bourgeoisies', 'brownies',\n    'budgies', 'caddies', 'calories', 'camaraderies', 'cockamamies',\n    'collies', 'cookies', 'coolies', 'cooties', 'coteries', 'crappies',\n    'curies', 'cutesies', 'dogies', 'eyrie', 'floozies', 'footsies',\n    'freebies', 'genies', 'goalies', 'groupies',\n    'hies', 'jalousies', 'junkies',\n    'kiddies', 'laddies', 'lassies', 'lies',\n    'lingeries', 'magpies', 'menageries', 'mommies', 'movies', 'neckties',\n    'newbies', 'nighties', 'oldies', 'organdies', 'overlies',\n    'pies', 'pinkies', 'pixies', 'potpies', 'prairies',\n    'quickies', 'reveries', 'rookies', 'rotisseries', 'softies', 'sorties',\n    'species', 'stymies', 'sweeties', 'ties', 'underlies', 'unties',\n    'veggies', 'vies', 'yuppies', 'zombies',\n)\n\n\nsi_sb_oes_oe_case = (\n    'Chloes', 'Crusoes', 'Defoes', 'Faeroes', 'Ivanhoes', 'Joes',\n    'McEnroes', 'Moes', 'Monroes', 'Noes', 'Poes', 'Roscoes',\n    'Tahoes', 'Tippecanoes', 'Zoes',\n)\n\nsi_sb_oes_oe = (\n    'aloes', 'backhoes', 'canoes',\n    'does', 'floes', 'foes', 'hoes', 'mistletoes',\n    'oboes', 'pekoes', 'roes', 'sloes',\n    'throes', 'tiptoes', 'toes', 'woes',\n)\n\nsi_sb_z_zes = (\n    \"quartzes\", \"topazes\",\n)\n\nsi_sb_zzes_zz = (\n    'buzzes', 'fizzes', 'frizzes', 'razzes'\n)\n\nsi_sb_ches_che_case = (\n    'Andromaches', 'Apaches', 'Blanches', 'Comanches',\n    'Nietzsches', 'Porsches', 'Roches',\n)\n\nsi_sb_ches_che = (\n    'aches', 'avalanches', 'backaches', 'bellyaches', 'caches',\n    'cloches', 'creches', 'douches', 'earaches', 'fiches',\n    'headaches', 'heartaches', 'microfiches',\n    'niches', 'pastiches', 'psyches', 'quiches',\n    'stomachaches', 'toothaches',\n)\n\nsi_sb_xes_xe = (\n    'annexes', 'axes', 'deluxes', 'pickaxes',\n)\n\nsi_sb_sses_sse_case = (\n    'Hesses', 'Jesses', 'Larousses', 'Matisses',\n)\nsi_sb_sses_sse = (\n    'bouillabaisses', 'crevasses', 'demitasses', 'impasses',\n    'mousses', 'posses',\n)\n\nsi_sb_ves_ve_case = (\n    'Clives', 'Palmolives',\n)\nsi_sb_ves_ve = (\n    'interweaves', 'weaves',\n\n    'olives',\n\n    'bivalves', 'dissolves', 'resolves', 'salves', 'twelves', 'valves',\n)\n\n\nplverb_special_s = enclose('|'.join(\n    [pl_sb_singular_s] +\n    pl_sb_uninflected_s +\n    list(pl_sb_irregular_s.keys()) + [\n        '(.*[csx])is',\n        '(.*)ceps',\n        '[A-Z].*s',\n    ]\n))\n\npl_sb_postfix_adj = {\n    'general': ['(?!major|lieutenant|brigadier|adjutant|.*star)\\S+'],\n    'martial': ['court'],\n}\n\nfor k in list(pl_sb_postfix_adj.keys()):\n    pl_sb_postfix_adj[k] = enclose(\n        enclose('|'.join(pl_sb_postfix_adj[k])) +\n        \"(?=(?:-|\\\\s+)%s)\" % k)\n\npl_sb_postfix_adj_stems = '(' + '|'.join(list(pl_sb_postfix_adj.values())) + ')(.*)'\n\n\n\nsi_sb_es_is = (\n    'amanuenses', 'amniocenteses', 'analyses', 'antitheses',\n    'apotheoses', 'arterioscleroses', 'atheroscleroses', 'axes',\n    'catalyses', 'catharses', 'chasses', 'cirrhoses',\n    'cocces', 'crises', 'diagnoses', 'dialyses', 'diereses',\n    'electrolyses', 'emphases', 'exegeses', 'geneses',\n    'halitoses', 'hydrolyses', 'hypnoses', 'hypotheses', 'hystereses',\n    'metamorphoses', 'metastases', 'misdiagnoses', 'mitoses',\n    'mononucleoses', 'narcoses', 'necroses', 'nemeses', 'neuroses',\n    'oases', 'osmoses', 'osteoporoses', 'paralyses', 'parentheses',\n    'parthenogeneses', 'periphrases', 'photosyntheses', 'probosces',\n    'prognoses', 'prophylaxes', 'prostheses', 'preces', 'psoriases',\n    'psychoanalyses', 'psychokineses', 'psychoses', 'scleroses',\n    'scolioses', 'sepses', 'silicoses', 'symbioses', 'synopses',\n    'syntheses', 'taxes', 'telekineses', 'theses', 'thromboses',\n    'tuberculoses', 'urinalyses',\n)\n\npl_prep_list = \"\"\"\n    about above across after among around at athwart before behind\n    below beneath beside besides between betwixt beyond but by\n    during except for from in into near of off on onto out over\n    since till to under until unto upon with\"\"\".split()\n\npl_prep_list_da = pl_prep_list + ['de', 'du', 'da']\n\npl_prep_bysize = bysize(pl_prep_list_da)\n\npl_prep = enclose('|'.join(pl_prep_list_da))\n\npl_sb_prep_dual_compound = r'(.*?)((?:-|\\s+)(?:' + pl_prep + r')(?:-|\\s+))a(?:-|\\s+)(.*)'\n\n\nsingular_pronoun_genders = set(['neuter',\n                                'feminine',\n                                'masculine',\n                                'gender-neutral',\n                                'feminine or masculine',\n                                'masculine or feminine'])\n\npl_pron_nom = {\n    \"i\":    \"we\", \"myself\":   \"ourselves\",\n    \"you\":  \"you\", \"yourself\": \"yourselves\",\n    \"she\":  \"they\", \"herself\":  \"themselves\",\n    \"he\":   \"they\", \"himself\":  \"themselves\",\n    \"it\":   \"they\", \"itself\":   \"themselves\",\n    \"they\": \"they\", \"themself\": \"themselves\",\n\n    \"mine\": \"ours\",\n    \"yours\": \"yours\",\n    \"hers\": \"theirs\",\n    \"his\": \"theirs\",\n    \"its\": \"theirs\",\n    \"theirs\": \"theirs\",\n}\n\nsi_pron = {}\nsi_pron['nom'] = dict([(v, k) for (k, v) in pl_pron_nom.items()])\nsi_pron['nom']['we'] = 'I'\n\n\npl_pron_acc = {\n    \"me\":   \"us\", \"myself\":   \"ourselves\",\n    \"you\":  \"you\", \"yourself\": \"yourselves\",\n    \"her\":  \"them\", \"herself\":  \"themselves\",\n    \"him\":  \"them\", \"himself\":  \"themselves\",\n    \"it\":   \"them\", \"itself\":   \"themselves\",\n    \"them\": \"them\", \"themself\": \"themselves\",\n}\n\npl_pron_acc_keys = enclose('|'.join(list(pl_pron_acc.keys())))\npl_pron_acc_keys_bysize = bysize(list(pl_pron_acc.keys()))\n\nsi_pron['acc'] = dict([(v, k) for (k, v) in pl_pron_acc.items()])\n\nfor thecase, plur, gend, sing in (\n    ('nom', 'they', 'neuter', 'it'),\n    ('nom', 'they', 'feminine', 'she'),\n    ('nom', 'they', 'masculine', 'he'),\n    ('nom', 'they', 'gender-neutral', 'they'),\n    ('nom', 'they', 'feminine or masculine', 'she or he'),\n    ('nom', 'they', 'masculine or feminine', 'he or she'),\n    ('nom', 'themselves', 'neuter', 'itself'),\n    ('nom', 'themselves', 'feminine', 'herself'),\n    ('nom', 'themselves', 'masculine', 'himself'),\n    ('nom', 'themselves', 'gender-neutral', 'themself'),\n    ('nom', 'themselves', 'feminine or masculine', 'herself or himself'),\n    ('nom', 'themselves', 'masculine or feminine', 'himself or herself'),\n    ('nom', 'theirs', 'neuter', 'its'),\n    ('nom', 'theirs', 'feminine', 'hers'),\n    ('nom', 'theirs', 'masculine', 'his'),\n    ('nom', 'theirs', 'gender-neutral', 'theirs'),\n    ('nom', 'theirs', 'feminine or masculine', 'hers or his'),\n    ('nom', 'theirs', 'masculine or feminine', 'his or hers'),\n    ('acc', 'them', 'neuter', 'it'),\n    ('acc', 'them', 'feminine', 'her'),\n    ('acc', 'them', 'masculine', 'him'),\n    ('acc', 'them', 'gender-neutral', 'them'),\n    ('acc', 'them', 'feminine or masculine', 'her or him'),\n    ('acc', 'them', 'masculine or feminine', 'him or her'),\n    ('acc', 'themselves', 'neuter', 'itself'),\n    ('acc', 'themselves', 'feminine', 'herself'),\n    ('acc', 'themselves', 'masculine', 'himself'),\n    ('acc', 'themselves', 'gender-neutral', 'themself'),\n    ('acc', 'themselves', 'feminine or masculine', 'herself or himself'),\n    ('acc', 'themselves', 'masculine or feminine', 'himself or herself'),\n):\n    try:\n        si_pron[thecase][plur][gend] = sing\n    except TypeError:\n        si_pron[thecase][plur] = {}\n        si_pron[thecase][plur][gend] = sing\n\n\nsi_pron_acc_keys = enclose('|'.join(list(si_pron['acc'].keys())))\nsi_pron_acc_keys_bysize = bysize(list(si_pron['acc'].keys()))\n\n\ndef get_si_pron(thecase, word, gender):\n    try:\n        sing = si_pron[thecase][word]\n    except KeyError:\n        raise  # not a pronoun\n    try:\n        return sing[gender]  # has several types due to gender\n    except TypeError:\n        return sing  # answer independent of gender\n\n\nplverb_irregular_pres = {\n    \"am\":   \"are\", \"are\":  \"are\", \"is\":  \"are\",\n    \"was\":  \"were\", \"were\": \"were\", \"was\":  \"were\",\n    \"have\": \"have\", \"have\": \"have\", \"has\":  \"have\",\n    \"do\":   \"do\", \"do\":   \"do\", \"does\": \"do\",\n}\n\nplverb_ambiguous_pres = {\n    \"act\":   \"act\", \"act\":   \"act\", \"acts\":    \"act\",\n    \"blame\": \"blame\", \"blame\": \"blame\", \"blames\":  \"blame\",\n    \"can\":   \"can\", \"can\":   \"can\", \"can\":     \"can\",\n    \"must\":  \"must\", \"must\":  \"must\", \"must\":    \"must\",\n    \"fly\":   \"fly\", \"fly\":   \"fly\", \"flies\":   \"fly\",\n    \"copy\":  \"copy\", \"copy\":  \"copy\", \"copies\":  \"copy\",\n    \"drink\": \"drink\", \"drink\": \"drink\", \"drinks\":  \"drink\",\n    \"fight\": \"fight\", \"fight\": \"fight\", \"fights\":  \"fight\",\n    \"fire\":  \"fire\", \"fire\":  \"fire\", \"fires\":   \"fire\",\n    \"like\":  \"like\", \"like\":  \"like\", \"likes\":   \"like\",\n    \"look\":  \"look\", \"look\":  \"look\", \"looks\":   \"look\",\n    \"make\":  \"make\", \"make\":  \"make\", \"makes\":   \"make\",\n    \"reach\": \"reach\", \"reach\": \"reach\", \"reaches\": \"reach\",\n    \"run\":   \"run\", \"run\":   \"run\", \"runs\":    \"run\",\n    \"sink\":  \"sink\", \"sink\":  \"sink\", \"sinks\":   \"sink\",\n    \"sleep\": \"sleep\", \"sleep\": \"sleep\", \"sleeps\":  \"sleep\",\n    \"view\":  \"view\", \"view\":  \"view\", \"views\":   \"view\",\n}\n\nplverb_ambiguous_pres_keys = enclose('|'.join(list(plverb_ambiguous_pres.keys())))\n\n\nplverb_irregular_non_pres = (\n    \"did\", \"had\", \"ate\", \"made\", \"put\",\n    \"spent\", \"fought\", \"sank\", \"gave\", \"sought\",\n    \"shall\", \"could\", \"ought\", \"should\",\n)\n\nplverb_ambiguous_non_pres = enclose('|'.join((\n    \"thought\", \"saw\", \"bent\", \"will\", \"might\", \"cut\",\n)))\n\n\npl_v_oes_oe = ('canoes', 'floes', 'oboes', 'roes', 'throes', 'woes')\npl_v_oes_oe_endings_size4 = ('hoes', 'toes')\npl_v_oes_oe_endings_size5 = ('shoes')\n\n\npl_count_zero = (\n    \"0\", \"no\", \"zero\", \"nil\"\n)\n\n\npl_count_one = (\n    \"1\", \"a\", \"an\", \"one\", \"each\", \"every\", \"this\", \"that\",\n)\n\npl_adj_special = {\n    \"a\":    \"some\", \"an\":    \"some\",\n    \"this\": \"these\", \"that\": \"those\",\n}\n\npl_adj_special_keys = enclose('|'.join(list(pl_adj_special.keys())))\n\npl_adj_poss = {\n    \"my\":    \"our\",\n    \"your\":  \"your\",\n    \"its\":   \"their\",\n    \"her\":   \"their\",\n    \"his\":   \"their\",\n    \"their\": \"their\",\n}\n\npl_adj_poss_keys = enclose('|'.join(list(pl_adj_poss.keys())))\n\n\n\n\nA_abbrev = r\"\"\"\n(?! FJO | [HLMNS]Y.  | RY[EO] | SQU\n  | ( F[LR]? | [HL] | MN? | N | RH? | S[CHKLMNPTVW]? | X(YL)?) [AEIOU])\n[FHLMNRSX][A-Z]\n\"\"\"\n\n\nA_y_cons = 'y(b[lor]|cl[ea]|fere|gg|p[ios]|rou|tt)'\n\n\nA_explicit_a = enclose('|'.join((\n    \"unabomber\", \"unanimous\", \"US\",\n)))\n\nA_explicit_an = enclose('|'.join((\n    \"euler\",\n    \"hour(?!i)\", \"heir\", \"honest\", \"hono[ur]\",\n    \"mpeg\",\n)))\n\nA_ordinal_an = enclose('|'.join((\n    \"[aefhilmnorsx]-?th\",\n)))\n\nA_ordinal_a = enclose('|'.join((\n    \"[bcdgjkpqtuvwyz]-?th\",\n)))\n\n\n\nnth = {\n    0: 'th',\n    1: 'st',\n    2: 'nd',\n    3: 'rd',\n    4: 'th',\n    5: 'th',\n    6: 'th',\n    7: 'th',\n    8: 'th',\n    9: 'th',\n    11: 'th',\n    12: 'th',\n    13: 'th',\n}\n\nordinal = dict(ty='tieth',\n               one='first',\n               two='second',\n               three='third',\n               five='fifth',\n               eight='eighth',\n               nine='ninth',\n               twelve='twelfth')\n\nordinal_suff = '|'.join(list(ordinal.keys()))\n\n\n\nunit = ['', 'one', 'two', 'three', 'four', 'five',\n        'six', 'seven', 'eight', 'nine']\nteen = ['ten', 'eleven', 'twelve', 'thirteen', 'fourteen',\n        'fifteen', 'sixteen', 'seventeen', 'eighteen', 'nineteen']\nten = ['', '', 'twenty', 'thirty', 'forty',\n       'fifty', 'sixty', 'seventy', 'eighty', 'ninety']\nmill = [' ', ' thousand', ' million', ' billion', ' trillion', ' quadrillion',\n        ' quintillion', ' sextillion', ' septillion', ' octillion',\n        ' nonillion', ' decillion']\n\n\n\ndef_classical = dict(\n    all=False,\n    zero=False,\n    herd=False,\n    names=True,\n    persons=False,\n    ancient=False,\n)\n\nall_classical = dict((k, True) for k in list(def_classical.keys()))\nno_classical = dict((k, False) for k in list(def_classical.keys()))\n\n\n\n\n\nclass engine:\n\n    def __init__(self):\n\n        self.classical_dict = def_classical.copy()\n        self.persistent_count = None\n        self.mill_count = 0\n        self.pl_sb_user_defined = []\n        self.pl_v_user_defined = []\n        self.pl_adj_user_defined = []\n        self.si_sb_user_defined = []\n        self.A_a_user_defined = []\n        self.thegender = 'neuter'\n\n    deprecated_methods = dict(pl='plural',\n                              plnoun='plural_noun',\n                              plverb='plural_verb',\n                              pladj='plural_adj',\n                              sinoun='single_noun',\n                              prespart='present_participle',\n                              numwords='number_to_words',\n                              plequal='compare',\n                              plnounequal='compare_nouns',\n                              plverbequal='compare_verbs',\n                              pladjequal='compare_adjs',\n                              wordlist='join',\n                              )\n\n    def __getattr__(self, meth):\n        if meth in self.deprecated_methods:\n            print3('%s() deprecated, use %s()' % (meth, self.deprecated_methods[meth]))\n            raise DeprecationWarning\n        raise AttributeError\n\n    def defnoun(self, singular, plural):\n        '''\n        Set the noun plural of singular to plural.\n\n        '''\n        self.checkpat(singular)\n        self.checkpatplural(plural)\n        self.pl_sb_user_defined.extend((singular, plural))\n        self.si_sb_user_defined.extend((plural, singular))\n        return 1\n\n    def defverb(self, s1, p1, s2, p2, s3, p3):\n        '''\n        Set the verb plurals for s1, s2 and s3 to p1, p2 and p3 respectively.\n\n        Where 1, 2 and 3 represent the 1st, 2nd and 3rd person forms of the verb.\n\n        '''\n        self.checkpat(s1)\n        self.checkpat(s2)\n        self.checkpat(s3)\n        self.checkpatplural(p1)\n        self.checkpatplural(p2)\n        self.checkpatplural(p3)\n        self.pl_v_user_defined.extend((s1, p1, s2, p2, s3, p3))\n        return 1\n\n    def defadj(self, singular, plural):\n        '''\n        Set the adjective plural of singular to plural.\n\n        '''\n        self.checkpat(singular)\n        self.checkpatplural(plural)\n        self.pl_adj_user_defined.extend((singular, plural))\n        return 1\n\n    def defa(self, pattern):\n        '''\n        Define the indefinate article as 'a' for words matching pattern.\n\n        '''\n        self.checkpat(pattern)\n        self.A_a_user_defined.extend((pattern, 'a'))\n        return 1\n\n    def defan(self, pattern):\n        '''\n        Define the indefinate article as 'an' for words matching pattern.\n\n        '''\n        self.checkpat(pattern)\n        self.A_a_user_defined.extend((pattern, 'an'))\n        return 1\n\n    def checkpat(self, pattern):\n        '''\n        check for errors in a regex pattern\n        '''\n        if pattern is None:\n            return\n        try:\n            match(pattern, '')\n        except reerror:\n            print3(\"\\nBad user-defined singular pattern:\\n\\t%s\\n\" % pattern)\n            raise BadUserDefinedPatternError\n\n    def checkpatplural(self, pattern):\n        '''\n        check for errors in a regex replace pattern\n        '''\n        return\n\n    def ud_match(self, word, wordlist):\n        for i in range(len(wordlist) - 2, -2, -2):  # backwards through even elements\n            mo = search(r'^%s$' % wordlist[i], word, IGNORECASE)\n            if mo:\n                if wordlist[i + 1] is None:\n                    return None\n                pl = resub(r'\\$(\\d+)', r'\\\\1', wordlist[i + 1])  # change $n to \\n for expand\n                return mo.expand(pl)\n        return None\n\n    def classical(self, **kwargs):\n        \"\"\"\n        turn classical mode on and off for various categories\n\n        turn on all classical modes:\n        classical()\n        classical(all=True)\n\n        turn on or off specific claassical modes:\n        e.g.\n        classical(herd=True)\n        classical(names=False)\n\n        By default all classical modes are off except names.\n\n        unknown value in args or key in kwargs rasies exception: UnknownClasicalModeError\n\n        \"\"\"\n        classical_mode = list(def_classical.keys())\n        if not kwargs:\n            self.classical_dict = all_classical.copy()\n            return\n        if 'all' in kwargs:\n            if kwargs['all']:\n                self.classical_dict = all_classical.copy()\n            else:\n                self.classical_dict = no_classical.copy()\n\n        for k, v in list(kwargs.items()):\n            if k in classical_mode:\n                self.classical_dict[k] = v\n            else:\n                raise UnknownClassicalModeError\n\n    def num(self, count=None, show=None):  # (;$count,$show)\n        '''\n        Set the number to be used in other method calls.\n\n        Returns count.\n\n        Set show to False to return '' instead.\n\n        '''\n        if count is not None:\n            try:\n                self.persistent_count = int(count)\n            except ValueError:\n                raise BadNumValueError\n            if (show is None) or show:\n                return str(count)\n        else:\n            self.persistent_count = None\n        return ''\n\n    def gender(self, gender):\n        '''\n        set the gender for the singular of plural pronouns\n\n        can be one of:\n        'neuter'                ('they' -> 'it')\n        'feminine'              ('they' -> 'she')\n        'masculine'             ('they' -> 'he')\n        'gender-neutral'        ('they' -> 'they')\n        'feminine or masculine' ('they' -> 'she or he')\n        'masculine or feminine' ('they' -> 'he or she')\n        '''\n        if gender in singular_pronoun_genders:\n            self.thegender = gender\n        else:\n            raise BadGenderError\n\n    def nummo(self, matchobject):\n        '''\n        num but take a matchobject\n        use groups 1 and 2 in matchobject\n        '''\n        return self.num(matchobject.group(1), matchobject.group(2))\n\n    def plmo(self, matchobject):\n        '''\n        plural but take a matchobject\n        use groups 1 and 3 in matchobject\n        '''\n        return self.plural(matchobject.group(1), matchobject.group(3))\n\n    def plnounmo(self, matchobject):\n        '''\n        plural_noun but take a matchobject\n        use groups 1 and 3 in matchobject\n        '''\n        return self.plural_noun(matchobject.group(1), matchobject.group(3))\n\n    def plverbmo(self, matchobject):\n        '''\n        plural_verb but take a matchobject\n        use groups 1 and 3 in matchobject\n        '''\n        return self.plural_verb(matchobject.group(1), matchobject.group(3))\n\n    def pladjmo(self, matchobject):\n        '''\n        plural_adj but take a matchobject\n        use groups 1 and 3 in matchobject\n        '''\n        return self.plural_adj(matchobject.group(1), matchobject.group(3))\n\n    def sinounmo(self, matchobject):\n        '''\n        singular_noun but take a matchobject\n        use groups 1 and 3 in matchobject\n        '''\n        return self.singular_noun(matchobject.group(1), matchobject.group(3))\n\n    def amo(self, matchobject):\n        '''\n        A but take a matchobject\n        use groups 1 and 3 in matchobject\n        '''\n        if matchobject.group(3) is None:\n            return self.a(matchobject.group(1))\n        return self.a(matchobject.group(1), matchobject.group(3))\n\n    def nomo(self, matchobject):\n        '''\n        NO but take a matchobject\n        use groups 1 and 3 in matchobject\n        '''\n        return self.no(matchobject.group(1), matchobject.group(3))\n\n    def ordinalmo(self, matchobject):\n        '''\n        ordinal but take a matchobject\n        use group 1\n        '''\n        return self.ordinal(matchobject.group(1))\n\n    def numwordsmo(self, matchobject):\n        '''\n        number_to_words but take a matchobject\n        use group 1\n        '''\n        return self.number_to_words(matchobject.group(1))\n\n    def prespartmo(self, matchobject):\n        '''\n        prespart but take a matchobject\n        use group 1\n        '''\n        return self.present_participle(matchobject.group(1))\n\n\n    def inflect(self, text):\n        '''\n        Perform inflections in a string.\n\n        e.g. inflect('The plural of cat is plural(cat)') returns\n        'The plural of cat is cats'\n\n        can use plural, plural_noun, plural_verb, plural_adj, singular_noun, a, an, no, ordinal,\n        number_to_words and prespart\n\n        '''\n        save_persistent_count = self.persistent_count\n        sections = splitre(r\"(num\\([^)]*\\))\", text)\n        inflection = []\n\n        for section in sections:\n            (section, count) = subn(r\"num\\(\\s*?(?:([^),]*)(?:,([^)]*))?)?\\)\", self.nummo, section)\n            if not count:\n                total = -1\n                while total:\n                    (section, total) = subn(\n                        r\"(?x)\\bplural     \\( ([^),]*) (, ([^)]*) )? \\)  \",\n                        self.plmo, section)\n                    (section, count) = subn(\n                        r\"(?x)\\bplural_noun   \\( ([^),]*) (, ([^)]*) )? \\)  \",\n                        self.plnounmo, section)\n                    total += count\n                    (section, count) = subn(\n                        r\"(?x)\\bplural_verb   \\( ([^),]*) (, ([^)]*) )? \\)  \",\n                        self.plverbmo, section)\n                    total += count\n                    (section, count) = subn(\n                        r\"(?x)\\bplural_adj \\( ([^),]*) (, ([^)]*) )? \\)  \",\n                        self.pladjmo, section)\n                    total += count\n                    (section, count) = subn(\n                        r\"(?x)\\bsingular_noun   \\( ([^),]*) (, ([^)]*) )? \\)  \",\n                        self.sinounmo, section)\n                    total += count\n                    (section, count) = subn(\n                        r\"(?x)\\ban?    \\( ([^),]*) (, ([^)]*) )? \\)  \",\n                        self.amo, section)\n                    total += count\n                    (section, count) = subn(\n                        r\"(?x)\\bno    \\( ([^),]*) (, ([^)]*) )? \\)  \",\n                        self.nomo, section)\n                    total += count\n                    (section, count) = subn(\n                        r\"(?x)\\bordinal        \\( ([^)]*) \\)            \",\n                        self.ordinalmo, section)\n                    total += count\n                    (section, count) = subn(\n                        r\"(?x)\\bnumber_to_words  \\( ([^)]*) \\)            \",\n                        self.numwordsmo, section)\n                    total += count\n                    (section, count) = subn(\n                        r\"(?x)\\bpresent_participle \\( ([^)]*) \\)            \",\n                        self.prespartmo, section)\n                    total += count\n\n            inflection.append(section)\n\n        self.persistent_count = save_persistent_count\n        return \"\".join(inflection)\n\n\n    def postprocess(self, orig, inflected):\n        \"\"\"\n        FIX PEDANTRY AND CAPITALIZATION :-)\n        \"\"\"\n        if '|' in inflected:\n            inflected = inflected.split('|')[self.classical_dict['all']]\n        if orig == \"I\":\n            return inflected\n        if orig == orig.upper():\n            return inflected.upper()\n        if orig[0] == orig[0].upper():\n            return '%s%s' % (inflected[0].upper(),\n                             inflected[1:])\n        return inflected\n\n    def partition_word(self, text):\n        mo = search(r'\\A(\\s*)(.+?)(\\s*)\\Z', text)\n        try:\n            return mo.group(1), mo.group(2), mo.group(3)\n        except AttributeError:  # empty string\n            return '', '', ''\n\n\n    def plural(self, text, count=None):\n        '''\n        Return the plural of text.\n\n        If count supplied, then return text if count is one of:\n            1, a, an, one, each, every, this, that\n        otherwise return the plural.\n\n        Whitespace at the start and end is preserved.\n\n        '''\n        pre, word, post = self.partition_word(text)\n        if not word:\n            return text\n        plural = self.postprocess(\n            word,\n            self._pl_special_adjective(word, count) or\n            self._pl_special_verb(word, count) or\n            self._plnoun(word, count))\n        return \"%s%s%s\" % (pre, plural, post)\n\n    def plural_noun(self, text, count=None):\n        '''\n        Return the plural of text, where text is a noun.\n\n        If count supplied, then return text if count is one of:\n            1, a, an, one, each, every, this, that\n        otherwise return the plural.\n\n        Whitespace at the start and end is preserved.\n\n        '''\n        pre, word, post = self.partition_word(text)\n        if not word:\n            return text\n        plural = self.postprocess(word, self._plnoun(word, count))\n        return \"%s%s%s\" % (pre, plural, post)\n\n    def plural_verb(self, text, count=None):\n        '''\n        Return the plural of text, where text is a verb.\n\n        If count supplied, then return text if count is one of:\n            1, a, an, one, each, every, this, that\n        otherwise return the plural.\n\n        Whitespace at the start and end is preserved.\n\n        '''\n        pre, word, post = self.partition_word(text)\n        if not word:\n            return text\n        plural = self.postprocess(word, self._pl_special_verb(word, count) or\n                                  self._pl_general_verb(word, count))\n        return \"%s%s%s\" % (pre, plural, post)\n\n    def plural_adj(self, text, count=None):\n        '''\n        Return the plural of text, where text is an adjective.\n\n        If count supplied, then return text if count is one of:\n            1, a, an, one, each, every, this, that\n        otherwise return the plural.\n\n        Whitespace at the start and end is preserved.\n\n        '''\n        pre, word, post = self.partition_word(text)\n        if not word:\n            return text\n        plural = self.postprocess(word, self._pl_special_adjective(word, count) or word)\n        return \"%s%s%s\" % (pre, plural, post)\n\n    def compare(self, word1, word2):\n        '''\n        compare word1 and word2 for equality regardless of plurality\n\n        return values:\n        eq - the strings are equal\n        p:s - word1 is the plural of word2\n        s:p - word2 is the plural of word1\n        p:p - word1 and word2 are two different plural forms of the one word\n        False - otherwise\n\n        '''\n        return (\n            self._plequal(word1, word2, self.plural_noun) or\n            self._plequal(word1, word2, self.plural_verb) or\n            self._plequal(word1, word2, self.plural_adj))\n\n    def compare_nouns(self, word1, word2):\n        '''\n        compare word1 and word2 for equality regardless of plurality\n        word1 and word2 are to be treated as nouns\n\n        return values:\n        eq - the strings are equal\n        p:s - word1 is the plural of word2\n        s:p - word2 is the plural of word1\n        p:p - word1 and word2 are two different plural forms of the one word\n        False - otherwise\n\n        '''\n        return self._plequal(word1, word2, self.plural_noun)\n\n    def compare_verbs(self, word1, word2):\n        '''\n        compare word1 and word2 for equality regardless of plurality\n        word1 and word2 are to be treated as verbs\n\n        return values:\n        eq - the strings are equal\n        p:s - word1 is the plural of word2\n        s:p - word2 is the plural of word1\n        p:p - word1 and word2 are two different plural forms of the one word\n        False - otherwise\n\n        '''\n        return self._plequal(word1, word2, self.plural_verb)\n\n    def compare_adjs(self, word1, word2):\n        '''\n        compare word1 and word2 for equality regardless of plurality\n        word1 and word2 are to be treated as adjectives\n\n        return values:\n        eq - the strings are equal\n        p:s - word1 is the plural of word2\n        s:p - word2 is the plural of word1\n        p:p - word1 and word2 are two different plural forms of the one word\n        False - otherwise\n\n        '''\n        return self._plequal(word1, word2, self.plural_adj)\n\n    def singular_noun(self, text, count=None, gender=None):\n        '''\n        Return the singular of text, where text is a plural noun.\n\n        If count supplied, then return the singular if count is one of:\n            1, a, an, one, each, every, this, that or if count is None\n        otherwise return text unchanged.\n\n        Whitespace at the start and end is preserved.\n\n        '''\n        pre, word, post = self.partition_word(text)\n        if not word:\n            return text\n        sing = self._sinoun(word, count=count, gender=gender)\n        if sing is not False:\n            plural = self.postprocess(word, self._sinoun(word, count=count, gender=gender))\n            return \"%s%s%s\" % (pre, plural, post)\n        return False\n\n    def _plequal(self, word1, word2, pl):\n        classval = self.classical_dict.copy()\n        self.classical_dict = all_classical.copy()\n        if word1 == word2:\n            return \"eq\"\n        if word1 == pl(word2):\n            return \"p:s\"\n        if pl(word1) == word2:\n            return \"s:p\"\n        self.classical_dict = no_classical.copy()\n        if word1 == pl(word2):\n            return \"p:s\"\n        if pl(word1) == word2:\n            return \"s:p\"\n        self.classical_dict = classval.copy()\n\n        if pl == self.plural or pl == self.plural_noun:\n            if self._pl_check_plurals_N(word1, word2):\n                return \"p:p\"\n            if self._pl_check_plurals_N(word2, word1):\n                return \"p:p\"\n        if pl == self.plural or pl == self.plural_adj:\n            if self._pl_check_plurals_adj(word1, word2):\n                return \"p:p\"\n        return False\n\n    def _pl_reg_plurals(self, pair, stems, end1, end2):\n        if search(r\"(%s)(%s\\|\\1%s|%s\\|\\1%s)\" % (stems, end1, end2, end2, end1), pair):\n            return True\n        return False\n\n    def _pl_check_plurals_N(self, word1, word2):\n        pair = \"%s|%s\" % (word1, word2)\n        if pair in list(pl_sb_irregular_s.values()):\n            return True\n        if pair in list(pl_sb_irregular.values()):\n            return True\n        if pair in list(pl_sb_irregular_caps.values()):\n            return True\n\n        for (stems, end1, end2) in (\n            (pl_sb_C_a_ata, \"as\", \"ata\"),\n            (pl_sb_C_is_ides, \"is\", \"ides\"),\n            (pl_sb_C_a_ae, \"s\", \"e\"),\n            (pl_sb_C_en_ina, \"ens\", \"ina\"),\n            (pl_sb_C_um_a, \"ums\", \"a\"),\n            (pl_sb_C_us_i, \"uses\", \"i\"),\n            (pl_sb_C_on_a, \"ons\", \"a\"),\n            (pl_sb_C_o_i_stems, \"os\", \"i\"),\n            (pl_sb_C_ex_ices, \"exes\", \"ices\"),\n            (pl_sb_C_ix_ices, \"ixes\", \"ices\"),\n            (pl_sb_C_i, \"s\", \"i\"),\n            (pl_sb_C_im, \"s\", \"im\"),\n            ('.*eau', \"s\", \"x\"),\n            ('.*ieu', \"s\", \"x\"),\n            ('.*tri', \"xes\", \"ces\"),\n            ('.{2,}[yia]n', \"xes\", \"ges\")\n        ):\n            if self._pl_reg_plurals(pair, stems, end1, end2):\n                return True\n        return False\n\n    def _pl_check_plurals_adj(self, word1, word2):\n        word1a = word1[:word1.rfind(\"'\")] if word1.endswith((\"'s\", \"'\")) else ''\n        word2a = word2[:word2.rfind(\"'\")] if word2.endswith((\"'s\", \"'\")) else ''\n\n\n        if word1a:\n            if word2a and (self._pl_check_plurals_N(word1a, word2a) or\n                           self._pl_check_plurals_N(word2a, word1a)):\n                return True\n\n\n        return False\n\n    def get_count(self, count=None):\n        if count is None and self.persistent_count is not None:\n            count = self.persistent_count\n\n        if count is not None:\n            count = 1 if ((str(count) in pl_count_one) or\n                          (self.classical_dict['zero'] and\n                           str(count).lower() in pl_count_zero)) else 2\n        else:\n            count = ''\n        return count\n\n    def _plnoun(self, word, count=None):\n        count = self.get_count(count)\n\n\n        if count == 1:\n            return word\n\n\n        value = self.ud_match(word, self.pl_sb_user_defined)\n        if value is not None:\n            return value\n\n\n        if word == '':\n            return word\n\n        lowerword = word.lower()\n\n        if lowerword in pl_sb_uninflected_complete:\n            return word\n\n        if word in pl_sb_uninflected_caps:\n            return word\n\n        for k, v in pl_sb_uninflected_bysize.items():\n            if lowerword[-k:] in v:\n                return word\n\n        if (self.classical_dict['herd'] and lowerword in pl_sb_uninflected_herd):\n            return word\n\n\n        mo = search(r\"^(?:%s)$\" % pl_sb_postfix_adj_stems, word, IGNORECASE)\n        if mo and mo.group(2) != '':\n            return \"%s%s\" % (self._plnoun(mo.group(1), 2), mo.group(2))\n\n        if ' a ' in lowerword or '-a-' in lowerword:\n            mo = search(r\"^(?:%s)$\" % pl_sb_prep_dual_compound, word, IGNORECASE)\n            if mo and mo.group(2) != '' and mo.group(3) != '':\n                return \"%s%s%s\" % (self._plnoun(mo.group(1), 2),\n                                   mo.group(2),\n                                   self._plnoun(mo.group(3)))\n\n        lowersplit = lowerword.split(' ')\n        if len(lowersplit) >= 3:\n            for numword in range(1, len(lowersplit) - 1):\n                if lowersplit[numword] in pl_prep_list_da:\n                    return ' '.join(\n                        lowersplit[:numword - 1] +\n                        [self._plnoun(lowersplit[numword - 1], 2)] + lowersplit[numword:])\n\n        lowersplit = lowerword.split('-')\n        if len(lowersplit) >= 3:\n            for numword in range(1, len(lowersplit) - 1):\n                if lowersplit[numword] in pl_prep_list_da:\n                    return ' '.join(\n                        lowersplit[:numword - 1] +\n                        [self._plnoun(lowersplit[numword - 1], 2) +\n                            '-' + lowersplit[numword] + '-']\n                        ) + ' '.join(lowersplit[(numword + 1):])\n\n\n        for k, v in pl_pron_acc_keys_bysize.items():\n            if lowerword[-k:] in v:  # ends with accusivate pronoun\n                for pk, pv in pl_prep_bysize.items():\n                    if lowerword[:pk] in pv:  # starts with a prep\n                        if lowerword.split() == [lowerword[:pk], lowerword[-k:]]:\n                            return lowerword[:-k] + pl_pron_acc[lowerword[-k:]]\n\n        try:\n            return pl_pron_nom[word.lower()]\n        except KeyError:\n            pass\n\n        try:\n            return pl_pron_acc[word.lower()]\n        except KeyError:\n            pass\n\n\n        wordsplit = word.split()\n        wordlast = wordsplit[-1]\n        lowerwordlast = wordlast.lower()\n\n        if wordlast in list(pl_sb_irregular_caps.keys()):\n            llen = len(wordlast)\n            return '%s%s' % (word[:-llen],\n                             pl_sb_irregular_caps[wordlast])\n\n        if lowerwordlast in list(pl_sb_irregular.keys()):\n            llen = len(lowerwordlast)\n            return '%s%s' % (word[:-llen],\n                             pl_sb_irregular[lowerwordlast])\n\n        if (' '.join(wordsplit[-2:])).lower() in list(pl_sb_irregular_compound.keys()):\n            llen = len(' '.join(wordsplit[-2:]))  # TODO: what if 2 spaces between these words?\n            return '%s%s' % (word[:-llen],\n                             pl_sb_irregular_compound[(' '.join(wordsplit[-2:])).lower()])\n\n        if lowerword[-3:] == 'quy':\n            return word[:-1] + 'ies'\n\n        if lowerword[-6:] == 'person':\n            if self.classical_dict['persons']:\n                return word + 's'\n            else:\n                return word[:-4] + 'ople'\n\n\n        if lowerword[-3:] == 'man':\n            for k, v in pl_sb_U_man_mans_bysize.items():\n                if lowerword[-k:] in v:\n                    return word + 's'\n            for k, v in pl_sb_U_man_mans_caps_bysize.items():\n                if word[-k:] in v:\n                    return word + 's'\n            return word[:-3] + 'men'\n        if lowerword[-5:] == 'mouse':\n            return word[:-5] + 'mice'\n        if lowerword[-5:] == 'louse':\n            return word[:-5] + 'lice'\n        if lowerword[-5:] == 'goose':\n            return word[:-5] + 'geese'\n        if lowerword[-5:] == 'tooth':\n            return word[:-5] + 'teeth'\n        if lowerword[-4:] == 'foot':\n            return word[:-4] + 'feet'\n\n        if lowerword == 'die':\n            return 'dice'\n\n\n        if lowerword[-4:] == 'ceps':\n            return word\n        if lowerword[-4:] == 'zoon':\n            return word[:-2] + 'a'\n        if lowerword[-3:] in ('cis', 'sis', 'xis'):\n            return word[:-2] + 'es'\n\n        for lastlet, d, numend, post in (\n            ('h', pl_sb_U_ch_chs_bysize, None, 's'),\n            ('x', pl_sb_U_ex_ices_bysize, -2, 'ices'),\n            ('x', pl_sb_U_ix_ices_bysize, -2, 'ices'),\n            ('m', pl_sb_U_um_a_bysize, -2, 'a'),\n            ('s', pl_sb_U_us_i_bysize, -2, 'i'),\n            ('n', pl_sb_U_on_a_bysize, -2, 'a'),\n            ('a', pl_sb_U_a_ae_bysize, None, 'e'),\n        ):\n            if lowerword[-1] == lastlet:  # this test to add speed\n                for k, v in d.items():\n                    if lowerword[-k:] in v:\n                        return word[:numend] + post\n\n\n        if (self.classical_dict['ancient']):\n            if lowerword[-4:] == 'trix':\n                return word[:-1] + 'ces'\n            if lowerword[-3:] in ('eau', 'ieu'):\n                return word + 'x'\n            if lowerword[-3:] in ('ynx', 'inx', 'anx') and len(word) > 4:\n                return word[:-1] + 'ges'\n\n            for lastlet, d, numend, post in (\n                ('n', pl_sb_C_en_ina_bysize, -2, 'ina'),\n                ('x', pl_sb_C_ex_ices_bysize, -2, 'ices'),\n                ('x', pl_sb_C_ix_ices_bysize, -2, 'ices'),\n                ('m', pl_sb_C_um_a_bysize, -2, 'a'),\n                ('s', pl_sb_C_us_i_bysize, -2, 'i'),\n                ('s', pl_sb_C_us_us_bysize, None, ''),\n                ('a', pl_sb_C_a_ae_bysize, None, 'e'),\n                ('a', pl_sb_C_a_ata_bysize, None, 'ta'),\n                ('s', pl_sb_C_is_ides_bysize, -1, 'des'),\n                ('o', pl_sb_C_o_i_bysize, -1, 'i'),\n                ('n', pl_sb_C_on_a_bysize, -2, 'a'),\n            ):\n                if lowerword[-1] == lastlet:  # this test to add speed\n                    for k, v in d.items():\n                        if lowerword[-k:] in v:\n                            return word[:numend] + post\n\n            for d, numend, post in (\n                (pl_sb_C_i_bysize, None, 'i'),\n                (pl_sb_C_im_bysize, None, 'im'),\n            ):\n                for k, v in d.items():\n                    if lowerword[-k:] in v:\n                        return word[:numend] + post\n\n\n        if lowerword in pl_sb_singular_s_complete:\n            return word + 'es'\n\n        for k, v in pl_sb_singular_s_bysize.items():\n            if lowerword[-k:] in v:\n                return word + 'es'\n\n        if lowerword[-2:] == 'es' and word[0] == word[0].upper():\n            return word + 'es'\n\n\n        if lowerword[-1] == 'z':\n            for k, v in pl_sb_z_zes_bysize.items():\n                if lowerword[-k:] in v:\n                    return word + 'es'\n\n            if lowerword[-2:-1] != 'z':\n                return word + 'zes'\n\n        if lowerword[-2:] == 'ze':\n            for k, v in pl_sb_ze_zes_bysize.items():\n                if lowerword[-k:] in v:\n                    return word + 's'\n\n        if lowerword[-2:] in ('ch', 'sh', 'zz', 'ss') or lowerword[-1] == 'x':\n            return word + 'es'\n\n\n\n        if lowerword[-3:] in ('elf', 'alf', 'olf'):\n            return word[:-1] + 'ves'\n        if lowerword[-3:] == 'eaf' and lowerword[-4:-3] != 'd':\n            return word[:-1] + 'ves'\n        if lowerword[-4:] in ('nife', 'life', 'wife'):\n            return word[:-2] + 'ves'\n        if lowerword[-3:] == 'arf':\n            return word[:-1] + 'ves'\n\n\n        if lowerword[-1] == 'y':\n            if lowerword[-2:-1] in 'aeiou' or len(word) == 1:\n                return word + 's'\n\n            if (self.classical_dict['names']):\n                if lowerword[-1] == 'y' and word[0] == word[0].upper():\n                    return word + 's'\n\n            return word[:-1] + 'ies'\n\n\n        if lowerword in pl_sb_U_o_os_complete:\n            return word + 's'\n\n        for k, v in pl_sb_U_o_os_bysize.items():\n            if lowerword[-k:] in v:\n                return word + 's'\n\n        if lowerword[-2:] in ('ao', 'eo', 'io', 'oo', 'uo'):\n            return word + 's'\n\n        if lowerword[-1] == 'o':\n            return word + 'es'\n\n\n        return \"%ss\" % word\n\n    def _pl_special_verb(self, word, count=None):\n        if (self.classical_dict['zero'] and\n                str(count).lower() in pl_count_zero):\n                return False\n        count = self.get_count(count)\n\n        if count == 1:\n            return word\n\n\n        value = self.ud_match(word, self.pl_v_user_defined)\n        if value is not None:\n            return value\n\n\n        lowerword = word.lower()\n        try:\n            firstword = lowerword.split()[0]\n        except IndexError:\n            return False  # word is ''\n\n        if firstword in list(plverb_irregular_pres.keys()):\n            return \"%s%s\" % (plverb_irregular_pres[firstword], word[len(firstword):])\n\n\n        if firstword in plverb_irregular_non_pres:\n            return word\n\n\n        if firstword.endswith(\"n't\") and firstword[:-3] in list(plverb_irregular_pres.keys()):\n            return \"%sn't%s\" % (plverb_irregular_pres[firstword[:-3]], word[len(firstword):])\n\n        if firstword.endswith(\"n't\"):\n            return word\n\n\n        mo = search(r\"^(%s)$\" % plverb_special_s, word)\n        if mo:\n            return False\n        if search(r\"\\s\", word):\n            return False\n        if lowerword == 'quizzes':\n            return 'quiz'\n\n\n        if lowerword[-4:] in ('ches', 'shes', 'zzes', 'sses') or \\\n                lowerword[-3:] == 'xes':\n            return word[:-2]\n\n\n        if lowerword[-3:] == 'ies' and len(word) > 3:\n            return lowerword[:-3] + 'y'\n\n        if (lowerword in pl_v_oes_oe or\n                lowerword[-4:] in pl_v_oes_oe_endings_size4 or\n                lowerword[-5:] in pl_v_oes_oe_endings_size5):\n                return word[:-1]\n\n        if lowerword.endswith('oes') and len(word) > 3:\n            return lowerword[:-2]\n\n        mo = search(r\"^(.*[^s])s$\", word, IGNORECASE)\n        if mo:\n            return mo.group(1)\n\n\n        return False\n\n    def _pl_general_verb(self, word, count=None):\n        count = self.get_count(count)\n\n        if count == 1:\n            return word\n\n\n        mo = search(r\"^(%s)((\\s.*)?)$\" % plverb_ambiguous_pres_keys, word, IGNORECASE)\n        if mo:\n            return \"%s%s\" % (plverb_ambiguous_pres[mo.group(1).lower()], mo.group(2))\n\n\n        mo = search(r\"^(%s)((\\s.*)?)$\" % plverb_ambiguous_non_pres, word, IGNORECASE)\n        if mo:\n            return word\n\n\n        return word\n\n    def _pl_special_adjective(self, word, count=None):\n        count = self.get_count(count)\n\n        if count == 1:\n            return word\n\n\n        value = self.ud_match(word, self.pl_adj_user_defined)\n        if value is not None:\n            return value\n\n\n        mo = search(r\"^(%s)$\" % pl_adj_special_keys,\n                    word, IGNORECASE)\n        if mo:\n            return \"%s\" % (pl_adj_special[mo.group(1).lower()])\n\n\n        mo = search(r\"^(%s)$\" % pl_adj_poss_keys,\n                    word, IGNORECASE)\n        if mo:\n            return \"%s\" % (pl_adj_poss[mo.group(1).lower()])\n\n        mo = search(r\"^(.*)'s?$\",\n                    word)\n        if mo:\n            pl = self.plural_noun(mo.group(1))\n            trailing_s = \"\" if pl[-1] == 's' else \"s\"\n            return \"%s'%s\" % (pl, trailing_s)\n\n\n        return False\n\n    def _sinoun(self, word, count=None, gender=None):\n        count = self.get_count(count)\n\n\n        if count == 2:\n            return word\n\n\n        try:\n            if gender is None:\n                gender = self.thegender\n            elif gender not in singular_pronoun_genders:\n                raise BadGenderError\n        except (TypeError, IndexError):\n            raise BadGenderError\n\n\n        value = self.ud_match(word, self.si_sb_user_defined)\n        if value is not None:\n            return value\n\n\n        if word == '':\n            return word\n\n        lowerword = word.lower()\n\n        if word in si_sb_ois_oi_case:\n            return word[:-1]\n\n        if lowerword in pl_sb_uninflected_complete:\n            return word\n\n        if word in pl_sb_uninflected_caps:\n            return word\n\n        for k, v in pl_sb_uninflected_bysize.items():\n            if lowerword[-k:] in v:\n                return word\n\n        if (self.classical_dict['herd'] and lowerword in pl_sb_uninflected_herd):\n            return word\n\n        if lowerword in pl_sb_C_us_us:\n            return word\n\n\n        mo = search(r\"^(?:%s)$\" % pl_sb_postfix_adj_stems, word, IGNORECASE)\n        if mo and mo.group(2) != '':\n            return \"%s%s\" % (self._sinoun(mo.group(1), 1, gender=gender), mo.group(2))\n\n\n        lowersplit = lowerword.split(' ')\n        if len(lowersplit) >= 3:\n            for numword in range(1, len(lowersplit) - 1):\n                if lowersplit[numword] in pl_prep_list_da:\n                    return ' '.join(lowersplit[:numword - 1] +\n                                    [self._sinoun(lowersplit[numword - 1], 1, gender=gender) or\n                                     lowersplit[numword - 1]] + lowersplit[numword:])\n\n        lowersplit = lowerword.split('-')\n        if len(lowersplit) >= 3:\n            for numword in range(1, len(lowersplit) - 1):\n                if lowersplit[numword] in pl_prep_list_da:\n                    return ' '.join(\n                        lowersplit[:numword - 1] +\n                        [(self._sinoun(lowersplit[numword - 1],\n                                       1,\n                                       gender=gender) or\n                         lowersplit[numword - 1]) +\n                            '-' + lowersplit[numword] + '-']\n                        ) + ' '.join(lowersplit[(numword + 1):])\n\n\n        for k, v in si_pron_acc_keys_bysize.items():\n            if lowerword[-k:] in v:  # ends with accusivate pronoun\n                for pk, pv in pl_prep_bysize.items():\n                    if lowerword[:pk] in pv:  # starts with a prep\n                        if lowerword.split() == [lowerword[:pk], lowerword[-k:]]:\n                            return lowerword[:-k] + get_si_pron('acc', lowerword[-k:], gender)\n\n        try:\n            return get_si_pron('nom', word.lower(), gender)\n        except KeyError:\n            pass\n\n        try:\n            return get_si_pron('acc', word.lower(), gender)\n        except KeyError:\n            pass\n\n\n        wordsplit = word.split()\n        wordlast = wordsplit[-1]\n        lowerwordlast = wordlast.lower()\n\n        if wordlast in list(si_sb_irregular_caps.keys()):\n            llen = len(wordlast)\n            return '%s%s' % (word[:-llen],\n                             si_sb_irregular_caps[wordlast])\n\n        if lowerwordlast in list(si_sb_irregular.keys()):\n            llen = len(lowerwordlast)\n            return '%s%s' % (word[:-llen],\n                             si_sb_irregular[lowerwordlast])\n\n        if (' '.join(wordsplit[-2:])).lower() in list(si_sb_irregular_compound.keys()):\n            llen = len(' '.join(wordsplit[-2:]))  # TODO: what if 2 spaces between these words?\n            return '%s%s' % (word[:-llen],\n                             si_sb_irregular_compound[(' '.join(wordsplit[-2:])).lower()])\n\n        if lowerword[-5:] == 'quies':\n            return word[:-3] + 'y'\n\n        if lowerword[-7:] == 'persons':\n            return word[:-1]\n        if lowerword[-6:] == 'people':\n            return word[:-4] + 'rson'\n\n\n        if lowerword[-4:] == 'mans':\n            for k, v in si_sb_U_man_mans_bysize.items():\n                if lowerword[-k:] in v:\n                    return word[:-1]\n            for k, v in si_sb_U_man_mans_caps_bysize.items():\n                if word[-k:] in v:\n                    return word[:-1]\n        if lowerword[-3:] == 'men':\n            return word[:-3] + 'man'\n        if lowerword[-4:] == 'mice':\n            return word[:-4] + 'mouse'\n        if lowerword[-4:] == 'lice':\n            return word[:-4] + 'louse'\n        if lowerword[-5:] == 'geese':\n            return word[:-5] + 'goose'\n        if lowerword[-5:] == 'teeth':\n            return word[:-5] + 'tooth'\n        if lowerword[-4:] == 'feet':\n            return word[:-4] + 'foot'\n\n        if lowerword == 'dice':\n            return 'die'\n\n\n        if lowerword[-4:] == 'ceps':\n            return word\n        if lowerword[-3:] == 'zoa':\n            return word[:-1] + 'on'\n\n        for lastlet, d, numend, post in (\n            ('s', si_sb_U_ch_chs_bysize, -1, ''),\n            ('s', si_sb_U_ex_ices_bysize, -4, 'ex'),\n            ('s', si_sb_U_ix_ices_bysize, -4, 'ix'),\n            ('a', si_sb_U_um_a_bysize, -1, 'um'),\n            ('i', si_sb_U_us_i_bysize, -1, 'us'),\n            ('a', si_sb_U_on_a_bysize, -1, 'on'),\n            ('e', si_sb_U_a_ae_bysize, -1, ''),\n        ):\n            if lowerword[-1] == lastlet:  # this test to add speed\n                for k, v in d.items():\n                    if lowerword[-k:] in v:\n                        return word[:numend] + post\n\n\n        if (self.classical_dict['ancient']):\n\n            if lowerword[-6:] == 'trices':\n                return word[:-3] + 'x'\n            if lowerword[-4:] in ('eaux', 'ieux'):\n                return word[:-1]\n            if lowerword[-5:] in ('ynges', 'inges', 'anges') and len(word) > 6:\n                return word[:-3] + 'x'\n\n            for lastlet, d, numend, post in (\n                ('a', si_sb_C_en_ina_bysize, -3, 'en'),\n                ('s', si_sb_C_ex_ices_bysize, -4, 'ex'),\n                ('s', si_sb_C_ix_ices_bysize, -4, 'ix'),\n                ('a', si_sb_C_um_a_bysize, -1, 'um'),\n                ('i', si_sb_C_us_i_bysize, -1, 'us'),\n                ('s', pl_sb_C_us_us_bysize, None, ''),\n                ('e', si_sb_C_a_ae_bysize, -1, ''),\n                ('a', si_sb_C_a_ata_bysize, -2, ''),\n                ('s', si_sb_C_is_ides_bysize, -3, 's'),\n                ('i', si_sb_C_o_i_bysize, -1, 'o'),\n                ('a', si_sb_C_on_a_bysize, -1, 'on'),\n                ('m', si_sb_C_im_bysize, -2, ''),\n                ('i', si_sb_C_i_bysize, -1, ''),\n            ):\n                if lowerword[-1] == lastlet:  # this test to add speed\n                    for k, v in d.items():\n                        if lowerword[-k:] in v:\n                            return word[:numend] + post\n\n\n        if (lowerword[-6:] == 'houses' or\n                word in si_sb_uses_use_case or\n                lowerword in si_sb_uses_use):\n            return word[:-1]\n\n\n        if word in si_sb_ies_ie_case or lowerword in si_sb_ies_ie:\n            return word[:-1]\n\n\n        if (lowerword[-5:] == 'shoes' or\n                word in si_sb_oes_oe_case or\n                lowerword in si_sb_oes_oe):\n            return word[:-1]\n\n\n        if (word in si_sb_sses_sse_case or\n                lowerword in si_sb_sses_sse):\n            return word[:-1]\n\n        if lowerword in si_sb_singular_s_complete:\n            return word[:-2]\n\n        for k, v in si_sb_singular_s_bysize.items():\n            if lowerword[-k:] in v:\n                return word[:-2]\n\n        if lowerword[-4:] == 'eses' and word[0] == word[0].upper():\n            return word[:-2]\n\n\n        if lowerword in si_sb_z_zes:\n            return word[:-2]\n\n        if lowerword in si_sb_zzes_zz:\n            return word[:-2]\n\n        if lowerword[-4:] == 'zzes':\n            return word[:-3]\n\n        if (word in si_sb_ches_che_case or\n                lowerword in si_sb_ches_che):\n            return word[:-1]\n\n        if lowerword[-4:] in ('ches', 'shes'):\n            return word[:-2]\n\n        if lowerword in si_sb_xes_xe:\n            return word[:-1]\n\n        if lowerword[-3:] == 'xes':\n            return word[:-2]\n\n\n        if (word in si_sb_ves_ve_case or\n                lowerword in si_sb_ves_ve):\n            return word[:-1]\n\n        if lowerword[-3:] == 'ves':\n            if lowerword[-5:-3] in ('el', 'al', 'ol'):\n                return word[:-3] + 'f'\n            if lowerword[-5:-3] == 'ea' and word[-6:-5] != 'd':\n                return word[:-3] + 'f'\n            if lowerword[-5:-3] in ('ni', 'li', 'wi'):\n                return word[:-3] + 'fe'\n            if lowerword[-5:-3] == 'ar':\n                return word[:-3] + 'f'\n\n\n        if lowerword[-2:] == 'ys':\n            if len(lowerword) > 2 and lowerword[-3] in 'aeiou':\n                return word[:-1]\n\n            if (self.classical_dict['names']):\n                if lowerword[-2:] == 'ys' and word[0] == word[0].upper():\n                    return word[:-1]\n\n        if lowerword[-3:] == 'ies':\n            return word[:-3] + 'y'\n\n\n        if lowerword[-2:] == 'os':\n\n            if lowerword in si_sb_U_o_os_complete:\n                return word[:-1]\n\n            for k, v in si_sb_U_o_os_bysize.items():\n                if lowerword[-k:] in v:\n                    return word[:-1]\n\n            if lowerword[-3:] in ('aos', 'eos', 'ios', 'oos', 'uos'):\n                return word[:-1]\n\n        if lowerword[-3:] == 'oes':\n            return word[:-2]\n\n\n        if word in si_sb_es_is:\n            return word[:-2] + 'is'\n\n\n        if lowerword[-1] == 's':\n            return word[:-1]\n\n\n        return False\n\n\n    def a(self, text, count=1):\n        '''\n        Return the appropriate indefinite article followed by text.\n\n        The indefinite article is either 'a' or 'an'.\n\n        If count is not one, then return count followed by text\n        instead of 'a' or 'an'.\n\n        Whitespace at the start and end is preserved.\n\n        '''\n        mo = search(r\"\\A(\\s*)(?:an?\\s+)?(.+?)(\\s*)\\Z\",\n                    text, IGNORECASE)\n        if mo:\n            word = mo.group(2)\n            if not word:\n                return text\n            pre = mo.group(1)\n            post = mo.group(3)\n            result = self._indef_article(word, count)\n            return \"%s%s%s\" % (pre, result, post)\n        return ''\n\n    an = a\n\n    def _indef_article(self, word, count):\n        mycount = self.get_count(count)\n\n        if mycount != 1:\n            return \"%s %s\" % (count, word)\n\n\n        value = self.ud_match(word, self.A_a_user_defined)\n        if value is not None:\n            return \"%s %s\" % (value, word)\n\n\n        for a in (\n                (r\"^(%s)\" % A_ordinal_a, \"a\"),\n                (r\"^(%s)\" % A_ordinal_an, \"an\"),\n        ):\n            mo = search(a[0], word, IGNORECASE)\n            if mo:\n                return \"%s %s\" % (a[1], word)\n\n\n        for a in (\n                (r\"^(%s)\" % A_explicit_an, \"an\"),\n                (r\"^[aefhilmnorsx]$\", \"an\"),\n                (r\"^[bcdgjkpqtuvwyz]$\", \"a\"),\n        ):\n            mo = search(a[0], word, IGNORECASE)\n            if mo:\n                return \"%s %s\" % (a[1], word)\n\n\n        for a in (\n                (r\"(%s)\" % A_abbrev, \"an\", VERBOSE),\n                (r\"^[aefhilmnorsx][.-]\", \"an\", IGNORECASE),\n                (r\"^[a-z][.-]\", \"a\", IGNORECASE),\n        ):\n            mo = search(a[0], word, a[2])\n            if mo:\n                return \"%s %s\" % (a[1], word)\n\n\n        mo = search(r\"^[^aeiouy]\", word, IGNORECASE)\n        if mo:\n            return \"a %s\" % word\n\n\n        for a in (\n                (r\"^e[uw]\", \"a\"),\n                (r\"^onc?e\\b\", \"a\"),\n                (r\"^onetime\\b\", \"a\"),\n                (r\"^uni([^nmd]|mo)\", \"a\"),\n                (r\"^u[bcfghjkqrst][aeiou]\", \"a\"),\n                (r\"^ukr\", \"a\"),\n                (r\"^(%s)\" % A_explicit_a, \"a\"),\n        ):\n            mo = search(a[0], word, IGNORECASE)\n            if mo:\n                return \"%s %s\" % (a[1], word)\n\n\n        mo = search(r\"^U[NK][AIEO]?\", word)\n        if mo:\n            return \"a %s\" % word\n\n\n        mo = search(r\"^[aeiou]\", word, IGNORECASE)\n        if mo:\n            return \"an %s\" % word\n\n\n        mo = search(r\"^(%s)\" % A_y_cons, word, IGNORECASE)\n        if mo:\n            return \"an %s\" % word\n\n        return \"a %s\" % word\n\n\n    def no(self, text, count=None):\n        '''\n        If count is 0, no, zero or nil, return 'no' followed by the plural\n        of text.\n\n        If count is one of:\n            1, a, an, one, each, every, this, that\n        return count followed by text.\n\n        Otherwise return count follow by the plural of text.\n\n        In the return value count is always followed by a space.\n\n        Whitespace at the start and end is preserved.\n\n        '''\n        if count is None and self.persistent_count is not None:\n            count = self.persistent_count\n\n        if count is None:\n            count = 0\n        mo = search(r\"\\A(\\s*)(.+?)(\\s*)\\Z\", text)\n        pre = mo.group(1)\n        word = mo.group(2)\n        post = mo.group(3)\n\n        if str(count).lower() in pl_count_zero:\n            return \"%sno %s%s\" % (pre, self.plural(word, 0), post)\n        else:\n            return \"%s%s %s%s\" % (pre, count, self.plural(word, count), post)\n\n\n    def present_participle(self, word):\n        '''\n        Return the present participle for word.\n\n        word is the 3rd person singular verb.\n\n        '''\n        plv = self.plural_verb(word, 2)\n\n        for pat, repl in (\n                (r\"ie$\", r\"y\"),\n                (r\"ue$\", r\"u\"),  # TODO: isn't ue$ -> u encompassed in the following rule?\n                (r\"([auy])e$\", r\"\\g<1>\"),\n                (r\"ski$\", r\"ski\"),\n                (r\"[^b]i$\", r\"\"),\n                (r\"^(are|were)$\", r\"be\"),\n                (r\"^(had)$\", r\"hav\"),\n                (r\"^(hoe)$\", r\"\\g<1>\"),\n                (r\"([^e])e$\", r\"\\g<1>\"),\n                (r\"er$\", r\"er\"),\n                (r\"([^aeiou][aeiouy]([bdgmnprst]))$\", \"\\g<1>\\g<2>\"),\n        ):\n            (ans, num) = subn(pat, repl, plv)\n            if num:\n                return \"%sing\" % ans\n        return \"%sing\" % ans\n\n\n    def ordinal(self, num):\n        '''\n        Return the ordinal of num.\n\n        num can be an integer or text\n\n        e.g. ordinal(1) returns '1st'\n        ordinal('one') returns 'first'\n\n        '''\n        if match(r\"\\d\", str(num)):\n            try:\n                num % 2\n                n = num\n            except TypeError:\n                if '.' in str(num):\n                    try:\n                        n = int(num[-1])\n\n                    except ValueError:  # ends with '.', so need to use whole string\n                        n = int(num[:-1])\n                else:\n                    n = int(num)\n            try:\n                post = nth[n % 100]\n            except KeyError:\n                post = nth[n % 10]\n            return \"%s%s\" % (num, post)\n        else:\n            mo = search(r\"(%s)\\Z\" % ordinal_suff, num)\n            try:\n                post = ordinal[mo.group(1)]\n                return resub(r\"(%s)\\Z\" % ordinal_suff, post, num)\n            except AttributeError:\n                return \"%sth\" % num\n\n    def millfn(self, ind=0):\n        if ind > len(mill) - 1:\n            print3(\"number out of range\")\n            raise NumOutOfRangeError\n        return mill[ind]\n\n    def unitfn(self, units, mindex=0):\n        return \"%s%s\" % (unit[units], self.millfn(mindex))\n\n    def tenfn(self, tens, units, mindex=0):\n        if tens != 1:\n            return \"%s%s%s%s\" % (ten[tens],\n                                 '-' if tens and units else '',\n                                 unit[units],\n                                 self.millfn(mindex))\n        return \"%s%s\" % (teen[units], mill[mindex])\n\n    def hundfn(self, hundreds, tens, units, mindex):\n        if hundreds:\n            andword = \" %s \" % self.number_args['andword'] if tens or units else ''\n            return \"%s hundred%s%s%s, \" % (unit[hundreds],  # use unit not unitfn as simpler\n                                           andword,\n                                           self.tenfn(tens, units),\n                                           self.millfn(mindex))\n        if tens or units:\n            return \"%s%s, \" % (self.tenfn(tens, units), self.millfn(mindex))\n        return ''\n\n    def group1sub(self, mo):\n        units = int(mo.group(1))\n        if units == 1:\n            return \" %s, \" % self.number_args['one']\n        elif units:\n            return \"%s, \" % unit[units]\n        else:\n            return \" %s, \" % self.number_args['zero']\n\n    def group1bsub(self, mo):\n        units = int(mo.group(1))\n        if units:\n            return \"%s, \" % unit[units]\n        else:\n            return \" %s, \" % self.number_args['zero']\n\n    def group2sub(self, mo):\n        tens = int(mo.group(1))\n        units = int(mo.group(2))\n        if tens:\n            return \"%s, \" % self.tenfn(tens, units)\n        if units:\n            return \" %s %s, \" % (self.number_args['zero'], unit[units])\n        return \" %s %s, \" % (self.number_args['zero'], self.number_args['zero'])\n\n    def group3sub(self, mo):\n        hundreds = int(mo.group(1))\n        tens = int(mo.group(2))\n        units = int(mo.group(3))\n        if hundreds == 1:\n            hunword = \" %s\" % self.number_args['one']\n        elif hundreds:\n            hunword = \"%s\" % unit[hundreds]\n        else:\n            hunword = \" %s\" % self.number_args['zero']\n        if tens:\n            tenword = self.tenfn(tens, units)\n        elif units:\n            tenword = \" %s %s\" % (self.number_args['zero'], unit[units])\n        else:\n            tenword = \" %s %s\" % (self.number_args['zero'], self.number_args['zero'])\n        return \"%s %s, \" % (hunword, tenword)\n\n    def hundsub(self, mo):\n        ret = self.hundfn(int(mo.group(1)), int(mo.group(2)), int(mo.group(3)), self.mill_count)\n        self.mill_count += 1\n        return ret\n\n    def tensub(self, mo):\n        return \"%s, \" % self.tenfn(int(mo.group(1)), int(mo.group(2)), self.mill_count)\n\n    def unitsub(self, mo):\n        return \"%s, \" % self.unitfn(int(mo.group(1)), self.mill_count)\n\n    def enword(self, num, group):\n\n        if group == 1:\n            num = resub(r\"(\\d)\", self.group1sub, num)\n        elif group == 2:\n            num = resub(r\"(\\d)(\\d)\", self.group2sub, num)\n            num = resub(r\"(\\d)\", self.group1bsub, num, 1)\n        elif group == 3:\n            num = resub(r\"(\\d)(\\d)(\\d)\", self.group3sub, num)\n            num = resub(r\"(\\d)(\\d)\", self.group2sub, num, 1)\n            num = resub(r\"(\\d)\", self.group1sub, num, 1)\n        elif int(num) == 0:\n            num = self.number_args['zero']\n        elif int(num) == 1:\n            num = self.number_args['one']\n        else:\n            num = num.lstrip().lstrip('0')\n            self.mill_count = 0\n            mo = search(r\"(\\d)(\\d)(\\d)(?=\\D*\\Z)\", num)\n            while mo:\n                num = resub(r\"(\\d)(\\d)(\\d)(?=\\D*\\Z)\", self.hundsub, num, 1)\n                mo = search(r\"(\\d)(\\d)(\\d)(?=\\D*\\Z)\", num)\n            num = resub(r\"(\\d)(\\d)(?=\\D*\\Z)\", self.tensub, num, 1)\n            num = resub(r\"(\\d)(?=\\D*\\Z)\", self.unitsub, num, 1)\n        return num\n\n    def blankfn(self, mo):\n        ''' do a global blank replace\n        TODO: surely this can be done with an option to resub\n              rather than this fn\n        '''\n        return ''\n\n    def commafn(self, mo):\n        ''' do a global ',' replace\n        TODO: surely this can be done with an option to resub\n              rather than this fn\n        '''\n        return ','\n\n    def spacefn(self, mo):\n        ''' do a global ' ' replace\n        TODO: surely this can be done with an option to resub\n              rather than this fn\n        '''\n        return ' '\n\n    def number_to_words(self, num, wantlist=False,\n                        group=0, comma=',', andword='and',\n                        zero='zero', one='one', decimal='point',\n                        threshold=None):\n        '''\n        Return a number in words.\n\n        group = 1, 2 or 3 to group numbers before turning into words\n        comma: define comma\n        andword: word for 'and'. Can be set to ''.\n            e.g. \"one hundred and one\" vs \"one hundred one\"\n        zero: word for '0'\n        one: word for '1'\n        decimal: word for decimal point\n        threshold: numbers above threshold not turned into words\n\n        parameters not remembered from last call. Departure from Perl version.\n        '''\n        self.number_args = dict(andword=andword, zero=zero, one=one)\n        num = '%s' % num\n\n        if (threshold is not None and float(num) > threshold):\n            spnum = num.split('.', 1)\n            while (comma):\n                (spnum[0], n) = subn(r\"(\\d)(\\d{3}(?:,|\\Z))\", r\"\\1,\\2\", spnum[0])\n                if n == 0:\n                    break\n            try:\n                return \"%s.%s\" % (spnum[0], spnum[1])\n            except IndexError:\n                return \"%s\" % spnum[0]\n\n        if group < 0 or group > 3:\n            raise BadChunkingOptionError\n        nowhite = num.lstrip()\n        if nowhite[0] == '+':\n            sign = \"plus\"\n        elif nowhite[0] == '-':\n            sign = \"minus\"\n        else:\n            sign = \"\"\n\n        myord = (num[-2:] in ('st', 'nd', 'rd', 'th'))\n        if myord:\n            num = num[:-2]\n        finalpoint = False\n        if decimal:\n            if group != 0:\n                chunks = num.split('.')\n            else:\n                chunks = num.split('.', 1)\n            if chunks[-1] == '':  # remove blank string if nothing after decimal\n                chunks = chunks[:-1]\n                finalpoint = True  # add 'point' to end of output\n        else:\n            chunks = [num]\n\n        first = 1\n        loopstart = 0\n\n        if chunks[0] == '':\n            first = 0\n            if len(chunks) > 1:\n                loopstart = 1\n\n        for i in range(loopstart, len(chunks)):\n            chunk = chunks[i]\n            chunk = resub(r\"\\D\", self.blankfn, chunk)\n            if chunk == \"\":\n                chunk = \"0\"\n\n            if group == 0 and (first == 0 or first == ''):\n                chunk = self.enword(chunk, 1)\n            else:\n                chunk = self.enword(chunk, group)\n\n            if chunk[-2:] == ', ':\n                chunk = chunk[:-2]\n            chunk = resub(r\"\\s+,\", self.commafn, chunk)\n\n            if group == 0 and first:\n                chunk = resub(r\", (\\S+)\\s+\\Z\", \" %s \\\\1\" % andword, chunk)\n            chunk = resub(r\"\\s+\", self.spacefn, chunk)\n            chunk = chunk.strip()\n            if first:\n                first = ''\n            chunks[i] = chunk\n\n        numchunks = []\n        if first != 0:\n            numchunks = chunks[0].split(\"%s \" % comma)\n\n        if myord and numchunks:\n            mo = search(r\"(%s)\\Z\" % ordinal_suff, numchunks[-1])\n            if mo:\n                numchunks[-1] = resub(r\"(%s)\\Z\" % ordinal_suff, ordinal[mo.group(1)],\n                                      numchunks[-1])\n            else:\n                numchunks[-1] += 'th'\n\n        for chunk in chunks[1:]:\n            numchunks.append(decimal)\n            numchunks.extend(chunk.split(\"%s \" % comma))\n\n        if finalpoint:\n            numchunks.append(decimal)\n\n        if wantlist:\n            if sign:\n                numchunks = [sign] + numchunks\n            return numchunks\n        elif group:\n            signout = \"%s \" % sign if sign else ''\n            return \"%s%s\" % (signout, \", \".join(numchunks))\n        else:\n            signout = \"%s \" % sign if sign else ''\n            num = \"%s%s\" % (signout, numchunks.pop(0))\n            if decimal is None:\n                first = True\n            else:\n                first = not num.endswith(decimal)\n            for nc in numchunks:\n                if nc == decimal:\n                    num += \" %s\" % nc\n                    first = 0\n                elif first:\n                    num += \"%s %s\" % (comma, nc)\n                else:\n                    num += \" %s\" % nc\n            return num\n\n\n    def join(self, words, sep=None, sep_spaced=True,\n             final_sep=None, conj='and', conj_spaced=True):\n        '''\n        Join words into a list.\n\n        e.g. join(['ant', 'bee', 'fly']) returns 'ant, bee, and fly'\n\n        options:\n        conj: replacement for 'and'\n        sep: separator. default ',', unless ',' is in the list then ';'\n        final_sep: final separator. default ',', unless ',' is in the list then ';'\n        conj_spaced: boolean. Should conj have spaces around it\n\n        '''\n        if not words:\n            return \"\"\n        if len(words) == 1:\n            return words[0]\n\n        if conj_spaced:\n            if conj == '':\n                conj = ' '\n            else:\n                conj = ' %s ' % conj\n\n        if len(words) == 2:\n            return \"%s%s%s\" % (words[0], conj, words[1])\n\n        if sep is None:\n            if ',' in ''.join(words):\n                sep = ';'\n            else:\n                sep = ','\n        if final_sep is None:\n            final_sep = sep\n\n        final_sep = \"%s%s\" % (final_sep, conj)\n\n        if sep_spaced:\n            sep += ' '\n\n        return \"%s%s%s\" % (sep.join(words[0:-1]), final_sep, words[-1])\n"]}